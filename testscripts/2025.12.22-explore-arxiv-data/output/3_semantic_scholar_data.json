{
  "paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082",
  "externalIds": {
    "ArXiv": "2312.00752",
    "DBLP": "journals/corr/abs-2312-00752",
    "CorpusId": 265551773
  },
  "corpusId": 265551773,
  "publicationVenue": {
    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
    "name": "arXiv.org",
    "alternate_names": [
      "ArXiv"
    ],
    "issn": "2331-8422",
    "url": "https://arxiv.org"
  },
  "url": "https://www.semanticscholar.org/paper/7bbc7595196a0606a07506c4fb1473e5e87f6082",
  "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
  "venue": "arXiv.org",
  "year": 2023,
  "referenceCount": 0,
  "citationCount": 4921,
  "influentialCitationCount": 875,
  "isOpenAccess": false,
  "openAccessPdf": {
    "url": "",
    "status": null,
    "license": null,
    "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.00752, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
  },
  "fieldsOfStudy": [
    "Computer Science"
  ],
  "s2FieldsOfStudy": [
    {
      "category": "Computer Science",
      "source": "external"
    },
    {
      "category": "Computer Science",
      "source": "s2-fos-model"
    }
  ],
  "tldr": {
    "model": "tldr@v2.0.0",
    "text": "This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba)."
  },
  "publicationTypes": [
    "JournalArticle"
  ],
  "publicationDate": "2023-12-01",
  "journal": {
    "name": "ArXiv",
    "volume": "abs/2312.00752"
  },
  "citationStyles": {
    "bibtex": "@Article{Gu2023MambaLS,\n author = {Albert Gu and Tri Dao},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Mamba: Linear-Time Sequence Modeling with Selective State Spaces},\n volume = {abs/2312.00752},\n year = {2023}\n}\n"
  },
  "embedding": {
    "model": "specter_v1",
    "vector": [
      -2.0231235027313232,
      -2.0275232791900635,
      0.595641016960144,
      2.441437244415283,
      2.8502583503723145,
      0.0010087192058563232,
      2.4882638454437256,
      -3.117842674255371,
      0.3828331232070923,
      -0.43925195932388306,
      1.2022792100906372,
      3.988278388977051,
      -0.6286842823028564,
      2.1826279163360596,
      -4.750374794006348,
      -0.83368319272995,
      -1.4401617050170898,
      2.9304187297821045,
      1.7378606796264648,
      2.12058687210083,
      -1.2753186225891113,
      2.0522592067718506,
      -3.179868459701538,
      -1.4012446403503418,
      -3.020737648010254,
      1.4310531616210938,
      2.554990530014038,
      1.5249159336090088,
      -4.482201099395752,
      3.6198458671569824,
      1.0970289707183838,
      -2.0209121704101562,
      4.582968711853027,
      -4.835243225097656,
      2.741954803466797,
      -2.1382646560668945,
      -3.8552873134613037,
      5.702019214630127,
      -3.6857495307922363,
      -1.3552355766296387,
      -1.8539555072784424,
      0.7656872868537903,
      -1.1130932569503784,
      -0.2674654424190521,
      0.6266061663627625,
      1.368933916091919,
      3.689063787460327,
      1.4848477840423584,
      -2.213355779647827,
      3.5393667221069336,
      1.870126724243164,
      -0.42871272563934326,
      -2.42768931388855,
      3.1457810401916504,
      3.0153591632843018,
      -1.134399175643921,
      0.45689475536346436,
      -1.8745895624160767,
      3.5411689281463623,
      -2.05519437789917,
      3.9024465084075928,
      6.235813617706299,
      0.655153751373291,
      0.9047110080718994,
      4.281978130340576,
      -4.405595779418945,
      -1.0879125595092773,
      3.310267448425293,
      3.420459747314453,
      4.117047309875488,
      0.6654903888702393,
      -5.874194145202637,
      0.140242338180542,
      -0.30078941583633423,
      -3.9410171508789062,
      2.890698194503784,
      -3.4572746753692627,
      -5.302028656005859,
      -0.2172752022743225,
      -3.154273509979248,
      -0.1239992082118988,
      3.066850185394287,
      1.9120911359786987,
      1.7006112337112427,
      3.9770185947418213,
      -2.6346020698547363,
      -5.768732070922852,
      0.49172794818878174,
      1.412366271018982,
      -3.8348963260650635,
      -0.9257962703704834,
      0.5462722778320312,
      -0.8338544368743896,
      0.39488595724105835,
      -2.163196086883545,
      -1.14287269115448,
      3.727773666381836,
      -1.8966397047042847,
      -3.5185043811798096,
      0.5940036177635193,
      1.7631289958953857,
      -0.6365220546722412,
      1.2561802864074707,
      -1.4180834293365479,
      4.638990879058838,
      -3.195842742919922,
      -0.35569748282432556,
      3.547300338745117,
      1.7126824855804443,
      -2.075481414794922,
      -3.0855536460876465,
      4.308809280395508,
      -0.6296361684799194,
      0.11480790376663208,
      -1.6152348518371582,
      -3.8436837196350098,
      -1.2512681484222412,
      1.3446327447891235,
      -1.5692338943481445,
      4.177433013916016,
      -1.0411040782928467,
      1.3854544162750244,
      -4.18629264831543,
      -0.40783676505088806,
      -3.7316389083862305,
      1.128859519958496,
      0.6579827070236206,
      1.0767487287521362,
      1.9430179595947266,
      -1.0468603372573853,
      1.734552264213562,
      -2.310096025466919,
      1.743238925933838,
      -2.287778854370117,
      0.7834806442260742,
      4.336559772491455,
      -5.030026912689209,
      1.5496906042099,
      -1.5194302797317505,
      0.2437998652458191,
      -1.5199098587036133,
      1.8761662244796753,
      1.2964322566986084,
      -0.8754218816757202,
      2.472215414047241,
      4.588690757751465,
      -0.07000327110290527,
      2.7087228298187256,
      -0.5299817323684692,
      5.811007976531982,
      2.8883426189422607,
      -1.8768422603607178,
      2.480328321456909,
      -0.10906955599784851,
      2.739309072494507,
      4.126912593841553,
      -6.854326248168945,
      0.8732962608337402,
      -1.8464137315750122,
      1.9935959577560425,
      0.2814817428588867,
      1.2586281299591064,
      -9.781702995300293,
      0.722769021987915,
      2.4796054363250732,
      -5.0166778564453125,
      -3.0753626823425293,
      -0.054786503314971924,
      -0.3371403217315674,
      2.3538765907287598,
      0.7796938419342041,
      1.0177977085113525,
      0.17399653792381287,
      3.4228639602661133,
      5.690051555633545,
      4.986676216125488,
      1.1869579553604126,
      -1.968218207359314,
      -1.9317691326141357,
      -0.9980291724205017,
      -1.8978757858276367,
      0.2841620445251465,
      -7.2181525230407715,
      2.3440537452697754,
      -1.9179121255874634,
      -3.4935126304626465,
      -4.090288162231445,
      -1.8264186382293701,
      -1.6989182233810425,
      0.9399263262748718,
      1.1027050018310547,
      -0.17385676503181458,
      7.6382155418396,
      6.953525543212891,
      4.005941390991211,
      -0.6522891521453857,
      2.6632277965545654,
      2.1463623046875,
      -7.446007251739502,
      0.3592321276664734,
      4.893213748931885,
      -0.3608019948005676,
      -1.3403027057647705,
      -0.30700138211250305,
      3.753152370452881,
      1.717839002609253,
      -2.490220546722412,
      3.3279623985290527,
      3.113107204437256,
      1.2922972440719604,
      2.0885305404663086,
      -1.6317554712295532,
      -0.20238256454467773,
      2.272958755493164,
      -3.9169092178344727,
      -4.634247303009033,
      -5.458991527557373,
      4.834603786468506,
      6.859025478363037,
      -0.5301505327224731,
      -0.381454735994339,
      -0.03998923301696777,
      0.33895793557167053,
      -2.0067336559295654,
      -2.155397653579712,
      -0.5931315422058105,
      2.363917827606201,
      -0.22653934359550476,
      1.0988391637802124,
      2.7976346015930176,
      -2.4993748664855957,
      -4.374944686889648,
      1.9963362216949463,
      -0.07195475697517395,
      -5.729085922241211,
      -0.962591290473938,
      -4.926847457885742,
      -1.0022799968719482,
      1.0163302421569824,
      -3.570341110229492,
      6.675490379333496,
      1.3858764171600342,
      2.16290283203125,
      2.441497802734375,
      3.974665641784668,
      -1.9882152080535889,
      -3.235314130783081,
      1.9559240341186523,
      1.238813877105713,
      -2.3752269744873047,
      -1.4682961702346802,
      -0.49125516414642334,
      5.559842109680176,
      2.115281343460083,
      1.0036367177963257,
      4.259489059448242,
      -0.13889247179031372,
      -1.0511380434036255,
      1.839384913444519,
      2.090658187866211,
      -2.662663698196411,
      3.77575945854187,
      3.8871655464172363,
      5.102545261383057,
      -2.4373764991760254,
      0.28754496574401855,
      1.5087261199951172,
      -2.947470188140869,
      -1.5131806135177612,
      3.2635042667388916,
      1.5480890274047852,
      2.131974697113037,
      -3.901914119720459,
      -5.263405799865723,
      -1.2013119459152222,
      -6.85336971282959,
      -4.50730037689209,
      0.14125937223434448,
      1.018608570098877,
      4.7767744064331055,
      3.0804741382598877,
      -2.690598487854004,
      -0.9636629223823547,
      -0.3055535852909088,
      -0.9686799645423889,
      -3.8722167015075684,
      -3.788367986679077,
      0.08836551755666733,
      -3.6576976776123047,
      -1.8773880004882812,
      -3.7505359649658203,
      2.8347113132476807,
      -5.7050395011901855,
      -1.4358692169189453,
      -2.552339792251587,
      2.7152838706970215,
      4.478885650634766,
      -1.8639672994613647,
      -0.8029191493988037,
      -1.4535504579544067,
      -2.345945358276367,
      0.675792932510376,
      1.198189616203308,
      2.666667938232422,
      0.5989799499511719,
      6.150766849517822,
      0.7203944325447083,
      -1.1367824077606201,
      -2.3466720581054688,
      -3.8894524574279785,
      -2.848020553588867,
      -0.484696626663208,
      5.82861328125,
      -4.159543514251709,
      0.010246742516756058,
      -0.7514044642448425,
      1.6056427955627441,
      -0.843716025352478,
      -3.7912023067474365,
      2.9572701454162598,
      -0.940944492816925,
      -1.4705784320831299,
      -4.9902191162109375,
      -2.6888654232025146,
      -0.24867117404937744,
      0.1153499186038971,
      0.6929477453231812,
      1.8402122259140015,
      -1.985384225845337,
      5.736605167388916,
      3.3107454776763916,
      2.8969202041625977,
      3.685448169708252,
      2.393686294555664,
      0.09739823639392853,
      -5.730808258056641,
      -0.7276924252510071,
      1.6687803268432617,
      1.1882597208023071,
      2.7580838203430176,
      -1.4786758422851562,
      7.506428241729736,
      1.2346389293670654,
      1.4652904272079468,
      3.327360153198242,
      0.08305048942565918,
      -1.3562226295471191,
      -3.085801124572754,
      -1.7728712558746338,
      -2.2365102767944336,
      1.6387540102005005,
      1.9281785488128662,
      2.9100043773651123,
      -4.010126113891602,
      1.5916211605072021,
      2.1885929107666016,
      1.5944387912750244,
      1.970600962638855,
      2.317126512527466,
      2.471592903137207,
      -1.8440450429916382,
      -3.95478892326355,
      2.5047953128814697,
      -2.9262006282806396,
      -0.8579546213150024,
      -3.0301403999328613,
      9.817571640014648,
      1.118159532546997,
      0.16928166151046753,
      -6.084743499755859,
      0.2937917709350586,
      -2.321533441543579,
      -2.133148193359375,
      2.6587815284729004,
      -0.26425901055336,
      -0.7057688236236572,
      2.816380023956299,
      -2.8896470069885254,
      0.6233594417572021,
      0.9284250736236572,
      0.043992526829242706,
      5.677793025970459,
      -1.0611226558685303,
      2.3629705905914307,
      -2.4123401641845703,
      1.0265345573425293,
      -1.8239825963974,
      3.2656373977661133,
      0.19084936380386353,
      0.571632981300354,
      0.22849339246749878,
      3.557401657104492,
      4.111506462097168,
      -1.358163833618164,
      -4.650293827056885,
      -5.1283159255981445,
      -2.933279275894165,
      -3.7379817962646484,
      -2.2729218006134033,
      1.7991591691970825,
      1.2503387928009033,
      1.7501877546310425,
      4.0270209312438965,
      4.1097822189331055,
      -5.253222942352295,
      -1.6644471883773804,
      2.9049084186553955,
      1.0521408319473267,
      0.5499194264411926,
      -0.23429220914840698,
      -2.135923147201538,
      -3.5764119625091553,
      -2.3671116828918457,
      -2.877253532409668,
      0.8020728230476379,
      -1.2119156122207642,
      3.194566011428833,
      0.9637855887413025,
      6.530978202819824,
      2.6782643795013428,
      -1.366013526916504,
      1.5578174591064453,
      4.709372520446777,
      4.053898334503174,
      -0.8757189512252808,
      2.909923791885376,
      2.8592658042907715,
      2.911602020263672,
      -2.3302736282348633,
      2.8957109451293945,
      -4.450990200042725,
      1.5217678546905518,
      -4.763880729675293,
      0.6513477563858032,
      1.1340082883834839,
      0.6469629406929016,
      0.9297467470169067,
      3.759974479675293,
      1.2002525329589844,
      -1.325446605682373,
      0.26638805866241455,
      4.079394340515137,
      -3.4522833824157715,
      2.7593696117401123,
      0.5324448347091675,
      0.9720736742019653,
      1.6623061895370483,
      2.930417776107788,
      -2.0917985439300537,
      -6.127403259277344,
      3.0911996364593506,
      -4.042355537414551,
      -5.870924472808838,
      -1.6246931552886963,
      1.289107084274292,
      -0.5012913346290588,
      -2.7019264698028564,
      -3.659855365753174,
      2.9801666736602783,
      0.30357426404953003,
      -2.3164501190185547,
      3.9985780715942383,
      -0.7716179490089417,
      1.8864895105361938,
      0.36488884687423706,
      0.6028904914855957,
      -2.859064817428589,
      -1.2297468185424805,
      -0.6584607362747192,
      1.0903817415237427,
      3.015277147293091,
      0.35645800828933716,
      -2.887044906616211,
      -0.2978125810623169,
      -1.4632948637008667,
      0.15683716535568237,
      0.4047743082046509,
      5.090147972106934,
      -0.795984148979187,
      -5.322028160095215,
      -2.9595589637756348,
      -1.9457752704620361,
      3.798948287963867,
      -2.7049593925476074,
      -1.3632925748825073,
      5.699374675750732,
      3.8621318340301514,
      1.698291540145874,
      -2.2135987281799316,
      -0.05276814103126526,
      0.5530054569244385,
      0.07195252180099487,
      5.3146281242370605,
      2.0596961975097656,
      0.5853805541992188,
      1.079831838607788,
      -3.887685775756836,
      2.094977617263794,
      0.9741645455360413,
      -1.6536016464233398,
      -2.2599074840545654,
      -3.273479461669922,
      -0.1514424979686737,
      -0.9950766563415527,
      -2.7186241149902344,
      3.849677085876465,
      3.8309805393218994,
      2.44140362739563,
      -4.975710868835449,
      0.3186348080635071,
      -2.6378185749053955,
      2.6692843437194824,
      -3.6860806941986084,
      3.122737407684326,
      -0.5985375046730042,
      1.8069069385528564,
      2.6671676635742188,
      0.4386218190193176,
      1.4550440311431885,
      1.0742734670639038,
      1.133169412612915,
      0.06540742516517639,
      0.5638197660446167,
      3.636176109313965,
      2.2689383029937744,
      1.3499665260314941,
      -5.588260173797607,
      3.10482120513916,
      0.6071683764457703,
      5.871433258056641,
      5.868335247039795,
      5.6622161865234375,
      2.400174140930176,
      -3.075021743774414,
      -0.7594727277755737,
      -3.1294138431549072,
      2.272120952606201,
      -0.10543602705001831,
      -3.311556816101074,
      -1.1591672897338867,
      0.4107651114463806,
      3.728551149368286,
      -1.5537643432617188,
      0.734999418258667,
      -1.8127652406692505,
      2.853477716445923,
      -4.134486675262451,
      3.0094656944274902,
      -1.7350754737854004,
      0.21564552187919617,
      -1.0301439762115479,
      -0.3376634120941162,
      0.8195141553878784,
      -2.964266061782837,
      -3.445935010910034,
      1.2151535749435425,
      -2.7623889446258545,
      -2.865719795227051,
      -3.645101547241211,
      2.7314000129699707,
      2.572751522064209,
      -0.40219342708587646,
      -0.46580037474632263,
      2.4248428344726562,
      -6.174843788146973,
      -0.8438162803649902,
      -1.4381240606307983,
      1.9238760471343994,
      0.09961724281311035,
      -0.10524928569793701,
      -1.6533467769622803,
      -1.699012041091919,
      -0.011247217655181885,
      2.1473543643951416,
      0.5598078966140747,
      3.4599738121032715,
      2.8886594772338867,
      5.496639728546143,
      -4.279292106628418,
      0.5886110067367554,
      -3.903467893600464,
      -4.895391464233398,
      -5.030583381652832,
      -5.1923627853393555,
      -1.0507960319519043,
      -2.503828763961792,
      -5.953019618988037,
      0.6111580729484558,
      -5.275876045227051,
      -1.1653650999069214,
      -0.4915895164012909,
      0.2489064633846283,
      -0.5026919841766357,
      -2.9563214778900146,
      1.0332367420196533,
      -3.949690818786621,
      5.390247821807861,
      2.896200656890869,
      -3.403046131134033,
      0.7025633454322815,
      2.077181816101074,
      1.7496581077575684,
      3.1296164989471436,
      -1.51334810256958,
      -1.8514635562896729,
      -0.8170884251594543,
      2.830954074859619,
      2.9248218536376953,
      -1.3416634798049927,
      2.615361213684082,
      3.2242441177368164,
      3.0454623699188232,
      16.43669891357422,
      1.845740795135498,
      -1.5045028924942017,
      2.2652711868286133,
      -1.9670463800430298,
      -3.8538808822631836,
      -2.3473708629608154,
      1.3218084573745728,
      0.48261120915412903,
      3.3334736824035645,
      -3.145697832107544,
      -3.0397682189941406,
      1.4366178512573242,
      0.2722620964050293,
      1.926192283630371,
      1.19971764087677,
      -3.1480700969696045,
      1.4186511039733887,
      -4.2701592445373535,
      -0.6319456100463867,
      -1.1637260913848877,
      0.019455671310424805,
      -1.1147370338439941,
      0.9303779006004333,
      -2.457207441329956,
      2.629772663116455,
      0.6197386980056763,
      0.09415531158447266,
      -4.729483127593994,
      2.800915002822876,
      3.0170514583587646,
      1.3921477794647217,
      -1.2740225791931152,
      -0.3420185446739197,
      -4.692469596862793,
      4.537487506866455,
      4.589261054992676,
      -0.7955858707427979,
      6.237997055053711,
      1.1087157726287842,
      -3.1271812915802,
      -0.12716174125671387,
      -0.6901147365570068,
      0.5232268571853638,
      -0.6335849761962891,
      5.286746978759766,
      -0.06415703892707825,
      -1.121451735496521,
      -1.6600701808929443,
      2.189887523651123,
      3.313749313354492,
      -2.0077881813049316,
      -1.7014353275299072,
      1.577101707458496,
      1.105734944343567,
      4.267295837402344,
      1.3625503778457642,
      -0.029026776552200317,
      2.73728084564209,
      -1.709825038909912,
      0.7156535387039185,
      -0.7915756106376648,
      -1.2681994438171387,
      -3.9038305282592773,
      -1.3558194637298584,
      -1.5261869430541992,
      -4.739871025085449,
      1.7088284492492676,
      3.639819622039795,
      0.19734248518943787,
      1.8138147592544556,
      4.091117858886719,
      1.2293115854263306,
      -5.580234527587891,
      -1.6490845680236816,
      -5.5308709144592285,
      1.7784967422485352,
      3.2669837474823,
      -0.7072019577026367,
      5.500129699707031,
      -3.0444982051849365,
      2.71488094329834,
      -2.840378999710083,
      -2.047823905944824,
      5.845884323120117,
      -1.330613613128662,
      6.7935075759887695,
      -0.9289698600769043,
      -4.623430252075195,
      3.7056288719177246,
      0.7523103952407837,
      0.22099988162517548,
      1.2082669734954834,
      4.023215293884277,
      4.597226142883301,
      -4.726464748382568,
      -1.6416869163513184,
      -4.587994575500488,
      -4.126764297485352,
      -3.68357515335083,
      5.451760292053223,
      2.3010897636413574,
      1.001105546951294,
      -4.017926216125488,
      0.9111238121986389,
      0.30500328540802,
      -1.4989445209503174,
      -3.3723087310791016,
      -2.9977986812591553,
      -2.9822793006896973,
      3.520012855529785,
      -4.518161296844482,
      -4.184017181396484,
      -0.730301022529602,
      1.0978424549102783,
      -2.4878103733062744,
      0.10983137786388397,
      5.315211772918701,
      -0.616089403629303,
      5.077728271484375,
      2.19209623336792,
      -0.27326029539108276,
      -1.6973023414611816,
      -4.435327053070068,
      -3.617859363555908,
      0.35138148069381714,
      0.7133326530456543,
      -0.974068284034729,
      0.7316692471504211,
      0.964091956615448,
      2.9112112522125244,
      -4.383609771728516,
      -0.539027750492096,
      -3.0005617141723633,
      -2.817953109741211,
      -4.609777927398682,
      0.9610681533813477,
      1.1812607049942017,
      -3.297600269317627,
      2.368582248687744,
      4.101475715637207,
      -3.611083507537842,
      -1.0002257823944092,
      10.487066268920898,
      -0.34104976058006287,
      2.2823092937469482,
      -1.9138433933258057,
      -1.865269660949707,
      -2.347423791885376,
      -1.679144263267517,
      -0.24448660016059875,
      -1.5076508522033691,
      2.819781541824341,
      -0.12806227803230286,
      0.3775174915790558,
      -3.4421589374542236
    ]
  },
  "authors": [
    {
      "authorId": "2269161650",
      "name": "Albert Gu",
      "affiliations": []
    },
    {
      "authorId": "2269146652",
      "name": "Tri Dao",
      "affiliations": []
    }
  ],
  "citations": [
    {
      "paperId": "dbc1968b300ee8d5190bb811927226766ef1013a",
      "externalIds": {
        "DOI": "10.1016/j.renene.2025.124851",
        "CorpusId": 283462569
      },
      "corpusId": 283462569,
      "title": "Physics-informed Mamba network for ultra-short-term photovoltaic power forecasting: integrating WGAN-GP augmentation and CEEMDAN-SST decomposition",
      "venue": "Renewable Energy",
      "year": 2026,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.renene.2025.124851?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.renene.2025.124851, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2395809149",
          "name": "Yanmei Li"
        },
        {
          "authorId": "2396481350",
          "name": "Yi Zhang"
        },
        {
          "authorId": "2395829328",
          "name": "Minghao Yin"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "cbe6e3552a6b0e1045beedf1cdd1446e2085d759",
      "externalIds": {
        "DOI": "10.1016/j.autcon.2025.106603",
        "CorpusId": 282022538
      },
      "corpusId": 282022538,
      "title": "Regional graph-based pavement condition data quality assessment in support of trustworthy highway infrastructure digital twins",
      "venue": "Automation in Construction",
      "year": 2026,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.autcon.2025.106603?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.autcon.2025.106603, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2314774266",
          "name": "Linjun Lu"
        },
        {
          "authorId": "2152161187",
          "name": "Yuandong Pan"
        },
        {
          "authorId": "2342321856",
          "name": "Brian Sheil"
        },
        {
          "authorId": "2315711853",
          "name": "Peihang Luo"
        },
        {
          "authorId": "2313401572",
          "name": "Ioannis Brilakis"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "30d9e3bf2025316e443557e68377ecb538c83e07",
      "externalIds": {
        "DOI": "10.1016/j.autcon.2025.106596",
        "CorpusId": 282043109
      },
      "corpusId": 282043109,
      "title": "Automated spalling characterization with a parallel laser line-camera and U-Net++ with Kolmogorov-Arnold networks (UPPKAN)",
      "venue": "Automation in Construction",
      "year": 2026,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.autcon.2025.106596?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.autcon.2025.106596, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2354131728",
          "name": "Chaobin Li"
        },
        {
          "authorId": "2385425509",
          "name": "Haowei Zhang"
        },
        {
          "authorId": "2247709173",
          "name": "R. Su"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "33104588eed8ee575820cf1d153d38a81218ec06",
      "externalIds": {
        "DOI": "10.1016/j.autcon.2025.106587",
        "CorpusId": 282235919
      },
      "corpusId": 282235919,
      "title": "Pixel-level multicategory semantic segmentation of visible seismic damage in bridge piers using an Attention-Mamba Transformer-based U-Net model",
      "venue": "Automation in Construction",
      "year": 2026,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.autcon.2025.106587?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.autcon.2025.106587, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2386751125",
          "name": "Ensieh Ali Bakhshi"
        },
        {
          "authorId": "2247757295",
          "name": "O. Yazdanpanah"
        },
        {
          "authorId": "96552854",
          "name": "K. Dolatshahi"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "aec36f4cf269906d1a0a7b5c3dd51dece301b04c",
      "externalIds": {
        "DBLP": "journals/eaai/SunDZ26",
        "DOI": "10.1016/j.engappai.2025.112788",
        "CorpusId": 282293609
      },
      "corpusId": 282293609,
      "title": "An innovative optimization strategy based on Mamba and generative adversarial networks for efficient and high-performance multimodal image fusion",
      "venue": "Engineering applications of artificial intelligence",
      "year": 2026,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.engappai.2025.112788?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.engappai.2025.112788, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2125099298",
          "name": "Yichen Sun"
        },
        {
          "authorId": "2275893198",
          "name": "Mingli Dong"
        },
        {
          "authorId": "2261072249",
          "name": "Lianqing Zhu"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "d7265e4788ec78371ca9fe92c32c56e4bd816d4d",
      "externalIds": {
        "DOI": "10.1016/j.engappai.2025.113010",
        "CorpusId": 282795022
      },
      "corpusId": 282795022,
      "title": "A camera-light detection and ranging sensor online extrinsic calibration network based on mamba-like linear attention mechanism for unstructured off-road environments",
      "venue": "Engineering applications of artificial intelligence",
      "year": 2026,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.engappai.2025.113010?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.engappai.2025.113010, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2306150209",
          "name": "Lang Wu"
        },
        {
          "authorId": "2390776127",
          "name": "Ren Xiao"
        },
        {
          "authorId": "2294522619",
          "name": "Huayan Pu"
        },
        {
          "authorId": "2292840954",
          "name": "Gang Wang"
        },
        {
          "authorId": "2236904041",
          "name": "Mingliang Zhou"
        },
        {
          "authorId": "2335822064",
          "name": "Jun Luo"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "b9ae7df0ccd254ac8e33d25a7f8cdcee33d77665",
      "externalIds": {
        "DBLP": "journals/asc/SongZZLX26",
        "DOI": "10.1016/j.asoc.2025.114209",
        "CorpusId": 282954150
      },
      "corpusId": 282954150,
      "title": "Dual-student co-training network using Mamba and unreliable sample learning with class-adaptation for hyperspectral image classification",
      "venue": "Applied Soft Computing",
      "year": 2026,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.asoc.2025.114209?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.asoc.2025.114209, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2304925517",
          "name": "Xiqun Song"
        },
        {
          "authorId": "2327966791",
          "name": "Feng Zhao"
        },
        {
          "authorId": "2291994275",
          "name": "Junjie Zhang"
        },
        {
          "authorId": "48446617",
          "name": "Hanqiang Liu"
        },
        {
          "authorId": "2339433591",
          "name": "Liuwei Xu"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "4e9869cd827418bf1b99d83f2680c4a4142fa26a",
      "externalIds": {
        "DOI": "10.1016/j.rse.2025.115137",
        "CorpusId": 283000669
      },
      "corpusId": 283000669,
      "title": "Faster, better, and more accurate mapping of burned areas using Sentinel-2 multispectral images",
      "venue": "Remote Sensing of Environment",
      "year": 2026,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.rse.2025.115137?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.rse.2025.115137, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2279028342",
          "name": "Peng Liu"
        },
        {
          "authorId": "2278139614",
          "name": "Yongxue Liu"
        },
        {
          "authorId": "2301040865",
          "name": "Xiaoxiao Guo"
        },
        {
          "authorId": "2392292795",
          "name": "Yuchen Liu"
        },
        {
          "authorId": "2300909663",
          "name": "Wanjing Zhao"
        },
        {
          "authorId": "2152922764",
          "name": "Wenxuan Xu"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "f66fb9190ac26e30076656d6241f02fc1c841af5",
      "externalIds": {
        "DOI": "10.1016/j.engappai.2025.113245",
        "CorpusId": 283342541
      },
      "corpusId": 283342541,
      "title": "Skin lesion segmentation network based on state space modeling and convolutional perception",
      "venue": "Engineering applications of artificial intelligence",
      "year": 2026,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.engappai.2025.113245?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.engappai.2025.113245, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2291384117",
          "name": "Hao Chen"
        },
        {
          "authorId": "2395849488",
          "name": "Weiping Ding"
        },
        {
          "authorId": "2395707671",
          "name": "Zhe Wang"
        },
        {
          "authorId": "2279658755",
          "name": "Haifei Zhang"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "3b626d5dd4d7790ffb445f29ab475cbfa438c427",
      "externalIds": {
        "DOI": "10.1016/j.engappai.2025.113291",
        "CorpusId": 283550652
      },
      "corpusId": 283550652,
      "title": "A self-adaptive transformer-enhanced physics-informed neural network for railway dynamics system",
      "venue": "Engineering applications of artificial intelligence",
      "year": 2026,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.engappai.2025.113291?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.engappai.2025.113291, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "113739993",
          "name": "Chengjia Han"
        },
        {
          "authorId": "2222289472",
          "name": "Shuai Qu"
        },
        {
          "authorId": "2396789599",
          "name": "Yun Yang"
        },
        {
          "authorId": "2294317258",
          "name": "Maggie Y. Gao"
        },
        {
          "authorId": "2152288031",
          "name": "Liwei Dong"
        },
        {
          "authorId": "2397288001",
          "name": "Fan Yang"
        },
        {
          "authorId": "2268195304",
          "name": "Tao Ma"
        },
        {
          "authorId": "2224378018",
          "name": "Yaowen Yang"
        },
        {
          "authorId": "2384512528",
          "name": "Wanming Zhai"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "ebfebac88f3524111ee0966be29c8cc45b17e6dd",
      "externalIds": {
        "DOI": "10.1016/j.infrared.2025.106325",
        "CorpusId": 283988708
      },
      "corpusId": 283988708,
      "title": "MTFusion: A semantic segmentation-guided infrared and visible image fusion network based on an attention-enhanced hybrid Mamba-Transformer architecture",
      "venue": "Infrared Physics &amp; Technology",
      "year": 2026,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.infrared.2025.106325?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.infrared.2025.106325, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2399253814",
          "name": "Yuhang Zhou"
        },
        {
          "authorId": "2375730177",
          "name": "Qimin Yang"
        },
        {
          "authorId": "2255402154",
          "name": "Kan Ren"
        },
        {
          "authorId": null,
          "name": "Qian Chen"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "4f09116e51a77a842631146aa42d37448b475cdf",
      "externalIds": {
        "DOI": "10.1080/21642583.2025.2503238",
        "CorpusId": 284049025
      },
      "corpusId": 284049025,
      "title": "UNet reimagined: advanced building extraction through dual attention and global-local synthesis",
      "venue": "",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1080/21642583.2025.2503238?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/21642583.2025.2503238, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2261432786",
          "name": "Yanjing Lei"
        },
        {
          "authorId": "2374008779",
          "name": "Wangjie Zhou"
        },
        {
          "authorId": "3374776",
          "name": "Sixian Chan"
        },
        {
          "authorId": "2155797490",
          "name": "Xiaolong Zhou"
        },
        {
          "authorId": "2281106065",
          "name": "Jie Hu"
        },
        {
          "authorId": "2399878774",
          "name": "Jinshan Xu"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "be21db641ead2f15c2d869785857ca9481f4186f",
      "externalIds": {
        "ArXiv": "2512.16723",
        "CorpusId": 283934255
      },
      "corpusId": 283934255,
      "title": "KOSS: Kalman-Optimal Selective State Spaces for Long-Term Sequence Modeling",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.16723, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2348948528",
          "name": "Lei Wang"
        },
        {
          "authorId": null,
          "name": "Xin Tan"
        },
        {
          "authorId": "2257343214",
          "name": "Mingwei Wang"
        },
        {
          "authorId": null,
          "name": "Ying Zhang"
        }
      ],
      "abstract": "Recent selective state space models (SSMs), such as Mamba and Mamba-2, have demonstrated strong performance in sequence modeling owing to input-dependent selection mechanisms. However, these mechanisms lack theoretical grounding and cannot support context-aware selection from latent state dynamics. To address these limitations, we propose KOSS, a Kalman-optimal Selective State Space model that formulates selection as latent state uncertainty minimization. Derived from estimation theory, KOSS adopts a continuous-time latent update driven by a Kalman gain that dynamically modulates information propagation based on content and context, enabling a closed-loop, context-aware selectivity mechanism. To ensure stable computation and near-linear scalability, KOSS employs global spectral differentiation for frequency-domain derivative estimation, along with a segment-wise scan for hardware-efficient processing. On a selective copying task with distractors, KOSS achieves over 79\\% accuracy while baselines drop below 20\\%, demonstrating robust context-aware selection. Furthermore, across nine long-term forecasting benchmarks, KOSS reduces MSE by 2.92--36.23\\% and consistently outperforms state-of-the-art models in both accuracy and stability. To assess real-world applicability, a case study on secondary surveillance radar (SSR) tracking confirms KOSS's robustness under irregular intervals and noisy conditions and demonstrates its effectiveness in real-world applications. Finally, supplementary experiments verify Kalman gain convergence and the frequency response of spectral differentiation, providing theoretical support for the proposed closed-loop design."
    },
    {
      "paperId": "4364c5faca76a7f13ee159c763f55b6264fa01d5",
      "externalIds": {
        "ArXiv": "2512.16315",
        "CorpusId": 283934157
      },
      "corpusId": 283934157,
      "title": "CPMamba: Selective State Space Models for MIMO Channel Prediction in High-Mobility Environments",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.16315, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": null,
          "name": "Sheng Luo"
        },
        {
          "authorId": "2399604820",
          "name": "Jiashu Xie"
        },
        {
          "authorId": "2374478604",
          "name": "Yueling Che"
        },
        {
          "authorId": "2374948290",
          "name": "Junmei Yao"
        },
        {
          "authorId": null,
          "name": "Jian Tian"
        },
        {
          "authorId": null,
          "name": "Daquan Feng"
        },
        {
          "authorId": "2274700269",
          "name": "Kaishun Wu"
        }
      ],
      "abstract": "Channel prediction is a key technology for improving the performance of various functions such as precoding, adaptive modulation, and resource allocation in MIMO-OFDM systems. Especially in high-mobility scenarios with fast time-varying channels, it is crucial for resisting channel aging and ensuring communication quality. However, existing methods suffer from high complexity and the inability to accurately model the temporal variations of channels. To address this issue, this paper proposes CPMamba -- an efficient channel prediction framework based on the selective state space model. The proposed CPMamba architecture extracts features from historical channel state information (CSI) using a specifically designed feature extraction and embedding network and employs stacked residual Mamba modules for temporal modeling. By leveraging an input-dependent selective mechanism to dynamically adjust state transitions, it can effectively capture the long-range dependencies between the CSIs while maintaining a linear computational complexity. Simulation results under the 3GPP standard channel model demonstrate that CPMamba achieves state-of-the-art prediction accuracy across all scenarios, along with superior generalization and robustness. Compared to existing baseline models, CPMamba reduces the number of parameters by approximately 50 percent while achieving comparable or better performance, thereby significantly lowering the barrier for practical deployment."
    },
    {
      "paperId": "cd68ce46c68d1ba3e4a3b7d83a9d0a289951dbc0",
      "externalIds": {
        "ArXiv": "2512.16313",
        "CorpusId": 283933792
      },
      "corpusId": 283933792,
      "title": "LaverNet: Lightweight All-in-one Video Restoration via Selective Propagation",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.16313, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2271570535",
          "name": "Haiyu Zhao"
        },
        {
          "authorId": "2399164706",
          "name": "Yiwen Shan"
        },
        {
          "authorId": "1781457837",
          "name": "Yuanbiao Gou"
        },
        {
          "authorId": null,
          "name": "Xi Peng"
        }
      ],
      "abstract": "Recent studies have explored all-in-one video restoration, which handles multiple degradations with a unified model. However, these approaches still face two challenges when dealing with time-varying degradations. First, the degradation can dominate temporal modeling, confusing the model to focus on artifacts rather than the video content. Second, current methods typically rely on large models to handle all-in-one restoration, concealing those underlying difficulties. To address these challenges, we propose a lightweight all-in-one video restoration network, LaverNet, with only 362K parameters. To mitigate the impact of degradations on temporal modeling, we introduce a novel propagation mechanism that selectively transmits only degradation-agnostic features across frames. Through LaverNet, we demonstrate that strong all-in-one restoration can be achieved with a compact network. Despite its small size, less than 1\\% of the parameters of existing models, LaverNet achieves comparable, even superior performance across benchmarks."
    },
    {
      "paperId": "4f3df1d0f0db3dbe2449972b8b9db3b4faa62e84",
      "externalIds": {
        "ArXiv": "2512.16791",
        "CorpusId": 283934280
      },
      "corpusId": 283934280,
      "title": "KineST: A Kinematics-guided Spatiotemporal State Space Model for Human Motion Tracking from Sparse Signals",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.16791, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": null,
          "name": "Shuting Zhao"
        },
        {
          "authorId": "2399782016",
          "name": "Zeyu Xiao"
        },
        {
          "authorId": "2323900033",
          "name": "Xinrong Chen"
        }
      ],
      "abstract": "Full-body motion tracking plays an essential role in AR/VR applications, bridging physical and virtual interactions. However, it is challenging to reconstruct realistic and diverse full-body poses based on sparse signals obtained by head-mounted displays, which are the main devices in AR/VR scenarios. Existing methods for pose reconstruction often incur high computational costs or rely on separately modeling spatial and temporal dependencies, making it difficult to balance accuracy, temporal coherence, and efficiency. To address this problem, we propose KineST, a novel kinematics-guided state space model, which effectively extracts spatiotemporal dependencies while integrating local and global pose perception. The innovation comes from two core ideas. Firstly, in order to better capture intricate joint relationships, the scanning strategy within the State Space Duality framework is reformulated into kinematics-guided bidirectional scanning, which embeds kinematic priors. Secondly, a mixed spatiotemporal representation learning approach is employed to tightly couple spatial and temporal contexts, balancing accuracy and smoothness. Additionally, a geometric angular velocity loss is introduced to impose physically meaningful constraints on rotational variations for further improving motion stability. Extensive experiments demonstrate that KineST has superior performance in both accuracy and temporal consistency within a lightweight framework. Project page: https://kaka-1314.github.io/KineST/"
    },
    {
      "paperId": "f642d5e43d0a0cbf918ac3e100393321ae03a694",
      "externalIds": {
        "ArXiv": "2512.16236",
        "CorpusId": 283934434
      },
      "corpusId": 283934434,
      "title": "The Evolution of Reranking Models in Information Retrieval: From Heuristic Methods to Large Language Models",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.16236, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "1500529150",
          "name": "Tejul Pandit"
        },
        {
          "authorId": "2305482990",
          "name": "Sakshi Mahendru"
        },
        {
          "authorId": "2141830016",
          "name": "Meet Raval"
        },
        {
          "authorId": "2363466474",
          "name": "Dhvani Upadhyay"
        }
      ],
      "abstract": "Reranking is a critical stage in contemporary information retrieval (IR) systems, improving the relevance of the user-presented final results by honing initial candidate sets. This paper is a thorough guide to examine the changing reranker landscape and offer a clear view of the advancements made in reranking methods. We present a comprehensive survey of reranking models employed in IR, particularly within modern Retrieval Augmented Generation (RAG) pipelines, where retrieved documents notably influence output quality. We embark on a chronological journey through the historical trajectory of reranking techniques, starting with foundational approaches, before exploring the wide range of sophisticated neural network architectures such as cross-encoders, sequence-generation models like T5, and Graph Neural Networks (GNNs) utilized for structural information. Recognizing the computational cost of advancing neural rerankers, we analyze techniques for enhancing efficiency, notably knowledge distillation for creating competitive, lighter alternatives. Furthermore, we map the emerging territory of integrating Large Language Models (LLMs) in reranking, examining novel prompting strategies and fine-tuning tactics. This survey seeks to elucidate the fundamental ideas, relative effectiveness, computational features, and real-world trade-offs of various reranking strategies. The survey provides a structured synthesis of the diverse reranking paradigms, highlighting their underlying principles and comparative strengths and weaknesses."
    },
    {
      "paperId": "27b90f3ff79b0cac40156eeef5dec0baa74b1919",
      "externalIds": {
        "ArXiv": "2512.15653",
        "CorpusId": 283920895
      },
      "corpusId": 283920895,
      "title": "Characterizing Mamba's Selective Memory using Auto-Encoders",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.15653, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "1580141820",
          "name": "Tamanna Hossain"
        },
        {
          "authorId": "2335670945",
          "name": "IV RobertL.Logan"
        },
        {
          "authorId": "2399033769",
          "name": "Ganesh Jagadeesan"
        },
        {
          "authorId": "2258313039",
          "name": "Sameer Singh"
        },
        {
          "authorId": "2086973507",
          "name": "Joel R. Tetreault"
        },
        {
          "authorId": "2263750204",
          "name": "Alejandro Jaimes"
        }
      ],
      "abstract": "State space models (SSMs) are a promising alternative to transformers for language modeling because they use fixed memory during inference. However, this fixed memory usage requires some information loss in the hidden state when processing long sequences. While prior work has studied the sequence length at which this information loss occurs, it does not characterize the types of information SSM language models (LMs) tend to forget. In this paper, we address this knowledge gap by identifying the types of tokens (e.g., parts of speech, named entities) and sequences (e.g., code, math problems) that are more frequently forgotten by SSM LMs. We achieve this by training an auto-encoder to reconstruct sequences from the SSM's hidden state, and measure information loss by comparing inputs with their reconstructions. We perform experiments using the Mamba family of SSM LMs (130M--1.4B) on sequences ranging from 4--256 tokens. Our results show significantly higher rates of information loss on math-related tokens (e.g., numbers, variables), mentions of organization entities, and alternative dialects to Standard American English. We then examine the frequency that these tokens appear in Mamba's pretraining data and find that less prevalent tokens tend to be the ones Mamba is most likely to forget. By identifying these patterns, our work provides clear direction for future research to develop methods that better control Mamba's ability to retain important information."
    },
    {
      "paperId": "eec76e8e9e0a7f3f493694762d7127541681a53e",
      "externalIds": {
        "ArXiv": "2512.15038",
        "CorpusId": 283920507
      },
      "corpusId": 283920507,
      "title": "LADY: Linear Attention for Autonomous Driving Efficiency without Transformers",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.15038, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2399102352",
          "name": "Jihao Huang"
        },
        {
          "authorId": null,
          "name": "Xi Xia"
        },
        {
          "authorId": "2399114156",
          "name": "Zhiyuan Li"
        },
        {
          "authorId": "2399194019",
          "name": "Tianle Liu"
        },
        {
          "authorId": "2110100448",
          "name": "Jingke Wang"
        },
        {
          "authorId": "2378870612",
          "name": "Junbo Chen"
        },
        {
          "authorId": "2600726",
          "name": "Tengju Ye"
        }
      ],
      "abstract": "End-to-end paradigms have demonstrated great potential for autonomous driving. Additionally, most existing methods are built upon Transformer architectures. However, transformers incur a quadratic attention cost, limiting their ability to model long spatial and temporal sequences-particularly on resource-constrained edge platforms. As autonomous driving inherently demands efficient temporal modeling, this challenge severely limits their deployment and real-time performance. Recently, linear attention mechanisms have gained increasing attention due to their superior spatiotemporal complexity. However, existing linear attention architectures are limited to self-attention, lacking support for cross-modal and cross-temporal interactions-both crucial for autonomous driving. In this work, we propose LADY, the first fully linear attention-based generative model for end-to-end autonomous driving. LADY enables fusion of long-range temporal context at inference with constant computational and memory costs, regardless of the history length of camera and LiDAR features. Additionally, we introduce a lightweight linear cross-attention mechanism that enables effective cross-modal information exchange. Experiments on the NAVSIM and Bench2Drive benchmarks demonstrate that LADY achieves state-of-the-art performance with constant-time and memory complexity, offering improved planning performance and significantly reduced computational cost. Additionally, the model has been deployed and validated on edge devices, demonstrating its practicality in resource-limited scenarios."
    },
    {
      "paperId": "83485e76d4cfebcdbb277b5c4f6044285b58b41c",
      "externalIds": {
        "ArXiv": "2512.15115",
        "CorpusId": 283920740
      },
      "corpusId": 283920740,
      "title": "How Many Heads Make an SSM? A Unified Framework for Attention and State Space Models",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.15115, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2399033212",
          "name": "Ali Ghodsi"
        }
      ],
      "abstract": "Sequence modeling has produced diverse architectures -- from classical recurrent neural networks to modern Transformers and state space models (SSMs) -- yet a unified theoretical understanding of expressivity and trainability trade-offs remains limited. We introduce a unified framework that represents a broad class of sequence maps via an input-dependent effective interaction operator $W_{ij}(X)$, making explicit two recurring construction patterns: (i) the Unified Factorized Framework (Explicit) (attention-style mixing), in which $W_{ij}(X)$ varies through scalar coefficients applied to shared value maps, and (ii) Structured Dynamics (Implicit) (state-space recurrences), in which $W_{ij}$ is induced by a latent dynamical system. Using this framework, we derive three theoretical results. First, we establish the Interaction Rank Gap: models in the Unified Factorized Framework, such as single-head attention, are constrained to a low-dimensional operator span and cannot represent certain structured dynamical maps. Second, we prove an Equivalence (Head-Count) Theorem showing that, within our multi-head factorized class, representing a linear SSM whose lag operators span a $k$-dimensional subspace on length-$n$ sequences requires and is achievable with $H=k$ heads. Third, we prove a Gradient Highway Result, showing that attention layers admit inputs with distance-independent gradient paths, whereas stable linear dynamics exhibit distance-dependent gradient attenuation. Together, these results formalize a fundamental trade-off between algebraic expressivity (interaction/operator span) and long-range gradient propagation, providing theoretical grounding for modern sequence architecture design."
    },
    {
      "paperId": "77800def211c37ad0dc54e7777bdd8d17296aa0c",
      "externalIds": {
        "ArXiv": "2512.15621",
        "CorpusId": 283920806
      },
      "corpusId": 283920806,
      "title": "OccSTeP: Benchmarking 4D Occupancy Spatio-Temporal Persistence",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.15621, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2382606558",
          "name": "Yu Zheng"
        },
        {
          "authorId": "2322450506",
          "name": "Jie Hu"
        },
        {
          "authorId": "8689702",
          "name": "Kailun Yang"
        },
        {
          "authorId": "2291072911",
          "name": "Jiaming Zhang"
        }
      ],
      "abstract": "Autonomous driving requires a persistent understanding of 3D scenes that is robust to temporal disturbances and accounts for potential future actions. We introduce a new concept of 4D Occupancy Spatio-Temporal Persistence (OccSTeP), which aims to address two tasks: (1) reactive forecasting:''what will happen next''and (2) proactive forecasting:\"what would happen given a specific future action\". For the first time, we create a new OccSTeP benchmark with challenging scenarios (e.g., erroneous semantic labels and dropped frames). To address this task, we propose OccSTeP-WM, a tokenizer-free world model that maintains a dense voxel-based scene state and incrementally fuses spatio-temporal context over time. OccSTeP-WM leverages a linear-complexity attention backbone and a recurrent state-space module to capture long-range spatial dependencies while continually updating the scene memory with ego-motion compensation. This design enables online inference and robust performance even when historical sensor input is missing or noisy. Extensive experiments prove the effectiveness of the OccSTeP concept and our OccSTeP-WM, yielding an average semantic mIoU of 23.70% (+6.56% gain) and occupancy IoU of 35.89% (+9.26% gain). The data and code will be open source at https://github.com/FaterYU/OccSTeP."
    },
    {
      "paperId": "069bb694545d70fa280c27e11f3cc507d7ef03dc",
      "externalIds": {
        "ArXiv": "2512.15261",
        "CorpusId": 283920901
      },
      "corpusId": 283920901,
      "title": "MMMamba: A Versatile Cross-Modal In Context Fusion Framework for Pan-Sharpening and Zero-Shot Image Enhancement",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.15261, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2312001381",
          "name": "Yingying Wang"
        },
        {
          "authorId": "2216499293",
          "name": "Xuanhua He"
        },
        {
          "authorId": null,
          "name": "Chen Wu"
        },
        {
          "authorId": "2368158852",
          "name": "Jialing Huang"
        },
        {
          "authorId": "2354134117",
          "name": "Suiyun Zhang"
        },
        {
          "authorId": null,
          "name": "Rui Liu"
        },
        {
          "authorId": "2399928104",
          "name": "Xinghao Ding"
        },
        {
          "authorId": "2399035357",
          "name": "Haoxuan Che"
        }
      ],
      "abstract": "Pan-sharpening aims to generate high-resolution multispectral (HRMS) images by integrating a high-resolution panchromatic (PAN) image with its corresponding low-resolution multispectral (MS) image. To achieve effective fusion, it is crucial to fully exploit the complementary information between the two modalities. Traditional CNN-based methods typically rely on channel-wise concatenation with fixed convolutional operators, which limits their adaptability to diverse spatial and spectral variations. While cross-attention mechanisms enable global interactions, they are computationally inefficient and may dilute fine-grained correspondences, making it difficult to capture complex semantic relationships. Recent advances in the Multimodal Diffusion Transformer (MMDiT) architecture have demonstrated impressive success in image generation and editing tasks. Unlike cross-attention, MMDiT employs in-context conditioning to facilitate more direct and efficient cross-modal information exchange. In this paper, we propose MMMamba, a cross-modal in-context fusion framework for pan-sharpening, with the flexibility to support image super-resolution in a zero-shot manner. Built upon the Mamba architecture, our design ensures linear computational complexity while maintaining strong cross-modal interaction capacity. Furthermore, we introduce a novel multimodal interleaved (MI) scanning mechanism that facilitates effective information exchange between the PAN and MS modalities. Extensive experiments demonstrate the superior performance of our method compared to existing state-of-the-art (SOTA) techniques across multiple tasks and benchmarks."
    },
    {
      "paperId": "0f7edffef6614598c85328747a88a886574b8ee8",
      "externalIds": {
        "ArXiv": "2512.15946",
        "CorpusId": 283933752
      },
      "corpusId": 283933752,
      "title": "AIE4ML: An End-to-End Framework for Compiling Neural Networks for the Next Generation of AMD AI Engines",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.15946, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2376123451",
          "name": "Dimitrios Danopoulos"
        },
        {
          "authorId": "2395667340",
          "name": "Enrico Lupi"
        },
        {
          "authorId": "2392532196",
          "name": "Chang Sun"
        },
        {
          "authorId": "31057353",
          "name": "S. Dittmeier"
        },
        {
          "authorId": "2399162611",
          "name": "Michael Kagan"
        },
        {
          "authorId": "19203702",
          "name": "V. Loncar"
        },
        {
          "authorId": "2399162374",
          "name": "Maurizio Pierini"
        }
      ],
      "abstract": "Efficient AI inference on AMD's Versal AI Engine (AIE) is challenging due to tightly coupled VLIW execution, explicit datapaths, and local memory management. Prior work focused on first-generation AIE kernel optimizations, without tackling full neural network execution across the 2D array. In this work, we present AIE4ML, the first comprehensive framework for converting AI models automatically into optimized firmware targeting the AIE-ML generation devices, also with forward compatibility for the newer AIE-MLv2 architecture. At the single-kernel level, we attain performance close to the architectural peak. At the graph and system levels, we provide a structured parallelization method that can scale across the 2D AIE-ML fabric and exploit its dedicated memory tiles to stay entirely on-chip throughout the model execution. As a demonstration, we designed a generalized and highly efficient linear-layer implementation with intrinsic support for fused bias addition and ReLU activation. Also, as our framework necessitates the generation of multi-layer implementations, our approach systematically derives deterministic, compact, and topology-optimized placements tailored to the physical 2D grid of the device through a novel graph placement and search algorithm. Finally, the framework seamlessly accepts quantized models imported from high-level tools such as hls4ml or PyTorch while preserving bit-exactness. In layer scaling benchmarks, we achieve up to 98.6% efficiency relative to the single-kernel baseline, utilizing 296 of 304 AIE tiles (97.4%) of the device with entirely on-chip data movement. With evaluations across real-world model topologies, we demonstrate that AIE4ML delivers GPU-class throughput under microsecond latency constraints, making it a practical companion for ultra-low-latency environments such as trigger systems in particle physics experiments."
    },
    {
      "paperId": "028bf92e58e355b05d308604030a191155fe6fc7",
      "externalIds": {
        "ArXiv": "2512.15811",
        "CorpusId": 283933627
      },
      "corpusId": 283933627,
      "title": "Keep the Core: Adversarial Priors for Significance-Preserving Brain MRI Segmentation",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.15811, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2335315167",
          "name": "Feifei Zhang"
        },
        {
          "authorId": null,
          "name": "Zhenhong Jia"
        },
        {
          "authorId": "2003816254",
          "name": "Sensen Song"
        },
        {
          "authorId": "2349843998",
          "name": "Fei Shi"
        },
        {
          "authorId": "2391013106",
          "name": "Aoxue Chen"
        },
        {
          "authorId": "2292141757",
          "name": "Dayong Ren"
        }
      ],
      "abstract": "Medical image segmentation is constrained by sparse pathological annotations. Existing augmentation strategies, from conventional transforms to random masking for self-supervision, are feature-agnostic: they often corrupt critical diagnostic semantics or fail to prioritize essential features. We introduce\"Keep the Core,\"a novel data-centric paradigm that uses adversarial priors to guide both augmentation and masking in a significance-preserving manner. Our approach uses SAGE (Sparse Adversarial Gated Estimator), an offline module identifying minimal tokens whose micro-perturbation flips segmentation boundaries. SAGE forges the Token Importance Map $W$ by solving an adversarial optimization problem to maximally degrade performance, while an $\\ell_1$ sparsity penalty encourages a compact set of sensitive tokens. The online KEEP (Key-region Enhancement \\&Preservation) module uses $W$ for a two-pronged augmentation strategy: (1) Semantic-Preserving Augmentation: High-importance tokens are augmented, but their original pixel values are strictly restored. (2) Guided-Masking Augmentation: Low-importance tokens are selectively masked for an $\\text{MAE}$-style reconstruction, forcing the model to learn robust representations from preserved critical features.\"Keep the Core\"is backbone-agnostic with no inference overhead. Extensive experiments show SAGE's structured priors and KEEP's region-selective mechanism are highly complementary, achieving state-of-the-art segmentation robustness and generalization on 2D medical datasets."
    },
    {
      "paperId": "6c7a5662ab4445078f6b55a81c0e09584d8ae268",
      "externalIds": {
        "ArXiv": "2512.15931",
        "CorpusId": 283933769
      },
      "corpusId": 283933769,
      "title": "BarcodeMamba+: Advancing State-Space Models for Fungal Biodiversity Research",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.15931, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": null,
          "name": "Tiancheng Gao"
        },
        {
          "authorId": "2265494913",
          "name": "Scott C. Lowe"
        },
        {
          "authorId": "2399159675",
          "name": "Brendan Furneaux"
        },
        {
          "authorId": "2399931943",
          "name": "Angel X Chang"
        },
        {
          "authorId": "2265498974",
          "name": "Graham W. Taylor"
        }
      ],
      "abstract": "Accurate taxonomic classification from DNA barcodes is a cornerstone of global biodiversity monitoring, yet fungi present extreme challenges due to sparse labelling and long-tailed taxa distributions. Conventional supervised learning methods often falter in this domain, struggling to generalize to unseen species and to capture the hierarchical nature of the data. To address these limitations, we introduce BarcodeMamba+, a foundation model for fungal barcode classification built on a powerful and efficient state-space model architecture. We employ a pretrain and fine-tune paradigm, which utilizes partially labelled data and we demonstrate this is substantially more effective than traditional fully-supervised methods in this data-sparse environment. During fine-tuning, we systematically integrate and evaluate a suite of enhancements--including hierarchical label smoothing, a weighted loss function, and a multi-head output layer from MycoAI--to specifically tackle the challenges of fungal taxonomy. Our experiments show that each of these components yields significant performance gains. On a challenging fungal classification benchmark with distinct taxonomic distribution shifts from the broad training set, our final model outperforms a range of existing methods across all taxonomic levels. Our work provides a powerful new tool for genomics-based biodiversity research and establishes an effective and scalable training paradigm for this challenging domain. Our code is publicly available at https://github.com/bioscan-ml/BarcodeMamba."
    },
    {
      "paperId": "68d4a1007538bb31006f4e7e69468b83f79875d5",
      "externalIds": {
        "ArXiv": "2512.14471",
        "CorpusId": 283909088
      },
      "corpusId": 283909088,
      "title": "Kinetic-Mamba: Mamba-Assisted Predictions of Stiff Chemical Kinetics",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.14471, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2314366633",
          "name": "Additi Pandey"
        },
        {
          "authorId": "2398979062",
          "name": "Liang Wei"
        },
        {
          "authorId": "4571269",
          "name": "H. Babaee"
        },
        {
          "authorId": "2307254295",
          "name": "G. Karniadakis"
        }
      ],
      "abstract": "Accurate chemical kinetics modeling is essential for combustion simulations, as it governs the evolution of complex reaction pathways and thermochemical states. In this work, we introduce Kinetic-Mamba, a Mamba-based neural operator framework that integrates the expressive power of neural operators with the efficient temporal modeling capabilities of Mamba architectures. The framework comprises three complementary models: (i) a standalone Mamba model that predicts the time evolution of thermochemical state variables from given initial conditions; (ii) a constrained Mamba model that enforces mass conservation while learning the state dynamics; and (iii) a regime-informed architecture employing two standalone Mamba models to capture dynamics across temperature-dependent regimes. We additionally develop a latent Kinetic-Mamba variant that evolves dynamics in a reduced latent space and reconstructs the full state on the physical manifold. We evaluate the accuracy and robustness of Kinetic-Mamba using both time-decomposition and recursive-prediction strategies. We further assess the extrapolation capabilities of the model on varied out-of-distribution datasets. Computational experiments on Syngas and GRI-Mech 3.0 reaction mechanisms demonstrate that our framework achieves high fidelity in predicting complex kinetic behavior using only the initial conditions of the state variables."
    },
    {
      "paperId": "8dd9931b66db83a60728fb8b51bc90c3ee5da347",
      "externalIds": {
        "ArXiv": "2512.14309",
        "CorpusId": 283909444
      },
      "corpusId": 283909444,
      "title": "PSMamba: Progressive Self-supervised Vision Mamba for Plant Disease Recognition",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.14309, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2249681876",
          "name": "Abdullah Al Mamun"
        },
        {
          "authorId": "2325709224",
          "name": "Miaohua Zhang"
        },
        {
          "authorId": "2345383570",
          "name": "David Ahmedt-Aristizabal"
        },
        {
          "authorId": "3028761",
          "name": "Zeeshan Hayder"
        },
        {
          "authorId": "2284014099",
          "name": "Mohammad Awrangjeb"
        }
      ],
      "abstract": "Self-supervised Learning (SSL) has become a powerful paradigm for representation learning without manual annotations. However, most existing frameworks focus on global alignment and struggle to capture the hierarchical, multi-scale lesion patterns characteristic of plant disease imagery. To address this gap, we propose PSMamba, a progressive self-supervised framework that integrates the efficient sequence modelling of Vision Mamba (VM) with a dual-student hierarchical distillation strategy. Unlike conventional single teacher-student designs, PSMamba employs a shared global teacher and two specialised students: one processes mid-scale views to capture lesion distributions and vein structures, while the other focuses on local views to capture fine-grained cues such as texture irregularities and early-stage lesions. This multi-granular supervision facilitates the joint learning of contextual and detailed representations, with consistency losses ensuring coherent cross-scale alignment. Experiments on three benchmark datasets show that PSMamba consistently outperforms state-of-the-art SSL methods, delivering superior accuracy and robustness in both domain-shifted and fine-grained scenarios."
    },
    {
      "paperId": "e12b4bf970379548462fd99a46b267da82df38d8",
      "externalIds": {
        "ArXiv": "2512.14253",
        "CorpusId": 283909324
      },
      "corpusId": 283909324,
      "title": "FLAME: Flow Enhanced Legendre Memory Models for General Time Series Forecasting",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.14253, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2294304168",
          "name": "Xingjian Wu"
        },
        {
          "authorId": "2326072358",
          "name": "Hanyin Cheng"
        },
        {
          "authorId": "2294602367",
          "name": "Xiangfei Qiu"
        },
        {
          "authorId": "2326559692",
          "name": "Zhengyu Li"
        },
        {
          "authorId": "8520788",
          "name": "Jilin Hu"
        },
        {
          "authorId": null,
          "name": "Chenjuan Guo"
        },
        {
          "authorId": "2284076479",
          "name": "Bin Yang"
        }
      ],
      "abstract": "In this work, we introduce FLAME, a family of extremely lightweight and capable Time Series Foundation Models, which support both deterministic and probabilistic forecasting via generative probabilistic modeling, thus ensuring both efficiency and robustness. FLAME utilizes the Legendre Memory for strong generalization capabilities. Through adapting variants of Legendre Memory, i.e., translated Legendre (LegT) and scaled Legendre (LegS), in the Encoding and Decoding phases, FLAME can effectively capture the inherent inductive bias within data and make efficient long-range inferences. To enhance the accuracy of probabilistic forecasting while keeping efficient, FLAME adopts a Normalization Flow based forecasting head, which can model the arbitrarily intricate distributions over the forecasting horizon in a generative manner. Comprehensive experiments on well-recognized benchmarks, including TSFM-Bench and ProbTS, demonstrate the consistent state-of-the-art zero-shot performance of FLAME on both deterministic and probabilistic forecasting tasks."
    },
    {
      "paperId": "af41d7742ca870f53a4dcf6363f018275f91e3bc",
      "externalIds": {
        "DOI": "10.29121/shodhkosh.v6.i2s.2025.6696",
        "CorpusId": 283940048
      },
      "corpusId": 283940048,
      "title": "PERFORMANCE DATA ANALYTICS FOR MUSIC INSTITUTIONS",
      "venue": "ShodhKosh Journal of Visual and Performing Arts",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.29121/shodhkosh.v6.i2s.2025.6696?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.29121/shodhkosh.v6.i2s.2025.6696, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2327019874",
          "name": "Subhash Kumar Verma"
        },
        {
          "authorId": "2088107023",
          "name": "Varalakshmi Dandu"
        },
        {
          "authorId": "2399256829",
          "name": "Debanjan Ghosh"
        },
        {
          "authorId": "2366815910",
          "name": "Sasmeeta Tripathy"
        },
        {
          "authorId": "2399261868",
          "name": "Abhiraj Malhotra"
        },
        {
          "authorId": "2399260835",
          "name": "Varun Ojha"
        }
      ],
      "abstract": "Potential to transform the music education system the use of data analytics in music education has the potential to radically transform the performance, pedagogy, and institutional effectiveness assessment system. The given paper will provide a detailed Performance Data Analytics framework, which will be used in the environment of the music institutions, considering both academic and artistic sides of the performance. It also discusses the way different sources of data (student assessment, the documentation of recitals, student attendance, peer rating, etc.) can be analyzed systematically towards arriving at actionable information. The research employs mixed research model that is a blend of the quantitative analysis that is using statistics, and the qualitative interpretation, in order to measure the performance outcomes. The computation and graphical representation of complicated data patterns are done in Python, R, SPSS, and Excel. The systematic analysis of the literature shows a lack of domain-specific analytics models in music education where the traditional data evaluation practices do not take into account data-driven insights. The suggested analytics framework proposes some Key Performance Indicators (KPIs) peculiar to musical performance including technical mastery, expressive interpretations, teamwork, and temporal stability. An example of this framework application with the help of selected music institutions to case study gives the evidence of correlations between the teaching methods, practices, and performance development."
    },
    {
      "paperId": "5ca3e2268f8948f8515d847d2058b8ee5db8170e",
      "externalIds": {
        "DOI": "10.1109/JIOT.2025.3616339",
        "CorpusId": 281815723
      },
      "corpusId": 281815723,
      "title": "MRMamba: Multi-Resolution State-Space Modeling for Robust Multisensor Traffic Attack Detection in Wireless Sensor Networks",
      "venue": "IEEE Internet of Things Journal",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JIOT.2025.3616339?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JIOT.2025.3616339, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2345237907",
          "name": "Siyuan Liu"
        },
        {
          "authorId": "2311372296",
          "name": "Qian He"
        },
        {
          "authorId": "2109060625",
          "name": "Yiting Chen"
        },
        {
          "authorId": "2345497104",
          "name": "Fan Zhang"
        },
        {
          "authorId": "2265372387",
          "name": "Anfeng Liu"
        }
      ],
      "abstract": "Wireless sensor networks (WSNs) face critical security challenges due to their distributed architecture, dynamic topologies, and complexity introduced by multisensor nodes. These factors result in highly variable traffic patterns, making stealthy and long-duration attacks difficult to detect. Recent Mamba-based sequence models have shown promise in capturing long-range dependencies with high efficiency, making them attractive for such scenarios. However, Mamba\u2019s inherent causal modeling limits its ability to fully exploit hierarchical patterns that are crucial for detecting subtle or dispersed attacks. In this work, we propose MRMamba, a multi-resolution state-space architecture that equips Mamba with noncausal and cross-scale modeling capabilities. Specifically, MRMamba introduces a multi-resolution state-space module that performs cross-scale attention fusion. This mechanism dynamically integrates context at multiple temporal resolutions and helps alleviate long-range dependency decay. Furthermore, a dual-branch architecture is constructed around MRMamba to model attacks from complementary perspectives, enabling more accurate and robust detection. Extensive experiments on five real-world public datasets show that MRMamba consistently outperforms existing attack detection methods. It improves the $F1$ score by up to 4.53% on CIC2023 and 6.6% on IoM2024 over the best of the compared methods."
    },
    {
      "paperId": "bebe299fdf13577074e0f2fdb0c7d841f4dae286",
      "externalIds": {
        "ArXiv": "2512.13665",
        "CorpusId": 283897469
      },
      "corpusId": 283897469,
      "title": "Grab-3D: Detecting AI-Generated Videos from 3D Geometric Temporal Consistency",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.13665, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2398792910",
          "name": "Wenhan Chen"
        },
        {
          "authorId": "1968574",
          "name": "Sezer Karaoglu"
        },
        {
          "authorId": "2256692141",
          "name": "Theo Gevers"
        }
      ],
      "abstract": "Recent advances in diffusion-based generation techniques enable AI models to produce highly realistic videos, heightening the need for reliable detection mechanisms. However, existing detection methods provide only limited exploration of the 3D geometric patterns present in generated videos. In this paper, we use vanishing points as an explicit representation of 3D geometry patterns, revealing fundamental discrepancies in geometric consistency between real and AI-generated videos. We introduce Grab-3D, a geometry-aware transformer framework for detecting AI-generated videos based on 3D geometric temporal consistency. To enable reliable evaluation, we construct an AI-generated video dataset of static scenes, allowing stable 3D geometric feature extraction. We propose a geometry-aware transformer equipped with geometric positional encoding, temporal-geometric attention, and an EMA-based geometric classifier head to explicitly inject 3D geometric awareness into temporal modeling. Experiments demonstrate that Grab-3D significantly outperforms state-of-the-art detectors, achieving robust cross-domain generalization to unseen generators."
    },
    {
      "paperId": "308a55ce32ce18d90f9cc5a4e711124710286415",
      "externalIds": {
        "ArXiv": "2512.13191",
        "CorpusId": 283896873
      },
      "corpusId": 283896873,
      "title": "CoRA: A Collaborative Robust Architecture with Hybrid Fusion for Efficient Perception",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.13191, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2316092279",
          "name": "Gonglong Chen"
        },
        {
          "authorId": "2256775161",
          "name": "Chaokun Zhang"
        },
        {
          "authorId": "2398810417",
          "name": "Pengcheng Lv"
        },
        {
          "authorId": "2398848455",
          "name": "Xiaohui Xie"
        }
      ],
      "abstract": "Collaborative perception has garnered significant attention as a crucial technology to overcome the perceptual limitations of single-agent systems. Many state-of-the-art (SOTA) methods have achieved communication efficiency and high performance via intermediate fusion. However, they share a critical vulnerability: their performance degrades under adverse communication conditions due to the misalignment induced by data transmission, which severely hampers their practical deployment. To bridge this gap, we re-examine different fusion paradigms, and recover that the strengths of intermediate and late fusion are not a trade-off, but a complementary pairing. Based on this key insight, we propose CoRA, a novel collaborative robust architecture with a hybrid approach to decouple performance from robustness with low communication. It is composed of two components: a feature-level fusion branch and an object-level correction branch. Its first branch selects critical features and fuses them efficiently to ensure both performance and scalability. The second branch leverages semantic relevance to correct spatial displacements, guaranteeing resilience against pose errors. Experiments demonstrate the superiority of CoRA. Under extreme scenarios, CoRA improves upon its baseline performance by approximately 19% in AP@0.7 with more than 5x less communication volume, which makes it a promising solution for robust collaborative perception."
    },
    {
      "paperId": "3091550e37d4cdd2f76cc1561ef6034178c03884",
      "externalIds": {
        "ArXiv": "2512.13368",
        "CorpusId": 283896845
      },
      "corpusId": 283896845,
      "title": "BlossomRec: Block-level Fused Sparse Attention Mechanism for Sequential Recommendations",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.13368, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": null,
          "name": "Mengyang Ma"
        },
        {
          "authorId": "2238125533",
          "name": "Xiaopeng Li"
        },
        {
          "authorId": "2211473272",
          "name": "Wanyu Wang"
        },
        {
          "authorId": null,
          "name": "Zhaocheng Du"
        },
        {
          "authorId": "2161309826",
          "name": "Jingtong Gao"
        },
        {
          "authorId": "2264224432",
          "name": "Pengyue Jia"
        },
        {
          "authorId": null,
          "name": "Yuyang Ye"
        },
        {
          "authorId": "2282243915",
          "name": "Yiqi Wang"
        },
        {
          "authorId": "2398813051",
          "name": "Yunpeng Weng"
        },
        {
          "authorId": null,
          "name": "Weihong Luo"
        },
        {
          "authorId": "2399037389",
          "name": "Xiao Han"
        },
        {
          "authorId": "2309219661",
          "name": "Xiangyu Zhao"
        }
      ],
      "abstract": "Transformer structures have been widely used in sequential recommender systems (SRS). However, as user interaction histories increase, computational time and memory requirements also grow. This is mainly caused by the standard attention mechanism. Although there exist many methods employing efficient attention and SSM-based models, these approaches struggle to effectively model long sequences and may exhibit unstable performance on short sequences. To address these challenges, we design a sparse attention mechanism, BlossomRec, which models both long-term and short-term user interests through attention computation to achieve stable performance across sequences of varying lengths. Specifically, we categorize user interests in recommendation systems into long-term and short-term interests, and compute them using two distinct sparse attention patterns, with the results combined through a learnable gated output. Theoretically, it significantly reduces the number of interactions participating in attention computation. Extensive experiments on four public datasets demonstrate that BlossomRec, when integrated with state-of-the-art Transformer-based models, achieves comparable or even superior performance while significantly reducing memory usage, providing strong evidence of BlossomRec's efficiency and effectiveness.The code is available at https://github.com/ronineume/BlossomRec."
    },
    {
      "paperId": "02205eafa4f518668a1d8f765ab9830b8310d72d",
      "externalIds": {
        "ArXiv": "2512.13921",
        "CorpusId": 283909556
      },
      "corpusId": 283909556,
      "title": "Sliding Window Recurrences for Sequence Models",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.13921, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2253505884",
          "name": "Dragos Secrieru"
        },
        {
          "authorId": "38798240",
          "name": "Garyk Brixi"
        },
        {
          "authorId": "1865800402",
          "name": "Y. Bengio"
        },
        {
          "authorId": "2293556912",
          "name": "Taiji Suzuki"
        },
        {
          "authorId": "2319233397",
          "name": "Michael Poli"
        },
        {
          "authorId": "2293395853",
          "name": "Stefano Massaroli"
        }
      ],
      "abstract": "Multi-hybrid architectures are poised to take over language modeling due to better quality and performance. We introduce a hierarchical decomposition framework for linear recurrences that allows us to develop algorithms aligned with GPU memory hierarchies, yielding Sliding Window Recurrences. We focus specifically on truncating recurrences to hardware-aligned windows which are naturally jagged, limiting costly inter-warp communication. Using SWR, we develop Phalanx layers that serve as drop-in replacements for windowed attention or linear recurrences. In 1B parameter multi-hybrid models, Phalanx achieves over 10-40% speedup across 4K to 32K context length over optimized Transformers while matching perplexity."
    },
    {
      "paperId": "f9883b81961272b7f25803b75e9adf2a3b512a90",
      "externalIds": {
        "DOI": "10.1007/s00371-025-04212-0",
        "CorpusId": 281748868
      },
      "corpusId": 281748868,
      "title": "A spatiotemporal bidirectional mamba network with global\u2013local skeletal enhancement for 3D human pose estimation",
      "venue": "The Visual Computer",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s00371-025-04212-0?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s00371-025-04212-0, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2383628567",
          "name": "Chuhan Wu"
        },
        {
          "authorId": "2383514679",
          "name": "Zan Wang"
        },
        {
          "authorId": "2383967405",
          "name": "Guixian Zhou"
        },
        {
          "authorId": "2383484827",
          "name": "Jiahao Hua"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "a347df826b3bc3524ef0d458617ee8d3bec8929b",
      "externalIds": {
        "ArXiv": "2512.15778",
        "CorpusId": 283934102
      },
      "corpusId": 283934102,
      "title": "RAMBO: Reliability Analysis for Mamba through Bit-flip attack Optimization",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.15778, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2276012822",
          "name": "Sanjay Das"
        },
        {
          "authorId": "2331643469",
          "name": "Swastik Bhattacharya"
        },
        {
          "authorId": "114996851",
          "name": "Shamik Kundu"
        },
        {
          "authorId": "2221949",
          "name": "Arnab Raha"
        },
        {
          "authorId": "2313349883",
          "name": "Souvik Kundu"
        },
        {
          "authorId": "2269747045",
          "name": "Kanad Basu"
        }
      ],
      "abstract": "State-space models (SSMs), exemplified by the Mamba architecture, have recently emerged as state-of-the-art sequence-modeling frameworks, offering linear-time scalability together with strong performance in long-context settings. Owing to their unique combination of efficiency, scalability, and expressive capacity, SSMs have become compelling alternatives to transformer-based models, which suffer from the quadratic computational and memory costs of attention mechanisms. As SSMs are increasingly deployed in real-world applications, it is critical to assess their susceptibility to both software- and hardware-level threats to ensure secure and reliable operation. Among such threats, hardware-induced bit-flip attacks (BFAs) pose a particularly severe risk by corrupting model parameters through memory faults, thereby undermining model accuracy and functional integrity. To investigate this vulnerability, we introduce RAMBO, the first BFA framework specifically designed to target Mamba-based architectures. Through experiments on the Mamba-1.4b model with LAMBADA benchmark, a cloze-style word-prediction task, we demonstrate that flipping merely a single critical bit can catastrophically reduce accuracy from 74.64% to 0% and increase perplexity from 18.94 to 3.75 x 10^6. These results demonstrate the pronounced fragility of SSMs to adversarial perturbations."
    },
    {
      "paperId": "1b83e9bec493360d2e23b2fc5a7c3fdcc1ac184a",
      "externalIds": {
        "ArXiv": "2512.12602",
        "CorpusId": 283896475
      },
      "corpusId": 283896475,
      "title": "Error-Free Linear Attention is a Free Lunch: Exact Solution from Continuous-Time Dynamics",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.12602, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2316236029",
          "name": "Jingdi Lei"
        },
        {
          "authorId": "2399176822",
          "name": "Di Zhang"
        },
        {
          "authorId": "1746416",
          "name": "Soujanya Poria"
        }
      ],
      "abstract": "Linear-time attention and State Space Models (SSMs) promise to solve the quadratic cost bottleneck in long-context language models employing softmax attention. We introduce Error-Free Linear Attention (EFLA), a numerically stable, fully parallelism and generalized formulation of the delta rule. Specifically, we formulate the online learning update as a continuous-time dynamical system and prove that its exact solution is not only attainable but also computable in linear time with full parallelism. By leveraging the rank-1 structure of the dynamics matrix, we directly derive the exact closed-form solution effectively corresponding to the infinite-order Runge-Kutta method. This attention mechanism is theoretically free from error accumulation, perfectly capturing the continuous dynamics while preserving the linear-time complexity. Through an extensive suite of experiments, we show that EFLA enables robust performance in noisy environments, achieving lower language modeling perplexity and superior downstream benchmark performance than DeltaNet without introducing additional parameters. Our work provides a new theoretical foundation for building high-fidelity, scalable linear-time attention models."
    },
    {
      "paperId": "408f58eda60679cb0531ee19554adb3392560867",
      "externalIds": {
        "ArXiv": "2512.12337",
        "CorpusId": 283897330
      },
      "corpusId": 283897330,
      "title": "SCIR: A Self-Correcting Iterative Refinement Framework for Enhanced Information Extraction Based on Schema",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.12337, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2399027677",
          "name": "Yushen Fang"
        },
        {
          "authorId": "2398855443",
          "name": "Jianjun Li"
        },
        {
          "authorId": null,
          "name": "Mingqian Ding"
        },
        {
          "authorId": null,
          "name": "Chang Liu"
        },
        {
          "authorId": "2340473592",
          "name": "Xin Zou"
        },
        {
          "authorId": "2398805017",
          "name": "Wenqi Yang"
        }
      ],
      "abstract": "Although Large language Model (LLM)-powered information extraction (IE) systems have shown impressive capabilities, current fine-tuning paradigms face two major limitations: high training costs and difficulties in aligning with LLM preferences. To address these issues, we propose a novel universal IE paradigm, the Self-Correcting Iterative Refinement (SCIR) framework, along with a Multi-task Bilingual (Chinese-English) Self-Correcting (MBSC) dataset containing over 100,000 entries. The SCIR framework achieves plug-and-play compatibility with existing LLMs and IE systems through its Dual-Path Self-Correcting module and feedback-driven optimization, thereby significantly reducing training costs. Concurrently, the MBSC dataset tackles the challenge of preference alignment by indirectly distilling GPT-4's capabilities into IE result detection models. Experimental results demonstrate that SCIR outperforms state-of-the-art IE methods across three key tasks: named entity recognition, relation extraction, and event extraction, achieving a 5.27 percent average improvement in span-based Micro-F1 while reducing training costs by 87 percent compared to baseline approaches. These advancements not only enhance the flexibility and accuracy of IE systems but also pave the way for lightweight and efficient IE paradigms."
    },
    {
      "paperId": "86c961814b330649a872f5ffd9b0e95aa419c266",
      "externalIds": {
        "ArXiv": "2512.12279",
        "CorpusId": 283896361
      },
      "corpusId": 283896361,
      "title": "WATOS: Efficient LLM Training Strategies and Architecture Co-exploration for Wafer-scale Chip",
      "venue": "",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.12279, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2290366430",
          "name": "Huizheng Wang"
        },
        {
          "authorId": "2380461203",
          "name": "Zichuan Wang"
        },
        {
          "authorId": "2382432192",
          "name": "Hongbin Wang"
        },
        {
          "authorId": null,
          "name": "Jingxiang Hou"
        },
        {
          "authorId": "2335175414",
          "name": "Taiquan Wei"
        },
        {
          "authorId": "2303489619",
          "name": "Chao Li"
        },
        {
          "authorId": "2337488819",
          "name": "Yang Hu"
        },
        {
          "authorId": "2301577046",
          "name": "Shouyi Yin"
        }
      ],
      "abstract": "Training large language models (LLMs) imposes extreme demands on computation, memory capacity, and interconnect bandwidth, driven by their ever-increasing parameter scales and intensive data movement. Wafer-scale integration offers a promising solution by densely integrating multiple single-die chips with high-speed die-to-die (D2D) interconnects. However, the limited wafer area necessitates trade-offs among compute, memory, and communication resources. Fully harnessing the potential of wafer-scale integration while mitigating its architectural constraints is essential for maximizing LLM training performance. This imposes significant challenges for the co-optimization of architecture and training strategies. Unfortunately, existing approaches all fall short in addressing these challenges. To bridge the gap, we propose WATOS, a co-exploration framework for LLM training strategy and wafer-scale architecture. We first define a highly configurable hardware template designed to explore optimal architectural parameters for wafer-scale chips. Based on it, we capitalize on the high D2D bandwidth and fine-grained operation advantages inherent to wafer-scale chips to explore optimal parallelism and resource allocation strategies, effectively addressing the memory underutilization issues during LLM training. Compared to the state-of-the-art (SOTA) LLM training framework Megatron and Cerebras'weight streaming wafer training strategy, WATOS can achieve an average overall throughput improvement of 2.74x and 1.53x across various LLM models, respectively. In addition, we leverage WATOS to reveal intriguing insights about wafer-scale architecture design with the training of LLM workloads."
    },
    {
      "paperId": "944ff6b7decd16fd694fbab2b6e8bbe3676b947e",
      "externalIds": {
        "ArXiv": "2512.11503",
        "CorpusId": 283883792
      },
      "corpusId": 283883792,
      "title": "TSkel-Mamba: Temporal Dynamic Modeling via State Space Model for Human Skeleton-based Action Recognition",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.11503, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2145509159",
          "name": "Yanan Liu"
        },
        {
          "authorId": "2398712860",
          "name": "Jun Liu"
        },
        {
          "authorId": "2313789388",
          "name": "Hao Zhang"
        },
        {
          "authorId": "2290029954",
          "name": "Dan Xu"
        },
        {
          "authorId": "2265553215",
          "name": "Hossein Rahmani"
        },
        {
          "authorId": "2284692619",
          "name": "M. Bennamoun"
        },
        {
          "authorId": "2286689955",
          "name": "Qi Ke"
        }
      ],
      "abstract": "Skeleton-based action recognition has garnered significant attention in the computer vision community. Inspired by the recent success of the selective state-space model (SSM) Mamba in modeling 1D temporal sequences, we propose TSkel-Mamba, a hybrid Transformer-Mamba framework that effectively captures both spatial and temporal dynamics. In particular, our approach leverages Spatial Transformer for spatial feature learning while utilizing Mamba for temporal modeling. Mamba, however, employs separate SSM blocks for individual channels, which inherently limits its ability to model inter-channel dependencies. To better adapt Mamba for skeleton data and enhance Mamba`s ability to model temporal dependencies, we introduce a Temporal Dynamic Modeling (TDM) block, which is a versatile plug-and-play component that integrates a novel Multi-scale Temporal Interaction (MTI) module. The MTI module employs multi-scale Cycle operators to capture cross-channel temporal interactions, a critical factor in action recognition. Extensive experiments on NTU-RGB+D 60, NTU-RGB+D 120, NW-UCLA and UAV-Human datasets demonstrate that TSkel-Mamba achieves state-of-the-art performance while maintaining low inference time, making it both efficient and highly effective."
    },
    {
      "paperId": "241d6767eb483038411a1220c3451b54b906a6d3",
      "externalIds": {
        "ArXiv": "2512.11362",
        "CorpusId": 283883381
      },
      "corpusId": 283883381,
      "title": "An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.11362, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2394302126",
          "name": "Chao Xu"
        },
        {
          "authorId": "2394460603",
          "name": "Suyu Zhang"
        },
        {
          "authorId": "2398638988",
          "name": "Yang Liu"
        },
        {
          "authorId": null,
          "name": "Baigui Sun"
        },
        {
          "authorId": "2373743156",
          "name": "Weihong Chen"
        },
        {
          "authorId": "2398641898",
          "name": "Bo Xu"
        },
        {
          "authorId": "2399104523",
          "name": "Qi Liu"
        },
        {
          "authorId": "2394129519",
          "name": "Juncheng Wang"
        },
        {
          "authorId": "2398710650",
          "name": "Shujun Wang"
        },
        {
          "authorId": null,
          "name": "Shan Luo"
        },
        {
          "authorId": "2398656764",
          "name": "Jan Peters"
        },
        {
          "authorId": "2336426746",
          "name": "Athanasios V. Vasilakos"
        },
        {
          "authorId": "1776444",
          "name": "S. Zafeiriou"
        },
        {
          "authorId": "2394258111",
          "name": "Jiankang Deng"
        }
      ],
      "abstract": "Vision-Language-Action (VLA) models are driving a revolution in robotics, enabling machines to understand instructions and interact with the physical world. This field is exploding with new models and datasets, making it both exciting and challenging to keep pace with. This survey offers a clear and structured guide to the VLA landscape. We design it to follow the natural learning path of a researcher: we start with the basic Modules of any VLA model, trace the history through key Milestones, and then dive deep into the core Challenges that define recent research frontier. Our main contribution is a detailed breakdown of the five biggest challenges in: (1) Representation, (2) Execution, (3) Generalization, (4) Safety, and (5) Dataset and Evaluation. This structure mirrors the developmental roadmap of a generalist agent: establishing the fundamental perception-action loop, scaling capabilities across diverse embodiments and environments, and finally ensuring trustworthy deployment-all supported by the essential data infrastructure. For each of them, we review existing approaches and highlight future opportunities. We position this paper as both a foundational guide for newcomers and a strategic roadmap for experienced researchers, with the dual aim of accelerating learning and inspiring new ideas in embodied intelligence. A live version of this survey, with continuous updates, is maintained on our \\href{https://suyuz1.github.io/Survery/}{project page}."
    },
    {
      "paperId": "2a97482301f61abf758f1240054acc9b7d2f1ea6",
      "externalIds": {
        "ArXiv": "2512.11543",
        "CorpusId": 283883525
      },
      "corpusId": 283883525,
      "title": "All-in-One ASR: Unifying Encoder-Decoder Models of CTC, Attention, and Transducer in Dual-Mode ASR",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.11543, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "3334164",
          "name": "Takafumi Moriya"
        },
        {
          "authorId": "2309247572",
          "name": "Masato Mimura"
        },
        {
          "authorId": "1500659599",
          "name": "Tomohiro Tanaka"
        },
        {
          "authorId": "153515534",
          "name": "Hiroshi Sato"
        },
        {
          "authorId": "2874205",
          "name": "Ryo Masumura"
        },
        {
          "authorId": "2398649817",
          "name": "Atsunori Ogawa"
        }
      ],
      "abstract": "This paper proposes a unified framework, All-in-One ASR, that allows a single model to support multiple automatic speech recognition (ASR) paradigms, including connectionist temporal classification (CTC), attention-based encoder-decoder (AED), and Transducer, in both offline and streaming modes. While each ASR architecture offers distinct advantages and trade-offs depending on the application, maintaining separate models for each scenario incurs substantial development and deployment costs. To address this issue, we introduce a multi-mode joiner that enables seamless integration of various ASR modes within a single unified model. Experiments show that All-in-One ASR significantly reduces the total model footprint while matching or even surpassing the recognition performance of individually optimized ASR models. Furthermore, joint decoding leverages the complementary strengths of different ASR modes, yielding additional improvements in recognition accuracy."
    },
    {
      "paperId": "e5308e2521d7f3ad91ac7580d833b405fec14042",
      "externalIds": {
        "ArXiv": "2512.11719",
        "CorpusId": 283883834
      },
      "corpusId": 283883834,
      "title": "Referring Change Detection in Remote Sensing Imagery",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.11719, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "34952444",
          "name": "Yilmaz Korkmaz"
        },
        {
          "authorId": "1734834200",
          "name": "Jay N. Paranjape"
        },
        {
          "authorId": "2269481396",
          "name": "C. D. Melo"
        },
        {
          "authorId": "2322026557",
          "name": "Vishal M. Patel"
        }
      ],
      "abstract": "Change detection in remote sensing imagery is essential for applications such as urban planning, environmental monitoring, and disaster management. Traditional change detection methods typically identify all changes between two temporal images without distinguishing the types of transitions, which can lead to results that may not align with specific user needs. Although semantic change detection methods have attempted to address this by categorizing changes into predefined classes, these methods rely on rigid class definitions and fixed model architectures, making it difficult to mix datasets with different label sets or reuse models across tasks, as the output channels are tightly coupled with the number and type of semantic classes. To overcome these limitations, we introduce Referring Change Detection (RCD), which leverages natural language prompts to detect specific classes of changes in remote sensing images. By integrating language understanding with visual analysis, our approach allows users to specify the exact type of change they are interested in. However, training models for RCD is challenging due to the limited availability of annotated data and severe class imbalance in existing datasets. To address this, we propose a two-stage framework consisting of (I) \\textbf{RCDNet}, a cross-modal fusion network designed for referring change detection, and (II) \\textbf{RCDGen}, a diffusion-based synthetic data generation pipeline that produces realistic post-change images and change maps for a specified category using only pre-change image, without relying on semantic segmentation masks and thereby significantly lowering the barrier to scalable data creation. Experiments across multiple datasets show that our framework enables scalable and targeted change detection. Project website is here: https://yilmazkorkmaz1.github.io/RCD."
    },
    {
      "paperId": "f75d2fe09107cfa1b476c60f6c5f48ef97941629",
      "externalIds": {
        "ArXiv": "2512.12087",
        "CorpusId": 283896277
      },
      "corpusId": 283896277,
      "title": "BLASST: Dynamic BLocked Attention Sparsity via Softmax Thresholding",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.12087, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2383328023",
          "name": "Jiayi Yuan"
        },
        {
          "authorId": "2249759423",
          "name": "Cameron Shinn"
        },
        {
          "authorId": "2399248292",
          "name": "Kai Xu"
        },
        {
          "authorId": "2399926292",
          "name": "Jingze Cui"
        },
        {
          "authorId": "1605991453",
          "name": "George Klimiashvili"
        },
        {
          "authorId": "2046958974",
          "name": "Guangxuan Xiao"
        },
        {
          "authorId": "2398806858",
          "name": "Perkz Zheng"
        },
        {
          "authorId": "2399157625",
          "name": "Bo Li"
        },
        {
          "authorId": "2304452188",
          "name": "Yuxin Zhou"
        },
        {
          "authorId": null,
          "name": "Zhouhai Ye"
        },
        {
          "authorId": "2398802934",
          "name": "Weijie You"
        },
        {
          "authorId": "2399803369",
          "name": "Tian Zheng"
        },
        {
          "authorId": "2398860206",
          "name": "Dominic Brown"
        },
        {
          "authorId": "2398854534",
          "name": "Pengbo Wang"
        },
        {
          "authorId": "2398803238",
          "name": "Richard Cai"
        },
        {
          "authorId": "32604218",
          "name": "Julien Demouth"
        },
        {
          "authorId": "2249760131",
          "name": "John D. Owens"
        },
        {
          "authorId": "2398857967",
          "name": "Xia Hu"
        },
        {
          "authorId": "2249530374",
          "name": "Song Han"
        },
        {
          "authorId": "2398794230",
          "name": "Timmy Liu"
        },
        {
          "authorId": "2394074732",
          "name": "Huizi Mao"
        }
      ],
      "abstract": "The growing demand for long-context inference capabilities in Large Language Models (LLMs) has intensified the computational and memory bottlenecks inherent to the standard attention mechanism. To address this challenge, we introduce BLASST, a drop-in sparse attention method that dynamically prunes the attention matrix without any pre-computation or proxy scores. Our method uses a fixed threshold and existing information from online softmax to identify negligible attention scores, skipping softmax computation, Value block loading, and the subsequent matrix multiplication. This fits seamlessly into existing FlashAttention kernel designs with negligible latency overhead. The approach is applicable to both prefill and decode stages across all attention variants (MHA, GQA, MQA, and MLA), providing a unified solution for accelerating long-context inference. We develop an automated calibration procedure that reveals a simple inverse relationship between optimal threshold and context length, enabling robust deployment across diverse scenarios. Maintaining high accuracy, we demonstrate a 1.62x speedup for prefill at 74.7% sparsity and a 1.48x speedup for decode at 73.2% sparsity on modern GPUs. Furthermore, we explore sparsity-aware training as a natural extension, showing that models can be trained to be inherently more robust to sparse attention patterns, pushing the accuracy-sparsity frontier even further."
    },
    {
      "paperId": "575b3d3a0b231d6462da3482d05550daf4efafef",
      "externalIds": {
        "ArXiv": "2512.10353",
        "CorpusId": 283737494
      },
      "corpusId": 283737494,
      "title": "Hybrid Transformer-Mamba Architecture for Weakly Supervised Volumetric Medical Segmentation",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.10353, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2238090441",
          "name": "Y. Lyu"
        },
        {
          "authorId": "46265547",
          "name": "Lian Xu"
        },
        {
          "authorId": "2338647839",
          "name": "Mohammed Bennamoun"
        },
        {
          "authorId": "3417987",
          "name": "F. Boussaid"
        },
        {
          "authorId": "2044351608",
          "name": "Coen Arrow"
        },
        {
          "authorId": "2363346213",
          "name": "Girish Dwivedi"
        }
      ],
      "abstract": "Weakly supervised semantic segmentation offers a label-efficient solution to train segmentation models for volumetric medical imaging. However, existing approaches often rely on 2D encoders that neglect the inherent volumetric nature of the data. We propose TranSamba, a hybrid Transformer-Mamba architecture designed to capture 3D context for weakly supervised volumetric medical segmentation. TranSamba augments a standard Vision Transformer backbone with Cross-Plane Mamba blocks, which leverage the linear complexity of state space models for efficient information exchange across neighboring slices. The information exchange enhances the pairwise self-attention within slices computed by the Transformer blocks, directly contributing to the attention maps for object localization. TranSamba achieves effective volumetric modeling with time complexity that scales linearly with the input volume depth and maintains constant memory usage for batch processing. Extensive experiments on three datasets demonstrate that TranSamba establishes new state-of-the-art performance, consistently outperforming existing methods across diverse modalities and pathologies. Our source code and trained models are openly accessible at: https://github.com/YihengLyu/TranSamba."
    },
    {
      "paperId": "6b0ef9604e589b6a9e31dc6f207ae1bd618d4da0",
      "externalIds": {
        "ArXiv": "2512.10252",
        "CorpusId": 283737841
      },
      "corpusId": 283737841,
      "title": "GDKVM: Echocardiography Video Segmentation via Spatiotemporal Key-Value Memory with Gated Delta Rule",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.10252, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2398703816",
          "name": "Rui Wang"
        },
        {
          "authorId": "2387815752",
          "name": "Yimu Sun"
        },
        {
          "authorId": "2387861818",
          "name": "Jingxing Guo"
        },
        {
          "authorId": "2293550809",
          "name": "Huisi Wu"
        },
        {
          "authorId": null,
          "name": "Jing Qin"
        }
      ],
      "abstract": "Accurate segmentation of cardiac chambers in echocardiography sequences is crucial for the quantitative analysis of cardiac function, aiding in clinical diagnosis and treatment. The imaging noise, artifacts, and the deformation and motion of the heart pose challenges to segmentation algorithms. While existing methods based on convolutional neural networks, Transformers, and space-time memory networks have improved segmentation accuracy, they often struggle with the trade-off between capturing long-range spatiotemporal dependencies and maintaining computational efficiency with fine-grained feature representation. In this paper, we introduce GDKVM, a novel architecture for echocardiography video segmentation. The model employs Linear Key-Value Association (LKVA) to effectively model inter-frame correlations, and introduces Gated Delta Rule (GDR) to efficiently store intermediate memory states. Key-Pixel Feature Fusion (KPFF) module is designed to integrate local and global features at multiple scales, enhancing robustness against boundary blurring and noise interference. We validated GDKVM on two mainstream echocardiography video datasets (CAMUS and EchoNet-Dynamic) and compared it with various state-of-the-art methods. Experimental results show that GDKVM outperforms existing approaches in terms of segmentation accuracy and robustness, while ensuring real-time performance. Code is available at https://github.com/wangrui2025/GDKVM."
    },
    {
      "paperId": "ed3db4c28b0f1abed25a8a34e13a50133ca4f658",
      "externalIds": {
        "ArXiv": "2512.10411",
        "CorpusId": 283737171
      },
      "corpusId": 283737171,
      "title": "Sliding Window Attention Adaptation",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.10411, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2275622434",
          "name": "Yijiong Yu"
        },
        {
          "authorId": "2253854104",
          "name": "Jiale Liu"
        },
        {
          "authorId": "2257162610",
          "name": "Qingyun Wu"
        },
        {
          "authorId": "2257095799",
          "name": "Huazheng Wang"
        },
        {
          "authorId": "2325878293",
          "name": "Ji Pei"
        }
      ],
      "abstract": "The self-attention mechanism in Transformer-based Large Language Models (LLMs) scales quadratically with input length, making long-context inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete SWA at inference-time for models pretrained with full attention (FA) causes severe long-context performance degradation due to training-inference mismatch. This makes us wonder: Can FA-pretrained LLMs be well adapted to SWA without pretraining? We investigate this by proposing Sliding Window Attention Adaptation (SWAA), a set of practical recipes that combine five methods for better adaptation: (1) applying SWA only during prefilling; (2) preserving\"sink\"tokens; (3) interleaving FA/SWA layers; (4) chain-of-thought (CoT); and (5) fine-tuning. Our experiments show that SWA adaptation is feasible while non-trivial: no single method suffices, yet specific synergistic combinations effectively recover the original long-context performance. We further analyze the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios, which can greatly and fundamentally accelerate LLM long-context inference speed by up to 100%. Our code is available at https://github.com/yuyijiong/sliding-window-attention-adaptation"
    },
    {
      "paperId": "4fd16df69ec65319e15a117afc5a4949891f5e3b",
      "externalIds": {
        "ArXiv": "2512.09492",
        "CorpusId": 283721551
      },
      "corpusId": 283721551,
      "title": "StateSpace-SSL: Linear-Time Self-supervised Learning for Plant Disease Detection",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.09492, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2249681876",
          "name": "Abdullah Al Mamun"
        },
        {
          "authorId": "2325709224",
          "name": "Miaohua Zhang"
        },
        {
          "authorId": "2345383570",
          "name": "David Ahmedt-Aristizabal"
        },
        {
          "authorId": "3028761",
          "name": "Zeeshan Hayder"
        },
        {
          "authorId": "2284014099",
          "name": "Mohammad Awrangjeb"
        }
      ],
      "abstract": "Self-supervised learning (SSL) is attractive for plant disease detection as it can exploit large collections of unlabeled leaf images, yet most existing SSL methods are built on CNNs or vision transformers that are poorly matched to agricultural imagery. CNN-based SSL struggles to capture disease patterns that evolve continuously along leaf structures, while transformer-based SSL introduces quadratic attention cost from high-resolution patches. To address these limitations, we propose StateSpace-SSL, a linear-time SSL framework that employs a Vision Mamba state-space encoder to model long-range lesion continuity through directional scanning across the leaf surface. A prototype-driven teacher-student objective aligns representations across multiple views, encouraging stable and lesion-aware features from labelled data. Experiments on three publicly available plant disease datasets show that StateSpace-SSL consistently outperforms the CNN- and transformer-based SSL baselines in various evaluation metrics. Qualitative analyses further confirm that it learns compact, lesion-focused feature maps, highlighting the advantage of linear state-space modelling for self-supervised plant disease representation learning."
    },
    {
      "paperId": "f56b12228283cf42cf8120a18fee60849b965f83",
      "externalIds": {
        "ArXiv": "2512.09238",
        "CorpusId": 283722209
      },
      "corpusId": 283722209,
      "title": "Training-free Context-adaptive Attention for Efficient Long Context Modeling",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.09238, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2238559407",
          "name": "Zeng You"
        },
        {
          "authorId": "1816749481",
          "name": "Yaofo Chen"
        },
        {
          "authorId": "2154723799",
          "name": "Shuhai Zhang"
        },
        {
          "authorId": "2365394290",
          "name": "Zhijie Qiu"
        },
        {
          "authorId": "2398494228",
          "name": "Tingyu Wu"
        },
        {
          "authorId": "2397715638",
          "name": "Yingjian Li"
        },
        {
          "authorId": "2288039852",
          "name": "Yaowei Wang"
        },
        {
          "authorId": "2287854562",
          "name": "Mingkui Tan"
        }
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks. These capabilities stem primarily from the self-attention mechanism, which enables modeling of long-range dependencies. However, the quadratic complexity of self-attention with respect to sequence length poses significant computational and memory challenges, especially as sequence length extends to extremes. While various sparse attention and KV cache compression methods have been proposed to improve efficiency, they often suffer from limitations such as reliance on fixed patterns, inability to handle both prefilling and decoding stages, or the requirement for additional training. In this paper, we propose Training-free Context-adaptive Attention (TCA-Attention), a training-free sparse attention mechanism that selectively attends to only the informative tokens for efficient long-context inference. Our method consists of two lightweight phases: i) an offline calibration phase that determines head-specific sparsity budgets via a single forward pass, and ii) an online token selection phase that adaptively retains core context tokens using a lightweight redundancy metric. TCA-Attention provides a unified solution that accelerates both prefilling and decoding while reducing KV cache memory footprint, without requiring parameter updates or architectural changes. Theoretical analysis shows that our approach maintains bounded approximation error. Extensive experiments demonstrate that TCA-Attention achieves a 2.8$\\times$ speedup and reduces KV cache by 61% at 128K context length while maintaining performance comparable to full attention across various benchmarks, offering a practical plug-and-play solution for efficient long-context inference."
    },
    {
      "paperId": "848118f4896dce0bae3cdde19961167358238ace",
      "externalIds": {
        "ArXiv": "2512.09892",
        "CorpusId": 283721453
      },
      "corpusId": 283721453,
      "title": "Provably Learning from Modern Language Models via Low Logit Rank",
      "venue": "",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.09892, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "3348246",
          "name": "Noah Golowich"
        },
        {
          "authorId": "2391493970",
          "name": "Allen Liu"
        },
        {
          "authorId": "2281640205",
          "name": "Abhishek Shetty"
        }
      ],
      "abstract": "While modern language models and their inner workings are incredibly complex, recent work (Golowich, Liu&Shetty; 2025) has proposed a simple and potentially tractable abstraction for them through the observation that empirically, these language models all seem to have approximately low logit rank. Roughly, this means that a matrix formed by the model's log probabilities of various tokens conditioned on certain sequences of tokens is well approximated by a low rank matrix. In this paper, our focus is on understanding how this structure can be exploited algorithmically for obtaining provable learning guarantees. Since low logit rank models can encode hard-to-learn distributions such as noisy parities, we study a query learning model with logit queries that reflects the access model for common APIs. Our main result is an efficient algorithm for learning any approximately low logit rank model from queries. We emphasize that our structural assumption closely reflects the behavior that is empirically observed in modern language models. Thus, our result gives what we believe is the first end-to-end learning guarantee for a generative model that plausibly captures modern language models."
    },
    {
      "paperId": "09085a00329fdc89ed7426e11f1ba0e0429224f6",
      "externalIds": {
        "ArXiv": "2512.10054",
        "CorpusId": 283736985
      },
      "corpusId": 283736985,
      "title": "Parallel Decoder Transformer: Model-Internal Parallel Decoding with Speculative Invariance via Note Conditioning",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.10054, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2397758876",
          "name": "Logan Robbins"
        }
      ],
      "abstract": "Autoregressive decoding in Large Language Models (LLMs) is inherently sequential, creating a latency bottleneck that scales linearly with output length. While ``Decomposition-and-Fill''methods like Skeleton-of-Thought attempt to parallelize generation via external orchestration, they suffer from \\textit{coherence drift} due to the lack of cross-stream communication. In this work, we introduce the \\textbf{Parallel Decoder Transformer (PDT)}, a parameter-efficient architecture that embeds coordination primitives directly into the inference process of a frozen pre-trained model. Instead of retraining the base model, PDT injects lightweight \\textit{Speculative Note Conditioning (SNC)} adapters that allow parallel decoding streams to synchronize via a shared, dynamic latent space. We formulate coordination as a \\textit{speculative consensus} problem, where sibling streams broadcast semantic ``notes''to a global bus, gated by a learned verification head. We validate our approach on a 50,000-step curriculum using a frozen 20B-parameter backbone. Our results demonstrate that PDT achieves effective self-correction, reaching \\textbf{77.8\\% precision} in coverage prediction and recovering approximate serial semantics without modifying the trunk weights. This establishes PDT as a scalable, efficient alternative to full model fine-tuning for structured parallel generation."
    },
    {
      "paperId": "07b9d899ed2506695b970873bdd4f61e759ffb11",
      "externalIds": {
        "DOI": "10.64898/2025.12.06.692772",
        "CorpusId": 283746384
      },
      "corpusId": 283746384,
      "title": "GUANinE v1.1 Reveals Complementarity of Supervised and Genomic Language Models",
      "venue": "bioRxiv",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.64898/2025.12.06.692772?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.64898/2025.12.06.692772, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2155667835",
          "name": "Eyes S. Robson"
        },
        {
          "authorId": "49063937",
          "name": "Nilah M. Ioannidis"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "19bd606d070cca75b11a455c3c3dc8e938ff8c7e",
      "externalIds": {
        "ArXiv": "2512.08829",
        "CorpusId": 283711609
      },
      "corpusId": 283711609,
      "title": "InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.08829, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2345961448",
          "name": "Hongyuan Tao"
        },
        {
          "authorId": "2060439659",
          "name": "Bencheng Liao"
        },
        {
          "authorId": "2144336691",
          "name": "Shaoyu Chen"
        },
        {
          "authorId": null,
          "name": "Haoran Yin"
        },
        {
          "authorId": "2261816376",
          "name": "Qian Zhang"
        },
        {
          "authorId": "2257432695",
          "name": "Wenyu Liu"
        },
        {
          "authorId": "2266175736",
          "name": "Xinggang Wang"
        }
      ],
      "abstract": "Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL."
    },
    {
      "paperId": "34214d70621612c0e6fbb967b3cf4af8b7ab6c2b",
      "externalIds": {
        "ArXiv": "2512.08161",
        "CorpusId": 283711984
      },
      "corpusId": 283711984,
      "title": "Fourier-RWKV: A Multi-State Perception Network for Efficient Image Dehazing",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.08161, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2312451862",
          "name": "Lirong Zheng"
        },
        {
          "authorId": "2271456053",
          "name": "Yanshan Li"
        },
        {
          "authorId": "2193031280",
          "name": "Rui Yu"
        },
        {
          "authorId": "2271730738",
          "name": "Kaihao Zhang"
        }
      ],
      "abstract": "Image dehazing is crucial for reliable visual perception, yet it remains highly challenging under real-world non-uniform haze conditions. Although Transformer-based methods excel at capturing global context, their quadratic computational complexity hinders real-time deployment. To address this, we propose Fourier Receptance Weighted Key Value (Fourier-RWKV), a novel dehazing framework based on a Multi-State Perception paradigm. The model achieves comprehensive haze degradation modeling with linear complexity by synergistically integrating three distinct perceptual states: (1) Spatial-form Perception, realized through the Deformable Quad-directional Token Shift (DQ-Shift) operation, which dynamically adjusts receptive fields to accommodate local haze variations; (2) Frequency-domain Perception, implemented within the Fourier Mix block, which extends the core WKV attention mechanism of RWKV from the spatial domain to the Fourier domain, preserving the long-range dependencies essential for global haze estimation while mitigating spatial attenuation; (3) Semantic-relation Perception, facilitated by the Semantic Bridge Module (SBM), which utilizes Dynamic Semantic Kernel Fusion (DSK-Fusion) to precisely align encoder-decoder features and suppress artifacts. Extensive experiments on multiple benchmarks demonstrate that Fourier-RWKV delivers state-of-the-art performance across diverse haze scenarios while significantly reducing computational overhead, establishing a favorable trade-off between restoration quality and practical efficiency. Code is available at: https://github.com/Dilizlr/Fourier-RWKV."
    },
    {
      "paperId": "75223fe2404b51c43a09883ffc816f0bbc66a4bc",
      "externalIds": {
        "DOI": "10.1007/s13755-025-00401-2",
        "CorpusId": 283717623,
        "PubMed": "41383823"
      },
      "corpusId": 283717623,
      "title": "Interpretable large language models for early prediction of antimicrobial multidrug resistance",
      "venue": "Health Information Science and Systems",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s13755-025-00401-2?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s13755-025-00401-2, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2397541176",
          "name": "Luc\u00eda Carmona-Martos"
        },
        {
          "authorId": "2339478148",
          "name": "Paula Mart\u00edn-Palomeque"
        },
        {
          "authorId": "2101260650",
          "name": "\u00d3scar Escudero-Arnanz"
        },
        {
          "authorId": "2290486876",
          "name": "Cristina Soguero-Ru\u00edz"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "9b2e96a201d003b9fbe6e21bd30d77812e3c5cf8",
      "externalIds": {
        "ArXiv": "2512.09152",
        "CorpusId": 283721671
      },
      "corpusId": 283721671,
      "title": "Understanding temperature tuning in energy-based models",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.09152, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2277761602",
          "name": "Peter W Fields"
        },
        {
          "authorId": "2965682",
          "name": "Wave Ngampruetikorn"
        },
        {
          "authorId": "2277756411",
          "name": "David J. Schwab"
        },
        {
          "authorId": "2258957633",
          "name": "S. E. Palmer"
        }
      ],
      "abstract": "Generative models of complex systems often require post-hoc parameter adjustments to produce useful outputs. For example, energy-based models for protein design are sampled at an artificially low''temperature''to generate novel, functional sequences. This temperature tuning is a common yet poorly understood heuristic used across machine learning contexts to control the trade-off between generative fidelity and diversity. Here, we develop an interpretable, physically motivated framework to explain this phenomenon. We demonstrate that in systems with a large''energy gap''- separating a small fraction of meaningful states from a vast space of unrealistic states - learning from sparse data causes models to systematically overestimate high-energy state probabilities, a bias that lowering the sampling temperature corrects. More generally, we characterize how the optimal sampling temperature depends on the interplay between data size and the system's underlying energy landscape. Crucially, our results show that lowering the sampling temperature is not always desirable; we identify the conditions where \\emph{raising} it results in better generative performance. Our framework thus casts post-hoc temperature tuning as a diagnostic tool that reveals properties of the true data distribution and the limits of the learned model."
    },
    {
      "paperId": "ec8e92eb414442a128c7142e53e8a723f4f4be82",
      "externalIds": {
        "DOI": "10.64898/2025.12.05.692480",
        "CorpusId": 283722913
      },
      "corpusId": 283722913,
      "title": "Encoding and Decoding of Brain Dynamic Functional Connectivity for ADHD Diagnosis",
      "venue": "bioRxiv",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.64898/2025.12.05.692480?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.64898/2025.12.05.692480, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2292599520",
          "name": "Deepank Girish"
        },
        {
          "authorId": "2265592289",
          "name": "Yi Hao Chan"
        },
        {
          "authorId": "29822768",
          "name": "Sukrit Gupta"
        },
        {
          "authorId": "2293273677",
          "name": "Jing Xia"
        },
        {
          "authorId": "2332585255",
          "name": "Jagath C. Rajapakse"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "14171ebc94e70ce645612fbc9e8cc1a4ce08450e",
      "externalIds": {
        "DOI": "10.3390/ai6120323",
        "CorpusId": 283781748
      },
      "corpusId": 283781748,
      "title": "An Adaptative Wavelet Time\u2013Frequency Transform with Mamba Network for OFDM Automatic Modulation Classification",
      "venue": "Applied Informatics",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/ai6120323?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/ai6120323, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2382573938",
          "name": "Hongji Xing"
        },
        {
          "authorId": "2118487433",
          "name": "Xiaogang Tang"
        },
        {
          "authorId": "2322962601",
          "name": "Lu Wang"
        },
        {
          "authorId": "2281978845",
          "name": "Binquan Zhang"
        },
        {
          "authorId": "2354592959",
          "name": "Yuepeng Li"
        }
      ],
      "abstract": "Background: With the development of wireless communication technologies, the rapid advancement of 5G and 6G communication systems has spawned an urgent demand for low latency and high data rates. Orthogonal Frequency Division Multiplexing (OFDM) communication using high-order digital modulation has become a key technology due to its characteristics, such as high reliability, high data rate, and low latency, and has been widely applied in various fields. As a component of cognitive radios, automatic modulation classification (AMC) plays an important role in remote sensing and electromagnetic spectrum sensing. However, under current complex channel conditions, there are issues such as low signal-to-noise ratio (SNR), Doppler frequency shift, and multipath propagation. Methods: Coupled with the inherent problem of indistinct characteristics in high-order modulation, these currently make it difficult for AMC to focus on OFDM and high-order digital modulation. Existing methods are mainly based on a single model-driven approach or data-driven approach. The Adaptive Wavelet Mamba Network (AWMN) proposed in this paper attempts to combine model-driven adaptive wavelet transform feature extraction with the Mamba deep learning architecture. A module based on the lifting wavelet scheme effectively captures discriminative time\u2013frequency features using learnable operations. Meanwhile, a Mamba network constructed based on the State Space Model (SSM) can capture long-term temporal dependencies. This network realizes a combination of model-driven and data-driven methods. Results: Tests conducted on public datasets and a custom-built real-time received OFDM dataset show that the proposed AWMN achieves a performance reaching higher accuracies of 62.39%, 64.50%, and 74.95% on the public Rml2016(a) and Rml2016(b) datasets and our formulated EVAS dataset, while maintaining a compact parameter size of 0.44 M. Conclusions: These results highlight its potential for improving the automatic modulation classification of high-order OFDM modulation in 5G/6G systems."
    },
    {
      "paperId": "2d0738ac90274c3499b707f0c23ab0dfa83d9588",
      "externalIds": {
        "ArXiv": "2512.07385",
        "CorpusId": 283693843
      },
      "corpusId": 283693843,
      "title": "How Far are Modern Trackers from UAV-Anti-UAV? A Million-Scale Benchmark and New Baseline",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.07385, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2247661930",
          "name": "Chunhui Zhang"
        },
        {
          "authorId": "2302827257",
          "name": "Li Liu"
        },
        {
          "authorId": "2353317268",
          "name": "Zhipeng Zhang"
        },
        {
          "authorId": "2396842882",
          "name": "Yong Wang"
        },
        {
          "authorId": "2269744771",
          "name": "Hao Wen"
        },
        {
          "authorId": "2302815087",
          "name": "Xi Zhou"
        },
        {
          "authorId": "2257167253",
          "name": "Shiming Ge"
        },
        {
          "authorId": "2246831804",
          "name": "Yanfeng Wang"
        }
      ],
      "abstract": "Unmanned Aerial Vehicles (UAVs) offer wide-ranging applications but also pose significant safety and privacy violation risks in areas like airport and infrastructure inspection, spurring the rapid development of Anti-UAV technologies in recent years. However, current Anti-UAV research primarily focuses on RGB, infrared (IR), or RGB-IR videos captured by fixed ground cameras, with little attention to tracking target UAVs from another moving UAV platform. To fill this gap, we propose a new multi-modal visual tracking task termed UAV-Anti-UAV, which involves a pursuer UAV tracking a target adversarial UAV in the video stream. Compared to existing Anti-UAV tasks, UAV-Anti-UAV is more challenging due to severe dual-dynamic disturbances caused by the rapid motion of both the capturing platform and the target. To advance research in this domain, we construct a million-scale dataset consisting of 1,810 videos, each manually annotated with bounding boxes, a language prompt, and 15 tracking attributes. Furthermore, we propose MambaSTS, a Mamba-based baseline method for UAV-Anti-UAV tracking, which enables integrated spatial-temporal-semantic learning. Specifically, we employ Mamba and Transformer models to learn global semantic and spatial features, respectively, and leverage the state space model's strength in long-sequence modeling to establish video-level long-term context via a temporal token propagation mechanism. We conduct experiments on the UAV-Anti-UAV dataset to validate the effectiveness of our method. A thorough experimental evaluation of 50 modern deep tracking algorithms demonstrates that there is still significant room for improvement in the UAV-Anti-UAV domain. The dataset and codes will be available at {\\color{magenta}https://github.com/983632847/Awesome-Multimodal-Object-Tracking}."
    },
    {
      "paperId": "9ff815fd587f255b9db70f9708e36c91d1efbc95",
      "externalIds": {
        "ArXiv": "2512.07352",
        "CorpusId": 283693321
      },
      "corpusId": 283693321,
      "title": "MultiAPI Spoof: A Multi-API Dataset and Local-Attention Network for Speech Anti-spoofing Detection",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.07352, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2381295367",
          "name": "Xueping Zhang"
        },
        {
          "authorId": "2382231660",
          "name": "Zhenshan Zhang"
        },
        {
          "authorId": "2381373338",
          "name": "Yechen Wang"
        },
        {
          "authorId": "2381311782",
          "name": "Linxi Li"
        },
        {
          "authorId": "2383115329",
          "name": "Liwei Jin"
        },
        {
          "authorId": "2381784971",
          "name": "Ming Li"
        }
      ],
      "abstract": "Existing speech anti-spoofing benchmarks rely on a narrow set of public models, creating a substantial gap from real-world scenarios in which commercial systems employ diverse, often proprietary APIs. To address this issue, we introduce MultiAPI Spoof, a multi-API audio anti-spoofing dataset comprising about 230 hours of synthetic speech generated by 30 distinct APIs, including commercial services, open-source models, and online platforms. Based on this dataset, we define the API tracing task, enabling fine-grained attribution of spoofed audio to its generation source. We further propose Nes2Net-LA, a local-attention enhanced variant of Nes2Net that improves local context modeling and fine-grained spoofing feature extraction. Experiments show that Nes2Net-LA achieves state-of-the-art performance and offers superior robustness, particularly under diverse and unseen spoofing conditions. Code \\footnote{https://github.com/XuepingZhang/MultiAPI-Spoof} and dataset \\footnote{https://xuepingzhang.github.io/MultiAPI-Spoof-Dataset/} have released."
    },
    {
      "paperId": "63c1f8812f0e3460c6808a82a34bb6623dc1c314",
      "externalIds": {
        "DOI": "10.1145/3769748.3773340",
        "CorpusId": 283673027
      },
      "corpusId": 283673027,
      "title": "Seeing in the Noisy Dark: A New Real-world Benchmark and An Efficient Method for Extreme Low-light Image Enhancement",
      "venue": "Proceedings of the 7th ACM International Conference on Multimedia in Asia",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3769748.3773340?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3769748.3773340, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2397298206",
          "name": "Jiashuo Liu"
        },
        {
          "authorId": null,
          "name": "Chenjian Chu"
        },
        {
          "authorId": "2397295846",
          "name": "Xiaohui Cao"
        },
        {
          "authorId": "2397189471",
          "name": "Junjie Kuang"
        }
      ],
      "abstract": "Despite significant progress in low-light image enhancement (LLIE), deep models trained on most existing datasets still struggle to handle more challenging real-world low-light scenarios. This is because these training data primarily focus on luminance enhancement, while neglecting the heavy and complex noise typically present in extreme dark environments. To this end, we contribute a new challenging benchmark dataset, LOL-Noise, which contains 5,000 low-noise/normal-clear high-resolution image pairs with extreme darkness and heavy noise captured in real-world scenarios. We also propose an efficient neighbor-aware wavelet state space model, called NW-Mamba, for LLIE. In contrast to existing Mamba-based methods that employ multi-direction scanning strategies for feature extraction, we introduce a single-direction neighbor-aware scanning method, which models spatial relationships by assigning similar tokens as neighbors within the scanning sequence. Extensive experiments demonstrate the value of our dataset and the effectiveness of our method. The dataset and code will be available to the public."
    },
    {
      "paperId": "86ffde4dfb4be5e75c1a3eb7c91d3cdc7f40c54a",
      "externalIds": {
        "ArXiv": "2512.07782",
        "CorpusId": 283693502
      },
      "corpusId": 283693502,
      "title": "GatedFWA: Linear Flash Windowed Attention with Gated Associative Memory",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.07782, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2397399826",
          "name": "Jiaxu Liu"
        },
        {
          "authorId": "2397822157",
          "name": "Yuhe Bai"
        },
        {
          "authorId": "4408876",
          "name": "C. Bouganis"
        }
      ],
      "abstract": "Modern autoregressive models rely on attention, yet the Softmax full attention in Transformers scales quadratically with sequence length. Sliding Window Attention (SWA) achieves linear-time encoding/decoding by constraining the attention pattern, but under an \\textit{Associative Memory} interpretation, its difference-style update renders the training objective effectively \\emph{unbounded}. In contrast, Softmax attention normalizes updates, leading to \\emph{memory shrinkage and gradient vanishing}. We propose GatedFWA: a Memory-\\underline{Gated} (\\underline{F}lash) \\underline{W}indowed \\underline{A}ttention mechanism that preserves SWAs efficiency while stabilizing memory updates and making gradient flow controllable. In essence, GatedFWA accumulate a per-token/head gate into a decay bias added to the attention logits, acting as a learnable contraction in the memory recurrence. We implement a fused one-pass gate preprocessing and a FlashAttention-compatible kernel that injects the gate under a sliding mask, ensuring I/O efficiency and numerical stability. On language modelling benchmarks, GatedFWA delivers competitive throughput with negligible overhead and better use of global context, and it integrates cleanly with token compression/selection methods such as NSA and generalizes to various autoregressive domains."
    },
    {
      "paperId": "c26d8448e377df3d4a36084ba341e86b73ec748e",
      "externalIds": {
        "ArXiv": "2512.07756",
        "CorpusId": 283694473
      },
      "corpusId": 283694473,
      "title": "UltrasODM: A Dual Stream Optical Flow Mamba Network for 3D Freehand Ultrasound Reconstruction",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.07756, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2397374004",
          "name": "Mayank Anand"
        },
        {
          "authorId": "1768634961",
          "name": "Ujair Alam"
        },
        {
          "authorId": "2397379743",
          "name": "Surya Prakash"
        },
        {
          "authorId": "90343034",
          "name": "Priya Shukla"
        },
        {
          "authorId": "2188707635",
          "name": "G. C. Nandi"
        },
        {
          "authorId": "2397384145",
          "name": "Domenec Puig"
        }
      ],
      "abstract": "Clinical ultrasound acquisition is highly operator-dependent, where rapid probe motion and brightness fluctuations often lead to reconstruction errors that reduce trust and clinical utility. We present UltrasODM, a dual-stream framework that assists sonographers during acquisition through calibrated per-frame uncertainty, saliency-based diagnostics, and actionable prompts. UltrasODM integrates (i) a contrastive ranking module that groups frames by motion similarity, (ii) an optical-flow stream fused with Dual-Mamba temporal modules for robust 6-DoF pose estimation, and (iii) a Human-in-the-Loop (HITL) layer combining Bayesian uncertainty, clinician-calibrated thresholds, and saliency maps highlighting regions of low confidence. When uncertainty exceeds the threshold, the system issues unobtrusive alerts suggesting corrective actions such as re-scanning highlighted regions or slowing the sweep. Evaluated on a clinical freehand ultrasound dataset, UltrasODM reduces drift by 15.2%, distance error by 12.1%, and Hausdorff distance by 10.1% relative to UltrasOM, while producing per-frame uncertainty and saliency outputs. By emphasizing transparency and clinician feedback, UltrasODM improves reconstruction reliability and supports safer, more trustworthy clinical workflows. Our code is publicly available at https://github.com/AnandMayank/UltrasODM."
    },
    {
      "paperId": "2552c8c0586ebde07966913569c73086a1d068b5",
      "externalIds": {
        "ArXiv": "2512.07806",
        "CorpusId": 283692936
      },
      "corpusId": 283692936,
      "title": "Multi-view Pyramid Transformer: Look Coarser to See Broader",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.07806, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2295666507",
          "name": "Gyeongjin Kang"
        },
        {
          "authorId": "2399106714",
          "name": "Seungkwon Yang"
        },
        {
          "authorId": "2163125327",
          "name": "Seungtae Nam"
        },
        {
          "authorId": "2295680391",
          "name": "Younggeun Lee"
        },
        {
          "authorId": "2397382534",
          "name": "Jungwoo Kim"
        },
        {
          "authorId": null,
          "name": "Eunbyung Park"
        }
      ],
      "abstract": "We propose Multi-view Pyramid Transformer (MVP), a scalable multi-view transformer architecture that directly reconstructs large 3D scenes from tens to hundreds of images in a single forward pass. Drawing on the idea of ``looking broader to see the whole, looking finer to see the details,\"MVP is built on two core design principles: 1) a local-to-global inter-view hierarchy that gradually broadens the model's perspective from local views to groups and ultimately the full scene, and 2) a fine-to-coarse intra-view hierarchy that starts from detailed spatial representations and progressively aggregates them into compact, information-dense tokens. This dual hierarchy achieves both computational efficiency and representational richness, enabling fast reconstruction of large and complex scenes. We validate MVP on diverse datasets and show that, when coupled with 3D Gaussian Splatting as the underlying 3D representation, it achieves state-of-the-art generalizable reconstruction quality while maintaining high efficiency and scalability across a wide range of view configurations."
    },
    {
      "paperId": "756c20050fa2addf1eea1cc91b949b0c0bc98a2d",
      "externalIds": {
        "ArXiv": "2512.07100",
        "CorpusId": 283693917
      },
      "corpusId": 283693917,
      "title": "Dual Refinement Cycle Learning: Unsupervised Text Classification of Mamba and Community Detection on Text Attributed Graph",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.07100, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2283763447",
          "name": "Hong Wang"
        },
        {
          "authorId": "2338857845",
          "name": "Yinglong Zhang"
        },
        {
          "authorId": "2397459521",
          "name": "Hanhan Guo"
        },
        {
          "authorId": null,
          "name": "Xuewen Xia"
        },
        {
          "authorId": "2152777692",
          "name": "Xing Xu"
        }
      ],
      "abstract": "Pretrained language models offer strong text understanding capabilities but remain difficult to deploy in real-world text-attributed networks due to their heavy dependence on labeled data. Meanwhile, community detection methods typically ignore textual semantics, limiting their usefulness in downstream applications such as content organization, recommendation, and risk monitoring. To overcome these limitations, we present Dual Refinement Cycle Learning (DRCL), a fully unsupervised framework designed for practical scenarios where no labels or category definitions are available. DRCL integrates structural and semantic information through a warm-start initialization and a bidirectional refinement cycle between a GCN-based Community Detection Module (GCN-CDM) and a Text Semantic Modeling Module (TSMM). The two modules iteratively exchange pseudo-labels, allowing semantic cues to enhance structural clustering and structural patterns to guide text representation learning without manual supervision. Across several text-attributed graph datasets, DRCL consistently improves the structural and semantic quality of discovered communities. Moreover, a Mamba-based classifier trained solely from DRCL's community signals achieves accuracy comparable to supervised models, demonstrating its potential for deployment in large-scale systems where labeled data are scarce or costly. The code is available at https://github.com/wuanghoong/DRCL.git."
    },
    {
      "paperId": "a4490e1e8acc533785e064183751928d11c14d77",
      "externalIds": {
        "ArXiv": "2512.06983",
        "CorpusId": 283693642
      },
      "corpusId": 283693642,
      "title": "On Memory: A comparison of memory mechanisms in world models",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.06983, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2316700374",
          "name": "Eli J. Laird"
        },
        {
          "authorId": "2316766271",
          "name": "Corey Clark"
        }
      ],
      "abstract": "World models enable agents to plan within imagined environments by predicting future states conditioned on past observations and actions. However, their ability to plan over long horizons is limited by the effective memory span of the backbone architecture. This limitation leads to perceptual drift in long rollouts, hindering the model's capacity to perform loop closures within imagined trajectories. In this work, we investigate the effective memory span of transformer-based world models through an analysis of several memory augmentation mechanisms. We introduce a taxonomy that distinguishes between memory encoding and memory injection mechanisms, motivating their roles in extending the world model's memory through the lens of residual stream dynamics. Using a state recall evaluation task, we measure the memory recall of each mechanism and analyze its respective trade-offs. Our findings show that memory mechanisms improve the effective memory span in vision transformers and provide a path to completing loop closures within a world model's imagination."
    },
    {
      "paperId": "b8a538e4380981dd1d9955f232e04d5f1c23bedb",
      "externalIds": {
        "ArXiv": "2512.06721",
        "CorpusId": 283693820
      },
      "corpusId": 283693820,
      "title": "ProAgent: Harnessing On-Demand Sensory Contexts for Proactive LLM Agent Systems",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.06721, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2267429742",
          "name": "Bufang Yang"
        },
        {
          "authorId": "2302513213",
          "name": "Lilin Xu"
        },
        {
          "authorId": "2362078956",
          "name": "Liekang Zeng"
        },
        {
          "authorId": "2311824199",
          "name": "Yunqi Guo"
        },
        {
          "authorId": "2302470640",
          "name": "Siyang Jiang"
        },
        {
          "authorId": "2363152955",
          "name": "Wenrui Lu"
        },
        {
          "authorId": "2294927293",
          "name": "Kaiwei Liu"
        },
        {
          "authorId": "2397384883",
          "name": "Hancheng Xiang"
        },
        {
          "authorId": "2302502130",
          "name": "Xiaofan Jiang"
        },
        {
          "authorId": "2238951251",
          "name": "Guoliang Xing"
        },
        {
          "authorId": "2261616783",
          "name": "Zhenyu Yan"
        }
      ],
      "abstract": "Large Language Model (LLM) agents are emerging to transform daily life. However, existing LLM agents primarily follow a reactive paradigm, relying on explicit user instructions to initiate services, which increases both physical and cognitive workload. In this paper, we propose ProAgent, the first end-to-end proactive agent system that harnesses massive sensory contexts and LLM reasoning to deliver proactive assistance. ProAgent first employs a proactive-oriented context extraction approach with on-demand tiered perception to continuously sense the environment and derive hierarchical contexts that incorporate both sensory and persona cues. ProAgent then adopts a context-aware proactive reasoner to map these contexts to user needs and tool calls, providing proactive assistance. We implement ProAgent on Augmented Reality (AR) glasses with an edge server and extensively evaluate it on a real-world testbed, a public dataset, and through a user study. Results show that ProAgent achieves up to 33.4% higher proactive prediction accuracy, 16.8% higher tool-calling F1 score, and notable improvements in user satisfaction over state-of-the-art baselines, marking a significant step toward proactive assistants. A video demonstration of ProAgent is available at https://youtu.be/pRXZuzvrcVs."
    },
    {
      "paperId": "51b011413ad5088c001409171e6b8598bcd1950a",
      "externalIds": {
        "ArXiv": "2512.06929",
        "CorpusId": 283694077
      },
      "corpusId": 283694077,
      "title": "Adaptive Normalization Mamba with Multi Scale Trend Decomposition and Patch MoE Encoding",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.06929, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2397376071",
          "name": "MinCheol Jeon"
        }
      ],
      "abstract": "Time series forecasting in real world environments faces significant challenges non stationarity, multi scale temporal patterns, and distributional shifts that degrade model stability and accuracy. This study propose AdaMamba, a unified forecasting architecture that integrates adaptive normalization, multi scale trend extraction, and contextual sequence modeling to address these challenges. AdaMamba begins with an Adaptive Normalization Block that removes non stationary components through multi scale convolutional trend extraction and channel wise recalibration, enabling consistent detrending and variance stabilization. The normalized sequence is then processed by a Context Encoder that combines patch wise embeddings, positional encoding, and a Mamba enhanced Transformer layer with a mixture of experts feed forward module, allowing efficient modeling of both long range dependencies and local temporal dynamics. A lightweight prediction head generates multi horizon forecasts, and a denormalization mechanism reconstructs outputs by reintegrating local trends to ensure robustness under varying temporal conditions. AdaMamba provides strong representational capacity with modular extensibility, supporting deterministic prediction and compatibility with probabilistic extensions. Its design effectively mitigates covariate shift and enhances predictive reliability across heterogeneous datasets. Experimental evaluations demonstrate that AdaMamba's combination of adaptive normalization and expert augmented contextual modeling yields consistent improvements in stability and accuracy over conventional Transformer based baselines."
    },
    {
      "paperId": "a699c77f1a0b270214ce2f9a1df112e18b125606",
      "externalIds": {
        "DOI": "10.1145/3743093.3771057",
        "CorpusId": 283567699
      },
      "corpusId": 283567699,
      "title": "Hybrid Mamba-Transformer Model for Lightweight Image Super-Resolution",
      "venue": "Proceedings of the 7th ACM International Conference on Multimedia in Asia",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3743093.3771057?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3743093.3771057, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2396676189",
          "name": "Szuchi Lu"
        },
        {
          "authorId": "2396726218",
          "name": "Shih-Hsuan Yang"
        }
      ],
      "abstract": "With the advancement of deep learning, single image super-resolution (SISR) methods based on Convolutional Neural Networks (CNNs) and Transformers have become mainstream. CNNs are effective at extracting local features but are limited in modeling long-range dependencies. In contrast, Transformers offer strong global modeling capabilities, yet their computational complexity grows quadratically with sequence length, which limits their practical use. Recently, emerging State Space Models (SSMs), such as Mamba, have demonstrated the ability to model long-range dependencies efficiently with linear complexity, offering a promising new direction for SISR model design. In this work, we adopt a hybrid Mamba-Transformer architecture, where Mamba and Transformer blocks are alternately stacked to fully leverage the strengths of both modules. Specifically, we first use the window-scaled cosine attention mechanism from Swin Transformer V2 to capture local image features, and use a lightweight generation network to dynamically generate relative position biases, adapting to varying input sizes while reducing parameter counts under large-window settings. We then incorporate a multi-scale Mamba module. One branch applies selective scanning at multiple scales to efficiently model long-range information with linear complexity, while the other branch uses 2D depthwise convolution to compensate for spatial information loss in sequence modeling. Finally, the outputs from both branches are fused to integrate sequence and spatial information, achieving a good balance between modeling capacity and computational efficiency. Experimental results show that the proposed method outperforms other lightweight Transformer-based super-resolution approaches. Compared with the hybrid Mamba-Transformer architecture used in MambaIRv2, our method achieves comparable PSNR and SSIM performance while reducing the number of model parameters by approximately 19% and lowering computational complexity by around 6%."
    },
    {
      "paperId": "8cac751867d3e425f50a0d7e0a12b09f834d01e8",
      "externalIds": {
        "ArXiv": "2512.06426",
        "CorpusId": 283693609
      },
      "corpusId": 283693609,
      "title": "When Gender is Hard to See: Multi-Attribute Support for Long-Range Recognition",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.06426, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2359633072",
          "name": "Nzakiese Mbongo"
        },
        {
          "authorId": "2280227620",
          "name": "Kailash A. Hambarde"
        },
        {
          "authorId": "2397376517",
          "name": "Hugo Proencca"
        }
      ],
      "abstract": "Accurate gender recognition from extreme long-range imagery remains a challenging problem due to limited spatial resolution, viewpoint variability, and loss of facial cues. For such purpose, we present a dual-path transformer framework that leverages CLIP to jointly model visual and attribute-driven cues for gender recognition at a distance. The framework integrates two complementary streams: (1) a direct visual path that refines a pre-trained CLIP image encoder through selective fine-tuning of its upper layers, and (2) an attribute-mediated path that infers gender from a set of soft-biometric prompts (e.g., hairstyle, clothing, accessories) aligned in the CLIP text-image space. Spatial channel attention modules further enhance discriminative localization under occlusion and low resolution. To support large-scale evaluation, we construct U-DetAGReID, a unified long-range gender dataset derived from DetReIDx and AG-ReID.v2, harmonized under a consistent ternary labeling scheme (Male, Female, Unknown). Extensive experiments suggest that the proposed solution surpasses state-of-the-art person-attribute and re-identification baselines across multiple metrics (macro-F1, accuracy, AUC), with consistent robustness to distance, angle, and height variations. Qualitative attention visualizations confirm interpretable attribute localization and responsible abstention behavior. Our results show that language-guided dual-path learning offers a principled, extensible foundation for responsible gender recognition in unconstrained long-range scenarios."
    },
    {
      "paperId": "685659bac1aba253decb8d3f499c5e06b7243446",
      "externalIds": {
        "ArXiv": "2512.06582",
        "CorpusId": 283693007
      },
      "corpusId": 283693007,
      "title": "QL-LSTM: A Parameter-Efficient LSTM for Stable Long-Sequence Modeling",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.06582, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2317786045",
          "name": "Isaac Kofi Nti"
        }
      ],
      "abstract": "Recurrent neural architectures such as LSTM and GRU remain widely used in sequence modeling, but they continue to face two core limitations: redundant gate-specific parameters and reduced ability to retain information across long temporal distances. This paper introduces the Quantum-Leap LSTM (QL-LSTM), a recurrent architecture designed to address both challenges through two independent components. The Parameter-Shared Unified Gating mechanism replaces all gate-specific transformations with a single shared weight matrix, reducing parameters by approximately 48 percent while preserving full gating behavior. The Hierarchical Gated Recurrence with Additive Skip Connections component adds a multiplication-free pathway that improves long-range information flow and reduces forget-gate degradation. We evaluate QL-LSTM on sentiment classification using the IMDB dataset with extended document lengths, comparing it to LSTM, GRU, and BiLSTM reference models. QL-LSTM achieves competitive accuracy while using substantially fewer parameters. Although the PSUG and HGR-ASC components are more efficient per time step, the current prototype remains limited by the inherent sequential nature of recurrent models and therefore does not yet yield wall-clock speed improvements without further kernel-level optimization."
    },
    {
      "paperId": "8dff2f635b582844edfc4bc9cc66e5907893e903",
      "externalIds": {
        "ArXiv": "2512.06421",
        "CorpusId": 283693518
      },
      "corpusId": 283693518,
      "title": "Rethinking Training Dynamics in Scale-wise Autoregressive Generation",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.06421, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": null,
          "name": "Gengze Zhou"
        },
        {
          "authorId": "2395895392",
          "name": "Chongjian Ge"
        },
        {
          "authorId": null,
          "name": "Hao Tan"
        },
        {
          "authorId": "2396713811",
          "name": "Feng Liu"
        },
        {
          "authorId": "2265727453",
          "name": "Yicong Hong"
        }
      ],
      "abstract": "Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation."
    },
    {
      "paperId": "ba5514281fa05b45add099b47f498d8cbc767a4d",
      "externalIds": {
        "ArXiv": "2512.06275",
        "CorpusId": 283694051
      },
      "corpusId": 283694051,
      "title": "FacePhys: State of the Heart Learning",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.06275, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2267685708",
          "name": "Kegang Wang"
        },
        {
          "authorId": "2204826271",
          "name": "Jiankai Tang"
        },
        {
          "authorId": "2267699941",
          "name": "Yuntao Wang"
        },
        {
          "authorId": "2267683550",
          "name": "Xin Liu"
        },
        {
          "authorId": "2353433633",
          "name": "Yuxuan Fan"
        },
        {
          "authorId": null,
          "name": "Jiatong Ji"
        },
        {
          "authorId": "2333361107",
          "name": "Yuanchun Shi"
        },
        {
          "authorId": "2315286830",
          "name": "D. McDuff"
        }
      ],
      "abstract": "Vital sign measurement using cameras presents opportunities for comfortable, ubiquitous health monitoring. Remote photoplethysmography (rPPG), a foundational technology, enables cardiac measurement through minute changes in light reflected from the skin. However, practical deployment is limited by the computational constraints of performing analysis on front-end devices and the accuracy degradation of transmitting data through compressive channels that reduce signal quality. We propose a memory efficient rPPG algorithm - \\emph{FacePhys} - built on temporal-spatial state space duality, which resolves the trilemma of model scalability, cross-dataset generalization, and real-time operation. Leveraging a transferable heart state, FacePhys captures subtle periodic variations across video frames while maintaining a minimal computational overhead, enabling training on extended video sequences and supporting low-latency inference. FacePhys establishes a new state-of-the-art, with a substantial 49\\% reduction in error. Our solution enables real-time inference with a memory footprint of 3.6 MB and per-frame latency of 9.46 ms -- surpassing existing methods by 83\\% to 99\\%. These results translate into reliable real-time performance in practical deployments, and a live demo is available at https://www.facephys.com/."
    },
    {
      "paperId": "321f3f10d7524203c20bf55d7d4bc73caf54b538",
      "externalIds": {
        "PubMedCentral": "12714595",
        "DOI": "10.3389/fendo.2025.1686248",
        "CorpusId": 283644921
      },
      "corpusId": 283644921,
      "title": "Thyroid intelligent diagnosis based on THMSNet",
      "venue": "Frontiers in Endocrinology",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12714595, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2397020646",
          "name": "Zhen Rao"
        },
        {
          "authorId": "2398986398",
          "name": "Tao Yu"
        },
        {
          "authorId": "2397146636",
          "name": "Xitan Yu"
        }
      ],
      "abstract": "Background Thyroid disease is a common endocrine disorder, with the differentiation between benign and malignant nodules being critical for clinical decision-making. Traditional diagnostic methods, such as ultrasound and TI-RADS classification, are limited by interobserver variability and time-consuming processes. While deep learning approaches such as CNNs and transformers have shown promise, they face challenges in multiscale feature extraction, global dependency modeling, and alignment with clinical standards. Methods We proposes THMSNet, a hybrid architecture that integrates a pyramid structure for multiscale feature extraction and Mamba for global long-range dependency modeling. The serial channel\u2013spatial attention module (SCSAM) enhances feature representation, whereas the truth\u2013value calibration (TVC) algorithm aligns model predictions with pathological standards. The system is evaluated on a public dataset of 7,288 thyroid ultrasound images (3,282 benign, 4,006 malignant) via five metrics: accuracy, precision, recall, F1 score, and AUROC. Results THMSNet achieves 91.15% accuracy, 93.28% recall, and 96.92% AUROC, outperforming ResNet (86.03% accuracy) and DenseNet (95.50% AUROC). Confidence intervals are calculated for key metrics, further strengthening the rigor of results. Ablation studies confirm the utility of each module, with the pyramid architecture (+7.83% accuracy), Mamba (+2.99%), SCSAM (+6.94%), and TVC (+6.94%) progressively contributing to performance improvements. Conclusion THMSNet provides a robust and clinically applicable solution for thyroid nodule diagnosis, combining advanced feature extraction, attention mechanisms, and probability calibration. Its high accuracy and interpretability make it a valuable tool for assisting radiologists in clinical practice."
    },
    {
      "paperId": "dc3263c0cffaa0500a94f6c75dd719d92e4141e1",
      "externalIds": {
        "ArXiv": "2512.05760",
        "CorpusId": 283672209
      },
      "corpusId": 283672209,
      "title": "Evolutionary System 2 Reasoning: An Empirical Proof",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.05760, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2257372380",
          "name": "Zeyuan Ma"
        },
        {
          "authorId": "2397349893",
          "name": "Wenqi Huang"
        },
        {
          "authorId": "2397474613",
          "name": "Guo-Huan Song"
        },
        {
          "authorId": "2152574829",
          "name": "Hongshu Guo"
        },
        {
          "authorId": "2351800796",
          "name": "Sijie Ma"
        },
        {
          "authorId": "2258331238",
          "name": "Zhiguang Cao"
        },
        {
          "authorId": null,
          "name": "Yue-Jiao Gong"
        }
      ],
      "abstract": "Machine intelligence marks the ultimate dream of making machines'intelligence comparable to human beings. While recent progress in Large Language Models (LLMs) show substantial specific skills for a wide array of downstream tasks, they more or less fall shorts in general intelligence. Following correlation between intelligence and system 2 reasoning (slow thinking), in this paper, we aim to answering a worthwhile research question: could machine intelligence such as LLMs be evolved to acquire reasoning ability (not specific skill) just like our human beings? To this end, we propose evolutionary reasoning optimization (ERO) framework which performs survival of the fittest over a population of LLMs to search for individual with strong reasoning ability. Given a reasoning task, ERO first initializes multiple LLMs as a population, after which an evolutionary strategy evolves the population to maximize quantified reasoning score of the best individual. Based on experiments on representative testsuites, we claim two surprising empirical discoveries: i) the latest LLMs such as GPT-5 still show limited system 2 reasoning ability; ii) with simple evolution-loop of ERO, a relatively weak model (Qwen-7B) could be enhanced to emerge powerful reasoning ability. Our project can be accessed at https://github.com/MetaEvo/ERO for reproduction needs."
    },
    {
      "paperId": "de8fd8e8378242f0ac7a54f565c12b2826890a3d",
      "externalIds": {
        "ArXiv": "2512.06204",
        "CorpusId": 283694115
      },
      "corpusId": 283694115,
      "title": "Quantifying Memory Use in Reinforcement Learning with Temporal Range",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.06204, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2397380153",
          "name": "Rodney Lafuente-Mercado"
        },
        {
          "authorId": "2303255708",
          "name": "Daniela Rus"
        },
        {
          "authorId": "67164720",
          "name": "T. Konstantin Rusch"
        }
      ],
      "abstract": "How much does a trained RL policy actually use its past observations? We propose \\emph{Temporal Range}, a model-agnostic metric that treats first-order sensitivities of multiple vector outputs across a temporal window to the input sequence as a temporal influence profile and summarizes it by the magnitude-weighted average lag. Temporal Range is computed via reverse-mode automatic differentiation from the Jacobian blocks $\\partial y_s/\\partial x_t\\in\\mathbb{R}^{c\\times d}$ averaged over final timesteps $s\\in\\{t+1,\\dots,T\\}$ and is well-characterized in the linear setting by a small set of natural axioms. Across diagnostic and control tasks (POPGym; flicker/occlusion; Copy-$k$) and architectures (MLPs, RNNs, SSMs), Temporal Range (i) remains small in fully observed control, (ii) scales with the task's ground-truth lag in Copy-$k$, and (iii) aligns with the minimum history window required for near-optimal return as confirmed by window ablations. We also report Temporal Range for a compact Long Expressive Memory (LEM) policy trained on the task, using it as a proxy readout of task-level memory. Our axiomatic treatment draws on recent work on range measures, specialized here to temporal lag and extended to vector-valued outputs in the RL setting. Temporal Range thus offers a practical per-sequence readout of memory dependence for comparing agents and environments and for selecting the shortest sufficient context."
    },
    {
      "paperId": "3ab4f71154028c50f546512448548b192a041269",
      "externalIds": {
        "DOI": "10.64898/2025.12.02.691895",
        "CorpusId": 283712729
      },
      "corpusId": 283712729,
      "title": "A dataset of differentiable biologically-derived single neuron models",
      "venue": "bioRxiv",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.64898/2025.12.02.691895?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.64898/2025.12.02.691895, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2397496645",
          "name": "Calvin Yeung"
        },
        {
          "authorId": "2325114979",
          "name": "Zhixin Lu"
        },
        {
          "authorId": "2397496472",
          "name": "Kris Ganjam"
        },
        {
          "authorId": "2327962582",
          "name": "Stefan Mihalas"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "a274e5a4ae76f906cafab69d705356b4cdf8c2ae",
      "externalIds": {
        "ArXiv": "2512.08979",
        "CorpusId": 283722234
      },
      "corpusId": 283722234,
      "title": "What Happens When: Learning Temporal Orders of Events in Videos",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.08979, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2131876149",
          "name": "Daechul Ahn"
        },
        {
          "authorId": "2282961855",
          "name": "Yura Choi"
        },
        {
          "authorId": "2392410229",
          "name": "Hyeonbeom Choi"
        },
        {
          "authorId": "2292035546",
          "name": "Seongwon Cho"
        },
        {
          "authorId": "2307030461",
          "name": "San Kim"
        },
        {
          "authorId": "2283130535",
          "name": "Jonghyun Choi"
        }
      ],
      "abstract": "Video Large Multimodal Models (VLMMs) have shown impressive performance in video understanding, yet their ability to accurately capture the temporal order of multiple events remains underexplored. We interestingly observe that, even when video frames are scrambled, models perform very well on the existing benchmarks by comprehensive experiments. This implies that VLMMs may not necessarily rely on accurate sequential processing of visual events, but instead depend on prior knowledge of typical scenarios to answer the question. To benchmark temporal understanding capabilities in VLMMs, we propose VECTOR, designed to explicitly assess a model's ability to identify the temporal order of events. On this benchmark, we observe that various VLMMs often fail to understand the orders of events. To address this, we propose MECOT (Multi-Event instruction fine-tuning with Chain-of-Thought), which (1) trains models on detailed, event-by-event video descriptions and (2) using chain-of-thought prompts at inference to enhance temporal awareness. MECOT outperforms prior arts on VECTOR as well as improving performance on existing video benchmarks, implying effectiveness of temporal understanding. We release our code, model and datasets."
    },
    {
      "paperId": "e3a91d28e97da22297e5b18be9709446dab62032",
      "externalIds": {
        "DOI": "10.1109/TIP.2025.3637729",
        "CorpusId": 283596248,
        "PubMed": "41348792"
      },
      "corpusId": 283596248,
      "title": "WMRNet: Wavelet Mamba With Reversible Structure for Infrared Small Target Detection",
      "venue": "IEEE Transactions on Image Processing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIP.2025.3637729?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIP.2025.3637729, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "3062588",
          "name": "Mingjin Zhang"
        },
        {
          "authorId": "2356679315",
          "name": "Xiaolong Li"
        },
        {
          "authorId": "2117062851",
          "name": "Jie Guo"
        },
        {
          "authorId": "2281831113",
          "name": "Yunsong Li"
        },
        {
          "authorId": "2281910078",
          "name": "Xinbo Gao"
        }
      ],
      "abstract": "Infrared small target detection (IRSTD) is of great practical significance in many real-world applications, such as maritime rescue and early warning systems, benefiting from the unique and excellent infrared imaging ability in adverse weather and low-light conditions. Nevertheless, segmenting small targets from the background remains a challenge. When the subsampling frequency during image processing does not satisfy the Nyquist criterion, the aliasing effect occurs, which makes it extremely difficult to identify small targets. To address this challenge, we propose a novel Wavelet Mamba with Reversible Structure Network (WMRNet) for infrared small target detection in this paper. Specifically, WMRNet consists of a Discrete Wavelet Mamba (DW-Mamba) module and a Third-order Difference Equation guided Reversible (TDE-Rev) structure. DW-Mamba employs the Discrete Wavelet Transform to decompose images into multiple subbands, integrating this information into the state equations of a state space model. This method minimizes frequency interference while preserving a global perspective, thereby effectively reducing background aliasing. The TDE-Rev aims to suppress edge aliasing effects by refining the target edges, which first processes features with an explicit neural structure derived from the second-order difference equations and then promotes feature interactions through a reversible structure. Extensive experiments on the public IRSTD-1k and SIRST datasets demonstrate that the proposed WMRNet outperforms the state-of-the-art methods."
    },
    {
      "paperId": "ff74b6b5b739cd2c453a614d9e222739f67b585a",
      "externalIds": {
        "ArXiv": "2512.15729",
        "CorpusId": 283934618
      },
      "corpusId": 283934618,
      "title": "TinyMyo: a Tiny Foundation Model for Flexible EMG Signal Processing at the Edge",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.15729, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2399163344",
          "name": "Matteo Fasulo"
        },
        {
          "authorId": "2322475080",
          "name": "Giusy Spacone"
        },
        {
          "authorId": "1734773394",
          "name": "T. Ingolfsson"
        },
        {
          "authorId": "2323432053",
          "name": "Yawei Li"
        },
        {
          "authorId": "2323368873",
          "name": "Luca Benini"
        },
        {
          "authorId": "12009833",
          "name": "A. Cossettini"
        }
      ],
      "abstract": "Surface electromyography (EMG) is a non-invasive sensing modality used in several domains, including biomechanics, rehabilitation, prosthetic control, and emerging human-machine interaction paradigms. Despite decades of use, significant challenges remain in achieving robust generalization across subjects, recording systems, and acquisition protocols. To tackle these challenges, foundation models (FMs) are gaining traction when targeting end-to-end applications based on EMG signals. Yet, existing EMG FMs remain limited to single downstream tasks and lack deployability on embedded platforms. In this work, we present TinyMyo, a lightweight FM based on a Transformer encoder architecture. The model is pre-trained in a self-supervised manner on publicly available datasets and achieves high reconstruction fidelity with only 3.6M parameters. With minimal task-specific head adaptations, the same backbone is used to tackle multiple downstream tasks, leveraging datasets acquired from diverse sensing locations and hardware platforms. We demonstrate generalization across hand gesture classification, hand kinematic regression, speech production and recognition, with performance comparable to or surpassing the state of the art (SoA), and model size below 5M parameters. We achieve SoA results compared to previous FM-based works on the NinaPro DB5 ($89.4\\pm0.16\\%$), UCI-EMG ($97.56\\pm0.32\\%$), and EPN-612 ($96.74\\pm0.09\\%$) datasets. We report, to the best of our knowledge, the first deployment of an EMG FM on an ultra-low-power microcontroller (GAP9), achieving an average power envelope of 36.45mW. By open-sourcing the pre-trained and the downstream task architectures (https://github.com/pulp-bio/BioFoundation), we aim to provide a flexible resource that can accelerate future research and serve as a common foundation for the EMG community."
    },
    {
      "paperId": "1b19726ccc2c9b90746f1b4624d2b171ccbb4095",
      "externalIds": {
        "DOI": "10.1007/s00530-025-02073-0",
        "CorpusId": 283479161
      },
      "corpusId": 283479161,
      "title": "Mamba-transformer for low-light image enhancement in HVI color space",
      "venue": "Multimedia Systems",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s00530-025-02073-0?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s00530-025-02073-0, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2395975504",
          "name": "Zepu Xu"
        },
        {
          "authorId": "2396693260",
          "name": "Shijie Hao"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "6afd1cc476a1a6be296291167ade2a18aff4cec3",
      "externalIds": {
        "DOI": "10.1007/s00530-025-02090-z",
        "CorpusId": 283479085
      },
      "corpusId": 283479085,
      "title": "A Multi-scale Cascaded Channel Attention Visual Mamba UNet model for medical image segmentation",
      "venue": "Multimedia Systems",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s00530-025-02090-z?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s00530-025-02090-z, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2395959884",
          "name": "Wu Shili"
        },
        {
          "authorId": "2395959174",
          "name": "Qiao Yalu"
        },
        {
          "authorId": "2395960952",
          "name": "Qian Chao"
        },
        {
          "authorId": "2396696438",
          "name": "Xinyou Zhang"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "8db1fbdb29d36f1205f46f753ab01101191a9b70",
      "externalIds": {
        "DOI": "10.1007/s00530-025-02080-1",
        "CorpusId": 283478554
      },
      "corpusId": 283478554,
      "title": "Mcdfuse: visible and infrared image fusion via Mamba-CNN for enhanced UAV perception",
      "venue": "Multimedia Systems",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s00530-025-02080-1?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s00530-025-02080-1, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2374058832",
          "name": "Yu Zhang"
        },
        {
          "authorId": "2395942760",
          "name": "Hao Cha"
        },
        {
          "authorId": "2396708952",
          "name": "HanRui Zhang"
        },
        {
          "authorId": "2395964412",
          "name": "XiongHui Li"
        },
        {
          "authorId": "2397621124",
          "name": "Zhuo Chen"
        },
        {
          "authorId": "2396441584",
          "name": "JiaMing Bai"
        },
        {
          "authorId": "2395954776",
          "name": "TingTing Fu"
        },
        {
          "authorId": "2395987019",
          "name": "Yong Cheng Du"
        },
        {
          "authorId": "2396298859",
          "name": "Xian Zhang"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "23d29f9fac5ea79fdfe04b79cd18c7cd8cc286bf",
      "externalIds": {
        "DOI": "10.1007/s00530-025-02068-x",
        "CorpusId": 283479113
      },
      "corpusId": 283479113,
      "title": "3D semantic segmentation for railway scenes via heterogeneous multimodal alignment and distillation",
      "venue": "Multimedia Systems",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s00530-025-02068-x?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s00530-025-02068-x, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2213970242",
          "name": "Ning Sun"
        },
        {
          "authorId": "2395976639",
          "name": "Yuchen Sun"
        },
        {
          "authorId": "2395967961",
          "name": "Maomao Sun"
        },
        {
          "authorId": "49722633",
          "name": "Jixin Liu"
        },
        {
          "authorId": "2307301607",
          "name": "Lei Chai"
        },
        {
          "authorId": "2135585366",
          "name": "Cong Wu"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "e08ce1a802701fe77dfc7838b916e67f558d0512",
      "externalIds": {
        "ArXiv": "2512.04690",
        "CorpusId": 283556731
      },
      "corpusId": 283556731,
      "title": "Recurrent Neural Networks with Linear Structures for Electricity Price Forecasting",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.04690, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "122325551",
          "name": "Souhir Ben Amor"
        },
        {
          "authorId": "40030503",
          "name": "F. Ziel"
        }
      ],
      "abstract": "We present a novel recurrent neural network architecture designed explicitly for day-ahead electricity price forecasting, aimed at improving short-term decision-making and operational management in energy systems. Our combined forecasting model embeds linear structures, such as expert models and Kalman filters, into recurrent networks, enabling efficient computation and enhanced interpretability. The design leverages the strengths of both linear and non-linear model structures, allowing it to capture all relevant stylised price characteristics in power markets, including calendar and autoregressive effects, as well as influences from load, renewable energy, and related fuel and carbon markets. For empirical testing, we use hourly data from the largest European electricity market spanning 2018 to 2025 in a comprehensive forecasting study, comparing our model against state-of-the-art approaches, particularly high-dimensional linear and neural network models. The proposed model achieves approximately 12% higher accuracy than leading benchmarks. We evaluate the contributions of the interpretable model components and conclude on the impact of combining linear and non-linear structures."
    },
    {
      "paperId": "71331a85d06c24f6924b47e9c5f6e2629fddbf35",
      "externalIds": {
        "DOI": "10.1007/s00371-025-04211-1",
        "CorpusId": 283462163
      },
      "corpusId": 283462163,
      "title": "MSMamba: enhancing medical image segmentation with a multi-scanning Mamba hybrid network",
      "venue": "The Visual Computer",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s00371-025-04211-1?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s00371-025-04211-1, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2395819730",
          "name": "Ruoyun Liu"
        },
        {
          "authorId": "2395815103",
          "name": "Jianshu Chao"
        },
        {
          "authorId": "2397563506",
          "name": "Jiahua Lai"
        },
        {
          "authorId": null,
          "name": "Qingwei Guo"
        },
        {
          "authorId": "2396664827",
          "name": "Ke Sun"
        },
        {
          "authorId": "2396072160",
          "name": "Zeyu Zhang"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "102e5c5a04305477a11245e7fa2da139aa52a207",
      "externalIds": {
        "ArXiv": "2512.03563",
        "CorpusId": 283466897
      },
      "corpusId": 283466897,
      "title": "State Space Models for Bioacoustics: A comparative Evaluation with Transformers",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.03563, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2316124879",
          "name": "Chengyu Tang"
        },
        {
          "authorId": "2292312652",
          "name": "Sanjeev Baskiyar"
        }
      ],
      "abstract": "In this study, we evaluate the efficacy of the Mamba model in the field of bioacoustics. We first pretrain a Mamba-based audio large language model (LLM) on a large corpus of audio data using self-supervised learning. We fine-tune and evaluate BioMamba on the BEANS benchmark, a collection of diverse bioacoustic tasks including classification and detection, and compare its performance and efficiency with multiple baseline models, including AVES, a state-of-the-art Transformer-based model. The results show that BioMamba achieves comparable performance with AVES while consumption significantly less VRAM, demonstrating its potential in this domain."
    },
    {
      "paperId": "67293b22341392376638589ef14a5d3a040ab5b3",
      "externalIds": {
        "DOI": "10.1007/s41060-025-00891-z",
        "CorpusId": 283461289
      },
      "corpusId": 283461289,
      "title": "MAWT: a market-contextual adaptive wavelet transformer for stock forecasting",
      "venue": "International Journal of Data Science and Analysis",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s41060-025-00891-z?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s41060-025-00891-z, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2350453104",
          "name": "Hao Guo"
        },
        {
          "authorId": "50880705",
          "name": "Yuefeng Cen"
        },
        {
          "authorId": "2187638384",
          "name": "Gang Cen"
        },
        {
          "authorId": "2338788077",
          "name": "Cheng Zhao"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "a1cf29c6d23817bdcafced189f2ebebded8be22b",
      "externalIds": {
        "ArXiv": "2512.03852",
        "CorpusId": 283466766
      },
      "corpusId": 283466766,
      "title": "Traffic Image Restoration under Adverse Weather via Frequency-Aware Mamba",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.03852, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2381018233",
          "name": "Liwen Pan"
        },
        {
          "authorId": "2397285909",
          "name": "Longguang Wang"
        },
        {
          "authorId": "2398827329",
          "name": "Guangwei Gao"
        },
        {
          "authorId": "2297418599",
          "name": "Jun Wang"
        },
        {
          "authorId": "2319820784",
          "name": "Jun Shi"
        },
        {
          "authorId": "2271464566",
          "name": "Juncheng Li"
        }
      ],
      "abstract": "Traffic image restoration under adverse weather conditions remains a critical challenge for intelligent transportation systems. Existing methods primarily focus on spatial-domain modeling but neglect frequency-domain priors. Although the emerging Mamba architecture excels at long-range dependency modeling through patch-wise correlation analysis, its potential for frequency-domain feature extraction remains unexplored. To address this, we propose Frequency-Aware Mamba (FAMamba), a novel framework that integrates frequency guidance with sequence modeling for efficient image restoration. Our architecture consists of two key components: (1) a Dual-Branch Feature Extraction Block (DFEB) that enhances local-global interaction via bidirectional 2D frequency-adaptive scanning, dynamically adjusting traversal paths based on sub-band texture distributions; and (2) a Prior-Guided Block (PGB) that refines texture details through wavelet-based high-frequency residual learning, enabling high-quality image reconstruction with precise details. Meanwhile, we design a novel Adaptive Frequency Scanning Mechanism (AFSM) for the Mamba architecture, which enables the Mamba to achieve frequency-domain scanning across distinct subgraphs, thereby fully leveraging the texture distribution characteristics inherent in subgraph structures. Extensive experiments demonstrate the efficiency and effectiveness of FAMamba."
    },
    {
      "paperId": "f69341e1d2d35ff91af555c83634e887c9250728",
      "externalIds": {
        "ArXiv": "2512.03350",
        "CorpusId": 283467131
      },
      "corpusId": 283467131,
      "title": "SeeU: Seeing the Unseen World via 4D Dynamics-aware Generation",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.03350, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2333755176",
          "name": "Yu Yuan"
        },
        {
          "authorId": "2164171797",
          "name": "Tharindu Wickremasinghe"
        },
        {
          "authorId": "28271623",
          "name": "Zeeshan Nadir"
        },
        {
          "authorId": "2333519848",
          "name": "Xijun Wang"
        },
        {
          "authorId": "51304180",
          "name": "Yiheng Chi"
        },
        {
          "authorId": "2333963506",
          "name": "Stanley Chan"
        }
      ],
      "abstract": "Images and videos are discrete 2D projections of the 4D world (3D space + time). Most visual understanding, prediction, and generation operate directly on 2D observations, leading to suboptimal performance. We propose SeeU, a novel approach that learns the continuous 4D dynamics and generate the unseen visual contents. The principle behind SeeU is a new 2D$\\to$4D$\\to$2D learning framework. SeeU first reconstructs the 4D world from sparse and monocular 2D frames (2D$\\to$4D). It then learns the continuous 4D dynamics on a low-rank representation and physical constraints (discrete 4D$\\to$continuous 4D). Finally, SeeU rolls the world forward in time, re-projects it back to 2D at sampled times and viewpoints, and generates unseen regions based on spatial-temporal context awareness (4D$\\to$2D). By modeling dynamics in 4D, SeeU achieves continuous and physically-consistent novel visual generation, demonstrating strong potentials in multiple tasks including unseen temporal generation, unseen spatial generation, and video editing."
    },
    {
      "paperId": "036a1cb97af3b2c953f6bcf477de8a9f5770c096",
      "externalIds": {
        "ArXiv": "2512.03796",
        "CorpusId": 283467219
      },
      "corpusId": 283467219,
      "title": "LSRS: Latent Scale Rejection Sampling for Visual Autoregressive Modeling",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.03796, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2386067739",
          "name": "Hong-Kai Zheng"
        },
        {
          "authorId": "2386328180",
          "name": "Piji Li"
        }
      ],
      "abstract": "Visual Autoregressive (VAR) modeling approach for image generation proposes autoregressive processing across hierarchical scales, decoding multiple tokens per scale in parallel. This method achieves high-quality generation while accelerating synthesis. However, parallel token sampling within a scale may lead to structural errors, resulting in suboptimal generated images. To mitigate this, we propose Latent Scale Rejection Sampling (LSRS), a method that progressively refines token maps in the latent scale during inference to enhance VAR models. Our method uses a lightweight scoring model to evaluate multiple candidate token maps sampled at each scale, selecting the high-quality map to guide subsequent scale generation. By prioritizing early scales critical for structural coherence, LSRS effectively mitigates autoregressive error accumulation while maintaining computational efficiency. Experiments demonstrate that LSRS significantly improves VAR's generation quality with minimal additional computational overhead. For the VAR-d30 model, LSRS increases the inference time by merely 1% while reducing its FID score from 1.95 to 1.78. When the inference time is increased by 15%, the FID score can be further reduced to 1.66. LSRS offers an efficient test-time scaling solution for enhancing VAR-based generation."
    },
    {
      "paperId": "9714242fec0b1d7216ffee01d2c325d5b1f4e14b",
      "externalIds": {
        "ArXiv": "2512.03424",
        "CorpusId": 283467085
      },
      "corpusId": 283467085,
      "title": "DM3D: Deformable Mamba via Offset-Guided Gaussian Sequencing for Point Cloud Understanding",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.03424, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2362566400",
          "name": "Bin Liu"
        },
        {
          "authorId": "2117977387",
          "name": "Chunyang Wang"
        },
        {
          "authorId": "46521904",
          "name": "Xuelian Liu"
        }
      ],
      "abstract": "State Space Models (SSMs) demonstrate significant potential for long-sequence modeling, but their reliance on input order conflicts with the irregular nature of point clouds. Existing approaches often rely on predefined serialization strategies, which cannot adjust based on diverse geometric structures. To overcome this limitation, we propose \\textbf{DM3D}, a deformable Mamba architecture for point cloud understanding. Specifically, DM3D introduces an offset-guided Gaussian sequencing mechanism that unifies local resampling and global reordering within a deformable scan. The Gaussian-based KNN Resampling (GKR) enhances structural awareness by adaptively reorganizing neighboring points, while the Gaussian-based Differentiable Reordering (GDR) enables end-to-end optimization of serialization order. Furthermore, a Tri-Path Frequency Fusion module enhances feature complementarity and reduces aliasing. Together, these components enable structure-adaptive serialization of point clouds. Extensive experiments on benchmark datasets show that DM3D achieves state-of-the-art performance in classification, few-shot learning, and part segmentation, demonstrating that adaptive serialization effectively unlocks the potential of SSMs for point cloud understanding. The code will be released at https://github.com/L1277471578/DM3D."
    },
    {
      "paperId": "fa985ea4c4f3565a57eacbd2cea1c41c20d5c67f",
      "externalIds": {
        "ArXiv": "2512.03637",
        "CorpusId": 283466586
      },
      "corpusId": 283466586,
      "title": "AaPE: Aliasing-aware Patch Embedding for Self-Supervised Audio Representation Learning",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.03637, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2395991005",
          "name": "Kohei Yamamoto"
        },
        {
          "authorId": "2395894251",
          "name": "Kosuke Okusa"
        }
      ],
      "abstract": "Transformer-based audio SSL (self-supervised learning) models often treat spectrograms as images, applying convolutional patchification with heavy temporal downsampling. This lowers the effective Nyquist frequency and introduces aliasing, while na\\\"ive low-pass filtering removes task-relevant high-frequency cues. In this study, we present Aliasing-aware Patch Embedding (AaPE), a drop-in patch stem that mitigates aliasing while preserving high-frequency information. AaPE augments standard patch tokens with features produced by a band-limited complex sinusoidal kernel using a two-sided exponential window that dynamically targets alias-prone bands. Frequency and decay parameters of the kernel are estimated from the input, enabling parallel, adaptive subband analysis whose outputs are fused with the standard patch tokens. AaPE integrates seamlessly into the masked teacher-student self-supervised learning. In addition, we combine a multi-mask strategy with a contrastive objective to enforce consistency across diverse mask patterns, stabilizing training. Pre-training on AudioSet followed by fine-tuning evaluation across diverse downstream benchmarks, which spanned categories, such as environmental sounds and other common audio domains. This approach yields state-of-the-art performance on a subset of tasks and competitive results across the remainder. Complementary linear probing evaluation mirrors this pattern, yielding clear gains on several benchmarks and strong performance elsewhere. The collective analysis of these results indicates that AaPE serves to mitigate the effects of aliasing without discarding of informative high-frequency content."
    },
    {
      "paperId": "3812edc6db687581e552bcc85e0fa9cfab3bb7d5",
      "externalIds": {
        "ArXiv": "2512.03704",
        "CorpusId": 283466344
      },
      "corpusId": 283466344,
      "title": "DZ-TDPO: Non-Destructive Temporal Alignment for Mutable State Tracking in Long-Context Dialogue",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.03704, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2396663372",
          "name": "Yijun Liao"
        }
      ],
      "abstract": "Long-context dialogue systems suffer from State Inertia, where static constraints prevent models from resolving conflicts between evolving user intents and established historical context. To address this, we propose DZ-TDPO, a non-destructive alignment framework that synergizes conflict-aware dynamic KL constraints with a calibrated temporal attention bias. Experiments on the Multi-Session Chat (MSC) dataset demonstrate that DZ-TDPO achieves state-of-the-art win rates (55.4% on Phi-3.5) while maintaining robust zero-shot generalization. Our scaling analysis reveals a\"Capacity-Stability Trade-off\": while smaller models incur an\"alignment tax\"(perplexity surge) to overcome historical inertia, the larger Qwen2.5-7B model achieves 50.8% win rate with negligible perplexity overhead. This confirms that TAI can be alleviated via precise attention regulation rather than destructive weight updates, preserving general capabilities (MMLU) across model scales. Code and data are available: https://github.com/lyj20071013/DZ-TDPO"
    },
    {
      "paperId": "6c1c3f2d5eb26d80fcd917cf23e2a78f2c77890e",
      "externalIds": {
        "DOI": "10.1145/3769102.3774633",
        "CorpusId": 283468672
      },
      "corpusId": 283468672,
      "title": "TinyBEV: Compact Temporal Fusion for Multi-View 3D Perception",
      "venue": "Proceedings of the Tenth ACM/IEEE Symposium on Edge Computing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3769102.3774633?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3769102.3774633, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2201312489",
          "name": "Hongyu Ke"
        },
        {
          "authorId": "2350989704",
          "name": "Jack Morris"
        },
        {
          "authorId": "2261677637",
          "name": "Yongkang Liu"
        },
        {
          "authorId": "2395906511",
          "name": "Satoshi Kitai"
        },
        {
          "authorId": "2076742",
          "name": "K. Oguchi"
        },
        {
          "authorId": "2332533873",
          "name": "Yi Ding"
        },
        {
          "authorId": "2261386394",
          "name": "Haoxin Wang"
        }
      ],
      "abstract": "Multi-view camera-based 3D object detection through unified Bird's Eye View (BEV) representation has become popular for autonomous driving due to its low cost, but efficiently inferring precise spatial and temporal information from cameras alone remains a significant challenge. Transformer-based approaches have shown substantial performance improvements but have the drawback of quadratic memory complexity \u2014 making these architectures ill-suited for edge deployment. Recently, State Space Models (SSMs) offer a more favorable balance of computational efficiency and performance in 2D vision, suggesting that they could help here as well. We present TinyBEV, an efficient BEV framework for multi-view 3D perception. For spatial modeling, we replace cross attention with SSMs that fusing BEV and camera images with linear complexity. For temporal modeling, we adopt a lightweight, linear-complexity history-fusion scheme that uses explicit time conditioning and channel-level aggregation instead of cross-frame attention. Both fusion strategies follow small constant scaling with respect to history length and enabling edge-friendly deployment. Experiments on NuScenes datasets demonstrate that TinyBEV is comparable with other state-of-the-art methods across diverse visual perception metrics with advantages in computational efficiency."
    },
    {
      "paperId": "4a4617355a258f0c6e3ae9fe94cec3ce8faeb927",
      "externalIds": {
        "ArXiv": "2512.02727",
        "CorpusId": 283457882
      },
      "corpusId": 283457882,
      "title": "DF-Mamba: Deformable State Space Modeling for 3D Hand Pose Estimation in Interactions",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.02727, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2395811913",
          "name": "Yifan Zhou"
        },
        {
          "authorId": "1515548135",
          "name": "Takehiko Ohkawa"
        },
        {
          "authorId": null,
          "name": "Guwenxiao Zhou"
        },
        {
          "authorId": "2336952485",
          "name": "Kanoko Goto"
        },
        {
          "authorId": "2313430818",
          "name": "Takumi Hirose"
        },
        {
          "authorId": "2323286046",
          "name": "Yusuke Sekikawa"
        },
        {
          "authorId": "2262445410",
          "name": "Nakamasa Inoue"
        }
      ],
      "abstract": "Modeling daily hand interactions often struggles with severe occlusions, such as when two hands overlap, which highlights the need for robust feature learning in 3D hand pose estimation (HPE). To handle such occluded hand images, it is vital to effectively learn the relationship between local image features (e.g., for occluded joints) and global context (e.g., cues from inter-joints, inter-hands, or the scene). However, most current 3D HPE methods still rely on ResNet for feature extraction, and such CNN's inductive bias may not be optimal for 3D HPE due to its limited capability to model the global context. To address this limitation, we propose an effective and efficient framework for visual feature extraction in 3D HPE using recent state space modeling (i.e., Mamba), dubbed Deformable Mamba (DF-Mamba). DF-Mamba is designed to capture global context cues beyond standard convolution through Mamba's selective state modeling and the proposed deformable state scanning. Specifically, for local features after convolution, our deformable scanning aggregates these features within an image while selectively preserving useful cues that represent the global context. This approach significantly improves the accuracy of structured 3D HPE, with comparable inference speed to ResNet-50. Our experiments involve extensive evaluations on five divergent datasets including single-hand and two-hand scenarios, hand-only and hand-object interactions, as well as RGB and depth-based estimation. DF-Mamba outperforms the latest image backbones, including VMamba and Spatial-Mamba, on all datasets and achieves state-of-the-art performance."
    },
    {
      "paperId": "7e9e2dd7cea8ba7847f99d736e443e4afee22eed",
      "externalIds": {
        "ArXiv": "2512.02643",
        "CorpusId": 283458384
      },
      "corpusId": 283458384,
      "title": "Leveraging Large-Scale Pretrained Spatial-Spectral Priors for General Zero-Shot Pansharpening",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.02643, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2281159660",
          "name": "Yongchuan Cui"
        },
        {
          "authorId": "2353478058",
          "name": "Peng Liu"
        },
        {
          "authorId": "2354191774",
          "name": "Yi Zeng"
        }
      ],
      "abstract": "Existing deep learning methods for remote sensing image fusion often suffer from poor generalization when applied to unseen datasets due to the limited availability of real training data and the domain gap between different satellite sensors. To address this challenge, we explore the potential of foundation models by proposing a novel pretraining strategy that leverages large-scale simulated datasets to learn robust spatial-spectral priors. Specifically, our approach first constructs diverse simulated datasets by applying various degradation operations (blur, noise, downsampling) and augmentations (bands generation, channel shuffling, high-pass filtering, color jittering, etc.) to natural images from ImageNet and remote sensing images from SkyScript. We then pretrain fusion models on these simulated data to learn generalizable spatial-spectral representations. The pretrained models are subsequently evaluated on six datasets (WorldView-2/3/4, IKONOS, QuickBird, GaoFen-2) using zero-shot and one-shot paradigms, with both full- and freeze-tuning approaches for fine-tuning. Extensive experiments on different network architectures including convolutional neural networks, Transformer, and Mamba demonstrate that our pretraining strategy significantly improves generalization performance across different satellite sensors and imaging conditions for various fusion models. The pretrained models achieve superior results in zero-shot scenarios and show remarkable adaptation capability with minimal real data in one-shot settings. Our work provides a practical solution for cross-domain pansharpening, establishes a new benchmark for generalization in remote sensing image fusion tasks, and paves the way for leveraging foundation models through advanced training strategies."
    },
    {
      "paperId": "20a785de745a47292f43568a51b3cbacc1ac019e",
      "externalIds": {
        "ArXiv": "2512.03020",
        "CorpusId": 283458185
      },
      "corpusId": 283458185,
      "title": "Unrolled Networks are Conditional Probability Flows in MRI Reconstruction",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.03020, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2372585729",
          "name": "Kehan Qi"
        },
        {
          "authorId": "2355334097",
          "name": "Saumya Gupta"
        },
        {
          "authorId": "2374369101",
          "name": "Qingqiao Hu"
        },
        {
          "authorId": "2106019843",
          "name": "Weimin Lyu"
        },
        {
          "authorId": "2327555452",
          "name": "Chao Chen"
        }
      ],
      "abstract": "Magnetic Resonance Imaging (MRI) offers excellent soft-tissue contrast without ionizing radiation, but its long acquisition time limits clinical utility. Recent methods accelerate MRI by under-sampling $k$-space and reconstructing the resulting images using deep learning. Unrolled networks have been widely used for the reconstruction task due to their efficiency, but suffer from unstable evolving caused by freely-learnable parameters in intermediate steps. In contrast, diffusion models based on stochastic differential equations offer theoretical stability in both medical and natural image tasks but are computationally expensive. In this work, we introduce flow ODEs to MRI reconstruction by theoretically proving that unrolled networks are discrete implementations of conditional probability flow ODEs. This connection provides explicit formulations for parameters and clarifies how intermediate states should evolve. Building on this insight, we propose Flow-Aligned Training (FLAT), which derives unrolled parameters from the ODE discretization and aligns intermediate reconstructions with the ideal ODE trajectory to improve stability and convergence. Experiments on three MRI datasets show that FLAT achieves high-quality reconstructions with up to $3\\times$ fewer iterations than diffusion-based generative models and significantly greater stability than unrolled networks."
    },
    {
      "paperId": "88a2ff205fdb97c45b2170ee0d05d671f22791aa",
      "externalIds": {
        "ArXiv": "2512.02557",
        "CorpusId": 283458068
      },
      "corpusId": 283458068,
      "title": "Deep Learning-Based Joint Uplink-Downlink CSI Acquisition for Next-Generation Upper Mid-Band Systems",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.02557, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2273360861",
          "name": "Xuan He"
        },
        {
          "authorId": "49032151",
          "name": "Hongwei Hou"
        },
        {
          "authorId": "2257099802",
          "name": "Yafei Wang"
        },
        {
          "authorId": "2257130827",
          "name": "Wenjin Wang"
        },
        {
          "authorId": "2362201211",
          "name": "Shi Jin"
        },
        {
          "authorId": "1760292",
          "name": "S. Chatzinotas"
        },
        {
          "authorId": "2295900707",
          "name": "Bj\u00f6rn E. Ottersten"
        }
      ],
      "abstract": "In next-generation wireless communication systems, the newly designated upper mid-band has attracted considerable attention, also called frequency range 3 (FR3), highlighting the need for downlink (DL) transmission design, which fundamentally relies on accurate CSI. However, CSI acquisition in FR3 systems faces significant challenges: the increased number of antennas and wider transmission bandwidth introduces prohibitive training overhead with traditional estimation approaches, as each probing captures only incomplete spatial-frequency observation, while higher carrier frequencies lead to faster temporal channel variation. To address these challenges, we propose a novel CSI acquisition framework that integrates CSI feedback, uplink (UL) and DL channel estimation, as well as channel prediction in the FR3 TDD massive MIMO systems. Specifically, we first develop the Joint UL and DL Channel Estimation Network (JUDCEN) to fuse incomplete observations based on the SRSs and CSI-RSs. By exploiting the complementary characteristics of preliminary UL and DL estimation features, obtained through initial UL estimation and quantized-feedback-assisted DL estimation, it enables full CSI reconstruction in the spatial domain. To mitigate the performance degradation in the feedback process, we propose the Transformer-MLP CSI Feedback Network (TMCFN), employing an MLP-based module to jointly exploit angle- and delay-domain features. Building upon the reconstructed full CSI, we further develop the Mamba-based Channel Prediction Network (MCPN), which exploits selective state-space model (SSM) mechanism to capture long-range temporal dynamics in the angle-delay domain for future CSI prediction. Simulation results demonstrate that the proposed framework consistently outperforms benchmarks in both CSI acquisition accuracy and transmission spectral efficiency with lower computational complexity."
    },
    {
      "paperId": "d3486f916e7a7b3694d7a617e4e9493f2e6ea644",
      "externalIds": {
        "ArXiv": "2512.02368",
        "CorpusId": 283458624
      },
      "corpusId": 283458624,
      "title": "Multi-Domain Enhanced Map-Free Trajectory Prediction with Selective Attention",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.02368, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "49602443",
          "name": "Wenyi Xiong"
        },
        {
          "authorId": "2352488330",
          "name": "Jian Chen"
        }
      ],
      "abstract": "Trajectory prediction is crucial for the reliability and safety of autonomous driving systems, yet it remains a challenging task in complex interactive scenarios. Existing methods often struggle to efficiently extract valuable scene information from redundant data, thereby reducing computational efficiency and prediction accuracy, especially when dealing with intricate agent interactions. To address these challenges, we propose a novel map-free trajectory prediction algorithm that achieves trajectory prediction across the temporal, spatial, and frequency domains. Specifically, in temporal information processing, We utilize a Mixture of Experts (MoE) mechanism to adaptively select critical frequency components. Concurrently, we extract these components and integrate multi-scale temporal features. Subsequently, a selective attention module is proposed to filter out redundant information in both temporal sequences and spatial interactions. Finally, we design a multimodal decoder. Under the supervision of patch-level and point-level losses, we obtain reasonable trajectory results. Experiments on Nuscences datasets demonstrate the superiority of our algorithm, validating its effectiveness in handling complex interactive scenarios."
    },
    {
      "paperId": "8d16b99357150e2cbfa4d73d50e2a37c946d00c6",
      "externalIds": {
        "ArXiv": "2512.02473",
        "CorpusId": 283458687
      },
      "corpusId": 283458687,
      "title": "WorldPack: Compressed Memory Improves Spatial Consistency in Video World Modeling",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.02473, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2290911034",
          "name": "Yuta Oshima"
        },
        {
          "authorId": "1715282",
          "name": "Yusuke Iwasawa"
        },
        {
          "authorId": "2327006455",
          "name": "Masahiro Suzuki"
        },
        {
          "authorId": "2320464508",
          "name": "Yutaka Matsuo"
        },
        {
          "authorId": "2052903664",
          "name": "Hiroki Furuta"
        }
      ],
      "abstract": "Video world models have attracted significant attention for their ability to produce high-fidelity future visual observations conditioned on past observations and navigation actions. Temporally- and spatially-consistent, long-term world modeling has been a long-standing problem, unresolved with even recent state-of-the-art models, due to the prohibitively expensive computational costs for long-context inputs. In this paper, we propose WorldPack, a video world model with efficient compressed memory, which significantly improves spatial consistency, fidelity, and quality in long-term generation despite much shorter context length. Our compressed memory consists of trajectory packing and memory retrieval; trajectory packing realizes high context efficiency, and memory retrieval maintains the consistency in rollouts and helps long-term generations that require spatial reasoning. Our performance is evaluated with LoopNav, a benchmark on Minecraft, specialized for the evaluation of long-term consistency, and we verify that WorldPack notably outperforms strong state-of-the-art models."
    },
    {
      "paperId": "a46cf6f4a20c0c6e1beefb2d397d5f422d1281ab",
      "externalIds": {
        "ArXiv": "2512.03040",
        "CorpusId": 283458212
      },
      "corpusId": 283458212,
      "title": "Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.03040, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2303589653",
          "name": "Zeqi Xiao"
        },
        {
          "authorId": "2376521660",
          "name": "Yiwei Zhao"
        },
        {
          "authorId": "2378839316",
          "name": "Lingxiao Li"
        },
        {
          "authorId": "2055985023",
          "name": "Yushi Lan"
        },
        {
          "authorId": "2397762569",
          "name": "Ning Yu"
        },
        {
          "authorId": "2386031805",
          "name": "Rahul Garg"
        },
        {
          "authorId": "2395768968",
          "name": "Roshni Cooper"
        },
        {
          "authorId": "2519704",
          "name": "M. H. Taghavi"
        },
        {
          "authorId": "2273731091",
          "name": "Xingang Pan"
        }
      ],
      "abstract": "We investigate whether video generative models can exhibit visuospatial intelligence, a capability central to human cognition, using only visual data. To this end, we present Video4Spatial, a framework showing that video diffusion models conditioned solely on video-based scene context can perform complex spatial tasks. We validate on two tasks: scene navigation - following camera-pose instructions while remaining consistent with 3D geometry of the scene, and object grounding - which requires semantic localization, instruction following, and planning. Both tasks use video-only inputs, without auxiliary modalities such as depth or poses. With simple yet effective design choices in the framework and data curation, Video4Spatial demonstrates strong spatial understanding from video context: it plans navigation and grounds target objects end-to-end, follows camera-pose instructions while maintaining spatial consistency, and generalizes to long contexts and out-of-domain environments. Taken together, these results advance video generative models toward general visuospatial reasoning."
    },
    {
      "paperId": "0ff1faecad6ac97a9be491749c914d952aa89651",
      "externalIds": {
        "ArXiv": "2512.02924",
        "CorpusId": 283458488
      },
      "corpusId": 283458488,
      "title": "AutoNeural: Co-Designing Vision-Language Models for NPU Inference",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.02924, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2398754918",
          "name": "Wei Chen"
        },
        {
          "authorId": "2395821705",
          "name": "Liangmin Wu"
        },
        {
          "authorId": "2395990476",
          "name": "Yunhai Hu"
        },
        {
          "authorId": "2294674012",
          "name": "Zhiyuan Li"
        },
        {
          "authorId": "2395854408",
          "name": "Zhiyuan Cheng"
        },
        {
          "authorId": "2396694393",
          "name": "Yicheng Qian"
        },
        {
          "authorId": "2396498690",
          "name": "Lingyue Zhu"
        },
        {
          "authorId": "2396513857",
          "name": "Zhipeng Hu"
        },
        {
          "authorId": "2397525542",
          "name": "Luoyi Liang"
        },
        {
          "authorId": "2396430189",
          "name": "Qiang Tang"
        },
        {
          "authorId": "2395829308",
          "name": "Zhen Liu"
        },
        {
          "authorId": "2397683733",
          "name": "Han Yang"
        }
      ],
      "abstract": "While Neural Processing Units (NPUs) offer high theoretical efficiency for edge AI, state-of-the-art Vision--Language Models (VLMs) tailored for GPUs often falter on these substrates. We attribute this hardware-model mismatch to two primary factors: the quantization brittleness of Vision Transformers (ViTs) and the I/O-bound nature of autoregressive attention mechanisms, which fail to utilize the high arithmetic throughput of NPUs. To bridge this gap, we propose AutoNeural, an NPU-native VLM architecture co-designed for integer-only inference. We replace the standard ViT encoder with a MobileNetV5-style backbone utilizing depthwise separable convolutions, which ensures bounded activation distributions for stable INT4/8/16 quantization. Complementing this, our language backbone integrates State-Space Model (SSM) principles with Transformer layers, employing efficient gated convolutions to achieve linear-time complexity. This hybrid design eliminates the heavy memory I/O overhead of Key-Value caching during generation. Our approach delivers substantial efficiency gains, reducing quantization error of vision encoder by up to 7x and end-to-end latency by 14x compared to conventional baselines. The AutoNeural also delivers 3x decoding speed and 4x longer context window than the baseline. We validate these improvements via a real-world automotive case study on the Qualcomm SA8295P SoC, demonstrating real-time performance for cockpit applications. Our results highlight that rethinking model topology specifically for NPU constraints is a prerequisite for robust multi-modal edge intelligence."
    },
    {
      "paperId": "e7be0396248f5907744174e35c355a2a5793e43e",
      "externalIds": {
        "ArXiv": "2512.03014",
        "CorpusId": 283458499
      },
      "corpusId": 283458499,
      "title": "Instant Video Models: Universal Adapters for Stabilizing Image-Based Networks",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.03014, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "1644885631",
          "name": "Matthew Dutson"
        },
        {
          "authorId": "2229814951",
          "name": "Nathan Labiosa"
        },
        {
          "authorId": "2395807889",
          "name": "Yin Li"
        },
        {
          "authorId": "2305465015",
          "name": "Mohit Gupta"
        }
      ],
      "abstract": "When applied sequentially to video, frame-based networks often exhibit temporal inconsistency - for example, outputs that flicker between frames. This problem is amplified when the network inputs contain time-varying corruptions. In this work, we introduce a general approach for adapting frame-based models for stable and robust inference on video. We describe a class of stability adapters that can be inserted into virtually any architecture and a resource-efficient training process that can be performed with a frozen base network. We introduce a unified conceptual framework for describing temporal stability and corruption robustness, centered on a proposed accuracy-stability-robustness loss. By analyzing the theoretical properties of this loss, we identify the conditions where it produces well-behaved stabilizer training. Our experiments validate our approach on several vision tasks including denoising (NAFNet), image enhancement (HDRNet), monocular depth (Depth Anything v2), and semantic segmentation (DeepLabv3+). Our method improves temporal stability and robustness against a range of image corruptions (including compression artifacts, noise, and adverse weather), while preserving or improving the quality of predictions."
    },
    {
      "paperId": "405c3120a855a2bb85348de24056482fea4b5a47",
      "externalIds": {
        "ArXiv": "2512.03111",
        "CorpusId": 283466339
      },
      "corpusId": 283466339,
      "title": "PanFoMa: A Lightweight Foundation Model and Benchmark for Pan-Cancer",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.03111, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2296943844",
          "name": "Xiaoshui Huang"
        },
        {
          "authorId": "2378213639",
          "name": "Tianlin Zhu"
        },
        {
          "authorId": "2042697551",
          "name": "Yifan Zuo"
        },
        {
          "authorId": "2396673056",
          "name": "Xue Xia"
        },
        {
          "authorId": "2395981176",
          "name": "Zonghan Wu"
        },
        {
          "authorId": "8247062",
          "name": "Jiebin Yan"
        },
        {
          "authorId": "2395894473",
          "name": "Dingli Hua"
        },
        {
          "authorId": "2395976672",
          "name": "Zongyi Xu"
        },
        {
          "authorId": "2279251297",
          "name": "Yuming Fang"
        },
        {
          "authorId": "2396107231",
          "name": "Jian Zhang"
        }
      ],
      "abstract": "Single-cell RNA sequencing (scRNA-seq) is essential for decoding tumor heterogeneity. However, pan-cancer research still faces two key challenges: learning discriminative and efficient single-cell representations, and establishing a comprehensive evaluation benchmark. In this paper, we introduce PanFoMa, a lightweight hybrid neural network that combines the strengths of Transformers and state-space models to achieve a balance between performance and efficiency. PanFoMa consists of a front-end local-context encoder with shared self-attention layers to capture complex, order-independent gene interactions; and a back-end global sequential feature decoder that efficiently integrates global context using a linear-time state-space model. This modular design preserves the expressive power of Transformers while leveraging the scalability of Mamba to enable transcriptome modeling, effectively capturing both local and global regulatory signals. To enable robust evaluation, we also construct a large-scale pan-cancer single-cell benchmark, PanFoMaBench, containing over 3.5 million high-quality cells across 33 cancer subtypes, curated through a rigorous preprocessing pipeline. Experimental results show that PanFoMa outperforms state-of-the-art models on our pan-cancer benchmark (+4.0\\%) and across multiple public tasks, including cell type annotation (+7.4\\%), batch integration (+4.0\\%) and multi-omics integration (+3.1\\%). The code is available at https://github.com/Xiaoshui-Huang/PanFoMa."
    },
    {
      "paperId": "2fc3d6bbb12599ca6407a8e24312e9cc9eff8aa2",
      "externalIds": {
        "ArXiv": "2512.01848",
        "CorpusId": 283448883
      },
      "corpusId": 283448883,
      "title": "Beyond SFT: Reinforcement Learning for Safer Large Reasoning Models with Better Reasoning Ability",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.01848, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2051552791",
          "name": "Jinghan Jia"
        },
        {
          "authorId": "2284064162",
          "name": "Nathalie Baracaldo"
        },
        {
          "authorId": "2365352540",
          "name": "Sijia Liu"
        }
      ],
      "abstract": "Large reasoning models (LRMs) extend large language models by generating explicit chain-of-thought (CoT) reasoning, significantly improving mathematical and logical problem solving. However, this explicit reasoning process also introduces new safety risks, as unsafe behaviors often emerge within intermediate reasoning trajectories, even when final answers appear harmless. Existing safety alignment approaches primarily rely on supervised fine-tuning (SFT) over safety-oriented long CoT datasets. While intuitive, we find that SFT produces inconsistent safety improvements, degrades reasoning ability, and generalizes poorly across model families. These limitations suggest that purely supervised approaches are insufficient for robust safety alignment in LRMs. To address this, we investigate reinforcement learning (RL) as a complementary optimization framework for LRM safety training. Unlike SFT, RL directly optimizes model policies with reward feedback, enabling more adaptive and stable alignment. Extensive experiments across multiple model families and benchmarks show that RL achieves stronger and more consistent safety gains while maintaining reasoning competence. Further analysis of reflection dynamics and token-level entropy reveals that RL suppresses unsafe exploratory reasoning while preserving reflective depth, leading to safer and more reliable reasoning processes."
    },
    {
      "paperId": "c6eab0cdc596424f1d59f5c612763c2d3117d696",
      "externalIds": {
        "DOI": "10.1109/TCSVT.2025.3588269",
        "CorpusId": 280054384
      },
      "corpusId": 280054384,
      "title": "LFSSMam: Efficient Aggregation of Multi-Spatial-Angular-Modal Information Using Selective SSM for Light Field Semantic Segmentation",
      "venue": "IEEE transactions on circuits and systems for video technology (Print)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2025.3588269?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2025.3588269, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2281961762",
          "name": "Wenbin Yan"
        },
        {
          "authorId": "2144213539",
          "name": "Hua Chen"
        },
        {
          "authorId": "2367467527",
          "name": "Qingwei Wu"
        },
        {
          "authorId": "2273200065",
          "name": "Xiaogang Zhang"
        },
        {
          "authorId": "2204791283",
          "name": "Qiu Fang"
        },
        {
          "authorId": "2292617347",
          "name": "Shengjie Hu"
        },
        {
          "authorId": "2284767452",
          "name": "Yaonan Wang"
        }
      ],
      "abstract": "Efficiently aggregating 4D light field information to achieve accurate semantic segmentation has always faced challenges in capturing long range dependency information (CNN-based) and the memory limitations of quadratic computational complexity (Transformer-based). Recently, the Mamba architecture, which utilizes the state space model (SSM), has achieved high performance under linear complexity in various vision tasks. However, directly applying Mamba to 4D light field scanning will lead to an inherent loss of multi-spatial-angular information. To address the above challenges, we introduce LFSSMam, a novel Light Field Semantic Segmentation architecture based on the selective state space model (Mamba). Firstly, LFSSMam presents an innovative spatial-angular selective scanning mechanism to decouple and scan 4D multi-dimensional light field data. It separately captures the rich spatial context, complementary angular and structural information of light field 2D slices within the state space. In addition, we design an SSM-attention Cross-Fusion Enhance Module to perform preferential scanning and fusion across multi-spatial-angular-modal light field information, adaptively aggregating and enhancing the central view features. Comprehensive experiments on synthetic and real world datasets demonstrate that LFSSMam achieves leading edge SOTA (State-Of-The-Art) performance (with a 6.97% improvement to LF-based methods) while reducing memory and computational complexity. This work provides valuable guidance for the efficient modeling and application of multi-spatial-angular information in light field semantic segmentation. Our code is available at https://github.com/HNU-WQW/LFSSMam"
    },
    {
      "paperId": "6c86740e684f970c2f4efb6595c3654c04478a4b",
      "externalIds": {
        "DBLP": "journals/eaai/GeLZY25",
        "DOI": "10.1016/j.engappai.2025.112077",
        "CorpusId": 280968575
      },
      "corpusId": 280968575,
      "title": "Identification of surface subsidence risk in deep foundation pits using a Mamba fusion model",
      "venue": "Engineering applications of artificial intelligence",
      "year": 2025,
      "citationCount": 3,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.engappai.2025.112077?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.engappai.2025.112077, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2168455751",
          "name": "Chenhe Ge"
        },
        {
          "authorId": "2261969319",
          "name": "Pengfei Li"
        },
        {
          "authorId": "2083690",
          "name": "Mingju Zhang"
        },
        {
          "authorId": "2260921298",
          "name": "Meng Yang"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "ca68115d3245d6184ca2e3c1b23a766e037eeb1e",
      "externalIds": {
        "DBLP": "journals/eaai/TangLZXS25",
        "DOI": "10.1016/j.engappai.2025.112480",
        "CorpusId": 281860393
      },
      "corpusId": 281860393,
      "title": "A mamba-quantum attention transformer-convolutional network for automated pest and disease detection",
      "venue": "Engineering applications of artificial intelligence",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.engappai.2025.112480?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.engappai.2025.112480, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2384854455",
          "name": "Dong Tang"
        },
        {
          "authorId": "2384150925",
          "name": "Zhihuan Liu"
        },
        {
          "authorId": "2369284641",
          "name": "Yirui Zeng"
        },
        {
          "authorId": "2384777765",
          "name": "Zhao Xu"
        },
        {
          "authorId": "2385459551",
          "name": "Wendong Su"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "16b525dd1230521ec684add6e9d7d14546de7492",
      "externalIds": {
        "ArXiv": "2512.01208",
        "CorpusId": 283449641
      },
      "corpusId": 283449641,
      "title": "Pay Attention Later: From Vector Space Diffusion to Linearithmic Spectral Phase-Locking",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.01208, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2243394370",
          "name": "Alper Y\u0131ld\u0131r\u0131m"
        },
        {
          "authorId": "2395678915",
          "name": ".Ibrahim Yucedaug"
        }
      ],
      "abstract": "Standard Transformers suffer from a\"Semantic Alignment Tax\", a prohibitive optimization cost required to organize a chaotic initialization into a coherent geometric map via local gradient diffusion. We hypothesize that this reliance on diffusive learning creates\"Catastrophic Rigidity\", rendering models unable to adapt to novel concepts without destroying their pre-trained reasoning capabilities. To isolate this phenomenon, we introduce Iterative Semantic Map Refinement (ISMR), a diagnostic protocol revealing that alignment is a fixed geometric barrier that scaling cannot solve; a 20-layer model overcomes this barrier no faster than a 1-layer model. We introduce the Phase-Resonant Intelligent Spectral Model (PRISM). PRISM encodes semantic identity as resonant frequencies in the complex domain (C^d) and replaces quadratic self-attention with linearithmic O(N log N) Gated Harmonic Convolutions. We validate PRISM on the WMT14 translation task. While the Standard Transformer maintains a slight edge in general competence on static benchmarks (23.88 vs 21.40 BLEU), it fails the\"Plasticity-Stability\"stress test completely. When injected with novel concepts, the Transformer suffers Catastrophic Forgetting, degrading by -10.55 BLEU points while achieving only 60% acquisition. In contrast, PRISM demonstrates Lossless Plasticity, achieving 96% 5-shot acquisition with negligible degradation (-0.84 BLEU). These results suggest that harmonic representations effectively decouple memory from reasoning, offering a structural solution to the plasticity-stability dilemma in real-time knowledge adaptation."
    },
    {
      "paperId": "e492f7087765a2b035b861c18d30bd752ef3e075",
      "externalIds": {
        "ArXiv": "2512.01298",
        "CorpusId": 283449128
      },
      "corpusId": 283449128,
      "title": "TBT-Former: Learning Temporal Boundary Distributions for Action Localization",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.01298, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2395665542",
          "name": "Thisara Rathnayaka"
        },
        {
          "authorId": "1935355",
          "name": "Uthayasanker Thayasivam"
        }
      ],
      "abstract": "Temporal Action Localization (TAL) remains a fundamental challenge in video understanding, aiming to identify the start time, end time, and category of all action instances within untrimmed videos. While recent single-stage, anchor-free models like ActionFormer have set a high standard by leveraging Transformers for temporal reasoning, they often struggle with two persistent issues: the precise localization of actions with ambiguous or\"fuzzy\"temporal boundaries and the effective fusion of multi-scale contextual information. In this paper, we introduce the Temporal Boundary Transformer (TBT-Former), a new architecture that directly addresses these limitations. TBT-Former enhances the strong ActionFormer baseline with three core contributions: (1) a higher-capacity scaled Transformer backbone with an increased number of attention heads and an expanded Multi-Layer Perceptron (MLP) dimension for more powerful temporal feature extraction; (2) a cross-scale feature pyramid network (FPN) that integrates a top-down pathway with lateral connections, enabling richer fusion of high-level semantics and low-level temporal details; and (3) a novel boundary distribution regression head. Inspired by the principles of Generalized Focal Loss (GFL), this new head recasts the challenging task of boundary regression as a more flexible probability distribution learning problem, allowing the model to explicitly represent and reason about boundary uncertainty. Within the paradigm of Transformer-based architectures, TBT-Former advances the formidable benchmark set by its predecessors, establishing a new level of performance on the highly competitive THUMOS14 and EPIC-Kitchens 100 datasets, while remaining competitive on the large-scale ActivityNet-1.3. Our code is available at https://github.com/aaivu/In21-S7-CS4681-AML-Research-Projects/tree/main/projects/210536K-Multi-Modal-Learning_Video-Understanding"
    },
    {
      "paperId": "9d6c091893370b7b6d3bea5451bd5cdc7a030b0a",
      "externalIds": {
        "ArXiv": "2512.01643",
        "CorpusId": 283449220
      },
      "corpusId": 283449220,
      "title": "ViT$^3$: Unlocking Test-Time Training in Vision",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.01643, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2159087148",
          "name": "Dongchen Han"
        },
        {
          "authorId": "2395717483",
          "name": "Yining Li"
        },
        {
          "authorId": "2397508711",
          "name": "Tianyu Li"
        },
        {
          "authorId": "2397444880",
          "name": "Zixuan Cao"
        },
        {
          "authorId": "2303300696",
          "name": "Ziming Wang"
        },
        {
          "authorId": "2390399160",
          "name": "Jun Song"
        },
        {
          "authorId": "2395799876",
          "name": "Yu Cheng"
        },
        {
          "authorId": "2396692298",
          "name": "Bo Zheng"
        },
        {
          "authorId": "2395712808",
          "name": "Gao Huang"
        }
      ],
      "abstract": "Test-Time Training (TTT) has recently emerged as a promising direction for efficient sequence modeling. TTT reformulates attention operation as an online learning problem, constructing a compact inner model from key-value pairs at test time. This reformulation opens a rich and flexible design space while achieving linear computational complexity. However, crafting a powerful visual TTT design remains challenging: fundamental choices for the inner module and inner training lack comprehensive understanding and practical guidelines. To bridge this critical gap, in this paper, we present a systematic empirical study of TTT designs for visual sequence modeling. From a series of experiments and analyses, we distill six practical insights that establish design principles for effective visual TTT and illuminate paths for future improvement. These findings culminate in the Vision Test-Time Training (ViT$^3$) model, a pure TTT architecture that achieves linear complexity and parallelizable computation. We evaluate ViT$^3$ across diverse visual tasks, including image classification, image generation, object detection, and semantic segmentation. Results show that ViT$^3$ consistently matches or outperforms advanced linear-complexity models (e.g., Mamba and linear attention variants) and effectively narrows the gap to highly optimized vision Transformers. We hope this study and the ViT$^3$ baseline can facilitate future work on visual TTT models. Code is available at https://github.com/LeapLabTHU/ViTTT."
    },
    {
      "paperId": "d96577e7b871acb78b5f87bbf42fc7d768d7a518",
      "externalIds": {
        "ArXiv": "2512.01563",
        "CorpusId": 283450567
      },
      "corpusId": 283450567,
      "title": "MasHeNe: A Benchmark for Head and Neck CT Mass Segmentation using Window-Enhanced Mamba with Frequency-Domain Integration",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.01563, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2187142992",
          "name": "Thao Thi Phuong Dao"
        },
        {
          "authorId": "1733079800",
          "name": "Tan-Cong Nguyen"
        },
        {
          "authorId": "2395656800",
          "name": "Nguyen Chi Thanh"
        },
        {
          "authorId": "2395678476",
          "name": "Truong Hoang Viet"
        },
        {
          "authorId": "51128412",
          "name": "Trong-Le Do"
        },
        {
          "authorId": "35505989",
          "name": "Mai-Khiem Tran"
        },
        {
          "authorId": "2270482502",
          "name": "Minh-Khoi Pham"
        },
        {
          "authorId": "2269907651",
          "name": "Trung-Nghia Le"
        },
        {
          "authorId": "2292757462",
          "name": "Minh-Triet Tran"
        },
        {
          "authorId": "2396674363",
          "name": "Thanh Dinh Le"
        }
      ],
      "abstract": "Head and neck masses are space-occupying lesions that can compress the airway and esophagus and may affect nerves and blood vessels. Available public datasets primarily focus on malignant lesions and often overlook other space-occupying conditions in this region. To address this gap, we introduce MasHeNe, an initial dataset of 3,779 contrast-enhanced CT slices that includes both tumors and cysts with pixel-level annotations. We also establish a benchmark using standard segmentation baselines and report common metrics to enable fair comparison. In addition, we propose the Windowing-Enhanced Mamba with Frequency integration (WEMF) model. WEMF applies tri-window enhancement to enrich the input appearance before feature extraction. It further uses multi-frequency attention to fuse information across skip connections within a U-shaped Mamba backbone. On MasHeNe, WEMF attains the best performance among evaluated methods, with a Dice of 70.45%, IoU of 66.89%, NSD of 72.33%, and HD95 of 5.12 mm. This model indicates stable and strong results on this challenging task. MasHeNe provides a benchmark for head-and-neck mass segmentation beyond malignancy-only datasets. The observed error patterns also suggest that this task remains challenging and requires further research. Our dataset and code are available at https://github.com/drthaodao3101/MasHeNe.git."
    },
    {
      "paperId": "f4e7dfe1f67e33e086457e05b648adb910894127",
      "externalIds": {
        "ArXiv": "2512.01913",
        "CorpusId": 283449465
      },
      "corpusId": 283449465,
      "title": "Disentangling Progress in Medical Image Registration: Beyond Trend-Driven Architectures towards Domain-Specific Strategies",
      "venue": "",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.01913, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2310852367",
          "name": "Bailiang Jian"
        },
        {
          "authorId": "2048020438",
          "name": "Jiazhen Pan"
        },
        {
          "authorId": "2395667691",
          "name": "Rohit Jena"
        },
        {
          "authorId": "2287928476",
          "name": "Morteza Ghahremani"
        },
        {
          "authorId": "2343779617",
          "name": "H. Li"
        },
        {
          "authorId": "2266407006",
          "name": "Daniel Rueckert"
        },
        {
          "authorId": "2273086658",
          "name": "Christian Wachinger"
        },
        {
          "authorId": "2304395613",
          "name": "Benedikt Wiestler"
        }
      ],
      "abstract": "Medical image registration drives quantitative analysis across organs, modalities, and patient populations. Recent deep learning methods often combine low-level\"trend-driven\"computational blocks from computer vision, such as large-kernel CNNs, Transformers, and state-space models, with high-level registration-specific designs like motion pyramids, correlation layers, and iterative refinement. Yet, their relative contributions remain unclear and entangled. This raises a central question: should future advances in registration focus on importing generic architectural trends or on refining domain-specific design principles? Through a modular framework spanning brain, lung, cardiac, and abdominal registration, we systematically disentangle the influence of these two paradigms. Our evaluation reveals that low-level\"trend-driven\"computational blocks offer only marginal or inconsistent gains, while high-level registration-specific designs consistently deliver more accurate, smoother, and more robust deformations. These domain priors significantly elevate the performance of a standard U-Net baseline, far more than variants incorporating\"trend-driven\"blocks, achieving an average relative improvement of $\\sim3\\%$. All models and experiments are released within a transparent, modular benchmark that enables plug-and-play comparison for new architectures and registration tasks (https://github.com/BailiangJ/rethink-reg). This dynamic and extensible platform establishes a common ground for reproducible and fair evaluation, inviting the community to isolate genuine methodological contributions from domain priors. Our findings advocate a shift in research emphasis: from following architectural trends to embracing domain-specific design principles as the true drivers of progress in learning-based medical image registration."
    },
    {
      "paperId": "69cf4b9df2f77f83c5921236e0dfb2e344d78ab9",
      "externalIds": {
        "ArXiv": "2512.01324",
        "CorpusId": 283449314
      },
      "corpusId": 283449314,
      "title": "Panda: Self-distillation of Reusable Sensor-level Representations for High Energy Physics",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.01324, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2344549413",
          "name": "Sam Young"
        },
        {
          "authorId": "2343742647",
          "name": "Kazuhiro Terao"
        }
      ],
      "abstract": "Liquid argon time projection chambers (LArTPCs) provide dense, high-fidelity 3D measurements of particle interactions and underpin current and future neutrino and rare-event experiments. Physics reconstruction typically relies on complex detector-specific pipelines that use tens of hand-engineered pattern recognition algorithms or cascades of task-specific neural networks that require extensive, labeled simulation that requires a careful, time-consuming calibration process. We introduce \\textbf{Panda}, a model that learns reusable sensor-level representations directly from raw unlabeled LArTPC data. Panda couples a hierarchical sparse 3D encoder with a multi-view, prototype-based self-distillation objective. On a simulated dataset, Panda substantially improves label efficiency and reconstruction quality, beating the previous state-of-the-art semantic segmentation model with 1,000$\\times$ fewer labels. We also show that a single set-prediction head 1/20th the size of the backbone with no physical priors trained on frozen outputs from Panda can result in particle identification that is comparable with state-of-the-art (SOTA) reconstruction tools. Full fine-tuning further improves performance across all tasks."
    },
    {
      "paperId": "8b9c6a08317001ba8127bbe12e89977763bbd9c8",
      "externalIds": {
        "ArXiv": "2512.01589",
        "CorpusId": 283450695
      },
      "corpusId": 283450695,
      "title": "Toward Content-based Indexing and Retrieval of Head and Neck CT with Abscess Segmentation",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.01589, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2187142992",
          "name": "Thao Thi Phuong Dao"
        },
        {
          "authorId": "1733079800",
          "name": "Tan-Cong Nguyen"
        },
        {
          "authorId": "51128412",
          "name": "Trong-Le Do"
        },
        {
          "authorId": "2395678476",
          "name": "Truong Hoang Viet"
        },
        {
          "authorId": "2395656800",
          "name": "Nguyen Chi Thanh"
        },
        {
          "authorId": "2395676931",
          "name": "Huynh Nguyen Thuan"
        },
        {
          "authorId": "2398647003",
          "name": "Do Vo Cong Nguyen"
        },
        {
          "authorId": "2270482502",
          "name": "Minh-Khoi Pham"
        },
        {
          "authorId": "35505989",
          "name": "Mai-Khiem Tran"
        },
        {
          "authorId": "2196925680",
          "name": "Viet-Tham Huynh"
        },
        {
          "authorId": "2238535257",
          "name": "Trong-Thuan Nguyen"
        },
        {
          "authorId": "2269907651",
          "name": "Trung-Nghia Le"
        },
        {
          "authorId": "2291390762",
          "name": "V\u00f5 Th\u00e0nh To\u00e0n"
        },
        {
          "authorId": "2196405720",
          "name": "Tam V. Nguyen"
        },
        {
          "authorId": "2292757462",
          "name": "Minh-Triet Tran"
        },
        {
          "authorId": "2396674363",
          "name": "Thanh Dinh Le"
        }
      ],
      "abstract": "Abscesses in the head and neck represent an acute infectious process that can potentially lead to sepsis or mortality if not diagnosed and managed promptly. Accurate detection and delineation of these lesions on imaging are essential for diagnosis, treatment planning, and surgical intervention. In this study, we introduce AbscessHeNe, a curated and comprehensively annotated dataset comprising 4,926 contrast-enhanced CT slices with clinically confirmed head and neck abscesses. The dataset is designed to facilitate the development of robust semantic segmentation models that can accurately delineate abscess boundaries and evaluate deep neck space involvement, thereby supporting informed clinical decision-making. To establish performance baselines, we evaluate several state-of-the-art segmentation architectures, including CNN, Transformer, and Mamba-based models. The highest-performing model achieved a Dice Similarity Coefficient of 0.39, Intersection-over-Union of 0.27, and Normalized Surface Distance of 0.67, indicating the challenges of this task and the need for further research. Beyond segmentation, AbscessHeNe is structured for future applications in content-based multimedia indexing and case-based retrieval. Each CT scan is linked with pixel-level annotations and clinical metadata, providing a foundation for building intelligent retrieval systems and supporting knowledge-driven clinical workflows. The dataset will be made publicly available at https://github.com/drthaodao3101/AbscessHeNe.git."
    },
    {
      "paperId": "84cca392be8d7e3133516540bb4d8f29d6f24c17",
      "externalIds": {
        "ArXiv": "2512.01383",
        "CorpusId": 283450013
      },
      "corpusId": 283450013,
      "title": "PointNet4D: A Lightweight 4D Point Cloud Video Backbone for Online and Offline Perception in Robotic Applications",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.01383, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2362119788",
          "name": "Yunze Liu"
        },
        {
          "authorId": "2274073759",
          "name": "Zifan Wang"
        },
        {
          "authorId": null,
          "name": "Peiran Wu"
        },
        {
          "authorId": "2174812277",
          "name": "Jiayang Ao"
        }
      ],
      "abstract": "Understanding dynamic 4D environments-3D space evolving over time-is critical for robotic and interactive systems. These applications demand systems that can process streaming point cloud video in real-time, often under resource constraints, while also benefiting from past and present observations when available. However, current 4D backbone networks rely heavily on spatiotemporal convolutions and Transformers, which are often computationally intensive and poorly suited to real-time applications. We propose PointNet4D, a lightweight 4D backbone optimized for both online and offline settings. At its core is a Hybrid Mamba-Transformer temporal fusion block, which integrates the efficient state-space modeling of Mamba and the bidirectional modeling power of Transformers. This enables PointNet4D to handle variable-length online sequences efficiently across different deployment scenarios. To enhance temporal understanding, we introduce 4DMAP, a frame-wise masked auto-regressive pretraining strategy that captures motion cues across frames. Our extensive evaluations across 9 tasks on 7 datasets, demonstrating consistent improvements across diverse domains. We further demonstrate PointNet4D's utility by building two robotic application systems: 4D Diffusion Policy and 4D Imitation Learning, achieving substantial gains on the RoboTwin and HandoverSim benchmarks."
    },
    {
      "paperId": "e19c248960d84655bf4c7cb8ea93e04f739e293c",
      "externalIds": {
        "ArXiv": "2512.01626",
        "DOI": "10.1109/TASLPRO.2025.3633044",
        "CorpusId": 283118342
      },
      "corpusId": 283118342,
      "title": "Parallel Delayed Memory Units for Enhanced Temporal Modeling in Biomedical and Bioacoustic Signal Analysis",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.01626, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "153146294",
          "name": "Pengfei Sun"
        },
        {
          "authorId": "2348586282",
          "name": "Wenyu Jiang"
        },
        {
          "authorId": "2073192795",
          "name": "P. Devos"
        },
        {
          "authorId": "2268479919",
          "name": "Dick Botteldooren"
        }
      ],
      "abstract": "Advanced deep learning architectures, particularly recurrent neural networks (RNNs), have been widely applied in audio, bioacoustic, and biomedical signal analysis, especially in data-scarce environments. While gated RNNs remain effective, they can be relatively over-parameterised and less training-efficient in some regimes [Ravanelli et al. (2017)], [Sun et al. (2024)], while linear RNNs tend to fall short in capturing the complexity inherent in bio-signals. To address these challenges, we propose the Parallel Delayed Memory Unit (PDMU), a delay-gated state-space module for short-term temporal credit assignment targeting audio and bioacoustic signals, which enhances short-term temporal state interactions and memory efficiency via a gated delay-line mechanism. Unlike previous Delayed Memory Units (DMU) that embed temporal dynamics into the delay-line architecture, the PDMU further compresses temporal information into vector representations using Legendre Memory Units (LMU). This design serves as a form of causal attention, allowing the model to dynamically adjust its reliance on past states and improve real-time learning performance. Notably, in low-information scenarios, the gating mechanism behaves similarly to skip connections by bypassing state decay and preserving early representations, thereby facilitating long-term memory retention. The PDMU is modular, supporting parallel training and sequential inference, and can be easily integrated into existing linear RNN frameworks. Furthermore, we introduce bidirectional, efficient, and spiking variants of the architecture, each offering additional gains in performance or energy efficiency. Experimental results on diverse audio and biomedical benchmarks demonstrate that the PDMU significantly enhances both memory capacity and overall model performance."
    },
    {
      "paperId": "739b540bb2e2cef09aafc01fe27a0bd40bca5c10",
      "externalIds": {
        "DOI": "10.1109/JSEN.2025.3622633",
        "CorpusId": 282331765
      },
      "corpusId": 282331765,
      "title": "Scale Sensitivity Mamba Network for Object Detection in Remote Sensing Images",
      "venue": "IEEE Sensors Journal",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JSEN.2025.3622633?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JSEN.2025.3622633, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2330680776",
          "name": "Qiyu Rong"
        },
        {
          "authorId": "2238627286",
          "name": "Hongyuan Jing"
        },
        {
          "authorId": "2330862727",
          "name": "Mengmeng Zhang"
        }
      ],
      "abstract": "In recent years, with the emergence of convolutional neural networks, object detection technology has made rapid progress. However, object detection in remote sensing images remains a significant challenge due to the characteristics of remote sensing images, such as complex geographical environments, insufficient feature representation for small objects, and large variation in object scale. These characteristics can lead to a significant decline in detector performance. In this article, to address these issues, we propose an efficient detector called the scale sensitivity Mamba network (SSMNet). SSMNet includes a intersection over union (IoU)-based bounding box loss function: Area-IoU (AIoU), along with two novel Mamba-based modules: the remote sensing Mamba block (RSMB) and the sparse feature fusion module (SFFM). The AIoU is used to make the detector more sensitive to the scale of the objects and to accelerate convergence on small object datasets. The RSMB leverages the powerful modeling capabilities of Mamba while maintaining the advantage of linear computational complexity to perform global context modeling on shallow features of remote sensing images. Meanwhile, the SFFM fuses the shallow features from the RSMB with the deep semantic features at the detection head to enhance the feature representation of small objects. Extensive experiments on three challenging remote sensing datasets, unicorn small object dataset (USOD), DIOR, and DOTA, reached 91.1% mAP50, 62.0% mAP50:95, and 24.6% mAP50:95, respectively, with only 30.4 giga floating point operations per second (GFLOPs), which demonstrates the effectiveness of our method. The code will be available at: https://github.com/BeStrongCode/SSMNet"
    },
    {
      "paperId": "b67d123b2c158f5ebc806bc39280bb1d58d4b47b",
      "externalIds": {
        "DOI": "10.1016/j.patcog.2025.112820",
        "CorpusId": 283485417
      },
      "corpusId": 283485417,
      "title": "MambaFusion: State-Space Model-Driven Object-Scene Fusion for Multi-Modal 3D Object Detection",
      "venue": "Pattern Recognition",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.patcog.2025.112820?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.patcog.2025.112820, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2331612258",
          "name": "Tong Ning"
        },
        {
          "authorId": "2153781757",
          "name": "Ke Lu"
        },
        {
          "authorId": "2331684556",
          "name": "Xirui Jiang"
        },
        {
          "authorId": null,
          "name": "Jian Xue"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "0395c979b7a9dc1b4fe4f0b5ac5bb5a8c347bc10",
      "externalIds": {
        "DOI": "10.1016/j.neucom.2025.132295",
        "CorpusId": 283515563
      },
      "corpusId": 283515563,
      "title": "SSMGNN: Spectral temporal graph neural network with state space models for multivariate time-series forecasting",
      "venue": "Neurocomputing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.neucom.2025.132295?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.neucom.2025.132295, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2396315207",
          "name": "Weize Zhou"
        },
        {
          "authorId": "2396130429",
          "name": "Fanzhi Zeng"
        },
        {
          "authorId": "2396390547",
          "name": "Tiegang Fan"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "37b4a7467b4c106380011582ff634f9719938139",
      "externalIds": {
        "DOI": "10.1109/TIE.2025.3581279",
        "CorpusId": 280074768
      },
      "corpusId": 280074768,
      "title": "Channel Independence Bidirectional Gated Mamba With Interactive Recurrent Mechanism for Time Series Forecasting",
      "venue": "IEEE transactions on industrial electronics (1982. Print)",
      "year": 2025,
      "citationCount": 4,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIE.2025.3581279?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIE.2025.3581279, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2258697897",
          "name": "Penghua Li"
        },
        {
          "authorId": "2336506253",
          "name": "Xinyou Zheng"
        },
        {
          "authorId": "2312764615",
          "name": "Sheng Xiang"
        },
        {
          "authorId": "2265178329",
          "name": "Jie Hou"
        },
        {
          "authorId": "2240266055",
          "name": "Yi Qin"
        },
        {
          "authorId": "2212572340",
          "name": "M. Kurboniyon"
        },
        {
          "authorId": "2374082116",
          "name": "Wei Ren"
        }
      ],
      "abstract": "Existing time series prediction models struggle with long-sequence data due to limitations in capturing long-range dependencies and processing multichannel information simultaneously. To address this gap, we first propose the channel independence bidirectional gated Mamba (CIBG-Mamba), a novel model enhancing gated recurrent neural networks by integrating Mamba into channel-independent recurrent units. At its core, CIBG-Mamba processes each data channel independently, with each channel utilizing three Mamba-enhanced recurrent units. This design improves memory capacity while enabling dynamic interchannel exchange, allowing the model to effectively handle multidimensional time series (MTS) data. Furthermore, an interactive recurrent mechanism (IRM) is introduced to optimize bidirectional information handling, employing channel-specific adaptive state transition functions. IRM significantly enhances the CIBG-Mamba's ability to capture complex time series dynamics across multiple dimensions. Finally, by fusing the above technologies, the prediction approach, i.e., the channel independence bidirectional gated Mamba with interactive recurrent mechanism (CIBG-Mamba-IRM) is constructed for time series forecasting. Additionally, a dataset with one year of data from five new energy vehicle air conditioners (NEVAC) is created. Through experiments on this dataset and five public time series datasets, our method proved effective in learning long-term dependencies and handling MTS data, achieving state-of-the-art results. Our code is available at https://github.com/XYouZheng/CIBG-Mamba."
    },
    {
      "paperId": "101e66cf2608dfc33bf5025fa28069825a49bd5e",
      "externalIds": {
        "DOI": "10.1109/TNSM.2025.3599168",
        "CorpusId": 280737438
      },
      "corpusId": 280737438,
      "title": "Urban Mobile Data Prediction With Geospatial Clustering and Dual Residual Learning",
      "venue": "IEEE Transactions on Network and Service Management",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TNSM.2025.3599168?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TNSM.2025.3599168, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2156109770",
          "name": "Hui-Lin Yang"
        },
        {
          "authorId": "2273942612",
          "name": "JeongJun Park"
        },
        {
          "authorId": "145589382",
          "name": "S. M. Raza"
        },
        {
          "authorId": "2287224763",
          "name": "Moonseong Kim"
        },
        {
          "authorId": "2273391254",
          "name": "Min Young Chung"
        },
        {
          "authorId": "2238313454",
          "name": "Hyunseung Choo"
        }
      ],
      "abstract": "The mobile network traffic patterns in urban areas significantly diverge depending on commercial and residential establishments. These regional traffic patterns provide crucial clues for predicting traffic patterns precisely. Previous studies have employed a combination of time-series and convolutional Deep Learning (DL) models to effectively capture the correlation of the regional features and traffic patterns. Despite promising results, these approaches are limited in identifying pattern similarities among sparsely located regions and can be further improved. To this end, this study proposes a GEospatial clustering and residual Convolutional temporal long Short-term memory (GECOS) framework consisting of clustering and DL components. The proposed Urbanflow Peak Clustering (UPC) component exploits the peak traffic times of daily mobile data to obtain the groups of cells with similar traffic patterns apart from their geographical diversity. The UPC improves the scalability of existing algorithms and enables DL components to improve their accuracy by recognizing unique regional patterns and localizing the training targets. The proposed Residual Convolutional TCN-LSTM (RCTL) serves as the DL component of GECOS that improves TCN-LSTM structure through layer-wise feature transfer and enhances long-term dependency learnability. The RCTL ensures more accurate capturing of extensive spatiotemporal features through structural enhancements. The experiments conducted on real-world mobile traffic data showcase 43% improvement by GECOS compared to state-of-the-art models, enabling precise traffic engineering policies by operators."
    },
    {
      "paperId": "4f5593a3734d876971fa33867852c94e761d7b02",
      "externalIds": {
        "DOI": "10.1016/j.neucom.2025.132348",
        "CorpusId": 283661225
      },
      "corpusId": 283661225,
      "title": "Quaternion-Based Inter-Channel Correlation Learning for Fine-Grained Medical Image Segmentation",
      "venue": "Neurocomputing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.neucom.2025.132348?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.neucom.2025.132348, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "1641734781",
          "name": "Bangcheng Zhan"
        },
        {
          "authorId": "123803451",
          "name": "E. Song"
        },
        {
          "authorId": "2118902798",
          "name": "Hong Liu"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "e96e167d67b641074cd4ce3abd2abe61cf7aee65",
      "externalIds": {
        "DOI": "10.1109/TAES.2025.3615169",
        "CorpusId": 281642072
      },
      "corpusId": 281642072,
      "title": "Efficient Multiaspect Compensation Network for Small Object Detection in SAR Images",
      "venue": "IEEE Transactions on Aerospace and Electronic Systems",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TAES.2025.3615169?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TAES.2025.3615169, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2299113388",
          "name": "Cong Li"
        },
        {
          "authorId": "2383255306",
          "name": "Pingui Zhang"
        },
        {
          "authorId": "2280817392",
          "name": "Y. Hei"
        },
        {
          "authorId": "2222295328",
          "name": "Wentao Li"
        },
        {
          "authorId": "2391122313",
          "name": "Zhu Xiao"
        }
      ],
      "abstract": "Deep learning has demonstrated its potential capability in object detection of synthetic aperture radar (SAR) images. However, the detection of small objects still faces unique challenges due to insufficient detailed information and complex background clutter distribution. Although existing methods alleviate these issues to some extent by extracting discriminative features, the detection performance is still highly unsatisfactory due to the inevitable information loss as neural networks deepen. In this work, a multiaspect compensation network is proposed to improve the detection accuracy of small objects, in which two key components interactive encoding enhancement (IEE) module and multiscale feature enhancement guided (MFEG) network are introduced. Specifically, IEE initially encodes the low-level details and high-level semantics from neighboring features to enhance the feature representation of small objects. Besides, MFEG performs global modeling using Mamba to extract high-frequency information in SAR images, then integrates it with higher level predicted features to compensate for information loss in multiscale features. Moreover, since the commonly used loss functions often exhibit insensitivity to small objects, a more effective scale and position sensitive loss function is designed to improve attention to small objects. Numerical results on the SAR datasets LS-SSDD-V1.0, HRSID, and SAR-AIRcraft-1.0 demonstrate the effectiveness and superiority of the proposed strategy."
    },
    {
      "paperId": "3fc2004dc0f26a1e5f078273deb56815be222eac",
      "externalIds": {
        "DOI": "10.1016/j.neunet.2025.108459",
        "CorpusId": 283766762,
        "PubMed": "41418561"
      },
      "corpusId": 283766762,
      "title": "fastiSSM: Fast inference of state space model with online model approximation in frequency-domain.",
      "venue": "Neural Networks",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.neunet.2025.108459?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.neunet.2025.108459, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2397994534",
          "name": "Jiaxuan Chen"
        },
        {
          "authorId": "2398619691",
          "name": "Yancheng Xie"
        },
        {
          "authorId": "2163841031",
          "name": "Hai Wang"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "c6e6b3a94069336e5554e926e205baea9f49518c",
      "externalIds": {
        "DOI": "10.1109/JBHI.2025.3612301",
        "CorpusId": 283701211,
        "PubMed": "41359721"
      },
      "corpusId": 283701211,
      "title": "Fractal Dimension of Resting-State EEG as a Biomarker for Autonomous Sensory Meridian Response (ASMR)",
      "venue": "IEEE journal of biomedical and health informatics",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JBHI.2025.3612301?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JBHI.2025.3612301, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2261955776",
          "name": "Shyamal Y. Dharia"
        },
        {
          "authorId": "2325558239",
          "name": "Camilo E. Valderrama"
        },
        {
          "authorId": "2321240733",
          "name": "Qian Liu"
        },
        {
          "authorId": "9601562",
          "name": "B. Fredborg"
        },
        {
          "authorId": "2350680872",
          "name": "Amy S. Desroches"
        },
        {
          "authorId": "2284469374",
          "name": "Stephen D. Smith"
        }
      ],
      "abstract": "Autonomous Sensory Meridian Response (ASMR) is an audio-visual phenomenon characterized by multisensory experiences in response to specific auditory stimuli, typically triggering a tingling sensation beginning in the scalp and neck and accompanied by decreased heart rate and deep relaxation. While prior electroencephalogram (EEG) studies have identified ASMR-related neural signatures in stimulus-based paradigms, resting-state differe nces between ASMR-sensitive (ASMR+) and non-sensitive (ASMR-) individuals remain unexplored. In this study, we apply Higuchi\u2019s fractal dimension (HFD) to eyes-open and eyes-closed resting-state EEG and demonstrate that ASMR+ participants exhibit significantly lower complexity in the delta (1\u20134Hz) and theta (4\u20138Hz) bands and higher complexity in the alpha (8\u201312Hz) band. Moreover, we train Transformer, Mamba, Random Forest and SVM classifiers on these HFD features to distinguish ASMR+ individuals from ASMR-, achieving F1 scores of 82.56%, 77.33%, 73.93%, and 70.85%, respectively. Finally, using an explainable-AI approach, we showed that ASMR+ participants had significantly lower hubness proportions (network connectivity) than ASMR-. These findings reveal novel resting-state biomarkers of ASMR sensitivity and lay the groundwork for rapid, noninvasive EEG-based screening in ASMR-augmented therapeutic applications. The code has been released on https://github.com/Shyamal-Dharia/Fractal-Dimension-of-Resting-State-EEG-as-a-Biomarker-for-Autonomous-Sensory-Meridian-Response-ASMR-GitHub."
    },
    {
      "paperId": "90d277e770f73951dd31ea5ded7ee2b9961c8256",
      "externalIds": {
        "DOI": "10.1016/j.eswa.2025.130693",
        "CorpusId": 283854222
      },
      "corpusId": 283854222,
      "title": "Lightweight UAV Image Super-Resolution Method Based on Large-Kernel Attention and Cross-Height Strategy",
      "venue": "Expert systems with applications",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.eswa.2025.130693?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.eswa.2025.130693, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2398339123",
          "name": "Tian Tian"
        },
        {
          "authorId": "2301845695",
          "name": "Wenzhong Yang"
        },
        {
          "authorId": "2334368311",
          "name": "Yabo Yin"
        },
        {
          "authorId": "2260435494",
          "name": "Danni Chen"
        },
        {
          "authorId": "2398737231",
          "name": "Xiaoming Tao"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "504de34d42934e61d98f7fa3ef35b13c0715f5ed",
      "externalIds": {
        "DOI": "10.1109/MGRS.2025.3588505",
        "CorpusId": 280477047
      },
      "corpusId": 280477047,
      "title": "From Orbit to Ground: A comprehensive review of multimodal self-supervised learning for remote sensing",
      "venue": "IEEE Geoscience and Remote Sensing Magazine",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/MGRS.2025.3588505?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/MGRS.2025.3588505, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2166536709",
          "name": "Lubin Bai"
        },
        {
          "authorId": "46448423",
          "name": "Xiuyuan Zhang"
        },
        {
          "authorId": "2311270252",
          "name": "Wei Qin"
        },
        {
          "authorId": "2375002365",
          "name": "Jiang Long"
        },
        {
          "authorId": "2138204673",
          "name": "Haoyu Wang"
        },
        {
          "authorId": "2233433997",
          "name": "Xiaoyan Dong"
        },
        {
          "authorId": "2239927323",
          "name": "Shihong Du"
        }
      ],
      "abstract": "Today, remote sensing (RS) can offer Earth observation data with various temporal\u2013spatial\u2013spectral characteristics by leveraging different kinds of sensors, forming a multimodal data framework. Shifting the perspective from orbit to ground, text, points of interest (POIs), street-view images, and other geospatial data can provide information related to but distinct from RS images. To enrich the information capacity and build a comprehensive understanding, multimodal learning in the RS field attempts to simultaneously process and apply data of various modalities. However, using multimodal approaches in a supervised manner often requires expensive human annotation, impeding the full utilization of the data\u2019s potential. To alleviate this issue, self-supervised learning (SSL) has become an attractive way to learn from unlabeled data as it can extract meaningful representations by designing effective pretext learning objectives. The label-free, feature-centric, and task-agnostic strengths allow SSL to easily scale up the data and model size, paving the way for RS multimodal foundation models (FMs). In this survey, we systematically review the evolving field of RS multimodal SSL. In terms of data modalities, this review not only covers research utilizing multimodal RS images but also includes studies that integrate RS images with other forms of geospatial data, providing a comprehensive overview of the data integration scenarios. At the methodology level, multimodal SSL requires the synergy of learning objectives and data fusion strategies. To provide a systematic framework for understanding the trends and challenges of RS multimodal SSL approaches, we present a structured methodology taxonomy in terms of a multimodal SSL objective and data fusion strategy. For each type of method, we summarize its characteristics, key elements, and common scenarios. Regarding the application areas, we categorize them into four classes, including image processing, image understanding, vision\u2013language understanding, and socioeconomic prediction. We also provide a systematic review of RS multimodal FMs based on SSL. Finally, we discuss challenges and future directions of RS multimodal SSL. It is our aspiration that this review will act as a starting point for researchers to examine the advancements and engage in the exploration of RS multimodal SSL studies."
    },
    {
      "paperId": "04672ad97c21f448cb7679d006927cf3d89247b3",
      "externalIds": {
        "ArXiv": "2512.01015",
        "CorpusId": 283448825
      },
      "corpusId": 283448825,
      "title": "Upper Approximation Bounds for Neural Oscillators",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.01015, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2264986285",
          "name": "Zifeng Huang"
        },
        {
          "authorId": "2395667373",
          "name": "Konstantin M. Zuev"
        },
        {
          "authorId": "2395712766",
          "name": "Yong Xia"
        },
        {
          "authorId": "2395667949",
          "name": "Michael Beer"
        }
      ],
      "abstract": "Neural oscillators, originating from the second-order ordinary differential equations (ODEs), have demonstrated competitive performance in stably learning causal mappings between long-term sequences or continuous temporal functions. However, theoretically quantifying the capacities of their neural network architectures remains a significant challenge. In this study, the neural oscillator consisting of a second-order ODE followed by a multilayer perceptron (MLP) is considered. Its upper approximation bound for approximating causal and uniformly continuous operators between continuous temporal function spaces and that for approximating uniformly asymptotically incrementally stable second-order dynamical systems are derived. The established proof method of the approximation bound for approximating the causal continuous operators can also be directly applied to state-space models consisting of a linear time-continuous complex recurrent neural network followed by an MLP. Theoretical results reveal that the approximation error of the neural oscillator for approximating the second-order dynamical systems scales polynomially with the reciprocals of the widths of two utilized MLPs, thus mitigating the curse of parametric complexity. The decay rates of two established approximation error bounds are validated through two numerical cases. These results provide a robust theoretical foundation for the effective application of the neural oscillator in science and engineering."
    },
    {
      "paperId": "5fc60b49a15b1e83e19083af38b86090e15d856e",
      "externalIds": {
        "ArXiv": "2512.00989",
        "CorpusId": 283449202
      },
      "corpusId": 283449202,
      "title": "Sleep Apnea Detection on a Wireless Multimodal Wearable Device Without Oxygen Flow Using a Mamba-based Deep Learning Approach",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.00989, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2395666278",
          "name": "Dominik Luszczynski"
        },
        {
          "authorId": "2336735907",
          "name": "Richard Fei Yin"
        },
        {
          "authorId": "2395668260",
          "name": "Nicholas Afonin"
        },
        {
          "authorId": "2297700462",
          "name": "Andrew S. P. Lim"
        }
      ],
      "abstract": "Objectives: We present and evaluate a Mamba-based deep-learning model for diagnosis and event-level characterization of sleep disordered breathing based on signals from the ANNE One, a non-intrusive dual-module wireless wearable system measuring chest electrocardiography, triaxial accelerometry, chest and finger temperature, and finger phototplethysmography. Methods: We obtained concurrent PSG and wearable sensor recordings from 384 adults attending a tertiary care sleep laboratory. Respiratory events in the PSG were manually annotated in accordance with AASM guidelines. Wearable sensor and PSG recordings were automatically aligned based on the ECG signal, alignment confirmed by visual inspection, and PSG-derived respiratory event labels were used to train and evaluate a deep sequential neural network based on the Mamba architecture. Results: In 57 recordings in our test set (mean age 56, mean AHI 10.8, 43.86\\% female) the model-predicted AHI was highly correlated with that derived form the PSG labels (R=0.95, p=8.3e-30, men absolute error 2.83). This performance did not vary with age or sex. At a threshold of AHI$>$5, the model had a sensitivity of 0.96, specificity of 0.87, and kappa of 0.82, and at a threshold of AHI$>$15, the model had a sensitivity of 0.86, specificity of 0.98, and kappa of 0.85. At the level of 30-sec epochs, the model had a sensitivity of 0.93 and specificity of 0.95, with a kappa of 0.68 regarding whether any given epoch contained a respiratory event. Conclusions: Applied to data from the ANNE One, a Mamba-based deep learning model can accurately predict AHI and identify SDB at clinically relevant thresholds, achieves good epoch- and event-level identification of individual respiratory events, and shows promise at physiological characterization of these events including event type (central vs. other) and event duration."
    },
    {
      "paperId": "953dc1632f40b23313cd200fed37e7feb7771805",
      "externalIds": {
        "DOI": "10.1007/s44352-025-00021-2",
        "CorpusId": 283353973
      },
      "corpusId": 283353973,
      "title": "A review of tooth AI segmentation on medical data",
      "venue": "Discover Imaging",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s44352-025-00021-2?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s44352-025-00021-2, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2395349595",
          "name": "Haotian Zhang"
        },
        {
          "authorId": "2219778373",
          "name": "Tianran Yuan"
        },
        {
          "authorId": "2388985254",
          "name": "Tingcheng Li"
        },
        {
          "authorId": "2297136896",
          "name": "Juan Du"
        },
        {
          "authorId": "2395596889",
          "name": "Maokang Ye"
        },
        {
          "authorId": "2399115472",
          "name": "Qian Jiang"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "24c0adad2446f3be29ce323fda136634bdce460c",
      "externalIds": {
        "ArXiv": "2512.00647",
        "CorpusId": 283450156
      },
      "corpusId": 283450156,
      "title": "MambaScope: Coarse-to-Fine Scoping for Efficient Vision Mamba",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.00647, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2395670390",
          "name": "Shanhui Liu"
        },
        {
          "authorId": "2395851168",
          "name": "Rui Xu"
        },
        {
          "authorId": "2395681499",
          "name": "Yunke Wang"
        }
      ],
      "abstract": "Vision Mamba has emerged as a promising and efficient alternative to Vision Transformers, yet its efficiency remains fundamentally constrained by the number of input tokens. Existing token reduction approaches typically adopt token pruning or merging to reduce computation. However, they inherently lead to information loss as they discard or compress token representations. This problem is further exacerbated when the same fine-grained token processing is uniformly applied across all images regardless of visual complexity. We observe that not all inputs require fine-grained processing: simple images can be effectively handled at a coarse resolution, while only complex ones require refinement. Based on this insight, we propose MambaScope, an adaptive framework for efficient inference for Vision Mamba. MambaScope first performs coarse-grained inference by dividing the input image into large patches, significantly reducing token length and computation. When the model's prediction confidence is low, selected regions are re-processed at a finer resolution to recover essential visual details with minimal additional cost. This dynamic resolution assignment strategy allows MambaScope to allocate computation adaptively according to image complexity, achieving efficient processing without compromising accuracy. Experiments across various vision tasks demonstrate that MambaScope outperforms both the baseline Vision Mamba and state-of-the-art token reduction techniques in terms of accuracy and efficiency."
    },
    {
      "paperId": "68d44410804b1f139b6f614c786eea792178e230",
      "externalIds": {
        "ArXiv": "2512.00283",
        "CorpusId": 283449798
      },
      "corpusId": 283449798,
      "title": "BioArc: Discovering Optimal Neural Architectures for Biological Foundation Models",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.00283, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2395778125",
          "name": "Yi Fang"
        },
        {
          "authorId": "2396184258",
          "name": "Haoran Xu"
        },
        {
          "authorId": "2396553389",
          "name": "Jiaxin Han"
        },
        {
          "authorId": "2307734628",
          "name": "Sirui Ding"
        },
        {
          "authorId": "2207006608",
          "name": "Yi\u2010Zheng Wang"
        },
        {
          "authorId": "2313580068",
          "name": "Yue Wang"
        },
        {
          "authorId": "2397425278",
          "name": "Xuan Wang"
        }
      ],
      "abstract": "Foundation models have revolutionized various fields such as natural language processing (NLP) and computer vision (CV). While efforts have been made to transfer the success of the foundation models in general AI domains to biology, existing works focus on directly adopting the existing foundation model architectures from general machine learning domains without a systematic design considering the unique physicochemical and structural properties of each biological data modality. This leads to suboptimal performance, as these repurposed architectures struggle to capture the long-range dependencies, sparse information, and complex underlying ``grammars''inherent to biological data. To address this gap, we introduce BioArc, a novel framework designed to move beyond intuition-driven architecture design towards principled, automated architecture discovery for biological foundation models. Leveraging Neural Architecture Search (NAS), BioArc systematically explores a vast architecture design space, evaluating architectures across multiple biological modalities while rigorously analyzing the interplay between architecture, tokenization, and training strategies. This large-scale analysis identifies novel, high-performance architectures, allowing us to distill a set of empirical design principles to guide future model development. Furthermore, to make the best of this set of discovered principled architectures, we propose and compare several architecture prediction methods that effectively and efficiently predict optimal architectures for new biological tasks. Overall, our work provides a foundational resource and a principled methodology to guide the creation of the next generation of task-specific and foundation models for biology."
    },
    {
      "paperId": "b716a08b1f602b5e796dfbe87d9f83e5d7fbdb45",
      "externalIds": {
        "ArXiv": "2512.00355",
        "CorpusId": 283448817
      },
      "corpusId": 283448817,
      "title": "SMamDiff: Spatial Mamba for Stochastic Human Motion Prediction",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.00355, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2293350785",
          "name": "Junqiao Fan"
        },
        {
          "authorId": "2395932913",
          "name": "Pengfei Liu"
        },
        {
          "authorId": "2395667988",
          "name": "Haocong Rao"
        }
      ],
      "abstract": "With intelligent room-side sensing and service robots widely deployed, human motion prediction (HMP) is essential for safe, proactive assistance. However, many existing HMP methods either produce a single, deterministic forecast that ignores uncertainty or rely on probabilistic models that sacrifice kinematic plausibility. Diffusion models improve the accuracy-diversity trade-off but often depend on multi-stage pipelines that are costly for edge deployment. This work focuses on how to ensure spatial-temporal coherence within a single-stage diffusion model for HMP. We introduce SMamDiff, a Spatial Mamba-based Diffusion model with two novel designs: (i) a residual-DCT motion encoding that subtracts the last observed pose before a temporal DCT, reducing the first DC component ($f=0$) dominance and highlighting informative higher-frequency cues so the model learns how joints move rather than where they are; and (ii) a stickman-drawing spatial-mamba module that processes joints in an ordered, joint-by-joint manner, making later joints condition on earlier ones to induce long-range, cross-joint dependencies. On Human3.6M and HumanEva, these coherence mechanisms deliver state-of-the-art results among single-stage probabilistic HMP methods while using less latency and memory than multi-stage diffusion baselines."
    },
    {
      "paperId": "2acda14762552fad50c4320af643cbff3968f685",
      "externalIds": {
        "ArXiv": "2512.00403",
        "CorpusId": 283450147
      },
      "corpusId": 283450147,
      "title": "SelfAI: Building a Self-Training AI System with LLM Agents",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.00403, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2395705897",
          "name": "Xiao Wu"
        },
        {
          "authorId": "2237742185",
          "name": "Ting-Zhu Huang"
        },
        {
          "authorId": "2294730119",
          "name": "Liang-Jian Deng"
        },
        {
          "authorId": "2396484586",
          "name": "Xiaobing Yu"
        },
        {
          "authorId": "2396683507",
          "name": "Yu Zhong"
        },
        {
          "authorId": "2206304280",
          "name": "Shangqi Deng"
        },
        {
          "authorId": "2384104335",
          "name": "Ufaq Khan"
        },
        {
          "authorId": "2355359931",
          "name": "Jianghao Wu"
        },
        {
          "authorId": "2362868437",
          "name": "Xiaofeng Liu"
        },
        {
          "authorId": "1630421985",
          "name": "Imran Razzak"
        },
        {
          "authorId": "2396422518",
          "name": "Xiaojun Chang"
        },
        {
          "authorId": "2346230834",
          "name": "Yutong Xie"
        }
      ],
      "abstract": "Recent work on autonomous scientific discovery has leveraged LLM-based agents to integrate problem specification, experiment planning, and execution into end-to-end systems. However, these frameworks are often confined to narrow application domains, offer limited real-time interaction with researchers, and lack principled mechanisms for determining when to halt exploration, resulting in inefficiencies, reproducibility challenges, and under-utilized human expertise. To address these gaps, we propose \\textit{SelfAI}, a general multi-agent platform that combines a User Agent for translating high-level research objectives into standardized experimental configurations, a Cognitive Agent powered by LLMs with optimal stopping criteria to iteratively refine hyperparameter searches, and an Experiment Manager responsible for orchestrating parallel, fault-tolerant training workflows across heterogeneous hardware while maintaining a structured knowledge base for continuous feedback. We further introduce two novel evaluation metrics, Score and $\\text{AUP}_D$, to quantify discovery efficiency and search diversity. Across regression, NLP, computer vision, scientific computing, medical imaging, and drug discovery benchmarks, SelfAI consistently achieves strong performance and reduces redundant trials compared to classical Bayesian optimization and LLM-based baselines, while enabling seamless interaction with human researchers."
    },
    {
      "paperId": "42ce484215bd2815b4bd5d9c3465e712cd45c159",
      "externalIds": {
        "ArXiv": "2512.00363",
        "CorpusId": 283449829
      },
      "corpusId": 283449829,
      "title": "MM-DETR: An Efficient Multimodal Detection Transformer with Mamba-Driven Dual-Granularity Fusion and Frequency-Aware Modality Adapters",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.00363, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2292470297",
          "name": "Jianhong Han"
        },
        {
          "authorId": "2108818776",
          "name": "Yupei Wang"
        },
        {
          "authorId": "2344842345",
          "name": "Yuan Zhang"
        },
        {
          "authorId": "2358044987",
          "name": "Liang Chen"
        }
      ],
      "abstract": "Multimodal remote sensing object detection aims to achieve more accurate and robust perception under challenging conditions by fusing complementary information from different modalities. However, existing approaches that rely on attention-based or deformable convolution fusion blocks still struggle to balance performance and lightweight design. Beyond fusion complexity, extracting modality features with shared backbones yields suboptimal representations due to insufficient modality-specific modeling, whereas dual-stream architectures nearly double the parameter count, ultimately limiting practical deployment. To this end, we propose MM-DETR, a lightweight and efficient framework for multimodal object detection. Specifically, we propose a Mamba-based dual granularity fusion encoder that reformulates global interaction as channel-wise dynamic gating and leverages a 1D selective scan for efficient cross-modal modeling with linear complexity. Following this design, we further reinterpret multimodal fusion as a modality completion problem. A region-aware 2D selective scanning completion branch is introduced to recover modality-specific cues, supporting fine-grained fusion along a bidirectional pyramid pathway with minimal overhead. To further reduce parameter redundancy while retaining strong feature extraction capability, a lightweight frequency-aware modality adapter is inserted into the shared backbone. This adapter employs a spatial-frequency co-expert structure to capture modality-specific cues, while a pixel-wise router dynamically balances expert contributions for efficient spatial-frequency fusion. Extensive experiments conducted on four multimodal benchmark datasets demonstrate the effectiveness and generalization capability of the proposed method."
    },
    {
      "paperId": "8e7ccb4f7eb102c3511c88b43a73f0af71c3f4ab",
      "externalIds": {
        "ArXiv": "2511.22849",
        "CorpusId": 283439587
      },
      "corpusId": 283439587,
      "title": "PerfMamba: Performance Analysis and Pruning of Selective State Space Models",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.22849, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "82043618",
          "name": "A. Asif"
        },
        {
          "authorId": "2395544096",
          "name": "Mobina Kashaniyan"
        },
        {
          "authorId": "2028798613",
          "name": "Sixing Yu"
        },
        {
          "authorId": "2249845183",
          "name": "J. P. Mu\u00f1oz"
        },
        {
          "authorId": "2266204425",
          "name": "Ali Jannesari"
        }
      ],
      "abstract": "Recent advances in sequence modeling have introduced selective SSMs as promising alternatives to Transformer architectures, offering theoretical computational efficiency and sequence processing advantages. A comprehensive understanding of selective SSMs in runtime behavior, resource utilization patterns, and scaling characteristics still remains unexplored, thus obstructing their optimal deployment and further architectural improvements. This paper presents a thorough empirical study of Mamba-1 and Mamba-2, systematically profiled for performance to assess the design principles that contribute to their efficiency in state-space modeling. A detailed analysis of computation patterns, memory access, I/O characteristics, and scaling properties was performed for sequence lengths ranging from 64 to 16384 tokens. Our findings show that the SSM component, a central part of the selective SSM architecture, demands a significant portion of computational resources compared to other components in the Mamba block. Based on these insights, we propose a pruning technique that selectively removes low-activity states within the SSM component, achieving measurable throughput and memory gains while maintaining accuracy within a moderate pruning regime. This approach results in performance improvements across varying sequence lengths, achieving a 1.14x speedup and reducing memory usage by 11.50\\%. These results offer valuable guidance for designing more efficient SSM architectures that can be applied to a wide range of real-world applications."
    },
    {
      "paperId": "eb1f251d7f53f0be0236028315ad96744da6f846",
      "externalIds": {
        "ArXiv": "2512.07884",
        "CorpusId": 283711331
      },
      "corpusId": 283711331,
      "title": "GSPN-2: Efficient Parallel Sequence Modeling",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.07884, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2292339349",
          "name": "Hongjun Wang"
        },
        {
          "authorId": "2259059712",
          "name": "Yitong Jiang"
        },
        {
          "authorId": "2397487413",
          "name": "Collin McCarthy"
        },
        {
          "authorId": "2397486399",
          "name": "David Wehr"
        },
        {
          "authorId": "2372190732",
          "name": "Hanrong Ye"
        },
        {
          "authorId": "2354885894",
          "name": "Xinhao Li"
        },
        {
          "authorId": "2290013060",
          "name": "K. Cheung"
        },
        {
          "authorId": "145965455",
          "name": "Wonmin Byeon"
        },
        {
          "authorId": "2343700058",
          "name": "Jinwei Gu"
        },
        {
          "authorId": "2397746674",
          "name": "Ke Chen"
        },
        {
          "authorId": "2398084698",
          "name": "Kai Han"
        },
        {
          "authorId": null,
          "name": "Hongxu Yin"
        },
        {
          "authorId": "2824500",
          "name": "Pavlo Molchanov"
        },
        {
          "authorId": "2376331457",
          "name": "Jan Kautz"
        },
        {
          "authorId": "2280548977",
          "name": "Sifei Liu"
        }
      ],
      "abstract": "Efficient vision transformer remains a bottleneck for high-resolution images and long-video related real-world applications. Generalized Spatial Propagation Network (GSPN) addresses this by replacing quadratic self-attention with a line-scan propagation scheme, bringing the cost close to linear in the number of rows or columns, while retaining accuracy. Despite this advancement, the existing GSPN implementation still suffers from (i) heavy overhead due to repeatedly launching GPU kernels, (ii) excessive data transfers from global GPU memory, and (iii) redundant computations caused by maintaining separate propagation weights for each channel. We introduce GSPN-2, a joint algorithm-system redesign. In particular, we eliminate thousands of micro-launches from the previous implementation into one single 2D kernel, explicitly pin one warp to each channel slice, and stage the previous column's activations in shared memory. On the model side, we introduce a compact channel propagation strategy that replaces per-channel matrices, trimming parameters, and align naturally with the affinity map used in transformer attention. Experiments demonstrate GSPN-2's effectiveness across image classification and text-to-image synthesis tasks, matching transformer-level accuracy with significantly lower computational cost. GSPN-2 establishes a new efficiency frontier for modeling global spatial context in vision applications through its unique combination of structured matrix transformations and GPU-optimized implementation. Project page: https://whj363636.github.io/GSPN2/"
    },
    {
      "paperId": "a0b766fb61746c938b6ef2d5ad4a95e7e13bf3eb",
      "externalIds": {
        "PubMedCentral": "12663430",
        "DOI": "10.1038/s41598-025-26897-y",
        "CorpusId": 283292308,
        "PubMed": "41315472"
      },
      "corpusId": 283292308,
      "title": "A novel deep neural model for efficient and scalable historical place image classification",
      "venue": "Scientific Reports",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBYNCND",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12663430, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2287961389",
          "name": "Md All Moon Tasir"
        },
        {
          "authorId": "2282687597",
          "name": "Mohd Nor Akmal Khalid"
        }
      ],
      "abstract": "Historical place image classification is a crucial intersection of artificial intelligence and cultural heritage preservation, essential for safeguarding architectural legacies in the digital age. Current AI classification systems often struggle with generalizability across regions and styles, require substantial computational resources, and lack interpretability for heritage professionals. This research introduces HistoNet, a hybrid deep learning framework designed to overcome these limitations. HistoNet integrates convolutional neural networks for local feature extraction and Transformer mechanisms for global context, while also using Mamba state-space models, which operate with linear time complexity O(L), thereby offering substantially higher computational efficiency than Transformer architectures that scale quadratically O(\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\hbox {L}^{2}$$\\end{document}). Enhanced with Squeeze-and-Excitation attention blocks and SHapley Additive exPlanations based interpretability, it provides pixel-level attribution maps for more accurate validation of classification decisions. Deployed through a user-friendly web interface with drag-and-drop functionality, HistoNet democratizes access by enabling researchers, conservationists, educators, and heritage professionals in resource-constrained regions to leverage advanced AI tools without requiring high-end hardware or specialized expertise. Testing shows that it achieves 95.66% accuracy on the Architectural Heritage Elements dataset and 96.46% on the Historical Building Recognition dataset, significantly outperforming models such as VGG16 and ResNet50. Moreover, the system demonstrates robust generalization, maintaining stable performance across Malacca\u2019s diverse lighting conditions, weather variations, and architectural styles, thereby ensuring practical reliability in real-world heritage environments. By combining efficiency, accessibility, and robustness, this research makes advanced heritage classification broadly usable and reliable, promoting sustainable development goals in cultural heritage conservation and bridging digital divides."
    },
    {
      "paperId": "20012ae06e81685d9e407978eaf43f3f0b40a15a",
      "externalIds": {
        "DOI": "10.1101/2025.02.19.638959",
        "CorpusId": 276581141
      },
      "corpusId": 276581141,
      "title": "scDNAm-GPT Captures Genome-wide CpG Dependencies in Single-cell DNA methylomes to Revolutionize Epigenetic Analysis",
      "venue": "bioRxiv",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 1,
      "openAccessPdf": {
        "url": "https://doi.org/10.1101/2025.02.19.638959",
        "status": "GREEN",
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1101/2025.02.19.638959?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1101/2025.02.19.638959, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2257114672",
          "name": "Chaoqi Liang"
        },
        {
          "authorId": "2283133268",
          "name": "Peng Ye"
        },
        {
          "authorId": "2239171857",
          "name": "Hongliang Yan"
        },
        {
          "authorId": "2312303732",
          "name": "Peng Zheng"
        },
        {
          "authorId": "2257137259",
          "name": "Jianle Sun"
        },
        {
          "authorId": "2347212102",
          "name": "Yanni Wang"
        },
        {
          "authorId": "2347876235",
          "name": "Yu Li"
        },
        {
          "authorId": "2258184764",
          "name": "Yuchen Ren"
        },
        {
          "authorId": "2347165245",
          "name": "Yuanpei Jiang"
        },
        {
          "authorId": "2387243479",
          "name": "Ran Wei"
        },
        {
          "authorId": "2347125011",
          "name": "Junjia Xiang"
        },
        {
          "authorId": "2347545055",
          "name": "Sizhe Zhang"
        },
        {
          "authorId": "2347165688",
          "name": "Linle Jiang"
        },
        {
          "authorId": "2257034953",
          "name": "Weiqiang Bai"
        },
        {
          "authorId": "2288224604",
          "name": "Xinzhu Ma"
        },
        {
          "authorId": "2312391029",
          "name": "Tao Chen"
        },
        {
          "authorId": "2243334363",
          "name": "Wangmeng Zuo"
        },
        {
          "authorId": "2334751712",
          "name": "Lei Bai"
        },
        {
          "authorId": "2257000758",
          "name": "Wanli Ouyang"
        },
        {
          "authorId": "2327517727",
          "name": "Jia Li"
        }
      ],
      "abstract": "Single-cell DNA methylomes are challenging to interpret because of sparse CpG coverage and the complexity of genome-wide sequences. We present scDNAm-GPT, a universal foundation model that uses context-aware CpG tokenization, a Mamba backbone, and cross-attention to capture both local and global DNA methylation patterns. Trained on over one million single cells from 35 human and mouse tissues, scDNAm-GPT enables accurate cell clustering, zero-shot prediction of CpG effects on gene expression, improved trajectory inference, and reference-free deconvolution of cell types from cell-free DNA. The model hierarchically learns regulatory features, and its attention maps highlight functionally relevant regions, demonstrating high biological interpretability. These results establish scDNAm-GPT as a scalable and generalizable framework for single-cell epigenomic analysis, offering new opportunities to dissect epigenetic regulation in health and disease. Code is available at GitHub (https://github.com/ChaoqiLiang/scDNAm-GPT)."
    },
    {
      "paperId": "f1bf76cf44578df1487fb4eff2424c3754d0a547",
      "externalIds": {
        "ArXiv": "2511.23319",
        "CorpusId": 283438743
      },
      "corpusId": 283438743,
      "title": "Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.23319, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2248371967",
          "name": "Xiang Hu"
        },
        {
          "authorId": "2264070330",
          "name": "Zhanchao Zhou"
        },
        {
          "authorId": "2376124865",
          "name": "Ruiqi Liang"
        },
        {
          "authorId": "2396436268",
          "name": "Zehuan Li"
        },
        {
          "authorId": "2324776486",
          "name": "Wei Wu"
        },
        {
          "authorId": "2388798534",
          "name": "Jianguo Li"
        }
      ],
      "abstract": "This work explores the challenge of building ``Machines that Can Remember'', framing long-term memory as the problem of efficient ultra-long context modeling. We argue that this requires three key properties: \\textbf{sparsity}, \\textbf{random-access flexibility}, and \\textbf{length generalization}. To address ultra-long-context modeling, we leverage Hierarchical Sparse Attention (HSA), a novel attention mechanism that satisfies all three properties. We integrate HSA into Transformers to build HSA-UltraLong, which is an 8B-parameter MoE model trained on over 8 trillion tokens and is rigorously evaluated on different tasks with in-domain and out-of-domain context lengths to demonstrate its capability in handling ultra-long contexts. Results show that our model performs comparably to full-attention baselines on in-domain lengths while achieving over 90\\% accuracy on most in-context retrieval tasks with contexts up to 16M. This report outlines our experimental insights and open problems, contributing a foundation for future research in ultra-long context modeling."
    },
    {
      "paperId": "98d28a4a5e8b9f7877da6d5aa53f786b790a405c",
      "externalIds": {
        "ArXiv": "2512.00208",
        "CorpusId": 283450009
      },
      "corpusId": 283450009,
      "title": "ReactionMamba: Generating Short&Long Human Reaction Sequences",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.00208, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2395675715",
          "name": "Hajra Anwar Beg"
        },
        {
          "authorId": "2093915168",
          "name": "Baptiste Chopin"
        },
        {
          "authorId": "2396564403",
          "name": "Hao Tang"
        },
        {
          "authorId": "2284074506",
          "name": "Mohamed Daoudi"
        }
      ],
      "abstract": "We present ReactionMamba, a novel framework for generating long 3D human reaction motions. Reaction-Mamba integrates a motion VAE for efficient motion encoding with Mamba-based state-space models to decode temporally consistent reactions. This design enables ReactionMamba to generate both short sequences of simple motions and long sequences of complex motions, such as dance and martial arts. We evaluate ReactionMamba on three datasets--NTU120-AS, Lindy Hop, and InterX--and demonstrate competitive performance in terms of realism, diversity, and long-sequence generation compared to previous methods, including InterFormer, ReMoS, and Ready-to-React, while achieving substantial improvements in inference speed."
    },
    {
      "paperId": "bc3c2b7f614883f46eb15b8a5f95f1bd52d3cfa5",
      "externalIds": {
        "DOI": "10.3390/batteries11120440",
        "CorpusId": 283403276
      },
      "corpusId": 283403276,
      "title": "Lithium-Ion Battery Lifetime Prediction Model Based on a Fusion Expert Network",
      "venue": "Batteries",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/batteries11120440?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/batteries11120440, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2318685847",
          "name": "Yawei Meng"
        },
        {
          "authorId": "2107459567",
          "name": "Qiang Sun"
        },
        {
          "authorId": "2264512802",
          "name": "Zhi Wang"
        },
        {
          "authorId": "2395359568",
          "name": "Qizheng Yang"
        },
        {
          "authorId": "2325688998",
          "name": "Yuchen Song"
        },
        {
          "authorId": "2395139450",
          "name": "Rui Xie"
        },
        {
          "authorId": "2354921516",
          "name": "Quanyi Liu"
        },
        {
          "authorId": "2395091198",
          "name": "Yang Lin"
        },
        {
          "authorId": "2345089030",
          "name": "Fei Ren"
        }
      ],
      "abstract": "Accurate prediction of the State of Health (SOH) of lithium-ion batteries is essential for improving the safety and longevity of energy storage systems. This paper introduces ExpertMixer, a novel model based on a fused expert network for SOH estimation. By combining the strengths of state space models and recurrent neural networks, the model effectively handles the joint optimization of long-sequence dependency modeling and complex dynamic feature extraction. To improve temporal representation, ExpertMixer utilizes sampling time-based rotary position encoding (RoPE). It consists of two expert modules: a Mamba module designed to capture global degradation trends and an LSTM module focused on modeling local dynamic fluctuations. These are adaptively fused through a learnable gating mechanism that supports multi-scale feature integration. Experiments performed on the NASA PCoE dataset show that ExpertMixer achieves optimal performance on the NASA L subset, with an average MAE of 1.047 and RMSE of 1.603. It surpasses the traditional CNN BiGRU model, which had an MAE of 2.286, by 54.2%, and improves upon the advanced SambaMixer model, which had an MAE of 1.072, by 2.3%. Under low-temperature conditions using Battery 47, the model reduces the prediction error for nonlinear degradation to an MAE of 0.539, significantly exceeding all compared methods. Ablation studies verify the effectiveness of the dual-expert structure and fusion mechanism; removing the gating module results in an 18.7% decrease in performance. This research offers a new framework for lithium battery life prediction that demonstrates improved accuracy and generalization capability, suggesting potential practical value for intelligent energy storage management."
    },
    {
      "paperId": "bd06a5897add192829b7af81f8d01fedd7532983",
      "externalIds": {
        "DOI": "10.1145/3778033",
        "CorpusId": 283291242
      },
      "corpusId": 283291242,
      "title": "MambaWDC: Efficient Weather Data Compression via Selective State Space Model",
      "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMCCAP)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3778033?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3778033, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2117690254",
          "name": "Xianxuan Lin"
        },
        {
          "authorId": "2257377068",
          "name": "Bailin Yang"
        },
        {
          "authorId": "2343273985",
          "name": "Zhigeng Pan"
        },
        {
          "authorId": "2341926273",
          "name": "Chuangxin Cai"
        },
        {
          "authorId": "2330051976",
          "name": "Shuang Wang"
        },
        {
          "authorId": "2341094555",
          "name": "Aditi Bhattarai"
        },
        {
          "authorId": "2340944823",
          "name": "Fan Meng"
        }
      ],
      "abstract": "The storage and transmission of large-scale meteorological data have become a significant bottleneck hindering the in-depth development of meteorological research. This study proposes a novel meteorological data compression framework characterized by innovative thinking. It embeds the Mamba structure during the feature extraction stage and integrates a spatio-temporal channel attention mechanism to construct an entropy coding mechanism that facilitates the sharing of high-dimensional meteorological data channel information. The framework achieves a compression rate of 300x and, more importantly, preserves the essential spatio-temporal features of meteorological data. Validation on the ERA5 dataset demonstrates that the proposed method surpasses existing techniques in terms of both data compression efficiency and reconstruction accuracy. The accuracy of meteorological analyses based on compressed data is comparable to that of the original data, offering a highly practical solution to the challenges of storing meteorological big data. Our source code are publicly available online at https://github.com/cike0cop/MambaComp."
    },
    {
      "paperId": "0edbf7b175d13ac8101aad5cbe093533850a5141",
      "externalIds": {
        "ArXiv": "2511.22095",
        "CorpusId": 283438334
      },
      "corpusId": 283438334,
      "title": "Binary-30K: A Heterogeneous Dataset for Deep Learning in Binary Analysis and Malware Detection",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.22095, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "50407992",
          "name": "M. Bommarito"
        }
      ],
      "abstract": "Deep learning research for binary analysis faces a critical infrastructure gap. Today, existing datasets target single platforms, require specialized tooling, or provide only hand-engineered features incompatible with modern neural architectures; no single dataset supports accessible research and pedagogy on realistic use cases. To solve this, we introduce Binary-30K, the first heterogeneous binary dataset designed for sequence-based models like transformers. Critically, Binary-30K covers Windows, Linux, macOS, and Android across 15+ CPU architectures. With 29,793 binaries and approximately 26.93% malware representation, Binary-30K enables research on platform-invariant detection, cross-target transfer learning, and long-context binary understanding. The dataset provides pre-computed byte-level BPE tokenization alongside comprehensive structural metadata, supporting both sequence modeling and structure-aware approaches. Platform-first stratified sampling ensures representative coverage across operating systems and architectures, while distribution via Hugging Face with official train/validation/test splits enables reproducible benchmarking. The dataset is publicly available at https://huggingface.co/datasets/mjbommar/binary-30k, providing an accessible resource for researchers, practitioners, and students alike."
    },
    {
      "paperId": "7e37f574903324e3dc06a73657fc1fa9feb44e65",
      "externalIds": {
        "ArXiv": "2511.22101",
        "CorpusId": 283438899
      },
      "corpusId": 283438899,
      "title": "Adaptive Dueling Double Deep Q-networks in Uniswap V3 Replication and Extension with Mamba",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.22101, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2395649011",
          "name": "Zhaofeng Zhang"
        }
      ],
      "abstract": "The report goes through the main steps of replicating and improving the article\"Adaptive Liquidity Provision in Uniswap V3 with Deep Reinforcement Learning.\"The replication part includes how to obtain data from the Uniswap Subgraph, details of the implementation, and comments on the results. After the replication, I propose a new structure based on the original model, which combines Mamba with DDQN and a new reward function. In this new structure, I clean the data again and introduce two new baselines for comparison. As a result, although the model has not yet been applied to all datasets, it shows stronger theoretical support than the original model and performs better in some tests."
    },
    {
      "paperId": "6e440b631f48699f163107e701ca38de1e697b96",
      "externalIds": {
        "ArXiv": "2511.22813",
        "CorpusId": 283439647
      },
      "corpusId": 283439647,
      "title": "Intelligent Neural Networks: From Layered Architectures to Graph-Organized Intelligence",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.22813, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2395540774",
          "name": "Antoine Salomon"
        }
      ],
      "abstract": "Biological neurons exhibit remarkable intelligence: they maintain internal states, communicate selectively with other neurons, and self-organize into complex graphs rather than rigid hierarchical layers. What if artificial intelligence could emerge from similarly intelligent computational units? We introduce Intelligent Neural Networks (INN), a paradigm shift where neurons are first-class entities with internal memory and learned communication patterns, organized in complete graphs rather than sequential layers. Each Intelligent Neuron combines selective state-space dynamics (knowing when to activate) with attention-based routing (knowing to whom to send signals), enabling emergent computation through graph-structured interactions. On the standard Text8 character modeling benchmark, INN achieves 1.705 Bit-Per-Character (BPC), significantly outperforming a comparable Transformer (2.055 BPC) and matching a highly optimized LSTM baseline. Crucially, a parameter-matched baseline of stacked Mamba blocks fails to converge (>3.4 BPC) under the same training protocol, demonstrating that INN's graph topology provides essential training stability. Ablation studies confirm this: removing inter-neuron communication degrades performance or leads to instability, proving the value of learned neural routing. This work demonstrates that neuron-centric design with graph organization is not merely bio-inspired -- it is computationally effective, opening new directions for modular, interpretable, and scalable neural architectures."
    },
    {
      "paperId": "913c066ceb4a36c3bd7698cbadbeb845e85b174b",
      "externalIds": {
        "ArXiv": "2511.21998",
        "CorpusId": 283439281
      },
      "corpusId": 283439281,
      "title": "Can Multi-Modal LLMs Provide Live Step-by-Step Task Guidance?",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.21998, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "3407762",
          "name": "Apratim Bhattacharyya"
        },
        {
          "authorId": "3443241",
          "name": "Bicheng Xu"
        },
        {
          "authorId": "2330587187",
          "name": "Sanjay Haresh"
        },
        {
          "authorId": "2264525112",
          "name": "Reza Pourreza"
        },
        {
          "authorId": "2265457053",
          "name": "Litian Liu"
        },
        {
          "authorId": "2193388695",
          "name": "Sunny Panchal"
        },
        {
          "authorId": "2220960963",
          "name": "Pulkit Madan"
        },
        {
          "authorId": "2251201357",
          "name": "Leonid Sigal"
        },
        {
          "authorId": "2264497937",
          "name": "Roland Memisevic"
        }
      ],
      "abstract": "Multi-modal Large Language Models (LLM) have advanced conversational abilities but struggle with providing live, interactive step-by-step guidance, a key capability for future AI assistants. Effective guidance requires not only delivering instructions but also detecting their successful execution, as well as identifying and alerting users to mistakes, all of which has to happen in real-time. This requires models that are not turn-based, but that can react asynchronously to a video stream, as well as video data showing users performing tasks including mistakes and their corrections. To this end, we introduce Qualcomm Interactive Cooking, a new benchmark and dataset built upon CaptainCook4D, which contains user mistakes during task execution. Our dataset and benchmark features densely annotated, timed instructions and feedback messages, specifically including mistake alerts precisely timestamped to their visual occurrence in the video. We evaluate state-of-the-art multi-modal LLMs on the Qualcomm Interactive Cooking benchmark and introduce LiveMamba, a streaming multi-modal LLM designed for interactive instructional guidance. This work provides the first dedicated benchmark and a strong baseline for developing and evaluating on live, situated coaching."
    },
    {
      "paperId": "138dde97190af8e02afd65f11559ee550e336b5e",
      "externalIds": {
        "ArXiv": "2511.21298",
        "CorpusId": 283262444
      },
      "corpusId": 283262444,
      "title": "PathMamba: A Hybrid Mamba-Transformer for Topologically Coherent Road Segmentation in Satellite Imagery",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.21298, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2394309269",
          "name": "Jules Decaestecker"
        },
        {
          "authorId": "2394309261",
          "name": "Nicolas Vigne"
        }
      ],
      "abstract": "Achieving both high accuracy and topological continuity in road segmentation from satellite imagery is a critical goal for applications ranging from urban planning to disaster response. State-of-the-art methods often rely on Vision Transformers, which excel at capturing global context, yet their quadratic complexity is a significant barrier to efficient deployment, particularly for on-board processing in resource-constrained platforms. In contrast, emerging State Space Models like Mamba offer linear-time efficiency and are inherently suited to modeling long, continuous structures. We posit that these architectures have complementary strengths. To this end, we introduce PathMamba, a novel hybrid architecture that integrates Mamba's sequential modeling with the Transformer's global reasoning. Our design strategically uses Mamba blocks to trace the continuous nature of road networks, preserving topological structure, while integrating Transformer blocks to refine features with global context. This approach yields topologically superior segmentation maps without the prohibitive scaling costs of pure attention-based models. Our experiments on the DeepGlobe Road Extraction and Massachusetts Roads datasets demonstrate that PathMamba sets a new state-of-the-art. Notably, it significantly improves topological continuity, as measured by the APLS metric, setting a new benchmark while remaining computationally competitive."
    },
    {
      "paperId": "412358d9a12e8521c6a3c1353db1f20fe48783c7",
      "externalIds": {
        "ArXiv": "2511.21132",
        "CorpusId": 283261959
      },
      "corpusId": 283261959,
      "title": "DeepRFTv2: Kernel-level Learning for Image Deblurring",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.21132, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2150466000",
          "name": "Xintian Mao"
        },
        {
          "authorId": "2298733354",
          "name": "Haofei Song"
        },
        {
          "authorId": "2394354422",
          "name": "Yin-Nian Liu"
        },
        {
          "authorId": "2238167289",
          "name": "Qingli Li"
        },
        {
          "authorId": "2152543577",
          "name": "Yan Wang"
        }
      ],
      "abstract": "It is well-known that if a network aims to learn how to deblur, it should understand the blur process. Blurring is naturally caused by the convolution of the sharp image with the blur kernel. Thus, allowing the network to learn the blur process in the kernel-level can significantly improve the image deblurring performance. But, current deep networks are still at the pixel-level learning stage, either performing end-to-end pixel-level restoration or stage-wise pseudo kernel-level restoration, failing to enable the deblur model to understand the essence of the blur. To this end, we propose Fourier Kernel Estimator (FKE), which considers the activation operation in Fourier space and converts the convolution problem in the spatial domain to a multiplication problem in Fourier space. Our FKE, jointly optimized with the deblur model, enables the network to learn the kernel-level blur process with low complexity and without any additional supervision. Furthermore, we change the convolution object of the kernel from ``image\"to network extracted ``feature\", whose rich semantic and structural information is more suitable to blur process learning. With the convolution of the feature and the estimated kernel, our model can learn the essence of blur in kernel-level. To further improve the efficiency of feature extraction, we design a decoupled multi-scale architecture with multiple hierarchical sub-unets with a reversible strategy, which allows better multi-scale encoding and decoding in low training memory. Extensive experiments indicate that our method achieves state-of-the-art motion deblurring results and show potential for handling other kernel-related problems. Analysis also shows our kernel estimator is able to learn physically meaningful kernels. The code will be available at https://github.com/DeepMed-Lab-ECNU/Single-Image-Deblur."
    },
    {
      "paperId": "a154b71bf6f5f64765c35bcbb8eb26c1682053a7",
      "externalIds": {
        "ArXiv": "2511.21029",
        "CorpusId": 283261735
      },
      "corpusId": 283261735,
      "title": "FlowerDance: MeanFlow for Efficient and Refined 3D Dance Generation",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.21029, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2197785729",
          "name": "Kaixing Yang"
        },
        {
          "authorId": "2258907001",
          "name": "Xulong Tang"
        },
        {
          "authorId": null,
          "name": "Ziqiao Peng"
        },
        {
          "authorId": "2395047857",
          "name": "Xiangyue Zhang"
        },
        {
          "authorId": "2394378192",
          "name": "Puwei Wang"
        },
        {
          "authorId": "2363366882",
          "name": "Jun He"
        },
        {
          "authorId": "2242580956",
          "name": "Hongyan Liu"
        }
      ],
      "abstract": "Music-to-dance generation aims to translate auditory signals into expressive human motion, with broad applications in virtual reality, choreography, and digital entertainment. Despite promising progress, the limited generation efficiency of existing methods leaves insufficient computational headroom for high-fidelity 3D rendering, thereby constraining the expressiveness of 3D characters during real-world applications. Thus, we propose FlowerDance, which not only generates refined motion with physical plausibility and artistic expressiveness, but also achieves significant generation efficiency on inference speed and memory utilization . Specifically, FlowerDance combines MeanFlow with Physical Consistency Constraints, which enables high-quality motion generation with only a few sampling steps. Moreover, FlowerDance leverages a simple but efficient model architecture with BiMamba-based backbone and Channel-Level Cross-Modal Fusion, which generates dance with efficient non-autoregressive manner. Meanwhile, FlowerDance supports motion editing, enabling users to interactively refine dance sequences. Extensive experiments on AIST++ and FineDance show that FlowerDance achieves state-of-the-art results in both motion quality and generation efficiency. Code will be released upon acceptance."
    },
    {
      "paperId": "644e400d6ad37795a028dfc581de893508df7030",
      "externalIds": {
        "ArXiv": "2511.21016",
        "CorpusId": 283261573
      },
      "corpusId": 283261573,
      "title": "Gated KalmaNet: A Fading Memory Layer Through Test-Time Ridge Regression",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.21016, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2395852927",
          "name": "Liangzu Peng"
        },
        {
          "authorId": "2381987066",
          "name": "Aditya Chattopadhyay"
        },
        {
          "authorId": "1914913470",
          "name": "L. Zancato"
        },
        {
          "authorId": "2335858090",
          "name": "Elvis Nunez"
        },
        {
          "authorId": "2335860934",
          "name": "Wei Xia"
        },
        {
          "authorId": "2075295257",
          "name": "S. Soatto"
        }
      ],
      "abstract": "As efficient alternatives to softmax Attention, linear State-Space Models (SSMs) achieve constant memory and linear compute, but maintain only a lossy, fading summary of the past, often leading to inferior performance in recall-oriented tasks. We propose Gated KalmaNet (GKA), a layer that accounts for the full past while maintaining SSM-style efficiency. We ground our approach in the Kalman Filter (KF) framework, which provides a principled solution for optimal inference in dynamical systems. We show that several existing SSM layers (DeltaNet, Gated DeltaNet, and Kimi Delta Attention) are approximations to the KF recurrence that assume identity error covariance, thereby ignoring how past measurements (keys and values) should optimally influence state updates. In contrast, GKA computes the exact Kalman gain by maintaining the full error covariance. Under a steady-state assumption that enables parallelization, this reduces to solving an online ridge regression problem with constant memory and linear compute cost. A critical insight is that standard KF equations are numerically unstable in low-precision environments (like bfloat16) and hard to parallelize on modern hardware. We address this through: (1) adaptive regularization with input-dependent gating to control the condition number of the ridge regression for numerical stability, and (2) Chebyshev Iteration, which we show is more stable than conventional iterative solvers in low-precision settings. We further develop hardware-aware chunk-wise kernels to enable efficient training. Empirically, GKA outperforms existing SSM layers (like Mamba2 and Gated DeltaNet) on short-context tasks and achieves more than 10\\% relative improvement on long-context RAG and LongQA tasks up to 128k tokens."
    },
    {
      "paperId": "40b883fe70955fd911e278edfd35cf525e487393",
      "externalIds": {
        "ArXiv": "2511.21580",
        "CorpusId": 283261940
      },
      "corpusId": 283261940,
      "title": "Harmonic-Percussive Disentangled Neural Audio Codec for Bandwidth Extension",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.21580, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2322808461",
          "name": "Beno\u00eet Gini\u00e8s"
        },
        {
          "authorId": "2321454936",
          "name": "Xiaoyu Bie"
        },
        {
          "authorId": "2442024",
          "name": "Olivier Fercoq"
        },
        {
          "authorId": "2060044798",
          "name": "G. Richard"
        }
      ],
      "abstract": "Bandwidth extension, the task of reconstructing the high-frequency components of an audio signal from its low-pass counterpart, is a long-standing problem in audio processing. While traditional approaches have evolved alongside the broader trends in signal processing, recent advances in neural architectures have significantly improved performance across a wide range of audio tasks, In this work, we extend these advances by framing bandwidth extension as an audio token prediction problem. Specifically, we train a transformer-based language model on the discrete representations produced by a disentangled neural audio codec, where the disentanglement is guided by a Harmonic-Percussive decomposition of the input signals, highlighting spectral structures particularly relevant for bandwidth extension. Our approach introduces a novel codec design that explicitly accounts for the downstream token prediction task, enabling a more effective coupling between codec structure and transformer modeling. This joint design yields high-quality reconstructions of the original signal, as measured by both objective metrics and subjective evaluations. These results highlight the importance of aligning codec disentanglement and representation learning with the generative modeling stage, and demonstrate the potential of global, representation-aware design for advancing bandwidth extension."
    },
    {
      "paperId": "46a4f843fa3d640d25669654899df730b2327f7c",
      "externalIds": {
        "ArXiv": "2511.21550",
        "CorpusId": 283262334
      },
      "corpusId": 283262334,
      "title": "MMA: A Momentum Mamba Architecture for Human Activity Recognition with Inertial Sensors",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.21550, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2267747797",
          "name": "Thai-Khanh Nguyen"
        },
        {
          "authorId": "2394308607",
          "name": "Uyen Vo"
        },
        {
          "authorId": "2321654026",
          "name": "T. M. Nguyen"
        },
        {
          "authorId": "103050940",
          "name": "Thieu N. Vo"
        },
        {
          "authorId": "2067953701",
          "name": "Trung-Hieu Le"
        },
        {
          "authorId": "2394309345",
          "name": "Cuong Pham"
        }
      ],
      "abstract": "Human activity recognition (HAR) from inertial sensors is essential for ubiquitous computing, mobile health, and ambient intelligence. Conventional deep models such as Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and transformers have advanced HAR but remain limited by vanishing or exloding gradients, high computational cost, and difficulty in capturing long-range dependencies. Structured state-space models (SSMs) like Mamba address these challenges with linear complexity and effective temporal modeling, yet they are restricted to first-order dynamics without stable longterm memory mechanisms. We introduce Momentum Mamba, a momentum-augmented SSM that incorporates second-order dynamics to improve stability of information flow across time steps, robustness, and long-sequence modeling. Two extensions further expand its capacity: Complex Momentum Mamba for frequency-selective memory scaling. Experiments on multiple HAR benchmarks demonstrate consistent gains over vanilla Mamba and Transformer baselines in accuracy, robustness, and convergence speed. With only moderate increases in training cost, momentum-augmented SSMs offer a favorable accuracy-efficiency balance, establishing them as a scalable paradigm for HAR and a promising principal framework for broader sequence modeling applications."
    },
    {
      "paperId": "5b747eea7c83a41fc1c10bd90412715c1987e925",
      "externalIds": {
        "ArXiv": "2511.21475",
        "CorpusId": 283261833
      },
      "corpusId": 283261833,
      "title": "MobileI2V: Fast and High-Resolution Image-to-Video on Mobile Devices",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.21475, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2293804584",
          "name": "Shuai Zhang"
        },
        {
          "authorId": "2394597394",
          "name": "Bao Tang"
        },
        {
          "authorId": "2375863065",
          "name": "Siyuan Yu"
        },
        {
          "authorId": "2394203376",
          "name": "Yueting Zhu"
        },
        {
          "authorId": "2292324963",
          "name": "Jingfeng Yao"
        },
        {
          "authorId": null,
          "name": "Ya Zou"
        },
        {
          "authorId": "2395600009",
          "name": "Shanglin Yuan"
        },
        {
          "authorId": "2376359717",
          "name": "Li Yu"
        },
        {
          "authorId": "2394197423",
          "name": "Wenyu Liu"
        },
        {
          "authorId": "2261738789",
          "name": "Xinggang Wang"
        }
      ],
      "abstract": "Recently, video generation has witnessed rapid advancements, drawing increasing attention to image-to-video (I2V) synthesis on mobile devices. However, the substantial computational complexity and slow generation speed of diffusion models pose significant challenges for real-time, high-resolution video generation on resource-constrained mobile devices. In this work, we propose MobileI2V, a 270M lightweight diffusion model for real-time image-to-video generation on mobile devices. The core lies in: (1) We analyzed the performance of linear attention modules and softmax attention modules on mobile devices, and proposed a linear hybrid architecture denoiser that balances generation efficiency and quality. (2) We design a time-step distillation strategy that compresses the I2V sampling steps from more than 20 to only two without significant quality loss, resulting in a 10-fold increase in generation speed. (3) We apply mobile-specific attention optimizations that yield a 2-fold speed-up for attention operations during on-device inference. MobileI2V enables, for the first time, fast 720p image-to-video generation on mobile devices, with quality comparable to existing models. Under one-step conditions, the generation speed of each frame of 720p video is less than 100 ms. Our code is available at: https://github.com/hustvl/MobileI2V."
    },
    {
      "paperId": "bf6c7961e3c097dbebb7e6cb2d1374a51c5219d4",
      "externalIds": {
        "ArXiv": "2511.21861",
        "CorpusId": 283438221
      },
      "corpusId": 283438221,
      "title": "Towards a Foundation Model for Partial Differential Equations Across Physics Domains",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.21861, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2220629622",
          "name": "Eduardo Soares"
        },
        {
          "authorId": "2344933939",
          "name": "Emilio Vital Brazil"
        },
        {
          "authorId": "2313730800",
          "name": "V. Shirasuna"
        },
        {
          "authorId": "2395534524",
          "name": "Breno W. Carvalho"
        },
        {
          "authorId": "2395536521",
          "name": "Cristiano Malossi"
        }
      ],
      "abstract": "We present PDE-FM, a modular foundation model for physics-informed machine learning that unifies spatial, spectral, and temporal reasoning across heterogeneous partial differential equation (PDE) systems. PDE-FM combines spatial-spectral tokenization, physics-aware conditioning, and a Mamba-based state-space backbone with an operator-theoretic decoder, enabling scalable and data-efficient modeling of complex physical dynamics. In contrast to task-specific neural operators, PDE-FM is pretrained once on diverse PDE datasets and can be transferred to new physical regimes without architectural or data-specific modifications. Evaluated on twelve 2D and 3D datasets from The Well benchmark - spanning hydrodynamic, radiative, elastic, and astrophysical phenomena - PDE-FM achieves state-of-the-art accuracy in six domains, reducing mean VRMSE by 46% relative to prior operator-learning baselines. The model demonstrates robust cross-physics generalization, excelling in turbulent and radiative systems while maintaining strong performance in linear and steady-state regimes. These results suggest that large-scale pretraining across diverse physical processes can yield transferable representations of dynamics, marking a step toward unified, foundation-level surrogates for multi-physics simulation and scientific discovery."
    },
    {
      "paperId": "4d8933bc03763d8e87870ee553ab2fbd96ac7684",
      "externalIds": {
        "ArXiv": "2511.20151",
        "CorpusId": 283251034
      },
      "corpusId": 283251034,
      "title": "Hybrid Convolution and Frequency State Space Network for Image Compression",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.20151, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2383950416",
          "name": "Haodong Pan"
        },
        {
          "authorId": "2394721530",
          "name": "Hao Wei"
        },
        {
          "authorId": "2382821107",
          "name": "Yusong Wang"
        },
        {
          "authorId": "2376131997",
          "name": "Nanning Zheng"
        },
        {
          "authorId": "2293778374",
          "name": "Caigui Jiang"
        }
      ],
      "abstract": "Learned image compression (LIC) has recently benefited from Transformer based and state space model (SSM) based architectures. Convolutional neural networks (CNNs) effectively capture local high frequency details, whereas Transformers and SSMs provide strong long range modeling capabilities but may cause structural information loss or ignore frequency characteristics that are crucial for compression. In this work we propose HCFSSNet, a Hybrid Convolution and Frequency State Space Network for LIC. HCFSSNet uses CNNs to extract local high frequency structures and introduces a Vision Frequency State Space (VFSS) block that models long range low frequency information. The VFSS block combines an Omni directional Neighborhood State Space (VONSS) module, which scans features horizontally, vertically and diagonally, with an Adaptive Frequency Modulation Module (AFMM) that applies content adaptive weighting of discrete cosine transform frequency components for more efficient bit allocation. To further reduce redundancy in the entropy model, we integrate AFMM with a Swin Transformer to form a Frequency Swin Transformer Attention Module (FSTAM) for frequency aware side information modeling. Experiments on the Kodak, Tecnick and CLIC Professional Validation datasets show that HCFSSNet achieves competitive rate distortion performance compared with recent SSM based codecs such as MambaIC, while using significantly fewer parameters. On Kodak, Tecnick and CLIC, HCFSSNet reduces BD rate over the VTM anchor by 18.06, 24.56 and 22.44 percent, respectively, providing an efficient and interpretable hybrid architecture for future learned image compression systems."
    },
    {
      "paperId": "19c0251a93b8c5bf462c6dbecd4e11d203366d67",
      "externalIds": {
        "DOI": "10.1007/s40747-025-02130-1",
        "CorpusId": 283246525
      },
      "corpusId": 283246525,
      "title": "MAC2STI: Mamba network with autoregressive clustering for two-stage spatio-temporal imputation",
      "venue": "Complex & Intelligent Systems",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s40747-025-02130-1?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s40747-025-02130-1, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2394291146",
          "name": "Jinyu Fan"
        },
        {
          "authorId": "2394384697",
          "name": "Jun Ma"
        },
        {
          "authorId": "2369618735",
          "name": "Hongtao Gai"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "f45aceda6032f99f56a8f6951542a3f2b625568d",
      "externalIds": {
        "ArXiv": "2511.20306",
        "CorpusId": 283250467
      },
      "corpusId": 283250467,
      "title": "TaCo: Capturing Spatio-Temporal Semantic Consistency in Remote Sensing Change Detection",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.20306, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2307434318",
          "name": "Han Guo"
        },
        {
          "authorId": "2107899982",
          "name": "Chenyang Liu"
        },
        {
          "authorId": "2276656129",
          "name": "Haotian Zhang"
        },
        {
          "authorId": "2152692813",
          "name": "Bo-Ying Chen"
        },
        {
          "authorId": "2430445",
          "name": "Zhengxia Zou"
        },
        {
          "authorId": "113515560",
          "name": "Z. Shi"
        }
      ],
      "abstract": "Remote sensing change detection (RSCD) aims to identify surface changes across bi-temporal satellite images. Most previous methods rely solely on mask supervision, which effectively guides spatial localization but provides limited constraints on the temporal semantic transitions. Consequently, they often produce spatially coherent predictions while still suffering from unresolved semantic inconsistencies. To address this limitation, we propose TaCo, a spatio-temporal semantic consistent network, which enriches the existing mask-supervised framework with a spatio-temporal semantic joint constraint. TaCo conceptualizes change as a semantic transition between bi-temporal states, in which one temporal feature representation can be derived from the other via dedicated transition features. To realize this, we introduce a Text-guided Transition Generator that integrates textual semantics with bi-temporal visual features to construct the cross-temporal transition features. In addition, we propose a spatio-temporal semantic joint constraint consisting of bi-temporal reconstruct constraints and a transition constraint: the former enforces alignment between reconstructed and original features, while the latter enhances discrimination for changes. This design can yield substantial performance gains without introducing any additional computational overhead during inference. Extensive experiments on six public datasets, spanning both binary and semantic change detection tasks, demonstrate that TaCo consistently achieves SOTA performance."
    },
    {
      "paperId": "31c8bbd456b32f65f682fd07ae6c9efaafa24faf",
      "externalIds": {
        "ArXiv": "2511.20278",
        "CorpusId": 283250663
      },
      "corpusId": 283250663,
      "title": "DAPointMamba: Domain Adaptive Point Mamba for Point Cloud Completion",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.20278, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2337770050",
          "name": "Yinghui Li"
        },
        {
          "authorId": "2277845503",
          "name": "Qianyu Zhou"
        },
        {
          "authorId": "2345379795",
          "name": "Di Shao"
        },
        {
          "authorId": "2285242012",
          "name": "Hao Yang"
        },
        {
          "authorId": "2337777026",
          "name": "Ye Zhu"
        },
        {
          "authorId": "3327913",
          "name": "Richard Dazeley"
        },
        {
          "authorId": "2267503492",
          "name": "Xuequan Lu"
        }
      ],
      "abstract": "Domain adaptive point cloud completion (DA PCC) aims to narrow the geometric and semantic discrepancies between the labeled source and unlabeled target domains. Existing methods either suffer from limited receptive fields or quadratic complexity due to using CNNs or vision Transformers. In this paper, we present the first work that studies the adaptability of State Space Models (SSMs) in DA PCC and find that directly applying SSMs to DA PCC will encounter several challenges: directly serializing 3D point clouds into 1D sequences often disrupts the spatial topology and local geometric features of the target domain. Besides, the overlook of designs in the learning domain-agnostic representations hinders the adaptation performance. To address these issues, we propose a novel framework, DAPointMamba for DA PCC, that exhibits strong adaptability across domains and has the advantages of global receptive fields and efficient linear complexity. It has three novel modules. In particular, Cross-Domain Patch-Level Scanning introduces patch-level geometric correspondences, enabling effective local alignment. Cross-Domain Spatial SSM Alignment further strengthens spatial consistency by modulating patch features based on cross-domain similarity, effectively mitigating fine-grained structural discrepancies. Cross-Domain Channel SSM Alignment actively addresses global semantic gaps by interleaving and aligning feature channels. Extensive experiments on both synthetic and real-world benchmarks demonstrate that our DAPointMamba outperforms state-of-the-art methods with less computational complexity and inference latency."
    },
    {
      "paperId": "328b6d9d0a3a743ba05cbb73379f733a63cedb6f",
      "externalIds": {
        "ArXiv": "2511.19963",
        "CorpusId": 283251126
      },
      "corpusId": 283251126,
      "title": "MambaEye: A Size-Agnostic Visual Encoder with Causal Sequential Processing",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.19963, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2287921589",
          "name": "Changho Choi"
        },
        {
          "authorId": "2288054154",
          "name": "Minho Kim"
        },
        {
          "authorId": "2394200168",
          "name": "Jinkyu Kim"
        }
      ],
      "abstract": "Despite decades of progress, a truly input-size agnostic visual encoder-a fundamental characteristic of human vision-has remained elusive. We address this limitation by proposing \\textbf{MambaEye}, a novel, causal sequential encoder that leverages the low complexity and causal-process based pure Mamba2 backbone. Unlike previous Mamba-based vision encoders that often employ bidirectional processing, our strictly unidirectional approach preserves the inherent causality of State Space Models, enabling the model to generate a prediction at any point in its input sequence. A core innovation is our use of relative move embedding, which encodes the spatial shift between consecutive patches, providing a strong inductive bias for translation invariance and making the model inherently adaptable to arbitrary image resolutions and scanning patterns. To achieve this, we introduce a novel diffusion-inspired loss function that provides dense, step-wise supervision, training the model to build confidence as it gathers more visual evidence. We demonstrate that MambaEye exhibits robust performance across a wide range of image resolutions, especially at higher resolutions such as $1536^2$ on the ImageNet-1K classification task. This feat is achieved while maintaining linear time and memory complexity relative to the number of patches."
    },
    {
      "paperId": "363d2c08cf141a7fd767a718a04642b534c03a35",
      "externalIds": {
        "ArXiv": "2511.19882",
        "CorpusId": 283251270
      },
      "corpusId": 283251270,
      "title": "ChessMamba: Structure-Aware Interleaving of State Spaces for Change Detection in Remote Sensing Images",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.19882, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": null,
          "name": "Lei Ding"
        },
        {
          "authorId": "2319865027",
          "name": "Tong Liu"
        },
        {
          "authorId": "2394351746",
          "name": "Xuanguang Liu"
        },
        {
          "authorId": "2144226631",
          "name": "Xiang-Ming Liu"
        },
        {
          "authorId": "2110878390",
          "name": "Haitao Guo"
        },
        {
          "authorId": "2346054904",
          "name": "Jun Lu"
        }
      ],
      "abstract": "Change detection (CD) in multitemporal remote sensing imagery presents significant challenges for fine-grained recognition, owing to heterogeneity and spatiotemporal misalignment. However, existing methodologies based on vision transformers or state-space models typically disrupt local structural consistency during temporal serialization, obscuring discriminative cues under misalignment and hindering reliable change localization. To address this, we introduce ChessMamba, a structure-aware framework leveraging interleaved state-space modeling for robust CD with multi-temporal inputs. ChessMamba integrates a SpatialMamba encoder with a lightweight cross-source interaction module, featuring two key innovations: (i) Chessboard interleaving with snake scanning order, which serializes multi-temporal features into a unified sequence within a single forward pass, thereby shortening interaction paths and enabling direct comparison for accurate change localization; and (ii) Structure-aware fusion via multi-dilated convolutions, selectively capturing center-and-corner neighborhood contexts within each mono-temporal. Comprehensive evaluations on three CD tasks, including binary CD, semantic CD and multimodal building damage assessment, demonstrate that ChessMamba effectively fuses heterogeneous features and achieves substantial accuracy improvements over state-of-the-art methods.The relevant code will be available at: github.com/DingLei14/ChessMamba."
    },
    {
      "paperId": "cfe95bcc18c14d0157015dd02a5a7501ad7ee901",
      "externalIds": {
        "ArXiv": "2511.19947",
        "CorpusId": 283250725
      },
      "corpusId": 283250725,
      "title": "Towards Edge General Intelligence: Knowledge Distillation for Mobile Agentic AI",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.19947, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2394249949",
          "name": "Yuxuan Wu"
        },
        {
          "authorId": "2394428206",
          "name": "Linghan Ma"
        },
        {
          "authorId": "2266463634",
          "name": "Ruichen Zhang"
        },
        {
          "authorId": "2376416224",
          "name": "Yinqiu Liu"
        },
        {
          "authorId": "2340230621",
          "name": "Dusit Niyato"
        },
        {
          "authorId": "2294324317",
          "name": "Shunpu Tang"
        },
        {
          "authorId": "2943819",
          "name": "Zehui Xiong"
        },
        {
          "authorId": "2376539119",
          "name": "Zhu Han"
        },
        {
          "authorId": "2395034414",
          "name": "Zhaohui Yang"
        },
        {
          "authorId": "2351811438",
          "name": "Kaibin Huang"
        },
        {
          "authorId": "2394295796",
          "name": "Zhaoyang Zhang"
        },
        {
          "authorId": "2344471834",
          "name": "Kai-Kit Wong"
        }
      ],
      "abstract": "Edge General Intelligence (EGI) represents a paradigm shift in mobile edge computing, where intelligent agents operate autonomously in dynamic, resource-constrained environments. However, the deployment of advanced agentic AI models on mobile and edge devices faces significant challenges due to limited computation, energy, and storage resources. To address these constraints, this survey investigates the integration of Knowledge Distillation (KD) into EGI, positioning KD as a key enabler for efficient, communication-aware, and scalable intelligence at the wireless edge. In particular, we emphasize KD techniques specifically designed for wireless communication and mobile networking, such as channel-aware self-distillation, cross-model Channel State Information (CSI) feedback distillation, and robust modulation/classification distillation. Furthermore, we review novel architectures natively suited for KD and edge deployment, such as Mamba, RWKV (Receptance, Weight, Key, Value) and Cross-Architecture distillation, which enhance generalization capabilities. Subsequently, we examine diverse applications in which KD-driven architectures enable EGI across vision, speech, and multimodal tasks. Finally, we highlight the key challenges and future directions for KD in EGI. This survey aims to provide a comprehensive reference for researchers exploring KD-driven frameworks for mobile agentic AI in the era of EGI."
    },
    {
      "paperId": "ea14b98deadfe9aa0039f3221c293412d7bb9645",
      "externalIds": {
        "ArXiv": "2511.18890",
        "CorpusId": 283243510
      },
      "corpusId": 283243510,
      "title": "Nemotron-Flash: Towards Latency-Optimal Hybrid Small Language Models",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.18890, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2357021225",
          "name": "Yonggan Fu"
        },
        {
          "authorId": "2331626352",
          "name": "Xin Dong"
        },
        {
          "authorId": "2355863041",
          "name": "Shizhe Diao"
        },
        {
          "authorId": "40897085",
          "name": "Matthijs Van Keirsbilck"
        },
        {
          "authorId": "2372190732",
          "name": "Hanrong Ye"
        },
        {
          "authorId": "145965455",
          "name": "Wonmin Byeon"
        },
        {
          "authorId": "1739118614",
          "name": "Yashaswi Karnati"
        },
        {
          "authorId": "2394070513",
          "name": "Lucas Liebenwein"
        },
        {
          "authorId": "2395035792",
          "name": "Hannah Zhang"
        },
        {
          "authorId": "2231395",
          "name": "Nikolaus Binder"
        },
        {
          "authorId": "2394070685",
          "name": "Maksim Khadkevich"
        },
        {
          "authorId": "2319280219",
          "name": "Alexander Keller"
        },
        {
          "authorId": "2376331457",
          "name": "Jan Kautz"
        },
        {
          "authorId": "2345376825",
          "name": "Yingyan Lin"
        },
        {
          "authorId": "2824500",
          "name": "Pavlo Molchanov"
        }
      ],
      "abstract": "Efficient deployment of small language models (SLMs) is essential for numerous real-world applications with stringent latency constraints. While previous work on SLM design has primarily focused on reducing the number of parameters to achieve parameter-optimal SLMs, parameter efficiency does not necessarily translate into proportional real-device speed-ups. This work aims to identify the key determinants of SLMs'real-device latency and offer generalizable principles and methodologies for SLM design and training when real-device latency is the primary consideration. Specifically, we identify two central architectural factors: depth-width ratios and operator choices. The former is crucial for small-batch-size latency, while the latter affects both latency and large-batch-size throughput. In light of this, we first study latency-optimal depth-width ratios, with the key finding that although deep-thin models generally achieve better accuracy under the same parameter budget, they may not lie on the accuracy-latency trade-off frontier. Next, we explore emerging efficient attention alternatives to evaluate their potential as candidate building operators. Using the identified promising operators, we construct an evolutionary search framework to automatically discover latency-optimal combinations of these operators within hybrid SLMs, thereby advancing the accuracy-latency frontier. In addition to architectural improvements, we further enhance SLM training using a weight normalization technique that enables more effective weight updates and improves final convergence. Combining these methods, we introduce a new family of hybrid SLMs, called Nemotron-Flash, which significantly advances the accuracy-efficiency frontier of state-of-the-art SLMs, e.g., achieving over +5.5% average accuracy, 1.3x/1.9x lower latency, and 18.7x/45.6x higher throughput compared to Qwen3-1.7B/0.6B, respectively."
    },
    {
      "paperId": "0875dc14412837e867eaca55388e1418661c00d1",
      "externalIds": {
        "ArXiv": "2511.18838",
        "CorpusId": 283243548
      },
      "corpusId": 283243548,
      "title": "FVAR: Visual Autoregressive Modeling via Next Focus Prediction",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.18838, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2394255006",
          "name": "Xiaofan Li"
        },
        {
          "authorId": "2358238780",
          "name": "Chenming Wu"
        },
        {
          "authorId": "2394248390",
          "name": "Yanpeng Sun"
        },
        {
          "authorId": "2394095103",
          "name": "Jiaming Zhou"
        },
        {
          "authorId": "2394071411",
          "name": "Delin Qu"
        },
        {
          "authorId": "2394996719",
          "name": "Yansong Qu"
        },
        {
          "authorId": "2390408236",
          "name": "Weihao Bo"
        },
        {
          "authorId": "2373696624",
          "name": "Haibao Yu"
        },
        {
          "authorId": "2359971076",
          "name": "Dingkang Liang"
        }
      ],
      "abstract": "Visual autoregressive models achieve remarkable generation quality through next-scale predictions across multi-scale token pyramids. However, the conventional method uses uniform scale downsampling to build these pyramids, leading to aliasing artifacts that compromise fine details and introduce unwanted jaggies and moir\\'e patterns. To tackle this issue, we present \\textbf{FVAR}, which reframes the paradigm from \\emph{next-scale prediction} to \\emph{next-focus prediction}, mimicking the natural process of camera focusing from blur to clarity. Our approach introduces three key innovations: \\textbf{1) Next-Focus Prediction Paradigm} that transforms multi-scale autoregression by progressively reducing blur rather than simply downsampling; \\textbf{2) Progressive Refocusing Pyramid Construction} that uses physics-consistent defocus kernels to build clean, alias-free multi-scale representations; and \\textbf{3) High-Frequency Residual Learning} that employs a specialized residual teacher network to effectively incorporate alias information during training while maintaining deployment simplicity. Specifically, we construct optical low-pass views using defocus point spread function (PSF) kernels with decreasing radius, creating smooth blur-to-clarity transitions that eliminate aliasing at its source. To further enhance detail generation, we introduce a High-Frequency Residual Teacher that learns from both clean structure and alias residuals, distilling this knowledge to a vanilla VAR deployment network for seamless inference. Extensive experiments on ImageNet demonstrate that FVAR substantially reduces aliasing artifacts, improves fine detail preservation, and enhances text readability, achieving superior performance with perfect compatibility to existing VAR frameworks."
    },
    {
      "paperId": "e5b3979467ae975ea5dc7949a45e7b3f7485d9d8",
      "externalIds": {
        "ArXiv": "2511.19134",
        "CorpusId": 283244985
      },
      "corpusId": 283244985,
      "title": "MambaRefine-YOLO: A Dual-Modality Small Object Detector for UAV Imagery",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.19134, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2394256977",
          "name": "Shuyu Cao"
        },
        {
          "authorId": "2394123784",
          "name": "Minxin Chen"
        },
        {
          "authorId": "2394112502",
          "name": "Yucheng Song"
        },
        {
          "authorId": "2395222392",
          "name": "Zhaozhong Chen"
        },
        {
          "authorId": "2395276010",
          "name": "Xinyou Zhang"
        }
      ],
      "abstract": "Small object detection in Unmanned Aerial Vehicle (UAV) imagery is a persistent challenge, hindered by low resolution and background clutter. While fusing RGB and infrared (IR) data offers a promising solution, existing methods often struggle with the trade-off between effective cross-modal interaction and computational efficiency. In this letter, we introduce MambaRefine-YOLO. Its core contributions are a Dual-Gated Complementary Mamba fusion module (DGC-MFM) that adaptively balances RGB and IR modalities through illumination-aware and difference-aware gating mechanisms, and a Hierarchical Feature Aggregation Neck (HFAN) that uses a ``refine-then-fuse''strategy to enhance multi-scale features. Our comprehensive experiments validate this dual-pronged approach. On the dual-modality DroneVehicle dataset, the full model achieves a state-of-the-art mAP of 83.2%, an improvement of 7.9% over the baseline. On the single-modality VisDrone dataset, a variant using only the HFAN also shows significant gains, demonstrating its general applicability. Our work presents a superior balance between accuracy and speed, making it highly suitable for real-world UAV applications."
    },
    {
      "paperId": "5cbe723d2d127da0ae40d648843e408b717c8b02",
      "externalIds": {
        "DOI": "10.1177/03611981251381308",
        "CorpusId": 283309151
      },
      "corpusId": 283309151,
      "title": "Self-Supervised State-Space Model for Real-Time Traffic Accident Forecasting Using eKAN Networks",
      "venue": "Transportation Research Record",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1177/03611981251381308?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1177/03611981251381308, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2320766742",
          "name": "Xin Tan"
        },
        {
          "authorId": "2397568056",
          "name": "Sheng Yan"
        },
        {
          "authorId": "2395331123",
          "name": "Shuyu Zhou"
        },
        {
          "authorId": "2320533703",
          "name": "Meng Zhao"
        }
      ],
      "abstract": "\n Accurate traffic accident prediction across different times and regions is vital for public safety. However, current approaches face two major challenges: (i) generalization\u2014existing models rely heavily on manually constructed multiview structures, such as points of interest (POI) distributions and road network densities, which are difficult to scale across cities because of their labor-intensive nature; and (ii) real-Time performance\u2014while some methods improve prediction accuracy through complex architectures, they often incur significant computational costs, hindering their real-time applicability. To address these challenges, we propose SSL-eKamba, an efficient self-supervised framework for traffic accident prediction. To improve generalization, we introduce two self-supervised auxiliary tasks that dynamically enhance traffic pattern representation by capturing spatiotemporal discrepancies. For real-time performance, we present eKamba, an optimized model based on the KAN architecture, which uses learnable univariate functions and applies a selective mechanism (selective SSM) to capture multivariate correlations. Extensive experiments on two real-world data sets demonstrate that SSL-eKamba significantly outperforms state-of-the-art baselines. Furthermore, this framework offers potential insights for other spatiotemporal tasks. The source code is available at\n https://github.com/KevinTan61/SSL-eKamba\n .\n"
    },
    {
      "paperId": "13c53a340c9bbe92914f63659fed689748dcf01f",
      "externalIds": {
        "DOI": "10.1080/10106049.2025.2583334",
        "CorpusId": 283308237
      },
      "corpusId": 283308237,
      "title": "HRRF: a hierarchical recursive reasoning framework for high-resolution remote sensing semantic segmentation",
      "venue": "Geocarto International",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/10106049.2025.2583334?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/10106049.2025.2583334, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2070797388",
          "name": "Guoxun Zheng"
        },
        {
          "authorId": "2243326081",
          "name": "Zhengang Jiang"
        },
        {
          "authorId": "2243445361",
          "name": "Hua Zhang"
        }
      ],
      "abstract": "Abstract Semantic segmentation of high-resolution remote sensing imagery is impeded by a trio of persistent challenges: the obscuration of small-scale objects, the fragmentation of linear geofeatures, and high-level topological inconsistencies. Existing methods often struggle to address these intertwined issues simultaneously, lacking a holistic reasoning process. To overcome these limitations, this paper proposes the Hierarchical Recursive Reasoning Framework (HRRF), a multi-stage deep learning system designed for comprehensive scene interpretation. First, a Dual-stream Context-Aware Encoder robustly captures multi-scale global and local features. Second, a Bi-directional Recursive Refinement Decoder (BRRD) employs a novel Recursive Flow-Modulated Inter-level Aggregation (RFMIA) module with recursive feedback to enhance detail and context. Third, a Graph-based Topological Consistency Correction (GTC) stage enforces high-level spatial-structural priors. Evaluations on the challenging ISPRS Potsdam, Vaihingen, and LoveDA datasets demonstrate that HRRF achieves state-of-the-art or highly competitive performance."
    },
    {
      "paperId": "a8c3e293e8c0bad10242c11ec1cd9ecb7bd7aed1",
      "externalIds": {
        "ArXiv": "2511.18380",
        "CorpusId": 283243962
      },
      "corpusId": 283243962,
      "title": "RNN as Linear Transformer: A Closer Investigation into Representational Potentials of Visual Mamba Models",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.18380, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2325758086",
          "name": "Timing Yang"
        },
        {
          "authorId": "2254265059",
          "name": "Guoyizhe Wei"
        },
        {
          "authorId": "2253485882",
          "name": "Alan L. Yuille"
        },
        {
          "authorId": "2269737429",
          "name": "Feng Wang"
        }
      ],
      "abstract": "Mamba has recently garnered attention as an effective backbone for vision tasks. However, its underlying mechanism in visual domains remains poorly understood. In this work, we systematically investigate Mamba's representational properties and make three primary contributions. First, we theoretically analyze Mamba's relationship to Softmax and Linear Attention, confirming that it can be viewed as a low-rank approximation of Softmax Attention and thereby bridging the representational gap between Softmax and Linear forms. Second, we introduce a novel binary segmentation metric for activation map evaluation, extending qualitative assessments to a quantitative measure that demonstrates Mamba's capacity to model long-range dependencies. Third, by leveraging DINO for self-supervised pretraining, we obtain clearer activation maps than those produced by standard supervised approaches, highlighting Mamba's potential for interpretability. Notably, our model also achieves a 78.5 percent linear probing accuracy on ImageNet, underscoring its strong performance. We hope this work can provide valuable insights for future investigations of Mamba-based vision architectures."
    },
    {
      "paperId": "be5279843a0cd7e6153555033c8833340f5aeb0d",
      "externalIds": {
        "DOI": "10.1007/s11760-025-04985-w",
        "CorpusId": 283228273
      },
      "corpusId": 283228273,
      "title": "Reference-based Image Super-Resolution with Mamba-Deformable Convolution Networks",
      "venue": "Signal, Image and Video Processing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11760-025-04985-w?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11760-025-04985-w, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2302080512",
          "name": "Daokuan Qu"
        },
        {
          "authorId": "2383301132",
          "name": "Jinshi Kang"
        },
        {
          "authorId": "2393950844",
          "name": "Rui Yao"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "d126ce89626023b57dee8e8f8039fc915478ff3c",
      "externalIds": {
        "ArXiv": "2511.18571",
        "CorpusId": 283243690
      },
      "corpusId": 283243690,
      "title": "SAMBA: Toward a Long-Context EEG Foundation Model via Spatial Embedding and Differential Mamba",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.18571, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2347301181",
          "name": "Jiazhen Hong"
        },
        {
          "authorId": "72057097",
          "name": "Geoff Mackellar"
        },
        {
          "authorId": "51058272",
          "name": "Soheila Ghane"
        }
      ],
      "abstract": "Long-sequence electroencephalogram (EEG) modeling is essential for developing generalizable EEG representation models. This need arises from the high sampling rate of EEG data and the long recording durations required to capture extended neurological patterns in brain activity. Transformer-based models have shown promise in modeling short sequences of a few seconds; however, their quadratic complexity limits scalability to longer contexts. Moreover, variability in electrode montage across available datasets, along with inter-subject differences in brain signals, pose significant challenges to developing a generalizable and robust foundation model. We propose \\textit{SAMBA}, a self-supervised learning framework with a Mamba-based U-shaped encoder-decoder architecture, which effectively captures long-range temporal dependencies and spatial variability in EEG data. Leveraging the inherent ability of Mamba in processing long context sizes, we introduce: (1) \\textit{Temporal Semantic Random Masking} for semantic-level sequence reconstruction, (2) a \\textit{Multi-Head Differential Mamba} module to suppress redundancy and emphasize salient temporal structures, and (3) a \\textit{Spatial-Adaptive Input Embedding} that learns unified embeddings in a three-dimensional Euclidean space, enabling robustness across devices. Experiments on thirteen EEG datasets across diverse tasks, electrode configurations, and sequence durations demonstrate that SAMBA consistently outperforms state-of-the-art methods while maintaining low memory consumption and inference time. We also show the learned spatial weight maps from our embedding module align closely with task-relevant neurophysiological regions, demonstrating the learnability and interpretability of SAMBA. These results highlight SAMBA's scalability and practical potential as a foundation model for real-time brain-computer interface applications."
    },
    {
      "paperId": "b588ab7dbbacf950b1979eabf1f45caaf0b1948b",
      "externalIds": {
        "ArXiv": "2511.18248",
        "CorpusId": 283244461
      },
      "corpusId": 283244461,
      "title": "Coherent Multi-Agent Trajectory Forecasting in Team Sports with CausalTraj",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.18248, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "1380272287",
          "name": "Wei Zhen Teoh"
        }
      ],
      "abstract": "Jointly forecasting trajectories of multiple interacting agents is a core challenge in sports analytics and other domains involving complex group dynamics. Accurate prediction enables realistic simulation and strategic understanding of gameplay evolution. Most existing models are evaluated solely on per-agent accuracy metrics (minADE, minFDE), which assess each agent independently on its best-of-k prediction. However these metrics overlook whether the model learns which predicted trajectories can jointly form a plausible multi-agent future. Many state-of-the-art models are designed and optimized primarily based on these metrics. As a result, they may underperform on joint predictions and also fail to generate coherent, interpretable multi-agent scenarios in team sports. We propose CausalTraj, a temporally causal, likelihood-based model that is built to generate jointly probable multi-agent trajectory forecasts. To better assess collective modeling capability, we emphasize joint metrics (minJADE, minJFDE) that measure joint accuracy across agents within the best generated scenario sample. Evaluated on the NBA SportVU, Basketball-U, and Football-U datasets, CausalTraj achieves competitive per-agent accuracy and the best recorded results on joint metrics, while yielding qualitatively coherent and realistic gameplay evolutions."
    },
    {
      "paperId": "c7659111d6a3f3876ba89c992ca06737e90f1b7b",
      "externalIds": {
        "ArXiv": "2511.18312",
        "CorpusId": 283244894
      },
      "corpusId": 283244894,
      "title": "DiM-TS: Bridge the Gap between Selective State Space Models and Time Series for Generative Modeling",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.18312, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2337351080",
          "name": "Zihao Yao"
        },
        {
          "authorId": "116194051",
          "name": "Jian-yong Zuo"
        },
        {
          "authorId": "2394085954",
          "name": "Yaying Zhang"
        }
      ],
      "abstract": "Time series data plays a pivotal role in a wide variety of fields but faces challenges related to privacy concerns. Recently, synthesizing data via diffusion models is viewed as a promising solution. However, existing methods still struggle to capture long-range temporal dependencies and complex channel interrelations. In this research, we aim to utilize the sequence modeling capability of a State Space Model called Mamba to extend its applicability to time series data generation. We firstly analyze the core limitations in State Space Model, namely the lack of consideration for correlated temporal lag and channel permutation. Building upon the insight, we propose Lag Fusion Mamba and Permutation Scanning Mamba, which enhance the model's ability to discern significant patterns during the denoising process. Theoretical analysis reveals that both variants exhibit a unified matrix multiplication framework with the original Mamba, offering a deeper understanding of our method. Finally, we integrate two variants and introduce Diffusion Mamba for Time Series (DiM-TS), a high-quality time series generation model that better preserves the temporal periodicity and inter-channel correlations. Comprehensive experiments on public datasets demonstrate the superiority of DiM-TS in generating realistic time series while preserving diverse properties of data."
    },
    {
      "paperId": "28716e45b29811511bd6d30b8c88ef5e91b8d8e7",
      "externalIds": {
        "ArXiv": "2511.18534",
        "CorpusId": 283244400
      },
      "corpusId": 283244400,
      "title": "HiFi-MambaV2: Hierarchical Shared-Routed MoE for High-Fidelity MRI Reconstruction",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.18534, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2366006856",
          "name": "Pengcheng Fang"
        },
        {
          "authorId": "2296732367",
          "name": "Hongli Chen"
        },
        {
          "authorId": "2394074324",
          "name": "Guangzhen Yao"
        },
        {
          "authorId": "2394666901",
          "name": "Jian Shi"
        },
        {
          "authorId": "144038289",
          "name": "Fangfang Tang"
        },
        {
          "authorId": "2367110322",
          "name": "Xiaohao Cai"
        },
        {
          "authorId": "30564537",
          "name": "Shanshan Shan"
        },
        {
          "authorId": "2375895940",
          "name": "Feng Liu"
        }
      ],
      "abstract": "Reconstructing high-fidelity MR images from undersampled k-space data requires recovering high-frequency details while maintaining anatomical coherence. We present HiFi-MambaV2, a hierarchical shared-routed Mixture-of-Experts (MoE) Mamba architecture that couples frequency decomposition with content-adaptive computation. The model comprises two core components: (i) a separable frequency-consistent Laplacian pyramid (SF-Lap) that delivers alias-resistant, stable low- and high-frequency streams; and (ii) a hierarchical shared-routed MoE that performs per-pixel top-1 sparse dispatch to shared experts and local routers, enabling effective specialization with stable cross-depth behavior. A lightweight global context path is fused into an unrolled, data-consistency-regularized backbone to reinforce long-range reasoning and preserve anatomical coherence. Evaluated on fastMRI, CC359, ACDC, M4Raw, and Prostate158, HiFi-MambaV2 consistently outperforms CNN-, Transformer-, and prior Mamba-based baselines in PSNR, SSIM, and NMSE across single- and multi-coil settings and multiple acceleration factors, consistently surpassing consistent improvements in high-frequency detail and overall structural fidelity. These results demonstrate that HiFi-MambaV2 enables reliable and robust MRI reconstruction."
    },
    {
      "paperId": "73c77840f97aee42e76f59bb6cd75b3c49124200",
      "externalIds": {
        "DOI": "10.3390/electronics14234594",
        "CorpusId": 283252699
      },
      "corpusId": 283252699,
      "title": "A Two-Stage End-to-End Framework for Robust Scene Text Spotting with Self-Calibrated Detection and Contextual Recognition",
      "venue": "Electronics",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/electronics14234594?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/electronics14234594, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2315451780",
          "name": "Yuning Cheng"
        },
        {
          "authorId": "2396746779",
          "name": "Jinhong Huang"
        },
        {
          "authorId": "2394839716",
          "name": "Io San Tai"
        },
        {
          "authorId": "2203223",
          "name": "S. Mondal"
        },
        {
          "authorId": "2394300590",
          "name": "Tianqi Wang"
        },
        {
          "authorId": "2282024624",
          "name": "H. M. D. Kabir"
        }
      ],
      "abstract": "End-to-end scene text detection and recognition, which involves detecting and recognizing text in natural images, still faces significant challenges, particularly in handling text of arbitrary shapes, complex backgrounds, and computational efficiency requirements. This paper proposes a novel and viable end-to-end OCR framework that synergistically combines a powerful detection network with advanced recognition models. For text detection, we develop a method called Text Contrast Self-Calibrated Network (TextCSCN), which employs pixel-wise supervised contrastive learning to extract more discriminative features. TextCSCN addresses long-range dependency modeling and limited receptive field issues through self-calibrated convolutions and Global Convolutional Networks (GCNs). We further introduce an efficient Mamba-based bidirectional module for boundary refinement, enhancing both accuracy and speed. For text recognition, our framework employs a Swin Transformer backbone with Bidirectional Feature Pyramid Networks (BiFPNs) for optimized multi-scale feature extraction. We propose a Pre-Gated Contextual Attention Gate (PCAG) mechanism to effectively fuse visual and linguistic information while minimizing noise and uncertainty in multi-modal integration. Experiments on challenging benchmarks including TotalText and CTW1500 demonstrate the effectiveness of our approach. Our detection module achieves state-of-the-art performance with an F-score of 88.21% on TotalText, and the complete end-to-end system shows comparable improvements in recognition accuracy, establishing new benchmarks for scene text spotting."
    },
    {
      "paperId": "5b790ea20b33331a57e5688481371ef207ad5e76",
      "externalIds": {
        "ArXiv": "2511.17929",
        "CorpusId": 283243715
      },
      "corpusId": 283243715,
      "title": "MambaTAD: When State-Space Models Meet Long-Range Temporal Action Detection",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.17929, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2391747371",
          "name": "Hui Lu"
        },
        {
          "authorId": "2394141950",
          "name": "Yi Yu"
        },
        {
          "authorId": "2299572755",
          "name": "Shijian Lu"
        },
        {
          "authorId": "2391718593",
          "name": "Deepu Rajan"
        },
        {
          "authorId": "2391714614",
          "name": "Boon Poh Ng"
        },
        {
          "authorId": "2372523183",
          "name": "A. Kot"
        },
        {
          "authorId": "2375076460",
          "name": "Xudong Jiang"
        }
      ],
      "abstract": "Temporal Action Detection (TAD) aims to identify and localize actions by determining their starting and ending frames within untrimmed videos. Recent Structured State-Space Models such as Mamba have demonstrated potential in TAD due to their long-range modeling capability and linear computational complexity. On the other hand, structured state-space models often face two key challenges in TAD, namely, decay of temporal context due to recursive processing and self-element conflict during global visual context modeling, which become more severe while handling long-span action instances. Additionally, traditional methods for TAD struggle with detecting long-span action instances due to a lack of global awareness and inefficient detection heads. This paper presents MambaTAD, a new state-space TAD model that introduces long-range modeling and global feature detection capabilities for accurate temporal action detection. MambaTAD comprises two novel designs that complement each other with superior TAD performance. First, it introduces a Diagonal-Masked Bidirectional State-Space (DMBSS) module which effectively facilitates global feature fusion and temporal action detection. Second, it introduces a global feature fusion head that refines the detection progressively with multi-granularity features and global awareness. In addition, MambaTAD tackles TAD in an end-to-end one-stage manner using a new state-space temporal adapter(SSTA) which reduces network parameters and computation cost with linear complexity. Extensive experiments show that MambaTAD achieves superior TAD performance consistently across multiple public benchmarks."
    },
    {
      "paperId": "0370d1c0b368dd0c06af0efb201bb630832cb510",
      "externalIds": {
        "ArXiv": "2511.18139",
        "CorpusId": 283244866
      },
      "corpusId": 283244866,
      "title": "Compact neural networks for astronomy with optimal transport bias correction",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.18139, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2394253767",
          "name": "Shuhuan Wang"
        },
        {
          "authorId": "2395542353",
          "name": "Yuzhen Xie"
        },
        {
          "authorId": "2394411707",
          "name": "Jiayi Li"
        }
      ],
      "abstract": "Astronomical imaging confronts an efficiency-resolution tradeoff that limits large-scale morphological classification and redshift prediction. We introduce WaveletMamba, a theory-driven framework integrating wavelet decomposition with state-space modeling, mathematical regularization, and multi-level bias correction. WaveletMamba achieves 81.72% +/- 0.53% classification accuracy at 64x64 resolution with only 3.54M parameters, delivering high-resolution performance (80.93% +/- 0.27% at 244x244) at low-resolution inputs with 9.7x computational efficiency gains. The framework exhibits Resolution Multistability, where models trained on low-resolution data achieve consistent accuracy across different input scales despite divergent internal representations. The framework's multi-level bias correction synergizes HK distance (distribution-level optimal transport) with Color-Aware Weighting (sample-level fine-tuning), achieving 22.96% Log-MSE improvement and 26.10% outlier reduction without explicit selection function modeling. Here, we show that mathematical rigor enables unprecedented efficiency and comprehensive bias correction in scientific AI, bridging computer vision and astrophysics to revolutionize interdisciplinary scientific discovery."
    },
    {
      "paperId": "1416ba788b0722d6ef909c5f21d71b58bedf46d8",
      "externalIds": {
        "ArXiv": "2511.17864",
        "CorpusId": 283244968
      },
      "corpusId": 283244968,
      "title": "Equivalence of Context and Parameter Updates in Modern Transformer Blocks",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.17864, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "10665767",
          "name": "Adrian Goldwaser"
        },
        {
          "authorId": "2268368067",
          "name": "Michael Munn"
        },
        {
          "authorId": "2268370161",
          "name": "Javier Gonzalvo"
        },
        {
          "authorId": "2268366331",
          "name": "Benoit Dherin"
        }
      ],
      "abstract": "Recent research has established that the impact of context in a vanilla transformer can be represented implicitly by forming a token-dependent, rank-1 patch to its MLP weights. This work extends that foundational theory to the diverse architectures of modern Large Language Models. We first demonstrate a precise, analytical solution for a Gemma-style transformer block, proving that the entire effect of a context can be perfectly mapped to rank-1 patches on its MLP weight matrices and a patch to the RMSNorm scale. We then generalize this result, providing a constructive proof and algorithm for multi-layer models. To unify these findings, we introduce a general framework centered on two core properties: input controllability and output controllability. We prove that a perfect implicit weight patch is possible for any MLP block where the inner function is input-controllable and the outer function is output-controllable. This provides a simpler and more powerful lens for understanding how transformer models transmute prompts into effective weights. This setup generalizes to a wide range of modern LLM architectures including gating, pre-/post-norm, mixture of experts and sequential/parallel transformer blocks."
    },
    {
      "paperId": "c6ee289f350b189c570a878e5568bd36f9d644e9",
      "externalIds": {
        "ArXiv": "2511.18028",
        "CorpusId": 283244858
      },
      "corpusId": 283244858,
      "title": "MambaX: Image Super-Resolution with State Predictive Control",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.18028, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2247815810",
          "name": "Chenyu Li"
        },
        {
          "authorId": "2317324077",
          "name": "Danfeng Hong"
        },
        {
          "authorId": "2253457070",
          "name": "Bing Zhang"
        },
        {
          "authorId": "2372103245",
          "name": "Zhaojie Pan"
        },
        {
          "authorId": "6611386",
          "name": "N. Yokoya"
        },
        {
          "authorId": "2245024954",
          "name": "J. Chanussot"
        }
      ],
      "abstract": "Image super-resolution (SR) is a critical technology for overcoming the inherent hardware limitations of sensors. However, existing approaches mainly focus on directly enhancing the final resolution, often neglecting effective control over error propagation and accumulation during intermediate stages. Recently, Mamba has emerged as a promising approach that can represent the entire reconstruction process as a state sequence with multiple nodes, allowing for intermediate intervention. Nonetheless, its fixed linear mapper is limited by a narrow receptive field and restricted flexibility, which hampers its effectiveness in fine-grained images. To address this, we created a nonlinear state predictive control model \\textbf{MambaX} that maps consecutive spectral bands into a latent state space and generalizes the SR task by dynamically learning the nonlinear state parameters of control equations. Compared to existing sequence models, MambaX 1) employs dynamic state predictive control learning to approximate the nonlinear differential coefficients of state-space models; 2) introduces a novel state cross-control paradigm for multimodal SR fusion; and 3) utilizes progressive transitional learning to mitigate heterogeneity caused by domain and modality shifts. Our evaluation demonstrates the superior performance of the dynamic spectrum-state representation model in both single-image SR and multimodal fusion-based SR tasks, highlighting its substantial potential to advance spectrally generalized modeling across arbitrary dimensions and modalities."
    },
    {
      "paperId": "13ea42a4879f39eb13cb0aae7c2d53e2c574826c",
      "externalIds": {
        "ArXiv": "2511.17388",
        "CorpusId": 283226311
      },
      "corpusId": 283226311,
      "title": "Selective Rotary Position Embedding",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.17388, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "48690344",
          "name": "Sajad Movahedi"
        },
        {
          "authorId": "2345692748",
          "name": "Timur Carstensen"
        },
        {
          "authorId": "2130931335",
          "name": "Arshia Afzal"
        },
        {
          "authorId": "2393926066",
          "name": "Frank Hutter"
        },
        {
          "authorId": "2326112647",
          "name": "Antonio Orvieto"
        },
        {
          "authorId": "1678641",
          "name": "V. Cevher"
        }
      ],
      "abstract": "Position information is essential for language modeling. In softmax transformers, Rotary Position Embeddings (\\textit{RoPE}) encode positions through \\textit{fixed-angle} rotations, while in linear transformers, order is handled via input-dependent (selective) gating that decays past key-value associations. Selectivity has generally been shown to improve language-related tasks. Inspired by this, we introduce \\textit{Selective RoPE}, an \\textit{input-dependent} rotary embedding mechanism, that generalizes \\textit{RoPE}, and enables rotation in \\textit{arbitrary angles} for both linear and softmax transformers. We show that softmax attention already performs a hidden form of these rotations on query-key pairs, uncovering an implicit positional structure. We further show that in state-space models and gated linear transformers, the real part manages forgetting while the imaginary part encodes positions through rotations. We validate our method by equipping gated transformers with \\textit{Selective RoPE}, demonstrating that its input-dependent rotations improve performance in language modeling and on difficult sequence tasks like copying, state tracking, and retrieval."
    },
    {
      "paperId": "e69ee502ce4e0f9e68acc5c6c863c800baa2e6be",
      "externalIds": {
        "ArXiv": "2511.17355",
        "CorpusId": 283225619
      },
      "corpusId": 283225619,
      "title": "UAM: A Unified Attention-Mamba Backbone of Multimodal Framework for Tumor Cell Classification",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.17355, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2394372464",
          "name": "Taixi Chen"
        },
        {
          "authorId": "2394011502",
          "name": "Jingyun Chen"
        },
        {
          "authorId": "2393986842",
          "name": "Nancy Guo"
        }
      ],
      "abstract": "Cell-level radiomics features provide fine-grained insights into tumor phenotypes and have the potential to significantly enhance diagnostic accuracy on hematoxylin and eosin (H&E) images. By capturing micro-level morphological and intensity patterns, these features support more precise tumor identification and improve AI interpretability by highlighting diagnostically relevant cells for pathologist review. However, most existing studies focus on slide-level or patch-level tumor classification, leaving cell-level radiomics analysis largely unexplored. Moreover, there is currently no dedicated backbone specifically designed for radiomics data. Inspired by the recent success of the Mamba architecture in vision and language domains, we introduce a Unified Attention-Mamba (UAM) backbone for cell-level classification using radiomics features. Unlike previous hybrid approaches that integrate Attention and Mamba modules in fixed proportions, our unified design flexibly combines their capabilities within a single cohesive architecture, eliminating the need for manual ratio tuning and improving encode capability. We develop two UAM variants to comprehensively evaluate the benefits of this unified structure. Building on this backbone, we further propose a multimodal UAM framework that jointly performs cell-level classification and image segmentation. Experimental results demonstrate that UAM achieves state-of-the-art performance across both tasks on public benchmarks, surpassing leading image-based foundation models. It improves cell classification accuracy from 74% to 78% ($n$=349,882 cells), and tumor segmentation precision from 75% to 80% ($n$=406 patches). These findings highlight the effectiveness and promise of UAM as a unified and extensible multimodal foundation for radiomics-driven cancer diagnosis."
    },
    {
      "paperId": "3666851edeb4fb8dd22bfd0e00dc7e5588fefea8",
      "externalIds": {
        "ArXiv": "2511.16983",
        "CorpusId": 283225826
      },
      "corpusId": 283225826,
      "title": "Feature Partitioning and Semantic Equalization for Intrinsic Robustness in Semantic Communication under Packet Loss",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.16983, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2393978264",
          "name": "Xiao Yang"
        },
        {
          "authorId": "2212254496",
          "name": "Shuai Ma"
        },
        {
          "authorId": "2391500868",
          "name": "Yong Liang"
        },
        {
          "authorId": "2282530693",
          "name": "Guangming Shi"
        }
      ],
      "abstract": "Semantic communication can improve transmission efficiency by focusing on task-relevant information. However, under packet-based communication protocols, any error typically results in the loss of an entire packet, making semantic communication particularly vulnerable to packet loss. Since high-dimensional semantic features must be partitioned into one-dimensional transmission units during packetization. A critical open question is how to partition semantic features to maximize robustness. To address this, we systematically investigate the performance of two mainstream architectures, Transformer and Convolutional neural networks (CNN), under various feature partitioning schemes. The results show that the Transformer architecture exhibits inherent robustness to packet loss when partitioned along the channel dimension. In contrast, the CNN-based baseline exhibits imbalanced channel utilization, causing severe degradation once dominant channels are lost. To enhance the CNN resilience, we propose a lightweight Semantic Equalization Mechanism (SEM) that balances channel contributions and prevents a few channels from dominating. SEM consists of two parallel approaches: a Dynamic Scale module that adaptively adjusts channel importance, and a Broadcast module that facilitates information interaction among channels. Experimental results demonstrate that CNN equipped with SEM achieve graceful degradation under packet loss (retaining about 85% of lossless PSNR at 40% packet loss), comparable to that of Transformer models. Our findings indicate that, under an appropriate partitioning strategy, maintaining a balanced semantic representation is a fundamental condition for achieving intrinsic robustness against packet loss. These insights may also extend to other modalities such as video and support practical semantic communication design."
    },
    {
      "paperId": "8ac6506c67ffc2d73da5b8e553117a9c6b74c257",
      "externalIds": {
        "DOI": "10.1088/1361-6501/ae2287",
        "CorpusId": 283235545
      },
      "corpusId": 283235545,
      "title": "Damage identification of wind turbine blade based on large language model and a novel interpretable Mamba model",
      "venue": "Measurement science and technology",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1088/1361-6501/ae2287?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1088/1361-6501/ae2287, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2347779934",
          "name": "ZhiTai Xing"
        },
        {
          "authorId": "2393993000",
          "name": "Ruibin Ban"
        },
        {
          "authorId": "2274377081",
          "name": "Ling Xiang"
        },
        {
          "authorId": "40262148",
          "name": "Aijun Hu"
        },
        {
          "authorId": "2145913725",
          "name": "Yue Zhang"
        }
      ],
      "abstract": "Accurate detection of wind turbine blade (WTB) health structure is critical for preventing equipment failures and ensuring operational safety. Recently, data-driven approaches based on supervisory control and data acquisition (SCADA) have gained traction due to its high timeliness and large capacity. However, SCADA data are often characterized by high dimensionality and complexity, which pose significant challenges for effective feature extraction and high-precision fault identification. To address these challenges, a novel strategy is proposed for WTB damage identification under SCADA data. Firstly, a feature selection and dimensionality reduction approach is proposed to preprocess the high timeliness and large capacity data based on large language model, which enables the extraction of key diagnostic parameters and descends SCADA data complexity. Then, a novel interpretable Mamba model named sparse deformable BiMamba (SDBiM) is proposed to efficiently capture essential long-range dependencies and spatial correlations inherent in SCADA data. The deformable perception convolutional architecture of SDBiM model can adapt sampling guided by learnable offsets, facilitating fine-grained feature extraction. In proposed SDBiM model, a new loss function called Sinusoidal adaptive focal loss is proposed to address class imbalance by dynamically adjusting sample weights during training. Finally, the novel strategy is proved through two real SCADA datasets of wind farm, and the interpretability of the proposed SDBiM model is analyzed by SHapley Additive exPlanations method which visualizes the contribution of input features. The results demonstrate the proposed method achieves over 99% classification accuracy in both single-fault and multi-fault scenarios, demonstrating strong robustness, generalization, and the ability to provide actionable insights for ensuring blade structural safety."
    },
    {
      "paperId": "b571b6072bdf31bdcc06cceac64f658753d15568",
      "externalIds": {
        "DOI": "10.1109/TIP.2025.3633561",
        "CorpusId": 283220772,
        "PubMed": "41269856"
      },
      "corpusId": 283220772,
      "title": "NDMamba: Dual-Prior State-Space Model for Nighttime Deraining",
      "venue": "IEEE Transactions on Image Processing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIP.2025.3633561?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIP.2025.3633561, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2393868628",
          "name": "Zhirui Liu"
        },
        {
          "authorId": "2125119000",
          "name": "Shangquan Sun"
        },
        {
          "authorId": "2325595264",
          "name": "Chaopeng Li"
        },
        {
          "authorId": "2386786121",
          "name": "Siying Zhu"
        },
        {
          "authorId": "2393865900",
          "name": "Xiaopeng Zhu"
        },
        {
          "authorId": "2276950332",
          "name": "Wenqi Ren"
        }
      ],
      "abstract": "Recent advancements in deep learning, particularly through Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), have led to significant progress in nighttime image deraining. However, current architectures still struggle to strike an optimal balance between computational efficiency and restoration performance. Moreover, existing methods often fail to fully exploit the intrinsic characteristics of low-light conditions and inadequately model the interaction between rain and illumination. To overcome these challenges, we propose NDMamba, a dual-prior-guided state-space model that addresses nighttime deraining by incorporating degradation cues related to both lighting and rain distribution. Inspired by the Retinex theory, which suggests that rain streak distribution is influenced by the reflectance component of a scene, we propose a Prior Extraction Module (PEM) to jointly model lighting conditions and rain degradation. Furthermore, we design a Prior-Guided Mamba Block (PGMB), which comprises a Lighting-Adaptive Vision State-Space Module (LVSSM) that incorporates illumination priors, and a Rain Distribution Guidance Module (RDGM) to enhance local features in a more refined manner. Extensive experiments demonstrate that NDMamba outperforms state-of-the-art methods on both synthetic and real-world benchmark datasets. Our code is publicly available at https://github.com/tandaily/NDMamba"
    },
    {
      "paperId": "c9f06af504c57431f23a45557d9a010ad84c55dd",
      "externalIds": {
        "ArXiv": "2511.16839",
        "CorpusId": 283225583
      },
      "corpusId": 283225583,
      "title": "Analysis of heart failure patient trajectories using sequence modeling",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.16839, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2393137695",
          "name": "Falk Dippel"
        },
        {
          "authorId": "2393244316",
          "name": "Yinan Yu"
        },
        {
          "authorId": "2247375344",
          "name": "Annika Rosengren"
        },
        {
          "authorId": "2247375042",
          "name": "Martin Lindgren"
        },
        {
          "authorId": "1400934058",
          "name": "C. Lundberg"
        },
        {
          "authorId": "2337623458",
          "name": "Erik Aerts"
        },
        {
          "authorId": "2281604424",
          "name": "M. Adiels"
        },
        {
          "authorId": "2393136284",
          "name": "Helen Sjoland"
        }
      ],
      "abstract": "Transformers have defined the state-of-the-art for clinical prediction tasks involving electronic health records (EHRs). The recently introduced Mamba architecture outperformed an advanced Transformer (Transformer++) based on Llama in handling long context lengths, while using fewer model parameters. Despite the impressive performance of these architectures, a systematic approach to empirically analyze model performance and efficiency under various settings is not well established in the medical domain. The performances of six sequence models were investigated across three architecture classes (Transformers, Transformers++, Mambas) in a large Swedish heart failure (HF) cohort (N = 42820), providing a clinically relevant case study. Patient data included diagnoses, vital signs, laboratories, medications and procedures extracted from in-hospital EHRs. The models were evaluated on three one-year prediction tasks: clinical instability (a readmission phenotype) after initial HF hospitalization, mortality after initial HF hospitalization and mortality after latest hospitalization. Ablations account for modifications of the EHR-based input patient sequence, architectural model configurations, and temporal preprocessing techniques for data collection. Llama achieves the highest predictive discrimination, best calibration, and showed robustness across all tasks, followed by Mambas. Both architectures demonstrate efficient representation learning, with tiny configurations surpassing other large-scaled Transformers. At equal model size, Llama and Mambas achieve superior performance using 25% less training data. This paper presents a first ablation study with systematic design choices for input tokenization, model configuration and temporal data preprocessing. Future model development in clinical prediction tasks using EHRs could build upon this study's recommendation as a starting point."
    },
    {
      "paperId": "a78ae074deb34696a71a6b0f29c1c5d0718b6652",
      "externalIds": {
        "PubMedCentral": "12635100",
        "DOI": "10.1038/s41598-025-24898-5",
        "CorpusId": 283102640,
        "PubMed": "41266658"
      },
      "corpusId": 283102640,
      "title": "Cotton leaf disease detection model focusing on small targets and comprehensive feature extraction",
      "venue": "Scientific Reports",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12635100, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "1888762",
          "name": "Halidanmu Abudukelimu"
        },
        {
          "authorId": "2393210230",
          "name": "Gengrong Zhang"
        },
        {
          "authorId": "2167010974",
          "name": "Abudukelimu Abulizi"
        },
        {
          "authorId": "2347481866",
          "name": "Junxiang Ye"
        },
        {
          "authorId": "3281666",
          "name": "Mayilamu Musideke"
        },
        {
          "authorId": "2321119025",
          "name": "Yaqing Shi"
        },
        {
          "authorId": "2393131166",
          "name": "Gulimire Awudan"
        }
      ],
      "abstract": "Cotton, as a globally important economic crop, requires early and accurate disease detection to ensure stable yield and promote sustainable development. However, due to the small size of certain leaf lesions, traditional detection methods often suffer from missed or false detections. To address this issue, we propose an improved YOLOv8-based model, CM-YOLO, aimed at enhancing the detection performance for small cotton leaf disease targets. Specifically, the SS2D module from VMamba is introduced into the backbone network to achieve comprehensive feature extraction through multi-directional scanning. Furthermore, the MSDA module is embedded prior to the SPPF module to reduce performance degradation caused by redundant computations and to enhance the model\u2019s focus on critical small targets. Finally, the original bounding box loss function is replaced with DIoU, enabling precise localization of small targets by optimizing anchor center point distances and accelerating model convergence. Experimental results demonstrate that CM-YOLO achieves superior performance in cotton leaf disease detection, with an mAP50 of 0.933 and a recall of 0.891. Compared with state-of-the-art methods, YOLOv8n and YOLOv11n achieve mAP50 values of 0.874 and 0.930, respectively, both lower than CM-YOLO, thereby validating the effectiveness of the proposed method. Additionally, generalization experiments indicate that the model maintains high detection accuracy and robustness across different plant datasets, highlighting its strong applicability in complex scenarios and providing a valuable reference for intelligent agricultural disease detection research."
    },
    {
      "paperId": "778f52f69b11189af19c0430be7a50f8c745d098",
      "externalIds": {
        "ArXiv": "2511.16161",
        "CorpusId": 283110089
      },
      "corpusId": 283110089,
      "title": "Simba: Towards High-Fidelity and Geometrically-Consistent Point Cloud Completion via Transformation Diffusion",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.16161, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2393231764",
          "name": "Lirui Zhang"
        },
        {
          "authorId": "2394070786",
          "name": "Zhengkai Zhao"
        },
        {
          "authorId": "2354143311",
          "name": "Zhi Zuo"
        },
        {
          "authorId": "2257001021",
          "name": "Pan Gao"
        },
        {
          "authorId": "2274191088",
          "name": "Jie Qin"
        }
      ],
      "abstract": "Point cloud completion is a fundamental task in 3D vision. A persistent challenge in this field is simultaneously preserving fine-grained details present in the input while ensuring the global structural integrity of the completed shape. While recent works leveraging local symmetry transformations via direct regression have significantly improved the preservation of geometric structure details, these methods suffer from two major limitations: (1) These regression-based methods are prone to overfitting which tend to memorize instant-specific transformations instead of learning a generalizable geometric prior. (2) Their reliance on point-wise transformation regression lead to high sensitivity to input noise, severely degrading their robustness and generalization. To address these challenges, we introduce Simba, a novel framework that reformulates point-wise transformation regression as a distribution learning problem. Our approach integrates symmetry priors with the powerful generative capabilities of diffusion models, avoiding instance-specific memorization while capturing robust geometric structures. Additionally, we introduce a hierarchical Mamba-based architecture to achieve high-fidelity upsampling. Extensive experiments across the PCN, ShapeNet, and KITTI benchmarks validate our method's state-of-the-art (SOTA) performance."
    },
    {
      "paperId": "1e3599f7c11d0129a2dc54bff827df1722714b14",
      "externalIds": {
        "ArXiv": "2511.16595",
        "CorpusId": 283110069
      },
      "corpusId": 283110069,
      "title": "TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.16595, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2109511660",
          "name": "Boshen Xu"
        },
        {
          "authorId": "2351211604",
          "name": "Zihan Xiao"
        },
        {
          "authorId": "2362865102",
          "name": "Jiaze Li"
        },
        {
          "authorId": "2317982861",
          "name": "Jianzhong Ju"
        },
        {
          "authorId": null,
          "name": "Zhenbo Luo"
        },
        {
          "authorId": "2317980688",
          "name": "Jian Luan"
        },
        {
          "authorId": "2393210561",
          "name": "Qin Jin"
        }
      ],
      "abstract": "We introduce TimeViper, a hybrid vision-language model designed to tackle challenges of long video understanding. Processing long videos demands both an efficient model architecture and an effective mechanism for handling extended temporal contexts. To this end, TimeViper adopts a hybrid Mamba-Transformer backbone that combines the efficiency of state-space models with the expressivity of attention mechanisms. Through this hybrid design, we reveal the vision-to-text information aggregation phenomenon, where information progressively flows from vision tokens to text tokens across increasing LLM depth, resulting in severe vision token redundancy. Motivated by this observation, we propose TransV, a token information transfer module that transfers and compresses vision tokens into instruction tokens while maintaining multimodal understanding capabilities. This design enables TimeViper to process hour-long videos exceeding 10,000 frames. Extensive experiments across multiple benchmarks demonstrate that TimeViper competes with state-of-the-art models while extending frame numbers. We further analyze attention behaviors of both Mamba and Transformer layers, offering new insights into hybrid model interpretability. This work represents an initial step towards developing, interpreting, and compressing hybrid Mamba-Transformer architectures."
    },
    {
      "paperId": "ea1c104922a0a498e9ba05a47232732913b3736e",
      "externalIds": {
        "ArXiv": "2511.16191",
        "CorpusId": 283110467
      },
      "corpusId": 283110467,
      "title": "CausalMamba: Interpretable State Space Modeling for Temporal Rumor Causality",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.16191, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2393214510",
          "name": "Xiaotong Zhan"
        },
        {
          "authorId": "2394066026",
          "name": "Xi Cheng"
        }
      ],
      "abstract": "Rumor detection on social media remains a challenging task due to the complex propagation dynamics and the limited interpretability of existing models. While recent neural architectures capture content and structural features, they often fail to reveal the underlying causal mechanisms of misinformation spread. We propose CausalMamba, a novel framework that integrates Mamba-based sequence modeling, graph convolutional networks (GCNs), and differentiable causal discovery via NOTEARS. CausalMamba learns joint representations of temporal tweet sequences and reply structures, while uncovering latent causal graphs to identify influential nodes within each propagation chain. Experiments on the Twitter15 dataset show that our model achieves competitive classification performance compared to strong baselines, and uniquely enables counterfactual intervention analysis. Qualitative results demonstrate that removing top-ranked causal nodes significantly alters graph connectivity, offering interpretable insights into rumor dynamics. Our framework provides a unified approach for rumor classification and influence analysis, paving the way for more explainable and actionable misinformation detection systems."
    },
    {
      "paperId": "85652dd6868a85572a1fa0d1f1f110d1d11501f9",
      "externalIds": {
        "ArXiv": "2511.16664",
        "CorpusId": 283110121
      },
      "corpusId": 283110121,
      "title": "Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.16664, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "113435906",
          "name": "Ali Taghibakhshi"
        },
        {
          "authorId": "1868568103",
          "name": "Sharath Turuvekere Sreenivas"
        },
        {
          "authorId": "31225166",
          "name": "Saurav Muralidharan"
        },
        {
          "authorId": "2393206647",
          "name": "Ruisi Cai"
        },
        {
          "authorId": "2291133225",
          "name": "Marcin Chochowski"
        },
        {
          "authorId": "1789266135",
          "name": "Ameya Mahabaleshwarkar"
        },
        {
          "authorId": "2283136281",
          "name": "Yoshi Suhara"
        },
        {
          "authorId": "1696821",
          "name": "O. Olabiyi"
        },
        {
          "authorId": "51918928",
          "name": "Daniel Korzekwa"
        },
        {
          "authorId": "66870756",
          "name": "M. Patwary"
        },
        {
          "authorId": "1911755",
          "name": "M. Shoeybi"
        },
        {
          "authorId": "2273651410",
          "name": "Jan Kautz"
        },
        {
          "authorId": "2264406909",
          "name": "Bryan Catanzaro"
        },
        {
          "authorId": "1491319211",
          "name": "Ashwath Aithal"
        },
        {
          "authorId": "1930128",
          "name": "Nima Tajbakhsh"
        },
        {
          "authorId": "2824500",
          "name": "Pavlo Molchanov"
        }
      ],
      "abstract": "Training a family of large language models targeting multiple scales and deployment objectives is prohibitively expensive, requiring separate training runs for each different size. Recent work on model compression through pruning and knowledge distillation has reduced this cost; however, this process still incurs hundreds of billions of tokens worth of training cost per compressed model. In this paper, we present Nemotron Elastic, a framework for building reasoning-oriented LLMs, including hybrid Mamba-Attention architectures, that embed multiple nested submodels within a single parent model, each optimized for different deployment configurations and budgets. Each of these submodels shares weights with the parent model and can be extracted zero-shot during deployment without additional training or fine-tuning. We enable this functionality through an end-to-end trained router, tightly coupled to a two-stage training curriculum designed specifically for reasoning models. We additionally introduce group-aware SSM elastification that preserves Mamba's structural constraints, heterogeneous MLP elastification, normalized MSE-based layer importance for improved depth selection, and knowledge distillation enabling simultaneous multi-budget optimization. We apply Nemotron Elastic to the Nemotron Nano V2 12B model, simultaneously producing a 9B and a 6B model using only 110B training tokens; this results in over 360x cost reduction compared to training model families from scratch, and around 7x compared to SoTA compression techniques. Each of the nested models performs on par or better than the SoTA in accuracy. Moreover, unlike other compression methods, the nested capability of our approach allows having a many-in-one reasoning model that has constant deployment memory against the number of models in the family."
    },
    {
      "paperId": "b73797fd7a96059232a6c7197a13600f2f3d4976",
      "externalIds": {
        "PubMedCentral": "12675397",
        "DOI": "10.3389/frai.2025.1687983",
        "CorpusId": 283204642,
        "PubMed": "41356667"
      },
      "corpusId": 283204642,
      "title": "StaBle-MambaNet: structure-aware and blur-guided lane detection with Mamba",
      "venue": "Frontiers in Artificial Intelligence",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12675397, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2274924540",
          "name": "Xiaoyu Zhang"
        },
        {
          "authorId": "2394297474",
          "name": "Hongwei Huang"
        },
        {
          "authorId": "2269759310",
          "name": "Xiting Peng"
        },
        {
          "authorId": "2397445577",
          "name": "Xiaoling Zhang"
        },
        {
          "authorId": "2393823484",
          "name": "Lexi Xu"
        },
        {
          "authorId": "2343149186",
          "name": "Yang Yang"
        }
      ],
      "abstract": "The perception system constitutes a critical component of autonomous driving, due to factors such as high-speed motion and complex illumination, camera-captured images often exhibit local blurring, leading to the degradation of lane structure clarity and even temporary disappearance of lane markings, which severely compromises the accuracy and robustness of lane detection. Traditional approaches typically adopt a two-stage strategy of \u201cimage enhancement followed by structural recognition\u201d Initially, the entire image undergoes deblurring or super-resolution reconstruction, followed by lane detection. However, such methods rely on the quality of full-image restoration, exhibit low processing efficiency, and struggle to determine whether the disappearance of lane markings is genuinely caused by image blurring. To address these challenges, this paper proposes an Inter-frame Stability-Aware Blur-enhanced Mamba Network (StaBle-MambaNet), which identifies blurred regions and assesses the presence of potential lane structures without relying on full-image restoration. The method first localizes blurred areas and employs a Structure-Aware Restoration Module to perform directional extrapolation and completion for potential lane line regions. Subsequently, the Blur-Guided Consistency Reasoning Module evaluates structural stability to identify genuine lane regions. Finally, enhanced features are constructed into a spatially continuous token sequence, which is fed into a lightweight state-space model, Mamba, to model the dynamic feature variations in blurred regions while preserving the vertical structural evolution of the image. Experimental results demonstrate that StaBle-MambaNet significantly outperforms existing mainstream methods across multiple public lane datasets (e.g., CULane and CurveLanes), particularly under challenging conditions such as nighttime, occlusion, and curved lanes, exhibiting clear advantages in both detection accuracy and structural stability."
    },
    {
      "paperId": "f9476004b9e50091d8d481f4d2e7e45f2949549e",
      "externalIds": {
        "ArXiv": "2511.17647",
        "CorpusId": 283245071
      },
      "corpusId": 283245071,
      "title": "MamTiff-CAD: Multi-Scale Latent Diffusion with Mamba+ for Complex Parametric Sequence",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.17647, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2395589772",
          "name": "Liyuan Deng"
        },
        {
          "authorId": "2372422653",
          "name": "Yunpeng Bai"
        },
        {
          "authorId": "2355038106",
          "name": "Yongkang Dai"
        },
        {
          "authorId": "2354604461",
          "name": "Xiaoshui Huang"
        },
        {
          "authorId": "2372619346",
          "name": "Hongping Gan"
        },
        {
          "authorId": "2355246398",
          "name": "Dongshuo Huang"
        },
        {
          "authorId": "2376092013",
          "name": "Jiacheng Hao"
        },
        {
          "authorId": "2356001820",
          "name": "Yilei Shi"
        }
      ],
      "abstract": "Parametric Computer-Aided Design (CAD) is crucial in industrial applications, yet existing approaches often struggle to generate long sequence parametric commands due to complex CAD models'geometric and topological constraints. To address this challenge, we propose MamTiff-CAD, a novel CAD parametric command sequences generation framework that leverages a Transformer-based diffusion model for multi-scale latent representations. Specifically, we design a novel autoencoder that integrates Mamba+ and Transformer, to transfer parameterized CAD sequences into latent representations. The Mamba+ block incorporates a forget gate mechanism to effectively capture long-range dependencies. The non-autoregressive Transformer decoder reconstructs the latent representations. A diffusion model based on multi-scale Transformer is then trained on these latent embeddings to learn the distribution of long sequence commands. In addition, we also construct a dataset that consists of long parametric sequences, which is up to 256 commands for a single CAD model. Experiments demonstrate that MamTiff-CAD achieves state-of-the-art performance on both reconstruction and generation tasks, confirming its effectiveness for long sequence (60-256) CAD model generation."
    },
    {
      "paperId": "decd1f7e19d4a9cf53dff076c9398b7e5c601fb8",
      "externalIds": {
        "DOI": "10.3390/rs17223769",
        "CorpusId": 283148702
      },
      "corpusId": 283148702,
      "title": "Fine-Grained Multispectral Fusion for Oriented Object Detection in Remote Sensing",
      "venue": "Remote Sensing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/rs17223769?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/rs17223769, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2393490797",
          "name": "Xin Lan"
        },
        {
          "authorId": "2301753040",
          "name": "Shaolin Zhang"
        },
        {
          "authorId": "2394249288",
          "name": "Yuhao Bai"
        },
        {
          "authorId": "2243779484",
          "name": "Xiaolin Qin"
        }
      ],
      "abstract": "Infrared\u2013visible-oriented object detection aims to combine the strengths of both infrared and visible images, overcoming the limitations of a single imaging modality to achieve more robust detection with oriented bounding boxes under diverse environmental conditions. However, current methods often suffer from two issues: (1) modality misalignment caused by hardware and annotation errors, leading to inaccurate feature fusion that degrades downstream task performance; and (2) insufficient directional priors in square convolutional kernels, impeding robust object detection with diverse directions, especially in densely packed scenes. To tackle these challenges, in this paper, we propose a novel method, Fine-Grained Multispectral Fusion (FGMF), for oriented object detection in the paired aerial images. Specifically, we design a dual-enhancement and fusion module (DEFM) to obtain the calibrated and complementary features through weighted addition and subtraction-based attention mechanisms. Furthermore, we propose an orientation aggregation module (OAM) that employs large rotated strip convolutions to capture directional context and long-range dependencies. Extensive experiments on the DroneVehicle and VEDAI datasets demonstrate the effectiveness of our proposed method, yielding impressive results with accuracies of 80.2% and 66.3%, respectively. These results highlight the effectiveness of FGMF in oriented object detection within complex remote sensing scenarios."
    },
    {
      "paperId": "2e4b3e6ee65deecb50e7d9e86b50e4aa9b8ee8b8",
      "externalIds": {
        "DOI": "10.3390/ai6110298",
        "CorpusId": 283194795
      },
      "corpusId": 283194795,
      "title": "AdaLite: A Distilled AdaBins Model for Depth Estimation on Resource-Limited Devices",
      "venue": "Applied Informatics",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/ai6110298?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/ai6110298, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2393700300",
          "name": "Mohammed Chaouki Ziara"
        },
        {
          "authorId": "11029952",
          "name": "Mohamed Elbahri"
        },
        {
          "authorId": "2307309762",
          "name": "Nasreddine Taleb"
        },
        {
          "authorId": "1748777",
          "name": "K. Kpalma"
        },
        {
          "authorId": "102519884",
          "name": "Sid Ahmed El Mehdi Ardjoun"
        }
      ],
      "abstract": "This paper presents AdaLite, a knowledge distillation framework for monocular depth estimation designed for efficient deployment on resource-limited devices, without relying on quantization or pruning. While large-scale depth estimation networks achieve high accuracy, their computational and memory demands hinder real-time use. To address this problem, a large model is adopted as a teacher, and a compact encoder\u2013decoder student with few trainable parameters is trained under a dual-supervision scheme that aligns its predictions with both teacher feature maps and ground-truth depths. AdaLite is evaluated on the NYUv2, SUN-RGBD and KITTI benchmarks using standard depth metrics and deployment-oriented measures, including inference latency. The distilled model achieves a 94% reduction in size and reaches 1.02 FPS on a Raspberry Pi 2 (2 GB CPU), while preserving 96.8% of the teacher\u2019s accuracy (\u03b41) and providing over 11\u00d7 faster inference. These results demonstrate the effectiveness of distillation-driven compression for real-time depth estimation in resource-limited environments. The code is publically available."
    },
    {
      "paperId": "97c1433f6535adc1ea43ea8912011438a31f9f9d",
      "externalIds": {
        "ArXiv": "2511.21726",
        "CorpusId": 283439130
      },
      "corpusId": 283439130,
      "title": "Goal-Directed Search Outperforms Goal-Agnostic Memory Compression in Long-Context Memory Tasks",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.21726, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2352079425",
          "name": "Yicong Zheng"
        },
        {
          "authorId": "2348450181",
          "name": "Kevin L. McKee"
        },
        {
          "authorId": "2238332426",
          "name": "Thomas Miconi"
        },
        {
          "authorId": "2395541920",
          "name": "Zacharie Bugaud"
        },
        {
          "authorId": "143826117",
          "name": "Mick Van Gelderen"
        },
        {
          "authorId": "2077048287",
          "name": "Jed McCaleb"
        }
      ],
      "abstract": "How to enable human-like long-term memory in large language models (LLMs) has been a central question for unlocking more general capabilities such as few-shot generalization. Existing memory frameworks and benchmarks focus on finding the optimal memory compression algorithm for higher performance in tasks that require recollection and sometimes further reasoning. However, such efforts have ended up building more human bias into the compression algorithm, through the search for the best prompts and memory architectures that suit specific benchmarks, rather than finding a general solution that would work on other data distributions. On the other hand, goal-directed search on uncompressed information could potentially exhibit superior performance because compression is lossy, and a predefined compression algorithm will not fit all raw data distributions. Here we present SUMER (Search in Uncompressed Memory via Experience Replay), an end-to-end reinforcement learning agent with verifiable reward (RLVR) that learns to use search tools to gather information and answer a target question. On the LoCoMo dataset for long-context conversation understanding, SUMER with Qwen2.5-7B-Instruct learned to use search tools and outperformed all other biased memory compression approaches and also the full-context baseline, reaching SOTA performance (43% gain over the prior best). We demonstrate that a simple search method applied to raw data outperforms goal-agnostic and biased compression algorithms in current long-context memory tasks, arguing for new paradigms and benchmarks that are more dynamic and autonomously scalable. Code for SUMER and all implemented baselines is publicly available at https://github.com/zycyc/SUMER."
    },
    {
      "paperId": "bed4ba158eb340f6c5b6fb715a638e95ccd26f21",
      "externalIds": {
        "DOI": "10.1109/CCISP67522.2025.11281978",
        "CorpusId": 283749299
      },
      "corpusId": 283749299,
      "title": "SelectiveSSM-LTMNet: A Medical Image Registration Network with Selective State Space and Long-Term Memory for Large Deformations",
      "venue": "2025 10th International Conference on Communication, Image and Signal Processing (CCISP)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CCISP67522.2025.11281978?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CCISP67522.2025.11281978, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2397996879",
          "name": "Boyan Li"
        },
        {
          "authorId": "2397985793",
          "name": "Xuan Xie"
        },
        {
          "authorId": null,
          "name": "Jiajie Ding"
        },
        {
          "authorId": null,
          "name": "Lin Zhang"
        }
      ],
      "abstract": "The proposed 'Mamba-Titans' network is named for its integration of Mamba-inspired Selective State Space Module (SelectiveSSM) and Titans-based Long-Term Memory (LTM) mechanism, aiming to address large deformation and cross-modal challenges in medical image registration. In medical image registration, traditional methods are difficult to effectively handle large deformation regions and modal differences. This paper proposes a registration network that integrates the Mamba Selective State Space Module (SelectiveSSM) and the Titans Long-Term Memory Mechanism (LTM). It achieves coarse-to-fine deformation field optimization through a multi-resolution architecture and employs a dual similarity loss function (MI + MIND + smooth regularization) to constrain feature alignment. Experimental results show that this method achieves an average DICE coefficient of $0.7901 \\pm 0.2042$ on the test set, an improvement of 0.04 compared to the original model; the average HD95 is reduced to $4.1993 \\pm 16.9101 ~\\text{mm}$ (a decrease of 2.8 mm), the target registration error (TRE) is $1.9337 \\pm 1.3665 ~\\text{mm}$ (a reduction of 0.8 mm), and the SSIM reaches $0.9155 \\pm 0.0540$ (an increase of 0.063), effectively verifying the adaptive attention ability of SelectiveSSM to deformation regions and the memory generalization ability of LTM for anatomical features, providing an efficient solution for large deformation medical image registration."
    },
    {
      "paperId": "82fafcd97bdbae69336ddc54e941b3693ce95ab8",
      "externalIds": {
        "DOI": "10.1109/CCISP67522.2025.11282113",
        "CorpusId": 283749395
      },
      "corpusId": 283749395,
      "title": "The Evolution of Medical Image Processing Technology and the In-Depth Integration of Clinical Value",
      "venue": "2025 10th International Conference on Communication, Image and Signal Processing (CCISP)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CCISP67522.2025.11282113?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CCISP67522.2025.11282113, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2320743771",
          "name": "Zhao Feng"
        },
        {
          "authorId": "2171902746",
          "name": "Xu Hua"
        }
      ],
      "abstract": "Medical image processing, as a bridge connecting imaging technology and clinical diagnosis and treatment, has evolved from early film digitization to a new era of intelligent analysis. Its core value lies not only in transforming abstract imaging signals into quantifiable diagnostic evidence but also in driving the transformation of disease diagnosis and treatment models from experience-driven to data-driven through technological innovation. With the deep integration of multimodal imaging technology, artificial intelligence algorithms, and clinical needs, medical image processing is facing unprecedented development opportunities and challenges, with each step of its technological evolution closely linked to patients' lives and health."
    },
    {
      "paperId": "a61a32b4c4ac527edabb30ab5cd124e27e0862e5",
      "externalIds": {
        "DOI": "10.1145/3777465",
        "CorpusId": 283162867
      },
      "corpusId": 283162867,
      "title": "MCFINet: A Cost-Efficient Multi-Channel Feature Integration Network for Surface Scenarios Image Super-Resolution",
      "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMCCAP)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3777465?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3777465, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2393580422",
          "name": "Liangcheng Zhao"
        },
        {
          "authorId": "2238322007",
          "name": "Yueying Wang"
        },
        {
          "authorId": "2333172047",
          "name": "Yuhao Qing"
        },
        {
          "authorId": "2296069081",
          "name": "Dan Zeng"
        },
        {
          "authorId": "2295276259",
          "name": "Li Xu"
        }
      ],
      "abstract": "Convolutional Neural Network (CNN) and vision Transformer (ViT) have revolutionized the field of image super - resolution (SR). However, their complexity poses challenges for resource - constrained scenarios, particularly due to the high computational demands of Transformers and their excessive reliance on global information. To tackle these challenges, we propose a Multi - channel Feature Integration Network (MCFINet), designed to maximize input pixel utilization while minimizing computational overhead. It integrates both local and global features within the channels, thereby exploiting their complementary advantages. First, the designed Feature Integration Block (FIB) effectively captures local information and improves visual quality by enhancing the mapping of non - local features. Subsequently, we utilize the Adaptive Channel Fusion Block (ACFB), which strengthens the interaction between features and channels while maintaining computational efficiency. Finally, for SR task on resource-constrained surface scenarios, we propose a more suitable pre-training method, which further boosts the model\u2019s learning ability. Evaluation results indicate that the proposed MCFINet achieves a better balance between lightweight design and high-quality restoration on both standard evaluation datasets and water surface target datasets. Specifically, compared to the traditional SwinIR-L, MCFINet reduces model training time and runtime by 12% on the test set, while also decreasing model complexity by 43%. Our codes are available at https://github.com/Lcasjz/MCFINet."
    },
    {
      "paperId": "36490d64e9dd700fe4969ea1a626579cacd7219d",
      "externalIds": {
        "ArXiv": "2511.15927",
        "CorpusId": 283110451
      },
      "corpusId": 283110451,
      "title": "Breaking the Bottleneck with DiffuApriel: High-Throughput Diffusion LMs with Mamba Backbone",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.15927, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2393253809",
          "name": "Vaibhav Singh"
        },
        {
          "authorId": "145191120",
          "name": "O. Ostapenko"
        },
        {
          "authorId": "2393210880",
          "name": "Pierre-Andr'e Noel"
        },
        {
          "authorId": "2294607304",
          "name": "Torsten Scholak"
        }
      ],
      "abstract": "Diffusion-based language models have recently emerged as a promising alternative to autoregressive generation, yet their reliance on Transformer backbones limits inference efficiency due to quadratic attention and KV-cache overhead. In this work, we introduce DiffuApriel, a masked diffusion language model built on a bidirectional Mamba backbone that combines the diffusion objective with linear-time sequence modeling. DiffuApriel matches the performance of Transformer-based diffusion models while achieving up to 4.4x higher inference throughput for long sequences with a 1.3B model. We further propose DiffuApriel-H, a hybrid variant that interleaves attention and mamba layers, offering up to 2.6x throughput improvement with balanced global and local context modeling. Our results demonstrate that bidirectional state-space architectures serve as strong denoisers in masked diffusion LMs, providing a practical and scalable foundation for faster, memory-efficient text generation."
    },
    {
      "paperId": "732f918a1c630af349684a5debbf0cf9189862d4",
      "externalIds": {
        "DBLP": "conf/ccs/WiedemeierKFZPC25",
        "DOI": "10.1145/3719027.3765040",
        "CorpusId": 281338023
      },
      "corpusId": 281338023,
      "title": "Walking The Last Mile: Studying Decompiler Output Correction in Practice",
      "venue": "Conference on Computer and Communications Security",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3719027.3765040?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3719027.3765040, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2219236005",
          "name": "Joshua Wiedemeier"
        },
        {
          "authorId": "2373603504",
          "name": "Simon Klancher"
        },
        {
          "authorId": "2380980269",
          "name": "Joel Flores"
        },
        {
          "authorId": "2367627203",
          "name": "Max Zheng"
        },
        {
          "authorId": "2381347285",
          "name": "Jaehyun Park"
        },
        {
          "authorId": "2367333612",
          "name": "Sang Kil Cha"
        },
        {
          "authorId": "3344254",
          "name": "Kangkook Jee"
        }
      ],
      "abstract": "The increasing prevalence of Python has spurred interest in decompiling Python PYC bytecode. This work presents the first large-scale study on human-assisted Python decompilation in the wild, leveraging extensive data from pylingual.io, spanning 181,646 PYC binaries, 9,003 user-submitted patches, and 393 accuracy-verified patches. We investigate how reverse engineers respond to inaccurate decompilation and identify factors influencing their efforts to achieve accurate decompilation. We complement this unprecedented observational data with a controlled user study that isolates the technical difficulty of patching imperfect Python decompilations. By contrasting real-world patching behavior with that of the controlled setting, we discover that reversers' decision to repair a decompilation result is more strongly driven by the semantic content of the program (e.g., malware binaries or malicious tools) than by the technical difficulty of the patch. That is, a reverser's motivation is more important than their expertise. Our study reveals common patterns observed in the patching process, including how users approached the patching task, the types of errors they encountered, and the strategies they employed to resolve them. We also examine the strengths and limitations of assistive tools in the pursuit of perfect decompilation. Our findings offer unique insights into the practical dynamics of human-decompiler interaction, providing actionable recommendations for integrating human intelligence into the decompilation workflow and demonstrating the research potential of reliable decompilation accuracy verification."
    },
    {
      "paperId": "1dc2ad7db494573862312f9aa1f68c769f2898c6",
      "externalIds": {
        "ArXiv": "2511.15077",
        "CorpusId": 283103033
      },
      "corpusId": 283103033,
      "title": "MambaTrack3D: A State Space Model Framework for LiDAR-Based Object Tracking under High Temporal Variation",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.15077, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "120675386",
          "name": "Shengjing Tian"
        },
        {
          "authorId": "2280947579",
          "name": "Yinan Han"
        },
        {
          "authorId": "2188129942",
          "name": "Xiantong Zhao"
        },
        {
          "authorId": "2393977358",
          "name": "Xuehu Liu"
        },
        {
          "authorId": "2393137686",
          "name": "Qi Lang"
        }
      ],
      "abstract": "Dynamic outdoor environments with high temporal variation (HTV) pose significant challenges for 3D single object tracking in LiDAR point clouds. Existing memory-based trackers often suffer from quadratic computational complexity, temporal redundancy, and insufficient exploitation of geometric priors. To address these issues, we propose MambaTrack3D, a novel HTV-oriented tracking framework built upon the state space model Mamba. Specifically, we design a Mamba-based Inter-frame Propagation (MIP) module that replaces conventional single-frame feature extraction with efficient inter-frame propagation, achieving near-linear complexity while explicitly modeling spatial relations across historical frames. Furthermore, a Grouped Feature Enhancement Module (GFEM) is introduced to separate foreground and background semantics at the channel level, thereby mitigating temporal redundancy in the memory bank. Extensive experiments on KITTI-HTV and nuScenes-HTV benchmarks demonstrate that MambaTrack3D consistently outperforms both HTV-oriented and normal-scenario trackers, achieving improvements of up to 6.5 success and 9.5 precision over HVTrack under moderate temporal gaps. On the standard KITTI dataset, MambaTrack3D remains highly competitive with state-of-the-art normal-scenario trackers, confirming its strong generalization ability. Overall, MambaTrack3D achieves a superior accuracy-efficiency trade-off, delivering robust performance across both specialized HTV and conventional tracking scenarios."
    },
    {
      "paperId": "09daa6c407f5552ea564fe20b5b7f10126674c92",
      "externalIds": {
        "ArXiv": "2511.15244",
        "CorpusId": 283103462
      },
      "corpusId": 283103462,
      "title": "Context Cascade Compression: Exploring the Upper Limits of Text Compression",
      "venue": "",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.15244, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2393428222",
          "name": "Fanfan Liu"
        },
        {
          "authorId": "2396374806",
          "name": "Haibo Qiu"
        }
      ],
      "abstract": "Million-level token inputs in long-context tasks pose significant computational and memory challenges for Large Language Models (LLMs). Recently, DeepSeek-OCR conducted research into the feasibility of Contexts Optical Compression and achieved preliminary results. Inspired by this, we introduce Context Cascade Compression C3 to explore the upper limits of text compression. Our method cascades two LLMs of different sizes to handle the compression and decoding tasks. Specifically, a small LLM, acting as the first stage, performs text compression by condensing a long context into a set of latent tokens (e.g., 32 or 64 in length), achieving a high ratio of text tokens to latent tokens. A large LLM, as the second stage, then executes the decoding task on this compressed context. Experiments show that at a 20x compression ratio (where the number of text tokens is 20 times the number of latent tokens), our model achieves 98% decoding accuracy, compared to approximately 60% for DeepSeek-OCR. When we further increase the compression ratio to 40x, the accuracy is maintained at around 93%. This indicates that in the domain of context compression, C3 Compression demonstrates superior performance and feasibility over optical character compression. C3 uses a simpler, pure-text pipeline that ignores factors like layout, color, and information loss from a visual encoder. This also suggests a potential upper bound for compression ratios in future work on optical character compression, OCR, and related fields. Codes and model weights are publicly accessible at https://github.com/liufanfanlff/C3-Context-Cascade-Compression"
    },
    {
      "paperId": "fa044e254099dc2ad6126968a2ab1ffa3e1ed345",
      "externalIds": {
        "ArXiv": "2511.15247",
        "CorpusId": 283103622
      },
      "corpusId": 283103622,
      "title": "Addressing the gravitational collapse of a massless scalar field with Physics-Informed Neural Networks",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.15247, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2393140034",
          "name": "Antonio Ferrer-S'anchez"
        },
        {
          "authorId": "2393140043",
          "name": "Nino Villanueva-Espinosa"
        },
        {
          "authorId": "2393370402",
          "name": "Carlos Hernani Morales"
        },
        {
          "authorId": "2393138995",
          "name": "Roberto Ruiz de Austri-Bazan"
        },
        {
          "authorId": "2207841190",
          "name": "J. Font"
        },
        {
          "authorId": "2377795740",
          "name": "J. D. Mart'in-Guerrero"
        },
        {
          "authorId": "1920350",
          "name": "M. Choptuik"
        }
      ],
      "abstract": "The gravitational collapse of a massless scalar field remains a demanding benchmark for numerical methods in numerical relativity, as it exhibits critical behavior at the boundary between dispersion and black hole formation. In this work we revisit this problem by relying on Physics-Informed Neural Networks (PINNs) as flexible solvers for partial differential equations, thereby providing a comparative assessment of several recent neural architectures. Building on the Einstein-massless-Klein-Gordon formulation in polar-areal coordinates, we consider four initial-value problems encompassing subcritical, critical, and supercritical regimes and use high-resolution finite-difference simulations as reference solutions. Our study is primarily comparative: we evaluate several state-of-the-art deep learning architectures, including vanilla and high-precision PINNs, sinusoidal-feature and quadratic-residual variants, and Kolmogorov-Arnold Networks, all trained under a common loss design that encodes the field equations, boundary conditions, and causal time-space enforcement, together with a novel adaptive spacetime sampling. Within this framework we also introduce ModPINN, a modest modification of standard PINNs that augments standard multilayer perceptrons with coordinate embeddings, quadratic layers, and other common ingredients in recent literature. This study shows that deep-learning-based methods can reproduce finite-difference solutions for the scalar field and the spacetime metric with competitive accuracy using significantly fewer collocation points than more traditional methodologies. While no single architecture dominates in all regimes, ModPINN achieves particularly stable and accurate solutions near criticality, indicating that suitably designed embeddings and adaptive sampling can enhance the robustness of PINNs for challenging gravitational-collapse scenarios."
    },
    {
      "paperId": "df69551b2083889b26d671619715e5ac34aa71af",
      "externalIds": {
        "ArXiv": "2511.15833",
        "CorpusId": 283109725
      },
      "corpusId": 283109725,
      "title": "EfficientSAM3: Progressive Hierarchical Distillation for Video Concept Segmentation from SAM1, 2, and 3",
      "venue": "",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.15833, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2283845582",
          "name": "Chengxi Zeng"
        },
        {
          "authorId": "2393248401",
          "name": "Yuxuan Jiang"
        },
        {
          "authorId": "2397438778",
          "name": "Aaron Zhang"
        }
      ],
      "abstract": "The Segment Anything Model 3 (SAM3) advances visual understanding with Promptable Concept Segmentation (PCS) across images and videos, but its unified architecture (shared vision backbone, DETR-style detector, dense-memory tracker) remains prohibitive for on-device use. We present EfficientSAM3, a family of efficient models built on Progressive Hierarchical Distillation (PHD) that transfers capability from SAM3 to lightweight students in three stages: (1) Encoder Distillation aligns image features via prompt-in-the-loop training on SA-1B; (2) Temporal Memory Distillation replaces dense memory with a compact Perceiver-based module trained on SA-V to compress and retrieve spatiotemporal features efficiently; and (3) End-to-End Fine-Tuning refines the full pipeline on the official SAM3 PCS data to preserve concept-level performance. PHD yields a spectrum of student variants using RepViT, TinyViT, and EfficientViT backbones, enabling on-device concept segmentation and tracking while maintaining high fidelity to teacher behavior. We benchmark on popular VOS datasets, and compare with varies of releated work, achieing strong performance-efficiency trade-offs."
    },
    {
      "paperId": "f9e3416f5cb2f207f35d0b372c94d453e7c41e23",
      "externalIds": {
        "ArXiv": "2511.15915",
        "CorpusId": 283109792
      },
      "corpusId": 283109792,
      "title": "AccelOpt: A Self-Improving LLM Agentic System for AI Accelerator Kernel Optimization",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.15915, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2342648907",
          "name": "Genghan Zhang"
        },
        {
          "authorId": "2353419706",
          "name": "Shaowei Zhu"
        },
        {
          "authorId": "2028616391",
          "name": "Anjiang Wei"
        },
        {
          "authorId": "2367119287",
          "name": "Zhenyu Song"
        },
        {
          "authorId": "2386820887",
          "name": "Allen Nie"
        },
        {
          "authorId": "2394131389",
          "name": "Zhen Jia"
        },
        {
          "authorId": "1920997",
          "name": "Nandita Vijaykumar"
        },
        {
          "authorId": "2253834098",
          "name": "Yida Wang"
        },
        {
          "authorId": "1746638",
          "name": "K. Olukotun"
        }
      ],
      "abstract": "We present AccelOpt, a self-improving large language model (LLM) agentic system that autonomously optimizes kernels for emerging AI acclerators, eliminating the need for expert-provided hardware-specific optimization knowledge. AccelOpt explores the kernel optimization space through iterative generation, informed by an optimization memory that curates experiences and insights from previously encountered slow-fast kernel pairs. We build NKIBench, a new benchmark suite of AWS Trainium accelerator kernels with varying complexity extracted from real-world LLM workloads to evaluate the effectiveness of AccelOpt. Our evaluation confirms that AccelOpt's capability improves over time, boosting the average percentage of peak throughput from $49\\%$ to $61\\%$ on Trainium 1 and from $45\\%$ to $59\\%$ on Trainium 2 for NKIBench kernels. Moreover, AccelOpt is highly cost-effective: using open-source models, it matches the kernel improvements of Claude Sonnet 4 while being $26\\times$ cheaper."
    },
    {
      "paperId": "c0f9a3e6fb80a5ac8ab17919cc4c3c27af9d0bf3",
      "externalIds": {
        "DOI": "10.1101/2025.01.24.634217",
        "CorpusId": 275932868
      },
      "corpusId": 275932868,
      "title": "RegFormer: A Single-Cell Foundation Model Powered by Gene Regulatory Hierarchies",
      "venue": "bioRxiv",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1101/2025.01.24.634217?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1101/2025.01.24.634217, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2274216312",
          "name": "Luni Hu"
        },
        {
          "authorId": "2276615746",
          "name": "Hua Qin"
        },
        {
          "authorId": "2345331843",
          "name": "Yilin Zhang"
        },
        {
          "authorId": "2393262595",
          "name": "Yi Lu"
        },
        {
          "authorId": "2332348082",
          "name": "Ping Qiu"
        },
        {
          "authorId": "2387739748",
          "name": "Zhihan Guo"
        },
        {
          "authorId": "2271356066",
          "name": "Lei Cao"
        },
        {
          "authorId": "2284749793",
          "name": "Wenjian Jiang"
        },
        {
          "authorId": "2332909055",
          "name": "Qianqian Chen"
        },
        {
          "authorId": "2342586943",
          "name": "Yanbang Shang"
        },
        {
          "authorId": "2195902683",
          "name": "Tianyi Xia"
        },
        {
          "authorId": "2297346236",
          "name": "Ziqing Deng"
        },
        {
          "authorId": "2257813434",
          "name": "Xun Xu"
        },
        {
          "authorId": "2257783139",
          "name": "Shuangsang Fang"
        },
        {
          "authorId": "2332460511",
          "name": "Yuxiang Li"
        },
        {
          "authorId": "2188149106",
          "name": "Yong Zhang"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "1f345a10d8b36695f775f7cc3c2f3c563553ef73",
      "externalIds": {
        "PubMedCentral": "12672870",
        "DOI": "10.3389/fpls.2025.1718258",
        "CorpusId": 283149856,
        "PubMed": "41346834"
      },
      "corpusId": 283149856,
      "title": "MTMixG-Net: mixture of Transformer and Mamba network with a dual-path gating mechanism for plant gene expression prediction",
      "venue": "Frontiers in Plant Science",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12672870, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": null,
          "name": "Fei Guo"
        },
        {
          "authorId": "2342723204",
          "name": "Wenjuan Li"
        },
        {
          "authorId": "2342958700",
          "name": "Aihong Lu"
        },
        {
          "authorId": "98403769",
          "name": "Rongzhen Feng"
        },
        {
          "authorId": "2342911401",
          "name": "Wu Fang"
        }
      ],
      "abstract": "Accurate prediction of plant gene expression is essential for elucidating the regulatory mechanisms underlying plant development and stress adaptation. Traditional experimental approaches such as microarrays and RNA sequencing have provided valuable insights but remain limited in capturing the complexity and diversity of genomic regulation. Recent advances in deep learning have shown promise, yet existing models often struggle to generalize across species and to efficiently model long-range dependencies within genomic sequences. To address these challenges, we propose MTMixG-Net, a novel deep learning framework that integrates Transformer and Mamba architectures with a gating mechanism for enhanced gene expression prediction. MTMixG-Net consists of three main modules: the mixture of Transformer and Mamba encoder (MTMixEnc), the dual-path gating mechanism (DPGM), and the residual CNN chain (ResCNNChn). The MTMixEnc combines the self-attention capacity of Transformers with the state-space efficiency of Mamba to capture multi-scale regulatory dependencies while maintaining low computational complexity. The DPGM adaptively refines feature selection through dynamic gating, allowing the model to focus on the most informative representations. Finally, the ResCNNChn leverages a sequence of residual CNN blocks to extract high-level features and further boost predictive accuracy. We validate MTMixG-Net on multiple plant genomic datasets, demonstrating its superior accuracy and computational efficiency compared to existing methods. Our results highlight the potential of MTMixG-Net as a powerful tool for advancing plant genomics research and crop improvement strategies."
    },
    {
      "paperId": "7fc3201218fb81952f56f1247da83723055b3cc3",
      "externalIds": {
        "DOI": "10.1145/3777469",
        "CorpusId": 283205750
      },
      "corpusId": 283205750,
      "title": "MatPose: A 2D Human Pose Estimation Model with Hybrid Mamba-Transformer",
      "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMCCAP)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3777469?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3777469, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "46520636",
          "name": "Wenjun Xie"
        },
        {
          "authorId": "2394359252",
          "name": "Kejun Chen"
        },
        {
          "authorId": "2238909236",
          "name": "Dong Wang"
        },
        {
          "authorId": "2193475528",
          "name": "Xiaoping Liu"
        }
      ],
      "abstract": "Recently, Mamba has gained widespread attention due to its ability to model long-range dependencies with linear computational complexity. To explore the application of Mamba in 2D human pose estimation, we propose MatPose, a Mamba-Transformer hybrid model specifically designed for efficient 2D human pose estimation. The model aims to combine Mamba\u2019s efficient modeling of long-range dependencies with the powerful global context modeling capabilities of the Transformer to effectively extract human pose keypoints. First, to address the lack of local features when Mamba is applied to computer vision tasks, we design a Cross-Stage Multi-Scale Convolution (CSMSC) module by integrating multi-scale convolution, cross-stage feature fusion, and spatial attention mechanisms to effectively extract local features. Then, to mitigate the long-range forgetting issue inherent in Mamba, we shorten the sequence length using the Conv-Reduce operation. In addition, we design a Channel Selection Attention (CSA) mechanism to compensate for the feature loss caused by the Conv-Reduce operation. Finally, to explore a suitable integration method for the Mamba-Transformer hybrid model in 2D human pose estimation, we conduct a comprehensive ablation study on the feasibility of integrating Mamba and Transformer models. Experimental results show that the proposed method, compared to the baseline model, improves performance while reducing computational overhead. On the COCO val2017 dataset, MatPose achieves an AP of 74.6 with only 5.18 GFLOPs, outperforming most existing human pose estimation models."
    },
    {
      "paperId": "76d3d69c37dc9af8ef15a31548a91b6cb7d11021",
      "externalIds": {
        "DOI": "10.1117/12.3073774",
        "CorpusId": 282027098
      },
      "corpusId": 282027098,
      "title": "GRAM: a graph reasoning and attention-enhanced Mamba model for detecting infrared small targets",
      "venue": "SPIE/COS Photonics Asia",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1117/12.3073774?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1117/12.3073774, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2385198115",
          "name": "Heming Qian"
        },
        {
          "authorId": "2392217625",
          "name": "Ting Sun"
        }
      ],
      "abstract": "Detecting tiny infrared objects constitutes a significant difficulty within computer vision , requiring the detection and precise localization of minute objects within thermal imagery that typically span just a few pixels. The extremely limited target scale, coupled with the frequently cluttered backgrounds in infrared scenes, makes the task highly demanding. To address these difficulties, we introduce GRAM-NET, a deep learning model that improves detection performance through the integration of multiple specially designed modules. In particular, GRAM-NET integrates a Context-aware Multi-Branch Semantic Aggregation (C-MSA) block and a graph-based reasoning component. The C-MSA block adopts a parallel multi-branch extraction mechanism to obtain representations across diverse scales and semantic levels, while channel reweighting is applied to emphasize information conducive to small target discrimination, thereby boosting saliency and reducing both noise and misclassification. Meanwhile, the graph reasoning component leverages structured relational modeling to capture long-range dependencies, further combining multi-scale graph construction with bidirectional fusion for improved cross-scale awareness. Extensive experiments on NUDT-SIRST, NUAA-SIRST, and IRSTD-1K indicate that GRAMNET achieves superior detection performance compared with current leading approaches."
    },
    {
      "paperId": "7288ae64953fd28234edb5ec680ad4017e974b92",
      "externalIds": {
        "ArXiv": "2511.14503",
        "CorpusId": 283080519
      },
      "corpusId": 283080519,
      "title": "Parameter Aware Mamba Model for Multi-task Dense Prediction",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.14503, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2392992395",
          "name": "Xinzhuo Yu"
        },
        {
          "authorId": "2936850",
          "name": "Yunzhi Zhuge"
        },
        {
          "authorId": "2340013573",
          "name": "Sitong Gong"
        },
        {
          "authorId": "2332354510",
          "name": "Lu Zhang"
        },
        {
          "authorId": "2338055979",
          "name": "Pingping Zhang"
        },
        {
          "authorId": "2252297278",
          "name": "Huchuan Lu"
        }
      ],
      "abstract": "Understanding the inter-relations and interactions between tasks is crucial for multi-task dense prediction. Existing methods predominantly utilize convolutional layers and attention mechanisms to explore task-level interactions. In this work, we introduce a novel decoder-based framework, Parameter Aware Mamba Model (PAMM), specifically designed for dense prediction in multi-task learning setting. Distinct from approaches that employ Transformers to model holistic task relationships, PAMM leverages the rich, scalable parameters of state space models to enhance task interconnectivity. It features dual state space parameter experts that integrate and set task-specific parameter priors, capturing the intrinsic properties of each task. This approach not only facilitates precise multi-task interactions but also allows for the global integration of task priors through the structured state space sequence model (S4). Furthermore, we employ the Multi-Directional Hilbert Scanning method to construct multi-angle feature sequences, thereby enhancing the sequence model's perceptual capabilities for 2D data. Extensive experiments on the NYUD-v2 and PASCAL-Context benchmarks demonstrate the effectiveness of our proposed method. Our code is available at https://github.com/CQC-gogopro/PAMM."
    },
    {
      "paperId": "2fd66e1b088a3c72b3e76104d733fe91c37a9dc3",
      "externalIds": {
        "PubMedCentral": "12669175",
        "DOI": "10.3389/fnins.2025.1699700",
        "CorpusId": 283120658,
        "PubMed": "41341263"
      },
      "corpusId": 283120658,
      "title": "Detection of leptomeningeal angiomas in brain MRI of Sturge-Weber syndrome using multi-scale multi-scan Mamba",
      "venue": "Frontiers in Neuroscience",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12669175, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2360612228",
          "name": "Weiqun Bao"
        },
        {
          "authorId": "2393241401",
          "name": "Chenghao Xue"
        },
        {
          "authorId": "2388649837",
          "name": "Ruisheng Su"
        },
        {
          "authorId": "2278934415",
          "name": "Xindan Hu"
        },
        {
          "authorId": "2393253880",
          "name": "Yuanning Li"
        },
        {
          "authorId": "2278823640",
          "name": "Xiaoqiang Wang"
        },
        {
          "authorId": "2278810014",
          "name": "Tao Tan"
        },
        {
          "authorId": "2278818405",
          "name": "Dake He"
        },
        {
          "authorId": "2394051880",
          "name": "Lin Xu"
        }
      ],
      "abstract": "Objectives Sturge-Weber syndrome (SWS) is a congenital neurological disorder occurring in the early childhood. Timely diagnosis of SWS is essential for proper medical intervention that prevents the development of various neurological issues. Leptomeningeal angiomas (LA) are the clinical manifestation of SWS. Detection of LA is currently performed by manual inspection of the magnetic resonance images (MRI) by experienced neurologist, which is time-consuming and lack of inter-rater consistency. The aim of the present study is to investigate automated LA detection in MRI of SWS patients. Methods A Mamba-based encoder-decoder architecture was employed in the present study. Particularly, a multi-scale multi-scan strategy was proposed to convert 3-D volume into 1-D sequence, enabling capturing long-range dependency with reduced computation complexity. Our dataset consists of 40 SWS patients with T1-enhanced MRI. The proposed model was first pre-trained on a public brain tumor segmentation (BraTS) dataset and then fine-tuned and tested on the SWS dataset using 5-fold cross validation. Results and conclusion Our results show excellent performance of the proposed method, e.g., Dice score of 91.53% and 78.67% for BraTS and SWS, respectively, outperforming several state-of-the-art methods as well as two neurologists. Mamba-based deep learning method can automatically identify LA in MRI images, enabling automated SWS diagnosis in clinical settings."
    },
    {
      "paperId": "7b5893b6026ba9c963662a783125d717b080c7db",
      "externalIds": {
        "DOI": "10.4081/jae.2025.1637",
        "CorpusId": 283194629
      },
      "corpusId": 283194629,
      "title": "Automated tea shoot picking using the YOLO network and Mamba images segmentation for top-view detection with a monocular camera",
      "venue": "Journal of Agricultural Engineering",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.4081/jae.2025.1637?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.4081/jae.2025.1637, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2367014736",
          "name": "Zhiqiang Wang"
        },
        {
          "authorId": "2386645808",
          "name": "Hui Niu"
        },
        {
          "authorId": "2325594202",
          "name": "Jing Zhang"
        },
        {
          "authorId": "2366797529",
          "name": "Wu Zhang"
        },
        {
          "authorId": "2367323503",
          "name": "Jian Mao"
        },
        {
          "authorId": "2387782663",
          "name": "Shengqi Zhang"
        },
        {
          "authorId": "2367558531",
          "name": "Jun Liu"
        },
        {
          "authorId": "2209166156",
          "name": "Guanpeng Zuo"
        },
        {
          "authorId": "2209032797",
          "name": "Zhe Zheng"
        },
        {
          "authorId": "2366972428",
          "name": "Zhenxiang Chi"
        }
      ],
      "abstract": "Detection and localization of tea shoots (one bud with two leaves) are critical steps in the automation of tea harvesting. Using red, green, blue-depth (RGB-D) camera to detect and locate tea shoots from side angles results in significant occlusion of tea shoots, as well as loss of depth information. To achieve automated, intelligent, and precise tea harvesting, this paper proposes a method for detecting and locating tea shoots from the top using a monocular camera. Firstly, the \u201cYou Only Look Once\u201d (YOLO) network is employed to detect tea shoots regions in images collected by the monocular camera and to crop individual tea shoot top images. For these cropped images, a U-shaped images segmentation model based on Mamba is proposed. This model achieves a mean intersection over union (MIoU) of 87.80% and an accuracy (ACC) of 95.63%, precisely locating the specific tea shoots top regions. The center of the circumscribed circle of this region is used as the position for the next step in the picking process, accurately guiding the picking effector to the top of the tea shoot. Finally, the picking effector, controlled by feedback signals from infrared sensors, performs up-and-down reciprocation and cutting actions to complete the picking process. This method effectively avoids the problem of depth information loss during localization with RGB-D camera. To verify the effectiveness of the proposed approach, picking experiments were conducted on HouKui tea within a simulated tea garden environment, achieving a tea shoot picking success rate of 75.54%. The results indicate that this method offers significant application value and provides a new perspective for the development of automated tea shoots picking."
    },
    {
      "paperId": "6e45aa3c728acc1689f8f9ec90cf632ac2d933f6",
      "externalIds": {
        "PubMedCentral": "12627797",
        "DOI": "10.1038/s41467-025-65077-4",
        "CorpusId": 275436972,
        "PubMed": "41253815"
      },
      "corpusId": 275436972,
      "title": "DNALONGBENCH: a benchmark suite for long-range DNA prediction tasks",
      "venue": "Nature Communications",
      "year": 2025,
      "citationCount": 4,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBYNCND",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12627797, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2329727261",
          "name": "Wenduo Cheng"
        },
        {
          "authorId": "6872825",
          "name": "Zhenqiao Song"
        },
        {
          "authorId": "2322446548",
          "name": "Yang Zhang"
        },
        {
          "authorId": "2317115824",
          "name": "Shike Wang"
        },
        {
          "authorId": "2254643264",
          "name": "Danqing Wang"
        },
        {
          "authorId": "2158084692",
          "name": "Muyu Yang"
        },
        {
          "authorId": "2256977467",
          "name": "Lei Li"
        },
        {
          "authorId": "2324134263",
          "name": "Jianxin Ma"
        }
      ],
      "abstract": "Modeling long-range DNA dependencies is crucial for understanding genome structure and function across diverse biological contexts. However, effectively capturing these dependencies, which may span millions of base pairs in tasks such as three-dimensional (3D) chromatin folding prediction, remains a major challenge. A comprehensive benchmark suite for evaluating tasks that rely on long-range dependencies is notably absent. To address this gap, we introduce DNALONGBENCH, a benchmark dataset covering five key genomics tasks with long-range dependencies up to 1 million base pairs: enhancer-target gene interaction, expression quantitative trait loci, 3D genome organization, regulatory sequence activity, and transcription initiation signals. We assess DNALONGBENCH using five methods: a task-specific expert model, a convolutional neural network (CNN)-based model, and three fine-tuned DNA foundation models \u2013 HyenaDNA, Caduceus-Ph, and Caduceus-PS. We envision DNALONGBENCH as a standardized resource to enable comprehensive comparisons and rigorous evaluations of emerging DNA sequence-based deep learning models that account for long-range dependencies. Long-range dependency benchmarks for DNA foundation models are scarce. Here, the authors present DNALONGBENCH to fill this gap, showing that current foundation models still lag behind expert models in capturing long-range genomic dependencies."
    },
    {
      "paperId": "0247003c04f4c7d74fa182fc28d5c438070fdd02",
      "externalIds": {
        "ArXiv": "2511.14390",
        "CorpusId": 283081056
      },
      "corpusId": 283081056,
      "title": "Accelerating Automatic Differentiation of Direct Form Digital Filters",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.14390, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "120977179",
          "name": "Chin-Yun Yu"
        },
        {
          "authorId": "2295987786",
          "name": "Gyorgy Fazekas"
        }
      ],
      "abstract": "We introduce a general formulation for automatic differentiation through direct form filters, yielding a closed-form backpropagation that includes initial condition gradients. The result is a single expression that can represent both the filter and its gradients computation while supporting parallelism. C++/CUDA implementations in PyTorch achieve at least 1000x speedup over naive Python implementations and consistently run fastest on the GPU. For the low-order filters commonly used in practice, exact time-domain filtering with analytical gradients outperforms the frequency-domain method in terms of speed. The source code is available at https://github.com/yoyolicoris/philtorch."
    },
    {
      "paperId": "3c9f2620f830af923f26b5a9c7f208d87e869a15",
      "externalIds": {
        "ArXiv": "2511.14969",
        "DOI": "10.13140/RG.2.2.33632.55045",
        "CorpusId": 283103285
      },
      "corpusId": 283103285,
      "title": "Quality-Controlled Multimodal Emotion Recognition in Conversations with Identity-Based Transfer Learning and MAMBA Fusion",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.14969, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2393070522",
          "name": "Zanxu Wang"
        },
        {
          "authorId": "2370702717",
          "name": "Homayoon Beigi"
        }
      ],
      "abstract": "This paper addresses data quality issues in multimodal emotion recognition in conversation (MERC) through systematic quality control and multi-stage transfer learning. We implement a quality control pipeline for MELD and IEMOCAP datasets that validates speaker identity, audio-text alignment, and face detection. We leverage transfer learning from speaker and face recognition, assuming that identity-discriminative embeddings capture not only stable acoustic and Facial traits but also person-specific patterns of emotional expression. We employ RecoMadeEasy(R) engines for extracting 512-dimensional speaker and face embeddings, fine-tune MPNet-v2 for emotion-aware text representations, and adapt these features through emotion-specific MLPs trained on unimodal datasets. MAMBA-based trimodal fusion achieves 64.8% accuracy on MELD and 74.3% on IEMOCAP. These results show that combining identity-based audio and visual embeddings with emotion-tuned text representations on a quality-controlled subset of data yields consistent competitive performance for multimodal emotion recognition in conversation and provides a basis for further improvement on challenging, low-frequency emotion classes."
    },
    {
      "paperId": "314fd4d3f899d06977f3b3621720178cb5649b94",
      "externalIds": {
        "DOI": "10.1109/TIP.2025.3631445",
        "CorpusId": 283079071,
        "PubMed": "41247886"
      },
      "corpusId": 283079071,
      "title": "MambaMatch: Establishing Reliable Correspondences via Multi-Scale State Space Model",
      "venue": "IEEE Transactions on Image Processing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIP.2025.3631445?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIP.2025.3631445, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2278424934",
          "name": "Xiangyang Miao"
        },
        {
          "authorId": "2163148650",
          "name": "Shunxing Chen"
        },
        {
          "authorId": "2284135319",
          "name": "Xinyu Liu"
        },
        {
          "authorId": "2276561052",
          "name": "Shiping Wang"
        },
        {
          "authorId": "2370884449",
          "name": "Songlin Du"
        },
        {
          "authorId": "2370555179",
          "name": "Lianghua He"
        },
        {
          "authorId": "2275617003",
          "name": "Guobao Xiao"
        }
      ],
      "abstract": "Correspondence pruning aims to identify inliers from correspondences severely disturbed by outliers. Although Transformers and graph neural networks have shown impressive results in this field, they are either limited by a narrow receptive field or encounter quadratic computational complexity. To tackle this challenge, this work pioneers the integration of state space model into correspondence pruning task, proposing a Mamba-based framework named MambaMatch. Specifically, to address the limitations of the Mamba architecture in local consensus modeling, we proposes a multi-scale scanning strategy. It first employs an adaptive clustering algorithm to map origin correspondences into spatially coherent feature clusters, constructing a dual-representation space encompassing both full-scale and clustered-scale features. Bidirectional scan operations are then performed at both scales: 1) full-scale scan preserves global structural context, and 2) clustered-scale scan enhances local consistency. Subsequently, a Multi-Scale Interaction layer is designed to dynamically fuse dual-scale features via a cross-attention mechanism, further integrated with a Gated Feed-Forward Network to significantly improve the network\u2019s feature discrimination capability. Extensive experiments validate that MambaMatch surpasses state-of-the-art approaches across multiple benchmarks for two-view geometry estimation. Furthermore, MambaMatch exhibits robust generalization across diverse scenarios, tasks, and feature extractors. The source code is available at: https://github.com/mxyttkx/MambaMatch"
    },
    {
      "paperId": "aeee2f35dada3eaff01705834a9cac9be40b50dc",
      "externalIds": {
        "DOI": "10.1007/s11554-025-01799-4",
        "CorpusId": 283057178
      },
      "corpusId": 283057178,
      "title": "PIDNet-LW: lightweight semantic segmentation for edge devices",
      "venue": "Journal of Real-Time Image Processing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11554-025-01799-4?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11554-025-01799-4, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2372076939",
          "name": "Siyuan Qian"
        },
        {
          "authorId": "2352520592",
          "name": "Dongfang Zhao"
        },
        {
          "authorId": "2392764014",
          "name": "Xun Zhang"
        },
        {
          "authorId": "2333839806",
          "name": "Zhentong Gao"
        },
        {
          "authorId": "2392750994",
          "name": "Hailong Wang"
        },
        {
          "authorId": "2350180972",
          "name": "Zhihao Guo"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "1f39a843f0830c08653407765c87cea520f514ef",
      "externalIds": {
        "ArXiv": "2511.12878",
        "CorpusId": 283071858
      },
      "corpusId": 283071858,
      "title": "Uni-Hand: Universal Hand Motion Forecasting in Egocentric Views",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.12878, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2311100490",
          "name": "Junyi Ma"
        },
        {
          "authorId": "2319612596",
          "name": "Wentao Bao"
        },
        {
          "authorId": "2155954703",
          "name": "Jingyi Xu"
        },
        {
          "authorId": "2233442485",
          "name": "Guanzhong Sun"
        },
        {
          "authorId": "2392888628",
          "name": "Yu Zheng"
        },
        {
          "authorId": "2365229022",
          "name": "Erhang Zhang"
        },
        {
          "authorId": "2287787818",
          "name": "Xieyuanli Chen"
        },
        {
          "authorId": "2268808281",
          "name": "Hesheng Wang"
        }
      ],
      "abstract": "Forecasting how human hands move in egocentric views is critical for applications like augmented reality and human-robot policy transfer. Recently, several hand trajectory prediction (HTP) methods have been developed to generate future possible hand waypoints, which still suffer from insufficient prediction targets, inherent modality gaps, entangled hand-head motion, and limited validation in downstream tasks. To address these limitations, we present a universal hand motion forecasting framework considering multi-modal input, multi-dimensional and multi-target prediction patterns, and multi-task affordances for downstream applications. We harmonize multiple modalities by vision-language fusion, global context incorporation, and task-aware text embedding injection, to forecast hand waypoints in both 2D and 3D spaces. A novel dual-branch diffusion is proposed to concurrently predict human head and hand movements, capturing their motion synergy in egocentric vision. By introducing target indicators, the prediction model can forecast the specific joint waypoints of the wrist or the fingers, besides the widely studied hand center points. In addition, we enable Uni-Hand to additionally predict hand-object interaction states (contact/separation) to facilitate downstream tasks better. As the first work to incorporate downstream task evaluation in the literature, we build novel benchmarks to assess the real-world applicability of hand motion forecasting algorithms. The experimental results on multiple publicly available datasets and our newly proposed benchmarks demonstrate that Uni-Hand achieves the state-of-the-art performance in multi-dimensional and multi-target hand motion forecasting. Extensive validation in multiple downstream tasks also presents its impressive human-robot policy transfer to enable robotic manipulation, and effective feature enhancement for action anticipation/recognition."
    },
    {
      "paperId": "f647f59b82f8b6f52871181d2b55d2613bcd58c4",
      "externalIds": {
        "ArXiv": "2511.13510",
        "CorpusId": 283072076
      },
      "corpusId": 283072076,
      "title": "Naga: Vedic Encoding for Deep State Space Models",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.13510, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2347348873",
          "name": "Melanie Schaller"
        },
        {
          "authorId": "2348464326",
          "name": "N. Janssen"
        },
        {
          "authorId": "2261857813",
          "name": "Bodo Rosenhahn"
        }
      ],
      "abstract": "This paper presents Naga, a deep State Space Model (SSM) encoding approach inspired by structural concepts from Vedic mathematics. The proposed method introduces a bidirectional representation for time series by jointly processing forward and time-reversed input sequences. These representations are then combined through an element-wise (Hadamard) interaction, resulting in a Vedic-inspired encoding that enhances the model's ability to capture temporal dependencies across distant time steps. We evaluate Naga on multiple long-term time series forecasting (LTSF) benchmarks, including ETTh1, ETTh2, ETTm1, ETTm2, Weather, Traffic, and ILI. The experimental results show that Naga outperforms 28 current state of the art models and demonstrates improved efficiency compared to existing deep SSM-based approaches. The findings suggest that incorporating structured, Vedic-inspired decomposition can provide an interpretable and computationally efficient alternative for long-range sequence modeling."
    },
    {
      "paperId": "e3fee9e04d1a6ef0abe51f97cf59a37620dbbd12",
      "externalIds": {
        "ArXiv": "2511.13138",
        "CorpusId": 283072350
      },
      "corpusId": 283072350,
      "title": "WinMamba: Multi-Scale Shifted Windows in State Space Model for 3D Object Detection",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.13138, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2392903331",
          "name": "Longhui Zheng"
        },
        {
          "authorId": "2279739229",
          "name": "Qiming Xia"
        },
        {
          "authorId": "2392909694",
          "name": "Xiaolu Chen"
        },
        {
          "authorId": "2392897599",
          "name": "Zhaoliang Liu"
        },
        {
          "authorId": "2392630346",
          "name": "Chenglu Wen"
        }
      ],
      "abstract": "3D object detection is critical for autonomous driving, yet it remains fundamentally challenging to simultaneously maximize computational efficiency and capture long-range spatial dependencies. We observed that Mamba-based models, with their linear state-space design, capture long-range dependencies at lower cost, offering a promising balance between efficiency and accuracy. However, existing methods rely on axis-aligned scanning within a fixed window, inevitably discarding spatial information. To address this problem, we propose WinMamba, a novel Mamba-based 3D feature-encoding backbone composed of stacked WinMamba blocks. To enhance the backbone with robust multi-scale representation, the WinMamba block incorporates a window-scale-adaptive module that compensates voxel features across varying resolutions during sampling. Meanwhile, to obtain rich contextual cues within the linear state space, we equip the WinMamba layer with a learnable positional encoding and a window-shift strategy. Extensive experiments on the KITTI and Waymo datasets demonstrate that WinMamba significantly outperforms the baseline. Ablation studies further validate the individual contributions of the WSF and AWF modules in improving detection accuracy. The code will be made publicly available."
    },
    {
      "paperId": "a49f99038505c4b39596d0b44cdc599e3f83eac4",
      "externalIds": {
        "ArXiv": "2511.12940",
        "CorpusId": 283072802
      },
      "corpusId": 283072802,
      "title": "Recurrent Autoregressive Diffusion: Global Memory Meets Local Attention",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.12940, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2364448349",
          "name": "Taiye Chen"
        },
        {
          "authorId": "2291077410",
          "name": "Zihan Ding"
        },
        {
          "authorId": "2393645156",
          "name": "Anjian Li"
        },
        {
          "authorId": "2392903301",
          "name": "Christina Zhang"
        },
        {
          "authorId": "2393019361",
          "name": "Zeqi Xiao"
        },
        {
          "authorId": "2392908633",
          "name": "Yisen Wang"
        },
        {
          "authorId": "2393887100",
          "name": "Chi Jin"
        }
      ],
      "abstract": "Recent advancements in video generation have demonstrated the potential of using video diffusion models as world models, with autoregressive generation of infinitely long videos through masked conditioning. However, such models, usually with local full attention, lack effective memory compression and retrieval for long-term generation beyond the window size, leading to issues of forgetting and spatiotemporal inconsistencies. To enhance the retention of historical information within a fixed memory budget, we introduce a recurrent neural network (RNN) into the diffusion transformer framework. Specifically, a diffusion model incorporating LSTM with attention achieves comparable performance to state-of-the-art RNN blocks, such as TTT and Mamba2. Moreover, existing diffusion-RNN approaches often suffer from performance degradation due to training-inference gap or the lack of overlap across windows. To address these limitations, we propose a novel Recurrent Autoregressive Diffusion (RAD) framework, which executes frame-wise autoregression for memory update and retrieval, consistently across training and inference time. Experiments on Memory Maze and Minecraft datasets demonstrate the superiority of RAD for long video generation, highlighting the efficiency of LSTM in sequence modeling."
    },
    {
      "paperId": "c60f19e24244b08fc55bc74fb01ea0b96860db06",
      "externalIds": {
        "ArXiv": "2511.14806",
        "CorpusId": 283103778
      },
      "corpusId": 283103778,
      "title": "MergeDNA: Context-aware Genome Modeling with Dynamic Tokenization through Token Merging",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.14806, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2335846875",
          "name": "Siyuan Li"
        },
        {
          "authorId": "2395842286",
          "name": "Kai Yu"
        },
        {
          "authorId": "2393497680",
          "name": "Anna Wang"
        },
        {
          "authorId": "2200082418",
          "name": "Zicheng Liu"
        },
        {
          "authorId": "2335747692",
          "name": "Chang Yu"
        },
        {
          "authorId": "2290974001",
          "name": "Jingbo Zhou"
        },
        {
          "authorId": "2338355053",
          "name": "Qirong Yang"
        },
        {
          "authorId": "2344948292",
          "name": "Yucheng Guo"
        },
        {
          "authorId": "2367550090",
          "name": "Xiaoming Zhang"
        },
        {
          "authorId": "2279656942",
          "name": "Stan Z. Li"
        }
      ],
      "abstract": "Modeling genomic sequences faces two unsolved challenges: the information density varies widely across different regions, while there is no clearly defined minimum vocabulary unit. Relying on either four primitive bases or independently designed DNA tokenizers, existing approaches with naive masked language modeling pre-training often fail to adapt to the varying complexities of genomic sequences. Leveraging Token Merging techniques, this paper introduces a hierarchical architecture that jointly optimizes a dynamic genomic tokenizer and latent Transformers with context-aware pre-training tasks. As for network structures, the tokenization module automatically chunks adjacent bases into words by stacking multiple layers of the differentiable token merging blocks with local-window constraints, then a Latent Encoder captures the global context of these merged words by full-attention blocks. Symmetrically employing a Latent Decoder and a Local Decoder, MergeDNA learns with two pre-training tasks: Merged Token Reconstruction simultaneously trains the dynamic tokenization module and adaptively filters important tokens, while Adaptive Masked Token Modeling learns to predict these filtered tokens to capture informative contents. Extensive experiments show that MergeDNA achieves superior performance on three popular DNA benchmarks and several multi-omics tasks with fine-tuning or zero-shot evaluation, outperforming typical tokenization methods and large-scale DNA foundation models."
    },
    {
      "paperId": "75ad2963463b56ae4c07a0222e2c0c059bddfa59",
      "externalIds": {
        "ArXiv": "2511.12572",
        "CorpusId": 283072480
      },
      "corpusId": 283072480,
      "title": "Through-Foliage Surface-Temperature Reconstruction for early Wildfire Detection",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.12572, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2268673176",
          "name": "Mohamed Youssef"
        },
        {
          "authorId": "2392872120",
          "name": "Lukas Brunner"
        },
        {
          "authorId": "2392874706",
          "name": "Klaus Rundhammer"
        },
        {
          "authorId": "2392872722",
          "name": "Gerald Czech"
        },
        {
          "authorId": "2238073307",
          "name": "Oliver Bimber"
        }
      ],
      "abstract": "We introduce a novel method for reconstructing surface temperatures through occluding forest vegetation by combining signal processing and machine learning. Our goal is to enable fully automated aerial wildfire monitoring using autonomous drones, allowing for the early detection of ground fires before smoke or flames are visible. While synthetic aperture (SA) sensing mitigates occlusion from the canopy and sunlight, it introduces thermal blur that obscures the actual surface temperatures. To address this, we train a visual state space model to recover the subtle thermal signals of partially occluded soil and fire hotspots from this blurred data. A key challenge was the scarcity of real-world training data. We overcome this by integrating a latent diffusion model into a vector quantized to generated a large volume of realistic surface temperature simulations from real wildfire recordings, which we further expanded through temperature augmentation and procedural thermal forest simulation. On simulated data across varied ambient and surface temperatures, forest densities, and sunlight conditions, our method reduced the RMSE by a factor of 2 to 2.5 compared to conventional thermal and uncorrected SA imaging. In field experiments focused on high-temperature hotspots, the improvement was even more significant, with a 12.8-fold RMSE gain over conventional thermal and a 2.6-fold gain over uncorrected SA images. We also demonstrate our model's generalization to other thermal signals, such as human signatures for search and rescue. Since simple thresholding is frequently inadequate for detecting subtle thermal signals, the morphological characteristics are equally essential for accurate classification. Our experiments demonstrated another clear advantage: we reconstructed the complete morphology of fire and human signatures, whereas conventional imaging is defeated by partial occlusion."
    },
    {
      "paperId": "1cf128282c7f0bb2d9e437efd1bd6a7195535766",
      "externalIds": {
        "ArXiv": "2511.12671",
        "CorpusId": 283072837
      },
      "corpusId": 283072837,
      "title": "DensePercept-NCSSD: Vision Mamba towards Real-time Dense Visual Perception with Non-Causal State Space Duality",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.12671, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2336865264",
          "name": "Tushar Anand"
        },
        {
          "authorId": "2397569613",
          "name": "Advik Sinha"
        },
        {
          "authorId": "2392904740",
          "name": "Abhijit Das"
        }
      ],
      "abstract": "In this work, we propose an accurate and real-time optical flow and disparity estimation model by fusing pairwise input images in the proposed non-causal selective state space for dense perception tasks. We propose a non-causal Mamba block-based model that is fast and efficient and aptly manages the constraints present in a real-time applications. Our proposed model reduces inference times while maintaining high accuracy and low GPU usage for optical flow and disparity map generation. The results and analysis, and validation in real-life scenario justify that our proposed model can be used for unified real-time and accurate 3D dense perception estimation tasks. The code, along with the models, can be found at https://github.com/vimstereo/DensePerceptNCSSD"
    },
    {
      "paperId": "7b2a4db9082ba3c0c0ccbd0fcfddeb261afe9aed",
      "externalIds": {
        "DBLP": "journals/iotj/YangLBLY25",
        "DOI": "10.1109/JIOT.2025.3557157",
        "CorpusId": 277567959
      },
      "corpusId": 277567959,
      "title": "An Enhanced Multiscale Collaborative Learning Network for Medical Image Segmentation in Internet of Medical Things",
      "venue": "IEEE Internet of Things Journal",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JIOT.2025.3557157?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JIOT.2025.3557157, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2247482656",
          "name": "Aimin Yang"
        },
        {
          "authorId": "2391103640",
          "name": "Jiahao Li"
        },
        {
          "authorId": "2161239980",
          "name": "Yunjie Bai"
        },
        {
          "authorId": "2370981612",
          "name": "Jie Li"
        },
        {
          "authorId": "2354073015",
          "name": "Xingwang Yang"
        }
      ],
      "abstract": "Multiorgan segmentation in the abdomen is a key area in medical image segmentation and is essential for accurate diagnosis and treatment of diseases. The diversity of abdominal organs, differences in size, and ambiguous boundaries pose significant challenges to segmentation tasks, making it difficult for existing models to meet clinical demands in complex scenarios. To address this, this article proposes a multiscale collaborative learning network (MSCL-Net) in conjunction with a medical Internet of Things (IoMT) platform, aiming to enhance the accuracy of abdominal organ segmentation. The introduction of IoMT technology dramatically expands the channels for obtaining medical image data. By collecting massive amounts of data in real-time from various scenarios, such as outpatient clinics and hospitals, doctors can quickly use the segmentation model to obtain preliminary segmentation results of CT images, which specialists can rapidly correct. This workflow improves clinical work efficiency and provides a large volume of high-quality data for training the model, further enhancing its accuracy. Regarding model design, the Feature Space Module is used for global feature extraction, the Feature Extraction Module captures local semantic information at different scales, and the Feature Aggregation Module performs deep integration of local and global information. A considerable number of experimental results based on the Synapse dataset demonstrate that MSCL-Net outperforms existing advanced algorithms and achieves 81.71% in the dice similarity coefficient (DSC) and 17.81 in the Hausdorff Distance at 95% (HD95) metrics, demonstrating its outstanding ability in medical image segmentation."
    },
    {
      "paperId": "31003b44815d3f4edf7205daf16aa9473b5fd561",
      "externalIds": {
        "DOI": "10.1109/JSEN.2025.3620206",
        "CorpusId": 282178737
      },
      "corpusId": 282178737,
      "title": "Efficient Ocular Artifacts Removal From EEG Recordings Using State-Space Model",
      "venue": "IEEE Sensors Journal",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JSEN.2025.3620206?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JSEN.2025.3620206, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2186982763",
          "name": "Jin Yin"
        },
        {
          "authorId": "2160953865",
          "name": "Xun Chen"
        },
        {
          "authorId": "2238369923",
          "name": "Aiping Liu"
        }
      ],
      "abstract": "Electroencephalography (EEG) signals are highly susceptible to various artifacts. Eliminating ocular artifacts is crucial for real-world applications, as they predominantly affect prefrontal channels, which are vital for long-term monitoring. Typically, ocular artifacts removal relies on effectively modeling complex relationships within long signal sequences. The mainstream approach currently relies on self-attention mechanisms, but their quadratic computational complexity limits practical applications, especially on mobile devices. To improve efficiency while maintaining effective long-sequence modeling, we introduce an efficient ocular artifacts removal network (EORNet) that leverages the state-space model (SSM) to achieve linear complexity in modeling temporal relationships. EORNet extracts a wide range of EEG features by leveraging the inherent interdependencies within each segment through supervised training, significantly reducing model complexity without compromising representational capabilities. Experimental results demonstrate that EORNet outperforms previous attention-based algorithms, increasing the signal-to-noise ratio by 1.486, reducing floating-point operations (FLOPs) by 36\u00d7, and lowering parameter count by 12\u00d7. Furthermore, EORNet exhibits a 2.57% improvement in motor imagery classification accuracy and a 2.03% enhancement in emotion recognition accuracy compared with raw data, demonstrating its effectiveness in brain\u2013computer interface tasks. This study emphasizes the capability of SSMs in effectively processing time-series biosignals."
    },
    {
      "paperId": "55accbc57e32c35fcba14103347ced0c7e529823",
      "externalIds": {
        "ArXiv": "2511.12043",
        "CorpusId": 283072181
      },
      "corpusId": 283072181,
      "title": "BudgetLeak: Membership Inference Attacks on RAG Systems via the Generation Budget Side Channel",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.12043, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2276333336",
          "name": "Hao Li"
        },
        {
          "authorId": "2363686178",
          "name": "Jiajun He"
        },
        {
          "authorId": "2363435217",
          "name": "Guangshuo Wang"
        },
        {
          "authorId": "2276275105",
          "name": "Dengguo Feng"
        },
        {
          "authorId": "2312376529",
          "name": "Zheng Li"
        },
        {
          "authorId": "2312376569",
          "name": "Min Zhang"
        }
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) enhances large language models by integrating external knowledge, but reliance on proprietary or sensitive corpora poses various data risks, including privacy leakage and unauthorized data usage. Membership inference attacks (MIAs) are a common technique to assess such risks, yet existing approaches underperform in RAG due to black-box constraints and the absence of strong membership signals. In this paper, we identify a previously unexplored side channel in RAG systems: the generation budget, which controls the maximum number of tokens allowed in a generated response. Varying this budget reveals observable behavioral patterns between member and non-member queries, as members gain quality more rapidly with larger budgets. Building on this insight, we propose BudgetLeak, a novel membership inference attack that probes responses under different budgets and analyzes metric evolution via sequence modeling or clustering. Extensive experiments across four datasets, three LLM generators, and two retrievers demonstrate that BudgetLeak consistently outperforms existing baselines, while maintaining high efficiency and practical viability. Our findings reveal a previously overlooked data risk in RAG systems and highlight the need for new defenses."
    },
    {
      "paperId": "1a88725c77c958c674c032fc4b2ba1244add1e9b",
      "externalIds": {
        "ArXiv": "2511.12193",
        "CorpusId": 283072466
      },
      "corpusId": 283072466,
      "title": "MMRINet: Efficient Mamba-Based Segmentation with Dual-Path Refinement for Low-Resource MRI Analysis",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.12193, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2332090045",
          "name": "Abdelrahman Elsayed"
        },
        {
          "authorId": "2331999870",
          "name": "Ahmed Jaheen"
        },
        {
          "authorId": "2392873275",
          "name": "Mohammad Yaqub"
        }
      ],
      "abstract": "Automated brain tumor segmentation in multi-parametric MRI remains challenging in resource-constrained settings where deep 3D networks are computationally prohibitive. We propose MMRINet, a lightweight architecture that replaces quadratic-complexity attention with linear-complexity Mamba state-space models for efficient volumetric context modeling. Novel Dual-Path Feature Refinement (DPFR) modules maximize feature diversity without additional data requirements, while Progressive Feature Aggregation (PFA) enables effective multi-scale fusion. In the BraTS-Lighthouse SSA 2025, our model achieves strong performance with an average Dice score of (0.752) and an average HD95 of (12.23) with only ~2.5M parameters, demonstrating efficient and accurate segmentation suitable for low-resource clinical environments. Our GitHub repository can be accessed here: github.com/BioMedIA-MBZUAI/MMRINet."
    },
    {
      "paperId": "3f0f24f7f0e41467c445825540748ddbac8b4a7f",
      "externalIds": {
        "ArXiv": "2511.12259",
        "CorpusId": 283072481
      },
      "corpusId": 283072481,
      "title": "A Disease-Aware Dual-Stage Framework for Chest X-ray Report Generation",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.12259, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2376156088",
          "name": "Puzhen Wu"
        },
        {
          "authorId": "2376101480",
          "name": "Hexin Dong"
        },
        {
          "authorId": "2394518993",
          "name": "Yi Lin"
        },
        {
          "authorId": "2387425584",
          "name": "Yihao Ding"
        },
        {
          "authorId": "2373512474",
          "name": "Yifan Peng"
        }
      ],
      "abstract": "Radiology report generation from chest X-rays is an important task in artificial intelligence with the potential to greatly reduce radiologists'workload and shorten patient wait times. Despite recent advances, existing approaches often lack sufficient disease-awareness in visual representations and adequate vision-language alignment to meet the specialized requirements of medical image analysis. As a result, these models usually overlook critical pathological features on chest X-rays and struggle to generate clinically accurate reports. To address these limitations, we propose a novel dual-stage disease-aware framework for chest X-ray report generation. In Stage~1, our model learns Disease-Aware Semantic Tokens (DASTs) corresponding to specific pathology categories through cross-attention mechanisms and multi-label classification, while simultaneously aligning vision and language representations via contrastive learning. In Stage~2, we introduce a Disease-Visual Attention Fusion (DVAF) module to integrate disease-aware representations with visual features, along with a Dual-Modal Similarity Retrieval (DMSR) mechanism that combines visual and disease-specific similarities to retrieve relevant exemplars, providing contextual guidance during report generation. Extensive experiments on benchmark datasets (i.e., CheXpert Plus, IU X-ray, and MIMIC-CXR) demonstrate that our disease-aware framework achieves state-of-the-art performance in chest X-ray report generation, with significant improvements in clinical accuracy and linguistic quality."
    },
    {
      "paperId": "a11210b311c05a9e42fc131a623f4c63d7c7a4a9",
      "externalIds": {
        "ArXiv": "2511.11243",
        "CorpusId": 283054758
      },
      "corpusId": 283054758,
      "title": "Arcee: Differentiable Recurrent State Chain for Generative Vision Modeling with Mamba SSMs",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.11243, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2392719285",
          "name": "Jitesh Chavan"
        },
        {
          "authorId": "2392719264",
          "name": "Rohit Lal"
        },
        {
          "authorId": "2392719715",
          "name": "Anand Kamat"
        },
        {
          "authorId": "2392906368",
          "name": "Mengjia Xu"
        }
      ],
      "abstract": "State-space models (SSMs), Mamba in particular, are increasingly adopted for long-context sequence modeling, providing linear-time aggregation via an input-dependent, causal selective-scan operation. Along this line, recent\"Mamba-for-vision\"variants largely explore multiple scan orders to relax strict causality for non-sequential signals (e.g., images). Rather than preserving cross-block memory, the conventional formulation of the selective-scan operation in Mamba reinitializes each block's state-space dynamics from zero, discarding the terminal state-space representation (SSR) from the previous block. Arcee, a cross-block recurrent state chain, reuses each block's terminal state-space representation as the initial condition for the next block. Handoff across blocks is constructed as a differentiable boundary map whose Jacobian enables end-to-end gradient flow across terminal boundaries. Key to practicality, Arcee is compatible with all prior\"vision-mamba\"variants, parameter-free, and incurs constant, negligible cost. As a modeling perspective, we view terminal SSR as a mild directional prior induced by a causal pass over the input, rather than an estimator of the non-sequential signal itself. To quantify the impact, for unconditional generation on CelebA-HQ (256$\\times$256) with Flow Matching, Arcee reduces FID$\\downarrow$ from $82.81$ to $15.33$ ($5.4\\times$ lower) on a single scan-order Zigzag Mamba baseline. Efficient CUDA kernels and training code will be released to support rigorous and reproducible research."
    },
    {
      "paperId": "a08e2a565371eb89d2c8f205fa2380280868f7ed",
      "externalIds": {
        "PubMedCentral": "12660190",
        "DOI": "10.3389/frai.2025.1664317",
        "CorpusId": 283088369,
        "PubMed": "41322471"
      },
      "corpusId": 283088369,
      "title": "A hybrid deep learning framework for SEM-based air pollutant analysis: Mamba integration and GAN-augmented training",
      "venue": "Frontiers in Artificial Intelligence",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12660190, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2394089613",
          "name": "Minyi Cao"
        },
        {
          "authorId": "2394088841",
          "name": "Derun Kong"
        },
        {
          "authorId": "50434628",
          "name": "Guoying Zhu"
        },
        {
          "authorId": "2296074807",
          "name": "Zhongwen Chen"
        }
      ],
      "abstract": "Air pollution poses severe threats to public health and ecological stability, making accurate analysis of airborne pollutant composition increasingly vital. In this paper, we propose a novel deep learning framework for efficient classification of pollutant components based on microscopic or spectral images. The proposed model integrates the recent Mamba mechanism, a state space model (SSM) architecture known for its superior long-range dependency modeling and linear computational complexity, into the image classification pipeline. By leveraging convolutional layers for local feature extraction and Mamba blocks for global semantic representation, our approach significantly improves both detection accuracy and inference speed compared to traditional CNN or Transformer-based baselines. To address the challenge of limited labeled data, we further introduce a generative adversarial network (GAN)-based data augmentation strategy. A CGAN is trained to synthesize realistic SEM-like particulate images, which are then incorporated into the training set to expand the training dataset. This integration of generative modeling effectively mitigates overfitting and strengthens the model's ability to generalize across varied pollutant types and imaging conditions. Experimental results on benchmark demonstrate the model's effectiveness in identifying common airborne constituents."
    },
    {
      "paperId": "d97d7abbbdcdd89945d514dedf875743e4d27e7d",
      "externalIds": {
        "ArXiv": "2511.11177",
        "CorpusId": 283055337
      },
      "corpusId": 283055337,
      "title": "Viper-F1: Fast and Fine-Grained Multimodal Understanding with Cross-Modal State-Space Modulation",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.11177, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2000457754",
          "name": "Quoc-Huy Trinh"
        },
        {
          "authorId": "2269541965",
          "name": "Mustapha Abdullahi"
        },
        {
          "authorId": "2392719840",
          "name": "Do Duy Hung Trinh"
        },
        {
          "authorId": "2392909679",
          "name": "Bo Zhao"
        },
        {
          "authorId": "2392719298",
          "name": "Debesh Jha"
        }
      ],
      "abstract": "Recent advances in multimodal large language models (MLLMs) have enabled impressive progress in vision-language understanding, yet their high computational cost limits deployment in resource-constrained scenarios such as robotic manipulation, personal assistants, and smart cameras. Most existing methods rely on Transformer-based cross-attention, whose quadratic complexity hinders efficiency. Moreover, small vision-language models often struggle to precisely capture fine-grained, task-relevant visual regions, leading to degraded performance on fine-grained reasoning tasks that limit their effectiveness in the real world. To address these issues, we introduce Viper-F1, a Hybrid State-Space Vision-Language Model that replaces attention with efficient Liquid State-Space Dynamics. To further enhance visual grounding, we propose a Token-Grid Correlation Module, which computes lightweight correlations between text tokens and image patches and modulates the state-space dynamics via FiLM conditioning. This enables the model to selectively emphasize visual regions relevant to the textual prompt while maintaining linear-time inference. Experimental results across multiple benchmarks demonstrate that Viper-F1 achieves accurate, fine-grained understanding with significantly improved efficiency."
    },
    {
      "paperId": "c5c987db64385f55d85455c1322043dbb5c7d1fb",
      "externalIds": {
        "DOI": "10.3390/bdcc9110289",
        "CorpusId": 283064950
      },
      "corpusId": 283064950,
      "title": "Speech Separation Using Advanced Deep Neural Network Methods: A Recent Survey",
      "venue": "Big Data and Cognitive Computing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/bdcc9110289?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/bdcc9110289, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2393037165",
          "name": "Zeng Wang"
        },
        {
          "authorId": null,
          "name": "Zhongqiang Luo"
        }
      ],
      "abstract": "Speech separation, as an important research direction in audio signal processing, has been widely studied by the academic community since its emergence in the mid-1990s. In recent years, with the rapid development of deep neural network technology, speech processing based on deep neural networks has shown outstanding performance in speech separation. While existing studies have surveyed the application of deep neural networks in speech separation from multiple dimensions including learning paradigms, model architectures, loss functions, and training strategies, current achievements still lack systematic comprehension of the field\u2019s developmental trajectory. To address this, this paper focuses on single-channel supervised speech separation tasks, proposing a technological evolution path \u201cU-Net\u2013TasNet\u2013Transformer\u2013Mamba\u201d as the main thread to systematically analyze the impact mechanisms of core architectural designs on separation performance across different stages. By reviewing the transition process from traditional methods to deep learning paradigms and delving into the improvements and integration of deep learning architectures at various stages, this paper summarizes milestone achievements, mainstream evaluation frameworks, and typical datasets in the field, while also providing prospects for future research directions. Through this detailed-focused review perspective, we aim to provide researchers in the speech separation field with a clearly articulated technical evolution map and practical reference."
    },
    {
      "paperId": "279d6ac123d8691d51a44f6472c54dfcec14ef17",
      "externalIds": {
        "DOI": "10.3390/rs17223711",
        "CorpusId": 283065340
      },
      "corpusId": 283065340,
      "title": "Balancing Accuracy and Efficiency: HWBENet for Water Body Extraction in Complex Rural Landscapes",
      "venue": "Remote Sensing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/rs17223711?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/rs17223711, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2301705940",
          "name": "Pengyu Lei"
        },
        {
          "authorId": "2392754716",
          "name": "Jiang Zhang"
        },
        {
          "authorId": "2307909039",
          "name": "Jizheng Yi"
        }
      ],
      "abstract": "The accurate and timely extraction of water bodies from high-resolution remote sensing imagery is vital for environmental monitoring, yet segmenting small, scattered, and irregularly shaped water bodies in complex rural landscapes remains a persistent challenge. While state-of-the-art deep learning models have advanced segmentation accuracy, they often achieve this at the cost of substantial computational overhead, limiting their practical application for large-scale monitoring. To address this trade-off between precision and efficiency, this paper introduces HWBENet, a novel hybrid network for water body extraction. HWBENet is built upon a lightweight MobileNetV3 encoder to ensure computational efficiency while preserving strong feature extraction capabilities. Its core innovation lies in two specifically designed modules. First, the Contextual Information Mining Module (CIMM) is proposed to enhance the network\u2019s ability to learn and fuse both global scene-level context and fine-grained local details, which is crucial for identifying fragmented water bodies. Second, an Edge Refinement Module (ERM) is integrated into the decoder, which uniquely leverages transformer mechanisms to sharpen boundary details by effectively fusing prior feature information with up-sampled features. Extensive experiments on challenging rural water body datasets demonstrate that HWBENet strikes a superior balance between accuracy and computational cost. The experimental results validate the finding that HWBENet is an efficient, accurate, and scalable solution, offering significant practical value for large-scale hydrological mapping in complex rural environments."
    },
    {
      "paperId": "ae2e0f82a4f382bade76a9e3c791e183b0d12fd8",
      "externalIds": {
        "DOI": "10.1117/12.3091214",
        "CorpusId": 283063584
      },
      "corpusId": 283063584,
      "title": "HIDN: hierarchical iterative decomposition network for long-term series forecasting",
      "venue": "International Conference on Advanced Algorithms and Image Processing Technologies",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1117/12.3091214?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1117/12.3091214, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2392755312",
          "name": "Zhenhao Chen"
        },
        {
          "authorId": "2341728379",
          "name": "Dong Tang"
        },
        {
          "authorId": "2392991964",
          "name": "Zhen Zhang"
        },
        {
          "authorId": "2393037890",
          "name": "Xiaofei Yan"
        }
      ],
      "abstract": "The current development of time series prediction methods has exposed the limitations in effectively capturing complex patterns over long-time spans. Current methods face challenges in balancing global trends and local fluctuations, particularly when dealing with diverse time characteristics and complex seasonal patterns. To address these challenges, we propose a Hierarchical Iterative Decomposition Network (HIDN), which combines time series decomposition and advanced feature extraction to enhance long-term time series prediction. Specifically, HIDN decomposes time series into different scale trend and seasonal components with hierarchical iterative decomposition. It allows for detailed analysis of each component and enables independent handling of trend and seasonal patterns. The decomposed trend components are processed using a multi-head attention mechanism, which effectively captures and models multi-scale time trend dependencies and correlations. Meanwhile, the seasonal components are modeled using a selective state space model (Mamba), which captures periodic variations. Finally, HIDN reconstructs prediction results by adaptively fusing processed trends and seasonal components. Experimental results demonstrate that HIDN performs well on various benchmark datasets, delivering higher accuracy."
    },
    {
      "paperId": "d5b686df9dad0fe5e77ccbacbf203beac5782e8c",
      "externalIds": {
        "ArXiv": "2511.11478",
        "CorpusId": 283055005
      },
      "corpusId": 283055005,
      "title": "Rethinking Progression of Memory State in Robotic Manipulation: An Object-Centric Perspective",
      "venue": "",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.11478, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2365209118",
          "name": "Nhat Chung"
        },
        {
          "authorId": "2219924862",
          "name": "Taisei Hanyu"
        },
        {
          "authorId": "2391730068",
          "name": "Toan Nguyen"
        },
        {
          "authorId": "2274935095",
          "name": "Huy Le"
        },
        {
          "authorId": "2392720716",
          "name": "Frederick Bumgarner"
        },
        {
          "authorId": "2393866989",
          "name": "Duy Minh Ho Nguyen"
        },
        {
          "authorId": "2065785558",
          "name": "Khoa T. Vo"
        },
        {
          "authorId": "1556433845",
          "name": "Kashu Yamazaki"
        },
        {
          "authorId": "2354559693",
          "name": "Chase Rainwater"
        },
        {
          "authorId": "2274938047",
          "name": "Tung Kieu"
        },
        {
          "authorId": "2257301403",
          "name": "Anh Nguyen"
        },
        {
          "authorId": "2323041712",
          "name": "Ngan Le"
        }
      ],
      "abstract": "As embodied agents operate in increasingly complex environments, the ability to perceive, track, and reason about individual object instances over time becomes essential, especially in tasks requiring sequenced interactions with visually similar objects. In these non-Markovian settings, key decision cues are often hidden in object-specific histories rather than the current scene. Without persistent memory of prior interactions (what has been interacted with, where it has been, or how it has changed) visuomotor policies may fail, repeat past actions, or overlook completed ones. To surface this challenge, we introduce LIBERO-Mem, a non-Markovian task suite for stress-testing robotic manipulation under object-level partial observability. It combines short- and long-horizon object tracking with temporally sequenced subgoals, requiring reasoning beyond the current frame. However, vision-language-action (VLA) models often struggle in such settings, with token scaling quickly becoming intractable even for tasks spanning just a few hundred frames. We propose Embodied-SlotSSM, a slot-centric VLA framework built for temporal scalability. It maintains spatio-temporally consistent slot identities and leverages them through two mechanisms: (1) slot-state-space modeling for reconstructing short-term history, and (2) a relational encoder to align the input tokens with action decoding. Together, these components enable temporally grounded, context-aware action prediction. Experiments show Embodied-SlotSSM's baseline performance on LIBERO-Mem and general tasks, offering a scalable solution for non-Markovian reasoning in object-centric robotic policies."
    },
    {
      "paperId": "79e1090e03dc4baff8ff355ee85f6cf054311de7",
      "externalIds": {
        "ArXiv": "2511.10011",
        "CorpusId": 282991698
      },
      "corpusId": 282991698,
      "title": "Reinforcing Trustworthiness in Multimodal Emotional Support Systems",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.10011, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2341580167",
          "name": "Huy M. Le"
        },
        {
          "authorId": "2392388774",
          "name": "Dat Tien Nguyen"
        },
        {
          "authorId": "2392258531",
          "name": "Ngan T. T. Vo"
        },
        {
          "authorId": "2392377325",
          "name": "Tuan D. Q. Nguyen"
        },
        {
          "authorId": "2392258882",
          "name": "Nguyen Binh Le"
        },
        {
          "authorId": "143740420",
          "name": "D. M. Nguyen"
        },
        {
          "authorId": "2267332831",
          "name": "Daniel Sonntag"
        },
        {
          "authorId": "2392261910",
          "name": "Lizi Liao"
        },
        {
          "authorId": "2341473281",
          "name": "Binh T. Nguyen"
        }
      ],
      "abstract": "In today's world, emotional support is increasingly essential, yet it remains challenging for both those seeking help and those offering it. Multimodal approaches to emotional support show great promise by integrating diverse data sources to provide empathetic, contextually relevant responses, fostering more effective interactions. However, current methods have notable limitations, often relying solely on text or converting other data types into text, or providing emotion recognition only, thus overlooking the full potential of multimodal inputs. Moreover, many studies prioritize response generation without accurately identifying critical emotional support elements or ensuring the reliability of outputs. To overcome these issues, we introduce \\textsc{ MultiMood}, a new framework that (i) leverages multimodal embeddings from video, audio, and text to predict emotional components and to produce responses responses aligned with professional therapeutic standards. To improve trustworthiness, we (ii) incorporate novel psychological criteria and apply Reinforcement Learning (RL) to optimize large language models (LLMs) for consistent adherence to these standards. We also (iii) analyze several advanced LLMs to assess their multimodal emotional support capabilities. Experimental results show that MultiMood achieves state-of-the-art on MESC and DFEW datasets while RL-driven trustworthiness improvements are validated through human and LLM evaluations, demonstrating its superior capability in applying a multimodal framework in this domain."
    },
    {
      "paperId": "eb7e035124cfaeddec40691d90b4eedbe176ea5c",
      "externalIds": {
        "PubMedCentral": "12614583",
        "DOI": "10.1371/journal.pone.0335908",
        "CorpusId": 283010143,
        "PubMed": "41231925"
      },
      "corpusId": 283010143,
      "title": "Spatiotemporal prediction of obesity rates and model interpretability analysis from a public health perspective",
      "venue": "PLoS ONE",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12614583, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2393008045",
          "name": "Weiyan Tan"
        },
        {
          "authorId": "2392382622",
          "name": "Bing Geng"
        },
        {
          "authorId": "2397171903",
          "name": "XiuGuang Bai"
        }
      ],
      "abstract": "This study, focusing on the assessment of obesity prevalence trends in public health management, proposes an improved Transformer model that integrates temporal embeddings with spatially-constrained feature dependencies rather than purely geographic adjacency. Using state-level data from the CDC BRFSS, the method first performs joint temporal\u2013health encoding (JTH) of obesity prevalence time series and health indicators. It then incorporates temporal decay and a learnable spatial constraint matrix (STA) into the attention mechanism, while employing dual-branch consistency training to enhance stability and generalization. We conducted comparative and ablation experiments on ten states, including Alaska and Alabama, and carried out independent validation on unseen states such as Guam and Idaho. The results show that the proposed approach outperforms representative models including MLP, LSTM, 1D-CNN, Mamba, iTransformer, and TimeMixer across metrics such as MAE, RMSE, sMAPE, R2, and MASE. Ablation experiments further demonstrate that JTH and STA contribute complementary improvements to model performance, while independent validation confirmed that the R2 values for all states exceeded 0.84. In addition, SHAP analysis was employed to illustrate the contributions and dependencies of key features, providing interpretable evidence to support, thereby guiding evidence-based resource allocation in obesity prevention and control."
    },
    {
      "paperId": "30e633ecd88bb8c665ba8e992bfd09a3f926191b",
      "externalIds": {
        "ArXiv": "2511.11725",
        "CorpusId": 283072428
      },
      "corpusId": 283072428,
      "title": "Do Blind Spots Matter for Word-Referent Mapping? A Computational Study with Infant Egocentric Video",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.11725, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2392898693",
          "name": "Zekai Shi"
        },
        {
          "authorId": "2393897908",
          "name": "Zhixi Cai"
        },
        {
          "authorId": "28101391",
          "name": "Kalin Stefanov"
        }
      ],
      "abstract": "Typically, children start to learn their first words between 6 and 9 months, linking spoken utterances to their visual referents. Without prior knowledge, a word encountered for the first time can be interpreted in countless ways; it might refer to any of the objects in the environment, their components, or attributes. Using longitudinal, egocentric, and ecologically valid data from the experience of one child, in this work, we propose a self-supervised and biologically plausible strategy to learn strong visual representations. Our masked autoencoder-based visual backbone incorporates knowledge about the blind spot in human eyes to define a novel masking strategy. This mask and reconstruct approach attempts to mimic the way the human brain fills the gaps in the eyes'field of view. This represents a significant shift from standard random masking strategies, which are difficult to justify from a biological perspective. The pretrained encoder is utilized in a contrastive learning-based video-text model capable of acquiring word-referent mappings. Extensive evaluation suggests that the proposed biologically plausible masking strategy is at least as effective as random masking for learning word-referent mappings from cross-situational and temporally extended episodes."
    },
    {
      "paperId": "53c0ee0cdf467ea012be3bdeda84131d60e88a25",
      "externalIds": {
        "DOI": "10.3390/bdcc9110285",
        "CorpusId": 283045899
      },
      "corpusId": 283045899,
      "title": "Cross-Lingual Bimodal Emotion Recognition with LLM-Based Label Smoothing",
      "venue": "Big Data and Cognitive Computing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/bdcc9110285?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/bdcc9110285, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "13166728",
          "name": "E. Ryumina"
        },
        {
          "authorId": "51265283",
          "name": "A. Axyonov"
        },
        {
          "authorId": "2392580351",
          "name": "Timur Abdulkadirov"
        },
        {
          "authorId": "2392584462",
          "name": "Darya Koryakovskaya"
        },
        {
          "authorId": "2238070291",
          "name": "D. Ryumin"
        }
      ],
      "abstract": "Bimodal emotion recognition based on audio and text is widely adopted in video-constrained real-world applications such as call centers and voice assistants. However, existing systems suffer from limited cross-domain generalization and monolingual bias. To address these limitations, a cross-lingual bimodal emotion recognition method is proposed, integrating Mamba-based temporal encoders for audio (Wav2Vec2.0) and text (Jina-v3) with a Transformer-based cross-modal fusion architecture (BiFormer). Three corpus-adaptive augmentation strategies are introduced: (1) Stacked Data Sampling, in which short utterances are concatenated to stabilize sequence length; (2) Label Smoothing Generation based on Large Language Model, where the Qwen3-4B model is prompted to detect subtle emotional cues missed by annotators, producing soft labels that reflect latent emotional co-occurrences; and (3) Text-to-Utterance Generation, in which emotionally labeled utterances are generated by ChatGPT-5 and synthesized into speech using the DIA-TTS model, enabling controlled creation of affective audio\u2013text pairs without human annotation. BiFormer is trained jointly on the English Multimodal EmotionLines Dataset and the Russian Emotional Speech Dialogs corpus, enabling cross-lingual transfer without parallel data. Experimental results show that the optimal data augmentation strategy is corpus-dependent: Stacked Data Sampling achieves the best performance on short, noisy English utterances, while Label Smoothing Generation based on Large Language Model better captures nuanced emotional expressions in longer Russian utterances. Text-to-Utterance Generation does not yield a measurable gain due to current limitations in expressive speech synthesis. When combined, the two best performing strategies produce complementary improvements, establishing new state-of-the-art performance in both monolingual and cross-lingual settings."
    },
    {
      "paperId": "cc58159e3777791e11ac5f3458a629ef31f173e8",
      "externalIds": {
        "PubMedCentral": "12646879",
        "DOI": "10.3389/or.2025.1592408",
        "CorpusId": 282995914,
        "PubMed": "41312358"
      },
      "corpusId": 282995914,
      "title": "Research on the application of a multi-model cascaded deep learning framework in the pathological diagnosis of osteosarcoma",
      "venue": "Oncology Reviews",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12646879, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2274957878",
          "name": "Hui Yao"
        },
        {
          "authorId": "2391878618",
          "name": "Mengxue Yang"
        },
        {
          "authorId": "2218145788",
          "name": "Xin Jiang"
        },
        {
          "authorId": "2386402758",
          "name": "Hao Jia"
        },
        {
          "authorId": "2392605815",
          "name": "Tao Sun"
        },
        {
          "authorId": "2382028772",
          "name": "Molin Li"
        },
        {
          "authorId": "2392850288",
          "name": "Taiping Wang"
        },
        {
          "authorId": "2248512566",
          "name": "Xuefeng Tang"
        }
      ],
      "abstract": "Introduction Osteosarcoma is the most common malignant tumor of bone tissue in adolescents, and precise pathological diagnosis is the primary foundation for establishing the most effective treatment plan. The pathological evaluation of tumor necrosis after chemotherapy is crucial for assessing therapeutic efficacy in osteosarcoma patients. However, pathologists often face several challenges during the diagnosis and evaluation process. Methods To address these needs, we designed and developed a multi-model cascaded deep learning framework utilizing an advanced Vision Mamba (ViM) model as the core network architecture. The study employed one of the most comprehensive osteosarcoma datasets, sourced from: (1) real-world data from 68 osteosarcoma patients collected at Chongqing General Hospital, and (2) publicly available osteosarcoma assessment data from the University of Texas Southwestern/UT Dallas. Pathological images were annotated using the Palgo pathology image artificial intelligence self-training platform according to algorithm requirements. A triple verification mechanism of annotation, review, and archiving was implemented, and Palgo\u2019s integrated interactive algorithm correction mechanism was used to continuously refine the data annotation process. Results and Discussion The model demonstrated Dice coefficient values of 0.83 or higher in tumor segmentation, osteosarcoma osteoid matrix segmentation, necrotic area segmentation, lung metastatic tumor segmentation, and lung metastatic osteoid matrix segmentation. For necrosis classification, overall osteosarcoma subtypes, and localized osteosarcoma subtypes, the area under the receiver operating characteristics curve (AUC), sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) all exceeded 90%. The proposed model exhibited excellent performance, indicating high potential for future clinical application in osteosarcoma patients. This framework shows promise for enhancing the precision and efficiency of pathological diagnosis and evaluation in osteosarcoma management."
    },
    {
      "paperId": "03a28e9f7c0e71518c992365358cfd6ede5f7f22",
      "externalIds": {
        "ArXiv": "2511.11681",
        "CorpusId": 283072569
      },
      "corpusId": 283072569,
      "title": "MPCM-Net: Multi-scale network integrates partial attention convolution with Mamba for ground-based cloud image segmentation",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.11681, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2264427779",
          "name": "Penghui Niu"
        },
        {
          "authorId": "2391959203",
          "name": "Jiashuai She"
        },
        {
          "authorId": "145076774",
          "name": "Taotao Cai"
        },
        {
          "authorId": "2264546564",
          "name": "Yajuan Zhang"
        },
        {
          "authorId": "2296578494",
          "name": "Ping Zhang"
        },
        {
          "authorId": "2334697357",
          "name": "Junhua Gu"
        },
        {
          "authorId": "2392140954",
          "name": "Jianxin Li"
        }
      ],
      "abstract": "Ground-based cloud image segmentation is a critical research domain for photovoltaic power forecasting. Current deep learning approaches primarily focus on encoder-decoder architectural refinements. However, existing methodologies exhibit several limitations:(1)they rely on dilated convolutions for multi-scale context extraction, lacking the partial feature effectiveness and interoperability of inter-channel;(2)attention-based feature enhancement implementations neglect accuracy-throughput balance; and (3)the decoder modifications fail to establish global interdependencies among hierarchical local features, limiting inference efficiency. To address these challenges, we propose MPCM-Net, a Multi-scale network that integrates Partial attention Convolutions with Mamba architectures to enhance segmentation accuracy and computational efficiency. Specifically, the encoder incorporates MPAC, which comprises:(1)a MPC block with ParCM and ParSM that enables global spatial interaction across multi-scale cloud formations, and (2)a MPA block combining ParAM and ParSM to extract discriminative features with reduced computational complexity. On the decoder side, a M2B is employed to mitigate contextual loss through a SSHD that maintains linear complexity while enabling deep feature aggregation across spatial and scale dimensions. As a key contribution to the community, we also introduce and release a dataset CSRC, which is a clear-label, fine-grained segmentation benchmark designed to overcome the critical limitations of existing public datasets. Extensive experiments on CSRC demonstrate the superior performance of MPCM-Net over state-of-the-art methods, achieving an optimal balance between segmentation accuracy and inference speed. The dataset and source code will be available at https://github.com/she1110/CSRC."
    },
    {
      "paperId": "baa37b0de4f962b10fe24746ee8d1258b9544af9",
      "externalIds": {
        "ArXiv": "2511.09596",
        "CorpusId": 282991971
      },
      "corpusId": 282991971,
      "title": "Making Every Head Count: Sparse Attention Without the Speed-Performance Trade-off",
      "venue": "",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.09596, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2393149823",
          "name": "Mingkuan Zhao"
        },
        {
          "authorId": "2392884828",
          "name": "Wentao Hu"
        },
        {
          "authorId": "2284031610",
          "name": "Jiayin Wang"
        },
        {
          "authorId": "2087800232",
          "name": "Xin Lai"
        },
        {
          "authorId": "2392725305",
          "name": "Tianchen Huang"
        },
        {
          "authorId": "2392265873",
          "name": "Yuheng Min"
        },
        {
          "authorId": "2392370182",
          "name": "Rui Yan"
        },
        {
          "authorId": "1471614160",
          "name": "Xiao-yi Zhu"
        }
      ],
      "abstract": "The design of Large Language Models (LLMs) has long been hampered by a fundamental conflict within their core attention mechanism: its remarkable expressivity is built upon a computational complexity of O(H N^2) that grows quadratically with the context size (N) and linearly with the number of heads (H). This standard implementation harbors significant computational redundancy, as all heads independently compute attention over the same sequence space. Existing sparse methods, meanwhile, often trade information integrity for computational efficiency. To resolve this efficiency-performance trade-off, we propose SPAttention, whose core contribution is the introduction of a new paradigm we term Principled Structural Sparsity. SPAttention does not merely drop connections but instead reorganizes the computational task by partitioning the total attention workload into balanced, non-overlapping distance bands, assigning each head a unique segment. This approach transforms the multi-head attention mechanism from H independent O(N^2) computations into a single, collaborative O(N^2) computation, fundamentally reducing complexity by a factor of H. The structured inductive bias compels functional specialization among heads, enabling a more efficient allocation of computational resources from redundant modeling to distinct dependencies across the entire sequence span. Our work demonstrates that thoughtfully designed structural sparsity can serve as an effective inductive bias that simultaneously improves both computational efficiency and model performance, opening a new avenue for the architectural design of next-generation, high-performance LLMs."
    },
    {
      "paperId": "f70c80352106e5d24e87539161b21154940b9f54",
      "externalIds": {
        "DBLP": "conf/vrst/PavelMSDQ25",
        "DOI": "10.1145/3756884.3766053",
        "CorpusId": 283558715
      },
      "corpusId": 283558715,
      "title": "PatchFusionVR: Multitask Prediction of User Gaze, Reaction Time, and Cognitive Load in Virtual Reality from Multimodal Signals",
      "venue": "Virtual Reality Software and Technology",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3756884.3766053?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3756884.3766053, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2396397254",
          "name": "San Antonio"
        },
        {
          "authorId": "70728740",
          "name": "Virtual Environments"
        },
        {
          "authorId": "2320322091",
          "name": "John Quarles"
        },
        {
          "authorId": "2396380704",
          "name": "Md Irfan Pavel"
        },
        {
          "authorId": "2396397555",
          "name": "Rasel Mahmud"
        },
        {
          "authorId": "51956600",
          "name": "Jyotirmay Nag Setu"
        },
        {
          "authorId": "2289925141",
          "name": "Kevin Desai"
        }
      ],
      "abstract": "Enhancing user experience and performance, including task load in immersive environments, requires accurate prediction of user gaze point, reaction time, and mental and physical load uptake. Current gaze prediction approaches focus primarily on motion-based information, lacking physiological data, which leads to poor prediction accuracy in highly dynamic virtual reality (VR) environments. Traditional cognitive load measurements rely on post-task analysis without proper multimodal data integration and fail to capture the real-time dynamics of user states during interaction. Likewise, reaction time or attention load are often assessed only after the interaction, without using real-time immersive sensor data, which limits adaptive responsiveness. To tackle these limitations, we leveraged a comprehensive multimodal dataset - VRWalking, which recorded timestamped eye-tracking metrics, physiological signals (heart rate and galvanic skin response), and behavioral performance data during real-time engagement in a VR environment. We developed a unified multitask model based on the MultiPatchFormer architecture, which processes multimodal VR signals through dual patch projection branches for gaze and classification inputs. The model employs multiscale patch embeddings, cross-attention between gaze and classification pathways, channel attention, and transformer encoders to jointly predict continuous user gaze and classify reaction time, cognitive load (mental load and physical load). Our methodology achieved excellent predictive performance: 95.64% for reaction time, 98.01% for mental load, and 97.45% for physical load, with a MAPE (Mean Absolute Percentage Error) of 15.24% for gaze prediction. We applied Shapley Additive explanations (SHAP) analysis to interpret the model\u2019s behavior across all features, including eye-tracking, head-tracking, and physiological signals. The analysis revealed which features most influenced the predictions of user gaze, reaction time, mental load, and physical load. Our methods, while based only on the VRWalking dataset, demonstrated strong performance across all tasks, suggesting promising potential for real-world VR applications such as interactive training systems that respond to user attention lapses, educational platforms that adapt to cognitive load, and performance assessments that consider physiological indicators."
    },
    {
      "paperId": "c30da07d045bbfccb490a519de05414739f9aee4",
      "externalIds": {
        "ArXiv": "2511.08769",
        "CorpusId": 282939775
      },
      "corpusId": 282939775,
      "title": "SSMRadNet : A Sample-wise State-Space Framework for Efficient and Ultra-Light Radar Segmentation and Object Detection",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.08769, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2391963161",
          "name": "Anuab Sen"
        },
        {
          "authorId": "2186470548",
          "name": "Mir Sayeed Mohammad"
        },
        {
          "authorId": "2391962088",
          "name": "Saibal Mukhopadhyay"
        }
      ],
      "abstract": "We introduce SSMRadNet, the first multi-scale State Space Model (SSM) based detector for Frequency Modulated Continuous Wave (FMCW) radar that sequentially processes raw ADC samples through two SSMs. One SSM learns a chirp-wise feature by sequentially processing samples from all receiver channels within one chirp, and a second SSM learns a representation of a frame by sequentially processing chirp-wise features. The latent representations of a radar frame are decoded to perform segmentation and detection tasks. Comprehensive evaluations on the RADIal dataset show SSMRadNet has 10-33x fewer parameters and 60-88x less computation (GFLOPs) while being 3.7x faster than state-of-the-art transformer and convolution-based radar detectors at competitive performance for segmentation tasks."
    },
    {
      "paperId": "edd6fd3a769c80c4a2f0ef766bd23bdc1fdff01c",
      "externalIds": {
        "ArXiv": "2511.08349",
        "CorpusId": 282921355
      },
      "corpusId": 282921355,
      "title": "Hybrid Quantum-Classical Selective State Space Artificial Intelligence",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.08349, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2391803669",
          "name": "Amin Ebrahimi"
        },
        {
          "authorId": "1732646",
          "name": "F. Haddadi"
        }
      ],
      "abstract": "Hybrid Quantum Classical (HQC) algorithms constitute one of the most effective paradigms for exploiting the computational advantages of quantum systems in large-scale numerical tasks. By operating in high-dimensional Hilbert spaces, quantum circuits enable exponential speed-ups and provide access to richer representations of cost landscapes compared to purely classical methods. These capabilities are particularly relevant for machine learning, where state-of-the-art models especially in Natural Language Processing (NLP) suffer from prohibitive time complexity due to massive matrix multiplications and high-dimensional optimization. In this manuscript, we propose a Hybrid Quantum Classical selection mechanism for the Mamba architecture, designed specifically for temporal sequence classification problems. Our approach leverages Variational Quantum Circuits (VQCs) as quantum gating modules that both enhance feature extraction and improve suppression of irrelevant information. This integration directly addresses the computational bottlenecks of deep learning architectures by exploiting quantum resources for more efficient representation learning. We analyze how introducing quantum subroutines into large language models (LLMs) impacts their generalization capability, expressivity, and parameter efficiency. The results highlight the potential of quantum-enhanced gating mechanisms as a path toward scalable, resource-efficient NLP models, in a limited simulation step. Within the first four epochs on a reshaped MNIST dataset with input format (batch, 784, d_model), our hybrid model achieved 24.6% accuracy while using one quantum layer and achieve higher expressivity, compared to 21.6% obtained by a purely classical selection mechanism. we state No founding"
    },
    {
      "paperId": "b4954fe7f4356de71f7e14db40a3a56c35a094cb",
      "externalIds": {
        "ArXiv": "2511.07948",
        "CorpusId": 282922463
      },
      "corpusId": 282922463,
      "title": "ReIDMamba: Learning Discriminative Features with Visual State Space Model for Person Re-Identification",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.07948, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2392347214",
          "name": "Hongyang Gu"
        },
        {
          "authorId": "2110077190",
          "name": "Qisong Yang"
        },
        {
          "authorId": "2294488760",
          "name": "Lei Pu"
        },
        {
          "authorId": "2391924193",
          "name": "Siming Han"
        },
        {
          "authorId": "2392053198",
          "name": "Yao Ding"
        }
      ],
      "abstract": "Extracting robust discriminative features is a critical challenge in person re-identification (ReID). While Transformer-based methods have successfully addressed some limitations of convolutional neural networks (CNNs), such as their local processing nature and information loss resulting from convolution and downsampling operations, they still face the scalability issue due to the quadratic increase in memory and computational requirements with the length of the input sequence. To overcome this, we propose a pure Mamba-based person ReID framework named ReIDMamba. Specifically, we have designed a Mamba-based strong baseline that effectively leverages fine-grained, discriminative global features by introducing multiple class tokens. To further enhance robust features learning within Mamba, we have carefully designed two novel techniques. First, the multi-granularity feature extractor (MGFE) module, designed with a multi-branch architecture and class token fusion, effectively forms multi-granularity features, enhancing both discrimination ability and fine-grained coverage. Second, the ranking-aware triplet regularization (RATR) is introduced to reduce redundancy in features from multiple branches, enhancing the diversity of multi-granularity features by incorporating both intra-class and inter-class diversity constraints, thus ensuring the robustness of person features. To our knowledge, this is the pioneering work that integrates a purely Mamba-driven approach into ReID research. Our proposed ReIDMamba model boasts only one-third the parameters of TransReID, along with lower GPU memory usage and faster inference throughput. Experimental results demonstrate ReIDMamba's superior and promising performance, achieving state-of-the-art performance on five person ReID benchmarks. Code is available at https://github.com/GuHY777/ReIDMamba."
    },
    {
      "paperId": "7f0c3ec4a94485ca8dc2bef723c2f2dbb6e4ec4b",
      "externalIds": {
        "ArXiv": "2511.07823",
        "CorpusId": 282921944
      },
      "corpusId": 282921944,
      "title": "CloudMamba: Grouped Selective State Spaces for Point Cloud Analysis",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.07823, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2115061274",
          "name": "Kanglin Qu"
        },
        {
          "authorId": "2242566919",
          "name": "Pan Gao"
        },
        {
          "authorId": "2297849571",
          "name": "Qun Dai"
        },
        {
          "authorId": "2391903861",
          "name": "Zhanzhi Ye"
        },
        {
          "authorId": "2380140560",
          "name": "Rui Ye"
        },
        {
          "authorId": "2294388486",
          "name": "Yuanhao Sun"
        }
      ],
      "abstract": "Due to the long-range modeling ability and linear complexity property, Mamba has attracted considerable attention in point cloud analysis. Despite some interesting progress, related work still suffers from imperfect point cloud serialization, insufficient high-level geometric perception, and overfitting of the selective state space model (S6) at the core of Mamba. To this end, we resort to an SSM-based point cloud network termed CloudMamba to address the above challenges. Specifically, we propose sequence expanding and sequence merging, where the former serializes points along each axis separately and the latter serves to fuse the corresponding higher-order features causally inferred from different sequences, enabling unordered point sets to adapt more stably to the causal nature of Mamba without parameters. Meanwhile, we design chainedMamba that chains the forward and backward processes in the parallel bidirectional Mamba, capturing high-level geometric information during scanning. In addition, we propose a grouped selective state space model (GS6) via parameter sharing on S6, alleviating the overfitting problem caused by the computational mode in S6. Experiments on various point cloud tasks validate CloudMamba's ability to achieve state-of-the-art results with significantly less complexity."
    },
    {
      "paperId": "a479488646a5b39993602e98b6ae9cf1ff09f940",
      "externalIds": {
        "DBLP": "journals/cluster/GaoFCFWY26",
        "DOI": "10.1007/s10586-025-05822-y",
        "CorpusId": 282944480
      },
      "corpusId": 282944480,
      "title": "Crossmodal Hierarchical Heterogeneous Graph Guided Mamba Network for remote sensing semantic segmentation",
      "venue": "Cluster Computing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10586-025-05822-y?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10586-025-05822-y, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2279688007",
          "name": "Rong Gao"
        },
        {
          "authorId": "2375979434",
          "name": "Qingyang Feng"
        },
        {
          "authorId": "2258558063",
          "name": "Jinshan Cao"
        },
        {
          "authorId": "2149115588",
          "name": "Xiaoxiao Feng"
        },
        {
          "authorId": "2350435553",
          "name": "Jing Wang"
        },
        {
          "authorId": "2292994104",
          "name": "Lingyu Yan"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "bbbe62cb63ac78a38ce2d3d0ec2f67f5d0684a01",
      "externalIds": {
        "ArXiv": "2511.06593",
        "DOI": "10.1109/TIP.2025.3632221",
        "CorpusId": 282911815,
        "PubMed": "41259195"
      },
      "corpusId": 282911815,
      "title": "Spatial-Frequency Enhanced Mamba for Multi-Modal Image Fusion",
      "venue": "IEEE Transactions on Image Processing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.06593, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2391818224",
          "name": "Hui Sun"
        },
        {
          "authorId": "2305146871",
          "name": "Long Lv"
        },
        {
          "authorId": "2275083655",
          "name": "Pingping Zhang"
        },
        {
          "authorId": "2391963741",
          "name": "Tongdan Tang"
        },
        {
          "authorId": "2212022757",
          "name": "Feng Tian"
        },
        {
          "authorId": "2254296822",
          "name": "Weibing Sun"
        },
        {
          "authorId": "2252297278",
          "name": "Huchuan Lu"
        }
      ],
      "abstract": "Multi-Modal Image Fusion (MMIF) aims to integrate complementary image information from different modalities to produce informative images. Previous deep learning-based MMIF methods generally adopt Convolutional Neural Networks (CNNs) or Transformers for feature extraction. However, these methods deliver unsatisfactory performances due to the limited receptive field of CNNs and the high computational cost of Transformers. Recently, Mamba has demonstrated a powerful potential for modeling long-range dependencies with linear complexity, providing a promising solution to MMIF. Unfortunately, Mamba lacks full spatial and frequency perceptions, which are very important for MMIF. Moreover, employing Image Reconstruction (IR) as an auxiliary task has been proven beneficial for MMIF. However, a primary challenge is how to leverage IR efficiently and effectively. To address the above issues, we propose a novel framework named Spatial-Frequency Enhanced Mamba Fusion (SFMFusion) for MMIF. More specifically, we first propose a three-branch structure to couple MMIF and IR, which can retain complete contents from source images. Then, we propose the Spatial-Frequency Enhanced Mamba Block (SFMB), which can enhance Mamba in both spatial and frequency domains for comprehensive feature extraction. Finally, we propose the Dynamic Fusion Mamba Block (DFMB), which can be deployed across different branches for dynamic feature fusion. Extensive experiments show that our method achieves better results than most state-of-the-art methods on six MMIF datasets. The source code is available at https://github.com/SunHui1216/SFMFusion"
    },
    {
      "paperId": "5795ecea42da0da2de1efb7b725da73db1cd3ef9",
      "externalIds": {
        "ArXiv": "2511.06976",
        "CorpusId": 282912832
      },
      "corpusId": 282912832,
      "title": "Rethinking Crystal Symmetry Prediction: A Decoupled Perspective",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.06976, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2325913827",
          "name": "Liheng Yu"
        },
        {
          "authorId": "2292741962",
          "name": "Zhe Zhao"
        },
        {
          "authorId": "2355125061",
          "name": "Xucong Wang"
        },
        {
          "authorId": "2391990062",
          "name": "Di Wu"
        },
        {
          "authorId": "2108814780",
          "name": "Pengkun Wang"
        }
      ],
      "abstract": "Efficiently and accurately determining the symmetry is a crucial step in the structural analysis of crystalline materials. Existing methods usually mindlessly apply deep learning models while ignoring the underlying chemical rules. More importantly, experiments show that they face a serious sub-property confusion SPC problem. To address the above challenges, from a decoupled perspective, we introduce the XRDecoupler framework, a problem-solving arsenal specifically designed to tackle the SPC problem. Imitating the thinking process of chemists, we innovatively incorporate multidimensional crystal symmetry information as superclass guidance to ensure that the model's prediction process aligns with chemical intuition. We further design a hierarchical PXRD pattern learning model and a multi-objective optimization approach to achieve high-quality representation and balanced optimization. Comprehensive evaluations on three mainstream databases (e.g., CCDC, CoREMOF, and InorganicData) demonstrate that XRDecoupler excels in performance, interpretability, and generalization."
    },
    {
      "paperId": "c26310e6b04590764e03c38472c1adae821af43d",
      "externalIds": {
        "ArXiv": "2511.07241",
        "CorpusId": 282912415
      },
      "corpusId": 282912415,
      "title": "4DSTR: Advancing Generative 4D Gaussians with Spatial-Temporal Rectification for High-Quality and Consistent 4D Generation",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.07241, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2180671462",
          "name": "Mengmeng Liu"
        },
        {
          "authorId": "2372512355",
          "name": "Jiuming Liu"
        },
        {
          "authorId": "2378900309",
          "name": "Yunpeng Zhang"
        },
        {
          "authorId": "2378999556",
          "name": "Jiangtao Li"
        },
        {
          "authorId": "2145240572",
          "name": "Michael Ying Yang"
        },
        {
          "authorId": "2374048379",
          "name": "Francesco Nex"
        },
        {
          "authorId": "2374424596",
          "name": "Hao Cheng"
        }
      ],
      "abstract": "Remarkable advances in recent 2D image and 3D shape generation have induced a significant focus on dynamic 4D content generation. However, previous 4D generation methods commonly struggle to maintain spatial-temporal consistency and adapt poorly to rapid temporal variations, due to the lack of effective spatial-temporal modeling. To address these problems, we propose a novel 4D generation network called 4DSTR, which modulates generative 4D Gaussian Splatting with spatial-temporal rectification. Specifically, temporal correlation across generated 4D sequences is designed to rectify deformable scales and rotations and guarantee temporal consistency. Furthermore, an adaptive spatial densification and pruning strategy is proposed to address significant temporal variations by dynamically adding or deleting Gaussian points with the awareness of their pre-frame movements. Extensive experiments demonstrate that our 4DSTR achieves state-of-the-art performance in video-to-4D generation, excelling in reconstruction quality, spatial-temporal consistency, and adaptation to rapid temporal movements."
    },
    {
      "paperId": "12dbef83b88ec730a0994dfb5f093cb1b60b2dc7",
      "externalIds": {
        "DBLP": "conf/cikm/QianSZWW0Y25",
        "DOI": "10.1145/3746252.3761175",
        "CorpusId": 282905600
      },
      "corpusId": 282905600,
      "title": "TCFMamba: Trajectory Collaborative Filtering Mamba for Debiased Point-of-Interest Recommendation",
      "venue": "International Conference on Information and Knowledge Management",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746252.3761175?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746252.3761175, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2391900812",
          "name": "Jin Qian"
        },
        {
          "authorId": "2308555518",
          "name": "Shiyu Song"
        },
        {
          "authorId": "2155052357",
          "name": "Xin Zhang"
        },
        {
          "authorId": "2940191",
          "name": "Dongjing Wang"
        },
        {
          "authorId": "2187747755",
          "name": "He Weng"
        },
        {
          "authorId": "2333518008",
          "name": "Haiping Zhang"
        },
        {
          "authorId": "2246234676",
          "name": "Dongjin Yu"
        }
      ],
      "abstract": "Next Point-of-Interest (POI) recommendation, which predicts users' future destinations based on their potential interests, has emerged as a critical task in location-based social networks (LBSNs). However, this task remains challenged by issues such as popularity bias, exposure bias, and limited representational capacity, all of which impede the accurate modeling of users and POIs, thereby restricting balanced and effective recommendations. Therefore, we propose Trajectory Collaborative Filtering Mamba (TCFMamba), which integrates two specially designed modules, i.e., Joint Learning of Static and Dynamic Representations (JLSDR) and Preference State Mamba Network (PSMN), for debiased Point-of-Interest recommendation."
    },
    {
      "paperId": "5dbfb11e13779b619928264f0b13b0569fe50d18",
      "externalIds": {
        "DBLP": "conf/cikm/WangLQDL0025",
        "DOI": "10.1145/3746252.3761344",
        "CorpusId": 282904732
      },
      "corpusId": 282904732,
      "title": "MFAE: Multimodal Feature Adaptive Enhancement for Fake News Video Detection",
      "venue": "International Conference on Information and Knowledge Management",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746252.3761344?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746252.3761344, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2381600278",
          "name": "Wenhao Wang"
        },
        {
          "authorId": "2342445681",
          "name": "Mingxin Li"
        },
        {
          "authorId": "2354013392",
          "name": "Jiao Qiao"
        },
        {
          "authorId": "2189365840",
          "name": "Haotong Du"
        },
        {
          "authorId": "48568389",
          "name": "Xianghua Li"
        },
        {
          "authorId": "2114086856",
          "name": "Chao Gao"
        },
        {
          "authorId": "2258426491",
          "name": "Zhen Wang"
        }
      ],
      "abstract": "With the rapid global growth of short video platforms, the spread of fake news has become increasingly prevalent, creating an urgent demand for effective automated detection methods. Current approaches typically rely on feature extractors to gather information from multiple modalities and then generate predictions through classifiers. However, these methods often fail to fully utilize the complex information across all modalities and overlook the potential for video manipulation, limiting their overall performance. To tackle these issues, MFAE is proposed, a novel framework for Multimodal Feature Adaptive Enhancement for Fake News Video Detection. The framework starts by extracting semantic and emotional features from the news, which are the basis for generating coarse multimodal representations. These representations are further refined through Adaptive Enhancement, a module specifically designed to strengthen the visual and audio modalities. Subsequently, spatial and temporal features are extracted separately, with temporal features undergoing additional refinement via a Temporal Enhancement module. The final result is obtained by feeding the individually enhanced features into the multimodal feature integration module for interaction Comprehensive experiments on two benchmark datasets highlight the exceptional performance of MFAE in detecting fake news on short video platforms. Specifically, the method achieves accuracy improvements of 2.21% and 4.35% on FakeSV and FakeTT, respectively."
    },
    {
      "paperId": "87de57958f00cd0d24ca31314bf9a5a8a3476aa8",
      "externalIds": {
        "DOI": "10.1007/s11760-025-04931-w",
        "CorpusId": 282891628
      },
      "corpusId": 282891628,
      "title": "BiMambaNet: An efficient crack segmentation method that combines attention mechanism with Mamba",
      "venue": "Signal, Image and Video Processing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11760-025-04931-w?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11760-025-04931-w, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2391560152",
          "name": "Fei Deng"
        },
        {
          "authorId": "2391642295",
          "name": "Junhong Ren"
        },
        {
          "authorId": "2368643126",
          "name": "Shaohui Yang"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "df837bf4664a01a98d7f21fbbc55c8f9d6022b7a",
      "externalIds": {
        "ArXiv": "2511.06717",
        "CorpusId": 282911161
      },
      "corpusId": 282911161,
      "title": "MRT: Learning Compact Representations with Mixed RWKV-Transformer for Extreme Image Compression",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.06717, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2391936042",
          "name": "Han Liu"
        },
        {
          "authorId": "1579756374",
          "name": "Hengyu Man"
        },
        {
          "authorId": "1932054882",
          "name": "Xingtao Wang"
        },
        {
          "authorId": "2261894020",
          "name": "Wenrui Li"
        },
        {
          "authorId": "2255028678",
          "name": "Debin Zhao"
        }
      ],
      "abstract": "Recent advances in extreme image compression have revealed that mapping pixel data into highly compact latent representations can significantly improve coding efficiency. However, most existing methods compress images into 2-D latent spaces via convolutional neural networks (CNNs) or Swin Transformers, which tend to retain substantial spatial redundancy, thereby limiting overall compression performance. In this paper, we propose a novel Mixed RWKV-Transformer (MRT) architecture that encodes images into more compact 1-D latent representations by synergistically integrating the complementary strengths of linear-attention-based RWKV and self-attention-based Transformer models. Specifically, MRT partitions each image into fixed-size windows, utilizing RWKV modules to capture global dependencies across windows and Transformer blocks to model local redundancies within each window. The hierarchical attention mechanism enables more efficient and compact representation learning in the 1-D domain. To further enhance compression efficiency, we introduce a dedicated RWKV Compression Model (RCM) tailored to the structure characteristics of the intermediate 1-D latent features in MRT. Extensive experiments on standard image compression benchmarks validate the effectiveness of our approach. The proposed MRT framework consistently achieves superior reconstruction quality at bitrates below 0.02 bits per pixel (bpp). Quantitative results based on the DISTS metric show that MRT significantly outperforms the state-of-the-art 2-D architecture GLC, achieving bitrate savings of 43.75%, 30.59% on the Kodak and CLIC2020 test datasets, respectively."
    },
    {
      "paperId": "4507041e0198a0490b0ff7388dcf4c4dcb1862fa",
      "externalIds": {
        "ArXiv": "2511.06716",
        "CorpusId": 282912866
      },
      "corpusId": 282912866,
      "title": "MirrorMamba: Towards Scalable and Robust Mirror Detection in Videos",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.06716, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2391711957",
          "name": "Rui Song"
        },
        {
          "authorId": "2144908984",
          "name": "Jiaying Lin"
        },
        {
          "authorId": "2265657606",
          "name": "Rynson W. H. Lau"
        }
      ],
      "abstract": "Video mirror detection has received significant research attention, yet existing methods suffer from limited performance and robustness. These approaches often over-rely on single, unreliable dynamic features, and are typically built on CNNs with limited receptive fields or Transformers with quadratic computational complexity. To address these limitations, we propose a new effective and scalable video mirror detection method, called MirrorMamba. Our approach leverages multiple cues to adapt to diverse conditions, incorporating perceived depth, correspondence and optical. We also introduce an innovative Mamba-based Multidirection Correspondence Extractor, which benefits from the global receptive field and linear complexity of the emerging Mamba spatial state model to effectively capture correspondence properties. Additionally, we design a Mamba-based layer-wise boundary enforcement decoder to resolve the unclear boundary caused by the blurred depth map. Notably, this work marks the first successful application of the Mamba-based architecture in the field of mirror detection. Extensive experiments demonstrate that our method outperforms existing state-of-the-art approaches for video mirror detection on the benchmark datasets. Furthermore, on the most challenging and representative image-based mirror detection dataset, our approach achieves state-of-the-art performance, proving its robustness and generalizability."
    },
    {
      "paperId": "fe790f21982a895a14e42f738484c7566906f58b",
      "externalIds": {
        "ArXiv": "2511.07343",
        "CorpusId": 282912108
      },
      "corpusId": 282912108,
      "title": "TNT: Improving Chunkwise Training for Test-Time Memorization",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.07343, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2000315380",
          "name": "Ze-Minghui Li"
        },
        {
          "authorId": "46211294",
          "name": "Ali Behrouz"
        },
        {
          "authorId": "2325194132",
          "name": "Yuan Deng"
        },
        {
          "authorId": "2249561001",
          "name": "Peilin Zhong"
        },
        {
          "authorId": "1471876925",
          "name": "Praneeth Kacham"
        },
        {
          "authorId": "2332101471",
          "name": "Mahdi Karami"
        },
        {
          "authorId": "1800298",
          "name": "Meisam Razaviyayn"
        },
        {
          "authorId": "1728881",
          "name": "V. Mirrokni"
        }
      ],
      "abstract": "Recurrent neural networks (RNNs) with deep test-time memorization modules, such as Titans and TTT, represent a promising, linearly-scaling paradigm distinct from Transformers. While these expressive models do not yet match the peak performance of state-of-the-art Transformers, their potential has been largely untapped due to prohibitively slow training and low hardware utilization. Existing parallelization methods force a fundamental conflict governed by the chunksize hyperparameter: large chunks boost speed but degrade performance, necessitating a fixed, suboptimal compromise. To solve this challenge, we introduce TNT, a novel training paradigm that decouples training efficiency from inference performance through a two-stage process. Stage one is an efficiency-focused pre-training phase utilizing a hierarchical memory. A global module processes large, hardware-friendly chunks for long-range context, while multiple parallel local modules handle fine-grained details. Crucially, by periodically resetting local memory states, we break sequential dependencies to enable massive context parallelization. Stage two is a brief fine-tuning phase where only the local memory modules are adapted to a smaller, high-resolution chunksize, maximizing accuracy with minimal overhead. Evaluated on Titans and TTT models, TNT achieves a substantial acceleration in training speed-up to 17 times faster than the most accurate baseline configuration - while simultaneously improving model accuracy. This improvement removes a critical scalability barrier, establishing a practical foundation for developing expressive RNNs and facilitating future work to close the performance gap with Transformers."
    },
    {
      "paperId": "0c5d4591226453e821e729ff03ad3a8df052c0cb",
      "externalIds": {
        "ArXiv": "2511.06785",
        "CorpusId": 282912172
      },
      "corpusId": 282912172,
      "title": "Resource Efficient Sleep Staging via Multi-Level Masking and Prompt Learning",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.06785, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2368939203",
          "name": "Lejun Ai"
        },
        {
          "authorId": "2391730160",
          "name": "Yulong Li"
        },
        {
          "authorId": "2392680543",
          "name": "Haodong Yi"
        },
        {
          "authorId": "2391723838",
          "name": "Jixuan Xie"
        },
        {
          "authorId": "2300043117",
          "name": "Yue Wang"
        },
        {
          "authorId": "2108771131",
          "name": "Jia Liu"
        },
        {
          "authorId": "2337817853",
          "name": "Min Chen"
        },
        {
          "authorId": "2391771870",
          "name": "Rui Wang"
        }
      ],
      "abstract": "Automatic sleep staging plays a vital role in assessing sleep quality and diagnosing sleep disorders. Most existing methods rely heavily on long and continuous EEG recordings, which poses significant challenges for data acquisition in resource-constrained systems, such as wearable or home-based monitoring systems. In this paper, we propose the task of resource-efficient sleep staging, which aims to reduce the amount of signal collected per sleep epoch while maintaining reliable classification performance. To solve this task, we adopt the masking and prompt learning strategy and propose a novel framework called Mask-Aware Sleep Staging (MASS). Specifically, we design a multi-level masking strategy to promote effective feature modeling under partial and irregular observations. To mitigate the loss of contextual information introduced by masking, we further propose a hierarchical prompt learning mechanism that aggregates unmasked data into a global prompt, serving as a semantic anchor for guiding both patch-level and epoch-level feature modeling. MASS is evaluated on four datasets, demonstrating state-of-the-art performance, especially when the amount of data is very limited. This result highlights its potential for efficient and scalable deployment in real-world low-resource sleep monitoring environments."
    },
    {
      "paperId": "eae6f5d34b7b9dbd5f00618875063b28d77268c4",
      "externalIds": {
        "ArXiv": "2511.06818",
        "CorpusId": 282912449
      },
      "corpusId": 282912449,
      "title": "Learning to Focus: Focal Attention for Selective and Scalable Transformers",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.06818, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2260341519",
          "name": "Dhananjay Ram"
        },
        {
          "authorId": "2335860934",
          "name": "Wei Xia"
        },
        {
          "authorId": "2075295257",
          "name": "S. Soatto"
        }
      ],
      "abstract": "Attention is a core component of transformer architecture, whether encoder-only, decoder-only, or encoder-decoder model. However, the standard softmax attention often produces noisy probability distribution, which can impair effective feature selection at every layer of these models, particularly for long contexts. We propose Focal Attention, a simple yet effective modification that sharpens the attention distribution by controlling the softmax temperature, either as a fixed hyperparameter or as a learnable parameter during training. This sharpening enables the model to concentrate on the most relevant tokens while suppressing irrelevant ones. Empirically, Focal Attention scales more favorably than standard transformer with respect to model size, training data, and context length. Across diverse benchmarks, it achieves the same accuracy with up to 42% fewer parameters or 33% less training data. On long-context tasks, it delivers substantial relative improvements ranging from 17% to 82%, demonstrating its effectiveness in real world applications."
    },
    {
      "paperId": "7de02b843ee97f99f8313f1bada102459df6c65f",
      "externalIds": {
        "DOI": "10.3390/jmse13112124",
        "CorpusId": 282955036
      },
      "corpusId": 282955036,
      "title": "OWTDNet: A Novel CNN-Mamba Fusion Network for Offshore Wind Turbine Detection in High-Resolution Remote Sensing Images",
      "venue": "Journal of Marine Science and Engineering",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/jmse13112124?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/jmse13112124, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2392080960",
          "name": "Pengcheng Sha"
        },
        {
          "authorId": "2392129771",
          "name": "Sujie Lu"
        },
        {
          "authorId": "2391994401",
          "name": "Zongjie Xu"
        },
        {
          "authorId": "2392359556",
          "name": "Jianhai Yu"
        },
        {
          "authorId": "2392233573",
          "name": "Lei Li"
        },
        {
          "authorId": "2239065166",
          "name": "Yibo Zou"
        },
        {
          "authorId": "2238919824",
          "name": "Linlin Zhao"
        }
      ],
      "abstract": "Real-time monitoring of offshore wind turbines (OWTs) through satellite remote sensing imagery is considered an essential process for large-scale infrastructure surveillance in ocean engineering. Current detection systems, however, are constrained by persistent technical limitations, including prohibitive deployment costs, insufficient discriminative power for learned features, and susceptibility to environmental interference. To address these challenges, a dual-branch architecture named OWTDNet is proposed, which integrates global contextual modeling via State Space Models (SSMs) with CNN-based local feature extraction for high-resolution OWTs detection. The primary branch utilizes a Mamba-structured encoder with linear computational complexity to establish long-range spatial dependencies, while an auxiliary Blurring-MobileNetv3 (B-Mv3) branch is designed to compensate for the local feature extraction deficiencies inherent in SSMs. Additionally, a novel Feature Alignment Module (FAM) is introduced to systematically coordinate cross-modal feature fusion between Mamba and CNN branches through channel-wise recalibration and position-aware alignment mechanisms. This module not only enables complementary feature integration but also enhances turbine-specific responses through attention-driven feature modulation. Comprehensive experimental validation demonstrated the superiority of the proposed framework, achieving a mean average precision (AP) of 47.1% on 40,000 \u00d7 40,000-pixel satellite imagery, while maintaining practical computational efficiency (127.7 s per image processing time)."
    },
    {
      "paperId": "af9060b7e81b505319adb46cc65eec213158b1c4",
      "externalIds": {
        "DBLP": "conf/cikm/WangYT25",
        "DOI": "10.1145/3746252.3761399",
        "CorpusId": 282901339
      },
      "corpusId": 282901339,
      "title": "Spatio-Temporal Wavelet Enhanced Attention Mamba for Stock Price Forecasting",
      "venue": "International Conference on Information and Knowledge Management",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746252.3761399?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746252.3761399, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2350429189",
          "name": "Shurui Wang"
        },
        {
          "authorId": "2313592335",
          "name": "Wenbo Yan"
        },
        {
          "authorId": "2291196599",
          "name": "Ying Tan"
        }
      ],
      "abstract": "Stock price forecasting remains a critical challenge due to market non-stationarity and the influence of multiple factors. Existing studies apply frequency domain analysis methods to mitigate the impacts of non-stationarity by decoupling high- and low-frequency variation patterns. However, these approaches primarily focus on single series decomposition while neglecting cross frequency interactions among different stocks. Moreover, as a key indicator of overall market trends, current methods inadequately utilize market index information. In this paper, we propose STEAM, a Spatio-Temporal Wavelet Enhanced Attention Mamba model. We introduce Discrete Wavelet Transform (DWT) to disentangle multi-frequency temporal features and propose Wavelet Enhanced Attention (WEA) to capture cross frequency spatial dependencies, effectively leveraging both local and global inter-stock relationships. To extract the synergistic spatio-temporal dependencies in stock data, AMamba module is designed that integrates WEA into the Mamba-2 architecture. Additionally, to further enhance the model's perception of macro-market conditions, we incorporate market index as a prefix, guiding predictions with holistic market information in both spatial and temporal dependencies learning. Extensive experiments across multiple national stock markets demonstrate that STEAM achieves state-of-the-art forecasting performance."
    },
    {
      "paperId": "ea1ec32d39baaa67c4325a11998dd6933247a4b6",
      "externalIds": {
        "DBLP": "conf/cikm/YaoDP25a",
        "DOI": "10.1145/3746252.3760875",
        "CorpusId": 282957553
      },
      "corpusId": 282957553,
      "title": "Rethinking Masked Image Modeling for Ultrasound Image Denoising",
      "venue": "International Conference on Information and Knowledge Management",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746252.3760875?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746252.3760875, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2328891127",
          "name": "Yuchong Yao"
        },
        {
          "authorId": "18145464",
          "name": "Nandakishor Desai"
        },
        {
          "authorId": "145389998",
          "name": "M. Palaniswami"
        }
      ],
      "abstract": "Ultrasound imaging serves as an important clinical diagnostic modality due to its non-invasive, radiation-free, and real-time capabilities. However, ultrasound images suffer from speckle noise that significantly compromises diagnostic accuracy and clinical interpretation. Traditional denoising methods are limited by speckle noise's signal-dependent nature, often removing important diagnostic features. While deep learning performs better, it requires large labelled datasets that are difficult to obtain due to privacy concerns and annotation costs. Self-supervised learning through masked image modeling (MIM) shows potential in addressing data scarcity, but conventional MIM, developed for high-level vision tasks, is unsuitable for low-level tasks like image denoising due to its framework architecture and learning strategy. To this end, we propose Image Denoising Masked Image Modeling (ID-MIM), the first MIM framework for ultrasound image denoising. ID-MIM incorporates a novel high-frequency oriented dual-branch masking and a specialized learning objective for noise reduction. Our encoder-only architecture features a multi-scale hierarchical transformer with dynamic skip connections, where the encoder directly performs denoising rather than relying on separate decoder reconstruction as in conventional MIM approaches. Extensive experiments demonstrate the superior performance of our ID-MIM framework across diverse noise scenarios, establishing new state-of-the-art results."
    },
    {
      "paperId": "ec77d74bb655fc636c5c049dc5f19952bfbab88f",
      "externalIds": {
        "DBLP": "conf/cikm/DingJXWCCDL25",
        "DOI": "10.1145/3746252.3761246",
        "CorpusId": 282884310
      },
      "corpusId": 282884310,
      "title": "Adaptive Bidirectional State Space Model for High-frequency Portfolio Management",
      "venue": "International Conference on Information and Knowledge Management",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746252.3761246?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746252.3761246, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2292176138",
          "name": "Wei Ding"
        },
        {
          "authorId": "2237387605",
          "name": "H. Jiang"
        },
        {
          "authorId": "2391305326",
          "name": "Ruibo Xiong"
        },
        {
          "authorId": "2322010050",
          "name": "Yongrong Wu"
        },
        {
          "authorId": "2391494865",
          "name": "Jingan Chen"
        },
        {
          "authorId": "2391299656",
          "name": "Lifan Chen"
        },
        {
          "authorId": "2391341018",
          "name": "Pengfei Ding"
        },
        {
          "authorId": "2113697471",
          "name": "Fan Lin"
        }
      ],
      "abstract": "State space models (SSMs) have recently shown great potential on long-range sequence modeling tasks. Benefiting from SSMs' low spatio-temporal overhead and powerful modeling capabilities, utilizing them for high-frequency portfolio management is an appealing research direction. However, representing financial data is challenging for SSMs due to: 1) the non-stationary nature of financial markets and 2) the requirement of asset correlations for financial understanding. In this paper, under a deep reinforcement learning (DRL) paradigm for high-frequency portfolio management, we propose a novel Adaptive Bidirectional State Space Model (ABSSM) to tackle the above challenges. Specifically, in order to cope with changing market conditions, we design an adaptive linear time-varying structure, which precisely captures domain shifts in temporal patterns through an input-dependent state transition matrix, thereby seizing fleeting arbitrage opportunities. Furthermore, we enhance this framework by constructing a bidirectional state space layer, which extracts asset correlations by compressing the global context. To the best of our knowledge, this is the first work that solves the high-frequency portfolio management problem by devising a specialized state space model in the DRL framework. Through extensive experiments on real-world data from the U.S., China, and cryptocurrency markets, we show that our proposed ABSSM significantly outperforms state-of-the-art benchmark methods in balancing profits and risks."
    },
    {
      "paperId": "148480e0fae7855cd45fd35ddf51028b7f938745",
      "externalIds": {
        "DBLP": "conf/cikm/MeiT0WG025",
        "DOI": "10.1145/3746252.3761239",
        "CorpusId": 282853851
      },
      "corpusId": 282853851,
      "title": "FinD3: A Dual 3D State Space Model with Dynamic Hypergraph for Financial Stock Prediction",
      "venue": "International Conference on Information and Knowledge Management",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746252.3761239?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746252.3761239, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2391262483",
          "name": "Jieyuan Mei"
        },
        {
          "authorId": "2328037616",
          "name": "Jindong Tian"
        },
        {
          "authorId": "2310571367",
          "name": "Ronghui Xu"
        },
        {
          "authorId": "2391751483",
          "name": "Hanyue Wei"
        },
        {
          "authorId": "2637615",
          "name": "Chenjuan Guo"
        },
        {
          "authorId": "2118583402",
          "name": "Bin Yang"
        }
      ],
      "abstract": "The financial market plays a crucial role in the modern economy by influencing capital allocation, corporate valuation, and investor behavior. However, its complex dependencies and non-stationary dynamics present significant challenges for financial stock prediction. Previous predictive approaches are typically categorized into Univariate Time Series (UTS) and Multivariate Time Series (MTS) paradigms. UTS methods overlook both cross-feature and cross-stock influences, while MTS methods can only capture one of these simultaneously. Although some recent approaches claim to model 3D Multivariate Time Series (3D-MTS) dependencies, they often discard substantial information and fail to capture the dynamics of the stock market. To address these limitations, we propose FinD3, a Financial 3D model using Dual cubic state spaces and Dynamic hypergraphs. To extract the inherent complex relationships in 3D-MTS, we propose a novel Dual Cubic State Space Model (DCSSM) to capture both cross-feature and cross-stock patterns. Furthermore, to more accurately reflect the dynamics of the stock market, we present an Evolving Hypergraph Attention (EHA) module, which captures dynamic changes in financial markets and updates the hypergraph based on a priori hypergraph. Experimental results demonstrate that FinD3 achieves state-of-the-art performance in quantitative trading performance on two real-world stock market datasets, offering a promising solution to practical quantitative trading challenges. The code is available at: https://github.com/decisionintelligence/FinD3."
    },
    {
      "paperId": "24e3c4279a3c91de0212e9436f5b5d79c480590a",
      "externalIds": {
        "DBLP": "conf/cikm/LiWLL25",
        "DOI": "10.1145/3746252.3760991",
        "CorpusId": 282906559
      },
      "corpusId": 282906559,
      "title": "Give Me Some SALT: Structure-Aware Link Modeling for Temporal Weighted Link Prediction",
      "venue": "International Conference on Information and Knowledge Management",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746252.3760991?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746252.3760991, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2351842874",
          "name": "Ting Li"
        },
        {
          "authorId": "2391956227",
          "name": "Hanchen Wang"
        },
        {
          "authorId": "2350860790",
          "name": "Yiran Li"
        },
        {
          "authorId": "2331005895",
          "name": "Xiaolei Liu"
        }
      ],
      "abstract": "In dynamic graph analysis, research has predominantly focused on temporal link prediction (TLP) for unweighted links, with growing interest in predicting temporal link weights in recent years. Temporal weighted link prediction (TWLP) aims to estimate both the existence and the link weights, which is naturally formulated as a regression task. The long-tail distribution and short-term randomness of link weights pose significant challenges for TWLP. In this paper, we introduce SALT, a Structure-Aware Link modeling for Temporal weighted link prediction, which consists of Weighted Link Encoder (WLE) and Temporal Link State Space Module (TLSSM). WLE encodes each snapshot into link-centric embeddings with common neighbor information, and addresses the long-tail issue by leveraging weights to adjust the embedding distribution. Additionally, TLSSM is designed to handle short-term randomness in temporal modeling. On eight datasets, our model achieves average reductions of 19.86% in RMSE and 24.61% in MAE compared to state-of-the-art baselines."
    },
    {
      "paperId": "61cf0959a9109ed88c2d4f5b815bb7fac3586ea2",
      "externalIds": {
        "DBLP": "conf/cikm/WuZQWD025",
        "DOI": "10.1145/3746252.3761116",
        "CorpusId": 282899762
      },
      "corpusId": 282899762,
      "title": "Beyond Return Conditioning: Multi-Scale Sequence Modeling and Advantage-Guided Policy Routing for Offline RL",
      "venue": "International Conference on Information and Knowledge Management",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746252.3761116?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746252.3761116, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2372571905",
          "name": "Kunbao Wu"
        },
        {
          "authorId": "2237961456",
          "name": "Xinning Zhu"
        },
        {
          "authorId": "2351717200",
          "name": "Yang Qin"
        },
        {
          "authorId": "2391657315",
          "name": "Tieru Wang"
        },
        {
          "authorId": "2293454122",
          "name": "Jianzhou Diao"
        },
        {
          "authorId": "2282096771",
          "name": "Zheng Hu"
        }
      ],
      "abstract": "Return-conditioned supervised learning (RCSL) in offline reinforcement learning (RL) leverages Transformers to extract behavioral patterns from offline datasets for decision-making. However, it suffers from inherent limitations in comprehensively capturing multi-scale temporal relationships in historical trajectories. Moreover, its return-conditioning mechanism offers limited guidance in exploiting high-quality behavioral patterns, often resulting in suboptimal action generation during inference. To address these challenges, we propose the Advantage Decision ConvMamba (ADCM), a method that integrates multi-scale sequence modeling (MSSM) with advantage policy guidance (APG). ADCM reconstructs historical sequences through patch partitioning and employs Mamba architecture together with causal convolutions to model sparse global dependencies and dense local Markovian dependencies for behavioral pattern discovery. By incorporating relative advantage action sampling based on the Mixture-of-Experts (MoE) framework, ADCM prioritizes high-quality actions during inference, thereby reducing reliance on low-quality behavioral patterns in the dataset. We evaluate ADCM on multiple offline RL benchmarks from D4RL. Experimental results show that ADCM achieves significant improvements over baseline models, with particularly strong performance on suboptimal datasets. The code for ADCM is available at https://github.com/iTom233/ADCM.git."
    },
    {
      "paperId": "fd90ccfcc0861975f1d5b68948d4d2ef96090305",
      "externalIds": {
        "DOI": "10.1007/s11554-025-01798-5",
        "CorpusId": 282892322
      },
      "corpusId": 282892322,
      "title": "SCS-Mamba: a lightweight Sobel-enhanced content-aware state space model with shared deformable adaptive head for efficient underwater object detection",
      "venue": "Journal of Real-Time Image Processing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11554-025-01798-5?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11554-025-01798-5, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2365053800",
          "name": "Yili Xu"
        },
        {
          "authorId": "2391851175",
          "name": "Xueqi Zhao"
        },
        {
          "authorId": "2365337986",
          "name": "Xuanxuan Xiao"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "319d62d143afaae4610e08768f46c2d8d61868b0",
      "externalIds": {
        "ArXiv": "2511.06172",
        "CorpusId": 282911965
      },
      "corpusId": 282911965,
      "title": "MambaOVSR: Multiscale Fusion with Global Motion Modeling for Chinese Opera Video Super-Resolution",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.06172, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2298415473",
          "name": "Hua Chang"
        },
        {
          "authorId": "2335449736",
          "name": "Xin Xu"
        },
        {
          "authorId": "2298496359",
          "name": "Wei Liu"
        },
        {
          "authorId": "2345914005",
          "name": "Wei Wang"
        },
        {
          "authorId": "2335165402",
          "name": "Xin Yuan"
        },
        {
          "authorId": "2327745061",
          "name": "Kuichao Jiang"
        }
      ],
      "abstract": "Chinese opera is celebrated for preserving classical art. However, early filming equipment limitations have degraded videos of last-century performances by renowned artists (e.g., low frame rates and resolution), hindering archival efforts. Although space-time video super-resolution (STVSR) has advanced significantly, applying it directly to opera videos remains challenging. The scarcity of datasets impedes the recovery of high frequency details, and existing STVSR methods lack global modeling capabilities, compromising visual quality when handling opera's characteristic large motions. To address these challenges, we pioneer a large scale Chinese Opera Video Clip (COVC) dataset and propose the Mamba-based multiscale fusion network for space-time Opera Video Super-Resolution (MambaOVSR). Specifically, MambaOVSR involves three novel components: the Global Fusion Module (GFM) for motion modeling through a multiscale alternating scanning mechanism, and the Multiscale Synergistic Mamba Module (MSMM) for alignment across different sequence lengths. Additionally, our MambaVR block resolves feature artifacts and positional information loss during alignment. Experimental results on the COVC dataset show that MambaOVSR significantly outperforms the SOTA STVSR method by an average of 1.86 dB in terms of PSNR. Dataset and Code will be publicly released."
    },
    {
      "paperId": "5302e40e71fabe10c403bcc3cff81b7363767ec9",
      "externalIds": {
        "ArXiv": "2511.06451",
        "CorpusId": 282911893
      },
      "corpusId": 282911893,
      "title": "A Risk-Neutral Neural Operator for Arbitrage-Free SPX-VIX Term Structures",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.06451, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2391719400",
          "name": "Jian'an Zhang"
        }
      ],
      "abstract": "We propose ARBITER, a risk-neutral neural operator for learning joint SPX-VIX term structures under no-arbitrage constraints. ARBITER maps market states to an operator that outputs implied volatility and variance curves while enforcing static arbitrage (calendar, vertical, butterfly), Lipschitz bounds, and monotonicity. The model couples operator learning with constrained decoders and is trained with extragradient-style updates plus projection. We introduce evaluation metrics for derivatives term structures (NAS, CNAS, NI, Dual-Gap, Stability Rate) and show gains over Fourier Neural Operator, DeepONet, and state-space sequence models on historical SPX and VIX data. Ablation studies indicate that tying the SPX and VIX legs reduces Dual-Gap and improves NI, Lipschitz projection stabilizes calibration, and selective state updates improve long-horizon generalization. We provide identifiability and approximation results and describe practical recipes for arbitrage-free interpolation and extrapolation across maturities and strikes."
    },
    {
      "paperId": "81422e47bca2578017c2a46882230457066bb2ef",
      "externalIds": {
        "ArXiv": "2511.05833",
        "CorpusId": 282911322
      },
      "corpusId": 282911322,
      "title": "TYrPPG: Uncomplicated and Enhanced Learning Capability rPPG for Remote Heart Rate Estimation",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.05833, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2392367894",
          "name": "Taixi Chen"
        },
        {
          "authorId": "2290850701",
          "name": "Yiu-ming Cheung"
        }
      ],
      "abstract": "Remote photoplethysmography (rPPG) can remotely extract physiological signals from RGB video, which has many advantages in detecting heart rate, such as low cost and no invasion to patients. The existing rPPG model is usually based on the transformer module, which has low computation efficiency. Recently, the Mamba model has garnered increasing attention due to its efficient performance in natural language processing tasks, demonstrating potential as a substitute for transformer-based algorithms. However, the Mambaout model and its variants prove that the SSM module, which is the core component of the Mamba model, is unnecessary for the vision task. Therefore, we hope to prove the feasibility of using the Mambaout-based module to remotely learn the heart rate. Specifically, we propose a novel rPPG algorithm called uncomplicated and enhanced learning capability rPPG (TYrPPG). This paper introduces an innovative gated video understanding block (GVB) designed for efficient analysis of RGB videos. Based on the Mambaout structure, this block integrates 2D-CNN and 3D-CNN to enhance video understanding for analysis. In addition, we propose a comprehensive supervised loss function (CSL) to improve the model's learning capability, along with its weakly supervised variants. The experiments show that our TYrPPG can achieve state-of-the-art performance in commonly used datasets, indicating its prospects and superiority in remote heart rate estimation. The source code is available at https://github.com/Taixi-CHEN/TYrPPG."
    },
    {
      "paperId": "8e685331f2c98dadcdeb250c3f167e42a1c117ef",
      "externalIds": {
        "ArXiv": "2511.05890",
        "CorpusId": 282912696
      },
      "corpusId": 282912696,
      "title": "Towards Frequency-Adaptive Learning for SAR Despeckling",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.05890, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2345324149",
          "name": "Ziqing Ma"
        },
        {
          "authorId": "2216747211",
          "name": "Chang Yang"
        },
        {
          "authorId": "2258668762",
          "name": "Zhichang Guo"
        },
        {
          "authorId": "2125095995",
          "name": "Yao Li"
        }
      ],
      "abstract": "Synthetic Aperture Radar (SAR) images are inherently corrupted by speckle noise, limiting their utility in high-precision applications. While deep learning methods have shown promise in SAR despeckling, most methods employ a single unified network to process the entire image, failing to account for the distinct speckle statistics associated with different spatial physical characteristics. It often leads to artifacts, blurred edges, and texture distortion. To address these issues, we propose SAR-FAH, a frequency-adaptive heterogeneous despeckling model based on a divide-and-conquer architecture. First, wavelet decomposition is used to separate the image into frequency sub-bands carrying different intrinsic characteristics. Inspired by their differing noise characteristics, we design specialized sub-networks for different frequency components. The tailored approach leverages statistical variations across frequencies, improving edge and texture preservation while suppressing noise. Specifically, for the low-frequency part, denoising is formulated as a continuous dynamic system via neural ordinary differential equations, ensuring structural fidelity and sufficient smoothness that prevents artifacts. For high-frequency sub-bands rich in edges and textures, we introduce an enhanced U-Net with deformable convolutions for noise suppression and enhanced features. Extensive experiments on synthetic and real SAR images validate the superior performance of the proposed model in noise removal and structural preservation."
    },
    {
      "paperId": "14f3234f15a83a41dcb47bf67ca0247ae1e8f165",
      "externalIds": {
        "DOI": "10.3390/jmse13112119",
        "CorpusId": 282985031
      },
      "corpusId": 282985031,
      "title": "MKAIS: A Hybrid Mamba\u2013KAN Neural Network for Vessel Trajectory Prediction",
      "venue": "Journal of Marine Science and Engineering",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/jmse13112119?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/jmse13112119, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2257126993",
          "name": "Caiquan Xiong"
        },
        {
          "authorId": "2313871212",
          "name": "Jiaming Li"
        },
        {
          "authorId": "2392070646",
          "name": "Yuzhe Zhuang"
        },
        {
          "authorId": "2294810853",
          "name": "Xinyun Wu"
        },
        {
          "authorId": "2294514856",
          "name": "Mao Luo"
        },
        {
          "authorId": "2366297532",
          "name": "Qi Wang"
        }
      ],
      "abstract": "Vessel trajectory prediction (VTP) plays a critical role in maritime safety and intelligent navigation. Existing methods struggle to simultaneously capture long-term dependencies and nonlinear dynamic patterns in vessel movements. To address this challenge, we propose MKAIS, a novel trajectory prediction model that integrates the selective state space modeling capability of Mamba with the strong nonlinear representation power of Kolmogorov\u2013Arnold Networks (KAN). Specifically, we design a feature-separated embedding strategy for AIS inputs (longitude, latitude, speed over ground, course over ground), followed by an MKAN module that jointly models global temporal dependencies and nonlinear dynamics. Experiments on the public ct_dma dataset demonstrate that MKAIS outperforms state-of-the-art baselines (LSTM, Transformer, TrAISformer, Mamba), achieving up to 16.65% improvement in the Haversine distance over 3 h prediction horizons. These results highlight the effectiveness and robustness of MKAIS for both short-term and long-term vessel trajectory prediction."
    },
    {
      "paperId": "072c6f63c0f7503d353f4dce92d431e197fb481e",
      "externalIds": {
        "DOI": "10.3390/ai6110289",
        "CorpusId": 282978073
      },
      "corpusId": 282978073,
      "title": "DVAD: A Dynamic Visual Adaptation Framework for Multi-Class Anomaly Detection",
      "venue": "Applied Informatics",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/ai6110289?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/ai6110289, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2233229926",
          "name": "Han Gao"
        },
        {
          "authorId": "2275356861",
          "name": "Huiyuan Luo"
        },
        {
          "authorId": "2052756227",
          "name": "Fei Shen"
        },
        {
          "authorId": "2258534471",
          "name": "Zhengtao Zhang"
        }
      ],
      "abstract": "Despite the superior performance of existing anomaly detection methods, they are often limited to single-class detection tasks, requiring separate models for each class. This constraint hinders their detection performance and deployment efficiency when applied to real-world multi-class data. In this paper, we propose a dynamic visual adaptation framework for multi-class anomaly detection, enabling the dynamic and adaptive capture of features based on multi-class data, thereby enhancing detection performance. Specifically, our method introduces a network plug-in, the Hyper AD Plug-in, which dynamically adjusts model parameters according to the input data to extract dynamic features. By leveraging the collaboration between the Mamba block, the CNN block, and the proposed Hyper AD Plug-in, we extract global, local, and dynamic features simultaneously. Furthermore, we incorporate the Mixture-of-Experts (MoE) module, which achieves a dynamic balance across different features through its dynamic routing mechanism and multi-expert collaboration. As a result, the proposed method achieves leading accuracy on the MVTec AD and VisA datasets, with image-level mAU-ROC scores of 98.8% and 95.1%, respectively."
    },
    {
      "paperId": "c0d27942eb3c15345a25b8fa725e76a4ce7fa140",
      "externalIds": {
        "DBLP": "journals/corr/abs-2511-05477",
        "ArXiv": "2511.05477",
        "DOI": "10.48550/arXiv.2511.05477",
        "CorpusId": 282890049
      },
      "corpusId": 282890049,
      "title": "GroupKAN: Rethinking Nonlinearity with Grouped Spline-based KAN Modeling for Efficient Medical Image Segmentation",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.05477, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2391653620",
          "name": "Guojie Li"
        },
        {
          "authorId": "2363677797",
          "name": "A. P. A. Majeed"
        },
        {
          "authorId": "2373625113",
          "name": "Muhammad Ateeq"
        },
        {
          "authorId": "2362330943",
          "name": "Anh Nguyen"
        },
        {
          "authorId": "2391726099",
          "name": "Fan Zhang"
        }
      ],
      "abstract": "Medical image segmentation requires models that are accurate, lightweight, and interpretable. Convolutional architectures lack adaptive nonlinearity and transparent decision-making, whereas Transformer architectures are hindered by quadratic complexity and opaque attention mechanisms. U-KAN addresses these challenges using Kolmogorov-Arnold Networks, achieving higher accuracy than both convolutional and attention-based methods, fewer parameters than Transformer variants, and improved interpretability compared to conventional approaches. However, its O(C^2) complexity due to full-channel transformations limits its scalability as the number of channels increases. To overcome this, we introduce GroupKAN, a lightweight segmentation network that incorporates two novel, structured functional modules: (1) Grouped KAN Transform, which partitions channels into G groups for multivariate spline mappings, reducing complexity to O(C^2/G), and (2) Grouped KAN Activation, which applies shared spline-based mappings within each channel group for efficient, token-wise nonlinearity. Evaluated on three medical benchmarks (BUSI, GlaS, and CVC), GroupKAN achieves an average IoU of 79.80 percent, surpassing U-KAN by +1.11 percent while requiring only 47.6 percent of the parameters (3.02M vs 6.35M), and shows improved interpretability."
    },
    {
      "paperId": "ca5edabde040f595eea680e642b7a14180e81899",
      "externalIds": {
        "ArXiv": "2511.04952",
        "DBLP": "journals/corr/abs-2511-04952",
        "DOI": "10.48550/arXiv.2511.04952",
        "CorpusId": 282890210
      },
      "corpusId": 282890210,
      "title": "LoPT: Lossless Parallel Tokenization Acceleration for Long Context Inference of Large Language Model",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.04952, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2391520065",
          "name": "Wei Shao"
        },
        {
          "authorId": "2383066909",
          "name": "Lingchao Zheng"
        },
        {
          "authorId": "2391652171",
          "name": "Pengyu Wang"
        },
        {
          "authorId": "2391521203",
          "name": "Peizhen Zheng"
        },
        {
          "authorId": "2383854073",
          "name": "Jun Li"
        },
        {
          "authorId": "2383326945",
          "name": "Yuwei Fan"
        }
      ],
      "abstract": "Long context inference scenarios have become increasingly important for large language models, yet they introduce significant computational latency. While prior research has optimized long-sequence inference through operators, model architectures, and system frameworks, tokenization remains an overlooked bottleneck. Existing parallel tokenization methods accelerate processing through text segmentation and multi-process tokenization, but they suffer from inconsistent results due to boundary artifacts that occur after merging. To address this, we propose LoPT, a novel Lossless Parallel Tokenization framework that ensures output identical to standard sequential tokenization. Our approach employs character-position-based matching and dynamic chunk length adjustment to align and merge tokenized segments accurately. Extensive experiments across diverse long-text datasets demonstrate that LoPT achieves significant speedup while guaranteeing lossless tokenization. We also provide theoretical proof of consistency and comprehensive analytical studies to validate the robustness of our method."
    },
    {
      "paperId": "b32086316c9ba449f6a5fbb8c745c5e55d832bcd",
      "externalIds": {
        "ArXiv": "2511.05313",
        "DBLP": "journals/corr/abs-2511-05313",
        "DOI": "10.48550/arXiv.2511.05313",
        "CorpusId": 282889713
      },
      "corpusId": 282889713,
      "title": "Attention and Compression is all you need for Controllably Efficient Language Models",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.05313, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2383855237",
          "name": "Jatin Prakash"
        },
        {
          "authorId": "81522899",
          "name": "Aahlad Puli"
        },
        {
          "authorId": "2304971319",
          "name": "Rajesh Ranganath"
        }
      ],
      "abstract": "The quadratic cost of attention in transformers motivated the development of efficient approaches: namely sparse and sliding window attention, convolutions and linear attention. Although these approaches result in impressive reductions in compute and memory, they often trade-off with quality, specifically in-context recall performance. Moreover, apriori fixing this quality-compute tradeoff means being suboptimal from the get-go: some downstream applications require more memory for in-context recall, while others require lower latency and memory. Further, these approaches rely on heuristic choices that artificially restrict attention, or require handcrafted and complex recurrent state update rules, or they must be carefully composed with attention at specific layers to form a hybrid architecture that complicates the design process, especially at scale. To address above issues, we propose Compress&Attend Transformer (CAT), a conceptually simple architecture employing two simple ingredients only: dense attention and compression. CAT decodes chunks of tokens by attending to compressed chunks of the sequence so far. Compression results in decoding from a reduced sequence length that yields compute and memory savings, while choosing a particular chunk size trades-off quality for efficiency. Moreover, CAT can be trained with multiple chunk sizes at once, unlocking control of quality-compute trade-offs directly at test-time without any retraining, all in a single adaptive architecture. In exhaustive evaluations on common language modeling tasks, in-context recall, and long-context understanding, a single adaptive CAT model outperforms existing efficient baselines, including hybrid architectures, across different compute-memory budgets. Further, a single CAT matches dense transformer in language modeling across model scales while being 1.4-3x faster and requiring 2-9x lower total memory usage."
    },
    {
      "paperId": "a452ef7f3ae5f364bfc564b07ca038d70c71af42",
      "externalIds": {
        "DOI": "10.1109/CSCloud66326.2025.00045",
        "CorpusId": 283469593
      },
      "corpusId": 283469593,
      "title": "Multi-Modal Image Fusion with State Space Mamba Networks",
      "venue": "International Conference on Cyber Security and Cloud Computing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CSCloud66326.2025.00045?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CSCloud66326.2025.00045, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2395969741",
          "name": "Yulin Gu"
        },
        {
          "authorId": "2395973811",
          "name": "Shijie Li"
        },
        {
          "authorId": "2397728827",
          "name": "Fengbo Wu"
        },
        {
          "authorId": "2397337117",
          "name": "Dazhong Wu"
        },
        {
          "authorId": "2399438026",
          "name": "Qing Cao"
        },
        {
          "authorId": "2399165543",
          "name": "Xin Chen"
        },
        {
          "authorId": "2396308571",
          "name": "Jiazhong Zhang"
        },
        {
          "authorId": "2395922940",
          "name": "Yiwei Lou"
        }
      ],
      "abstract": "To tackle problems of structural redundancy, detail loss, and limited cross-modal feature fusion in infrared and visible image fusion, we introduce a two-stage fusion network based on state-space modeling with Mamba. Specially, the proposed network uses a Transformer-CNN decoupled encoder to extract global semantic and local texture features, while the Mamba module dynamically fuses base and detail information. With its selective state-space modeling mechanism, Mamba captures cross-modal dependencies efficiently under linear computational complexity, boosting feature expressiveness and reducing redundancy. The final fused output is refined using channel mixing, residual convolution, and channel compression modules. Training follows a two-phase scheme to separately optimize feature decoupling reconstruction and the quality of the fused image. Experiments on the MSRS, TNO, and RoadScene datasets demonstrate superior performance across 8 metrics, with notable advantages in VIF, SSIM, and MI. The proposed approach holds significant promise for Multi-modal applications, including remote sensing, smart grid systems, and digital twin modeling."
    },
    {
      "paperId": "83e5ad9cf61042bddfc2bfeae2f7fd5c5f10a438",
      "externalIds": {
        "DOI": "10.1145/3774887",
        "CorpusId": 282759461
      },
      "corpusId": 282759461,
      "title": "3DMambaComplete: Structured State Space Model for High-Efficiency Point Cloud Completion",
      "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMCCAP)",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3774887?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3774887, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2295949017",
          "name": "Yixuan Li"
        },
        {
          "authorId": "50709644",
          "name": "Lipeng Ma"
        },
        {
          "authorId": "2260833505",
          "name": "Weidong Yang"
        },
        {
          "authorId": "2281945704",
          "name": "Ben Fei"
        }
      ],
      "abstract": "Point cloud completion seeks to reconstruct a complete and high-fidelity point cloud from an incomplete and low-quality input. Current methods predominantly rely on Transformer architectures for feature extraction. However, these approaches face two major limitations, including the computational complexity associated with the attention mechanism and the potential loss of fine-grained details during pooling operations. These issues hinder their performance on large-scale and highly fragmented point clouds. To overcome these challenges, we propose 3DMambaComplete, a novel point cloud completion method based on the selective State Space Model (SSM), particularly leveraging the Mamba architecture. Unlike traditional Transformer-based methods, 3DMambaComplete utilizes Mamba\u2019s linear-time complexity to efficiently extract global features with significantly reduced computational overhead. Furthermore, we introduce the concepts of discriminative nodes, referred to as hyperpoints, along with dynamic offsets, to improve reconstruction quality. Specifically, the HyperPoint Generation Module encodes the downsampled features of the point cloud using the Mamba Encoder, producing a set of hyperpoints that capture critical information. Subsequently, the HyperPoint Spread Module disperses these hyperpoints across various spatial locations employing dynamic offsets to mitigate aggregation. Finally, the Point Deformation Module implements a deformation technique to transform the 2D mesh into a detailed 3D structure, resulting in high-quality point cloud completions. Experiments on widely-used benchmark datasets show that 3DMambaComplete outperforms existing point cloud completion techniques in both quantitative and qualitative evaluations."
    },
    {
      "paperId": "51695af35e27cdbe56a2abefe56294c60e19e38a",
      "externalIds": {
        "PubMedCentral": "12592510",
        "DOI": "10.1038/s41598-025-22634-7",
        "CorpusId": 282763344,
        "PubMed": "41198740"
      },
      "corpusId": 282763344,
      "title": "A large language model for delirium prediction in the intensive care unit using structured electronic health records",
      "venue": "Scientific Reports",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBYNCND",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12592510, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2145513131",
          "name": "M. Contreras"
        },
        {
          "authorId": "49244137",
          "name": "S. Kapoor"
        },
        {
          "authorId": "1600426974",
          "name": "Jiaqing Zhang"
        },
        {
          "authorId": "2190753243",
          "name": "Andrea Davidson"
        },
        {
          "authorId": "1982722",
          "name": "Yuanfang Ren"
        },
        {
          "authorId": "1660825597",
          "name": "Ziyuan Guan"
        },
        {
          "authorId": "1390018896",
          "name": "T. Ozrazgat-Baslanti"
        },
        {
          "authorId": "2336951414",
          "name": "Jessica Sena"
        },
        {
          "authorId": "1404347990",
          "name": "Subhash Nerella"
        },
        {
          "authorId": "5484714",
          "name": "A. Bihorac"
        },
        {
          "authorId": "2067149174",
          "name": "Parisa Rashidi"
        }
      ],
      "abstract": "Delirium is an acute syndrome characterized by fluctuating attention, cognitive impairment, and severe disorganization of behavior, which has been shown to affect up to 31% of patients in the intensive care unit (ICU). Early detection can enable timely interventions and improved health outcomes. While artificial intelligence (AI) models have shown great potential for ICU delirium prediction using structured electronic health records (EHR), most studies have either not leveraged state-of-the-art AI models, been limited to single-center cohorts, or relied on small datasets for development and validation. In this study, we introduce DeLLiriuM, a novel LLM-based delirium prediction model that utilizes EHR data from the first 24 hours of ICU admission to estimate a patient\u2019s risk of developing delirium for the remainder of their ICU stay. We developed and validated DeLLiriuM using ICU admissions from 104,303 patients across 195 hospitals in three large databases: the eICU Collaborative Research Database, the Medical Information Mart for Intensive Care (MIMIC)-IV, and the University of Florida\u2019s Integrated Data Repository. Our DeLLiriuM model achieved superior performance compared to all baseline models on the external validation set, measured by the area under the receiver operating characteristic curve (AUROC) and the area under the precision-recall curve (AUPRC) metric. DeLLiriuM attained AUROC 82.4 (95% confidence interval 81.8\u201383.0) and AUPRC 11.8 (95% confidence interval 11.3\u201312.4) across 77,543 patients spanning 194 hospitals. Our approach of transforming structured EHR data into an unstructured text format, the primary data modality for LLMs, enables our DeLLiriuM model to capture clinical contextual information, resulting in improved predictive performance. To the best of our knowledge, DeLLiriuM is the first LLM-based delirium prediction tool for the ICU that utilizes structured EHR data with LLMs rather than clinical notes with LLMs or traditional structured feature representations used in AI models."
    },
    {
      "paperId": "a1add58931d2bf3b2c6fd0ecfad43a56c296290f",
      "externalIds": {
        "PubMedCentral": "12592499",
        "DOI": "10.1038/s41598-025-22862-x",
        "CorpusId": 282763199,
        "PubMed": "41198912"
      },
      "corpusId": 282763199,
      "title": "PUNet: a lightweight parallel U-Net architecture integrating Mamba\u2013CNN for high-precision image segmentation",
      "venue": "Scientific Reports",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBYNCND",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12592499, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2387131717",
          "name": "Zhaoyan Xie"
        },
        {
          "authorId": "2390802793",
          "name": "Xiaowei Li"
        },
        {
          "authorId": "2390959018",
          "name": "Hongyao Ma"
        },
        {
          "authorId": "2381789518",
          "name": "Sihao Wu"
        },
        {
          "authorId": "2381533726",
          "name": "Dayou Cui"
        }
      ],
      "abstract": "Real-time high-precision image segmentation on mobile and edge devices remains challenging due to the limited ability of traditional convolutional networks to model long-range dependencies and their high computational cost at high resolutions. We propose PUNet, a lightweight parallel U-Net variant that integrates depthwise separable convolutions (DSConv) with a structured state-space Mamba module in a dual-path encoder. The core component, the parallel structured state-space encoder, employs two branches to efficiently capture local spatial features (via DSConv) and model global semantic dependencies (via the visual Mamba layer). At the same time, a squeeze-and-excitation skip connection adaptively fuses multi-scale features. With only 0.26 M parameters and linear computational complexity, PUNet enables real-time inference on resource-constrained platforms. Experiments on the CamVid and CRACK500 datasets demonstrate superior performance, achieving validation Dice scores of 0.9208 and 0.7902, and mean Intersection-over-Union of 0.8643 and 0.6612, respectively, significantly outperforming other lightweight models."
    },
    {
      "paperId": "d8a1557fe1b978df7ff79d6feba1b7bb529c6648",
      "externalIds": {
        "DBLP": "journals/mva/DongYXW26",
        "DOI": "10.1007/s00138-025-01738-0",
        "CorpusId": 282763288
      },
      "corpusId": 282763288,
      "title": "A Mamba-optimized decoder for improved medical image segmentation",
      "venue": "Machine Vision and Applications",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s00138-025-01738-0?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s00138-025-01738-0, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2324917354",
          "name": "Zhiqi Dong"
        },
        {
          "authorId": "2324941207",
          "name": "Weibin Yang"
        },
        {
          "authorId": "2324957061",
          "name": "Mingyuan Xu"
        },
        {
          "authorId": "2290063748",
          "name": "Pengwei Wang"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "4dc6324e599739b424e6a0afb07eba1bb7a56d3e",
      "externalIds": {
        "ArXiv": "2511.04234",
        "DBLP": "journals/corr/abs-2511-04234",
        "DOI": "10.48550/arXiv.2511.04234",
        "CorpusId": 282812282
      },
      "corpusId": 282812282,
      "title": "Reusing Pre-Training Data at Test Time is a Compute Multiplier",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.04234, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "46372713",
          "name": "Alex Fang"
        },
        {
          "authorId": "2376132925",
          "name": "Thomas Voice"
        },
        {
          "authorId": "2238621132",
          "name": "Ruoming Pang"
        },
        {
          "authorId": "2253541812",
          "name": "Ludwig Schmidt"
        },
        {
          "authorId": "2238621478",
          "name": "Tom Gunter"
        }
      ],
      "abstract": "Large language models learn from their vast pre-training corpora, gaining the ability to solve an ever increasing variety of tasks; yet although researchers work to improve these datasets, there is little effort to understand how efficient the pre-training apparatus is at extracting ideas and knowledge from the data. In this work, we use retrieval augmented generation along with test-time compute as a way to quantify how much dataset value was left behind by the process of pre-training, and how this changes across scale. We demonstrate that pre-training then retrieving from standard and largely open-sourced datasets results in significant accuracy gains in MMLU, Math-500, and SimpleQA, which persist through decontamination. For MMLU we observe that retrieval acts as a ~5x compute multiplier versus pre-training alone. We show that these results can be further improved by leveraging additional compute at test time to parse the retrieved context, demonstrating a 10 percentage point improvement on MMLU for the public LLaMA 3.1 8B model. Overall, our results suggest that today's pre-training methods do not make full use of the information in existing pre-training datasets, leaving significant room for progress."
    },
    {
      "paperId": "cd1d87e7e31a05a82b6b25bf231f831c39006ef1",
      "externalIds": {
        "DBLP": "journals/corr/abs-2511-04670",
        "ArXiv": "2511.04670",
        "DOI": "10.48550/arXiv.2511.04670",
        "CorpusId": 282812799
      },
      "corpusId": 282812799,
      "title": "Cambrian-S: Towards Spatial Supersensing in Video",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 9,
      "influentialCitationCount": 1,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.04670, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2308234341",
          "name": "Shusheng Yang"
        },
        {
          "authorId": "2362644269",
          "name": "Jihan Yang"
        },
        {
          "authorId": "2363502482",
          "name": "Pinzhi Huang"
        },
        {
          "authorId": "113880966",
          "name": "Ellis Brown"
        },
        {
          "authorId": "2391532027",
          "name": "Zihao Yang"
        },
        {
          "authorId": "2391486148",
          "name": "Yue Yu"
        },
        {
          "authorId": "2383183297",
          "name": "Shengbang Tong"
        },
        {
          "authorId": "2343000199",
          "name": "Zihan Zheng"
        },
        {
          "authorId": "2391002556",
          "name": "Yifan Xu"
        },
        {
          "authorId": "2390943718",
          "name": "Muhan Wang"
        },
        {
          "authorId": "2308980940",
          "name": "Daohan Lu"
        },
        {
          "authorId": "2308037493",
          "name": "Rob Fergus"
        },
        {
          "authorId": "2265899558",
          "name": "Yann LeCun"
        },
        {
          "authorId": "2336426513",
          "name": "Fei-Fei Li"
        },
        {
          "authorId": "2282918539",
          "name": "Saining Xie"
        }
      ],
      "abstract": "We argue that progress in true multimodal intelligence calls for a shift from reactive, task-driven systems and brute-force long context towards a broader paradigm of supersensing. We frame spatial supersensing as four stages beyond linguistic-only understanding: semantic perception (naming what is seen), streaming event cognition (maintaining memory across continuous experiences), implicit 3D spatial cognition (inferring the world behind pixels), and predictive world modeling (creating internal models that filter and organize information). Current benchmarks largely test only the early stages, offering narrow coverage of spatial cognition and rarely challenging models in ways that require true world modeling. To drive progress in spatial supersensing, we present VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatial recall) and VSC (continual visual spatial counting). These tasks require arbitrarily long video inputs yet are resistant to brute-force context expansion. We then test data scaling limits by curating VSI-590K and training Cambrian-S, achieving +30% absolute improvement on VSI-Bench without sacrificing general capabilities. Yet performance on VSI-SUPER remains limited, indicating that scale alone is insufficient for spatial supersensing. We propose predictive sensing as a path forward, presenting a proof-of-concept in which a self-supervised next-latent-frame predictor leverages surprise (prediction error) to drive memory and event segmentation. On VSI-SUPER, this approach substantially outperforms leading proprietary baselines, showing that spatial supersensing requires models that not only see but also anticipate, select, and organize experience."
    },
    {
      "paperId": "838738a6110fe54acbfde1839e0851c77d1bf8b6",
      "externalIds": {
        "DBLP": "journals/corr/abs-2511-04797",
        "ArXiv": "2511.04797",
        "DOI": "10.48550/arXiv.2511.04797",
        "CorpusId": 282890104
      },
      "corpusId": 282890104,
      "title": "3D Gaussian Point Encoders",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.04797, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2391521691",
          "name": "Jim James"
        },
        {
          "authorId": "2391521029",
          "name": "Ben Wilson"
        },
        {
          "authorId": "2249160289",
          "name": "Simon Lucey"
        },
        {
          "authorId": "2391518228",
          "name": "James Hays"
        }
      ],
      "abstract": "In this work, we introduce the 3D Gaussian Point Encoder, an explicit per-point embedding built on mixtures of learned 3D Gaussians. This explicit geometric representation for 3D recognition tasks is a departure from widely used implicit representations such as PointNet. However, it is difficult to learn 3D Gaussian encoders in end-to-end fashion with standard optimizers. We develop optimization techniques based on natural gradients and distillation from PointNets to find a Gaussian Basis that can reconstruct PointNet activations. The resulting 3D Gaussian Point Encoders are faster and more parameter efficient than traditional PointNets. As in the 3D reconstruction literature where there has been considerable interest in the move from implicit (e.g., NeRF) to explicit (e.g., Gaussian Splatting) representations, we can take advantage of computational geometry heuristics to accelerate 3D Gaussian Point Encoders further. We extend filtering techniques from 3D Gaussian Splatting to construct encoders that run 2.7 times faster as a comparable accuracy PointNet while using 46% less memory and 88% fewer FLOPs. Furthermore, we demonstrate the effectiveness of 3D Gaussian Point Encoders as a component in Mamba3D, running 1.27 times faster and achieving a reduction in memory and FLOPs by 42% and 54% respectively. 3D Gaussian Point Encoders are lightweight enough to achieve high framerates on CPU-only devices."
    },
    {
      "paperId": "6980d2558ebeb31a963c84a2dcd4559ab89f65eb",
      "externalIds": {
        "DOI": "10.1111/mice.70118",
        "CorpusId": 282862593
      },
      "corpusId": 282862593,
      "title": "A crack detection and quantification framework for high\u2010resolution images using Mamba and unmanned devices",
      "venue": "Computer-Aided Civil and Infrastructure Engineering",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1111/mice.70118?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1111/mice.70118, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2391690396",
          "name": "Yanguang Zhu"
        },
        {
          "authorId": "2314184256",
          "name": "Jiangpeng Shu"
        },
        {
          "authorId": "2297631177",
          "name": "Wei Ding"
        },
        {
          "authorId": "2391347061",
          "name": "Chuan Yue"
        },
        {
          "authorId": "2391498714",
          "name": "Yongqiang Lu"
        },
        {
          "authorId": "2391276279",
          "name": "Jiahao Zhang"
        }
      ],
      "abstract": "In structural defects inspection, the quantitative detection of slender cracks remains a significant challenge. Existing methods suffer from low segmentation accuracy for complex boundaries and high computational demands for high\u2010resolution (HR) images, making them unsuitable for the current scenarios where unmanned devices are widely deployed. To address the above\u2010mentioned limitations, a crack detection and quantification framework based on multi\u2010scale convolution\u2010enhanced Mamba (MCMamba) and an HR image calibration method is proposed. The MCMamba is designed based on the Mamba architecture and the calibration method using variable step\u2010size moving least squares is proposed to fit the scale field of HR images, enabling precise crack segmentation and quantification. The MCMamba is trained on an established dataset, and the framework is further field\u2010tested using a climbing robot and Unmanned Aerial Vehicle (UAV), achieving accuracy with less than 10% error for cracks thinner than 0.2 mm. This framework improves crack detection accuracy and demonstrates its advantages in quantifying slender cracks on large\u2010scale bridges in engineering practice."
    },
    {
      "paperId": "6ea97e21523bbc708e498b74e2eb977ab25d5c95",
      "externalIds": {
        "DBLP": "journals/corr/abs-2511-03260",
        "ArXiv": "2511.03260",
        "DOI": "10.48550/arXiv.2511.03260",
        "CorpusId": 282757837
      },
      "corpusId": 282757837,
      "title": "Enhancing Medical Image Segmentation via Heat Conduction Equation",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.03260, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2398002400",
          "name": "Rong Wu"
        },
        {
          "authorId": "2391051841",
          "name": "Yim-Sang Yu"
        }
      ],
      "abstract": "Medical image segmentation has been significantly advanced by deep learning architectures, notably U-Net variants. However, existing models struggle to achieve efficient global context modeling and long-range dependency reasoning under practical computational budgets simultaneously. In this work, we propose a novel hybrid architecture utilizing U-Mamba with Heat Conduction Equation. Our model combines Mamba-based state-space modules for efficient long-range reasoning with Heat Conduction Operators (HCOs) in the bottleneck layers, simulating frequency-domain thermal diffusion for enhanced semantic abstraction. Experimental results on multimodal abdominal CT and MRI datasets demonstrate that the proposed model consistently outperforms strong baselines, validating its effectiveness and generalizability. It suggest that blending state-space dynamics with heat-based global diffusion offers a scalable and interpretable solution for medical segmentation tasks."
    },
    {
      "paperId": "bef434b41343118e1cc5044cf44f878855fcf064",
      "externalIds": {
        "DOI": "10.3390/systems13110991",
        "CorpusId": 282922788
      },
      "corpusId": 282922788,
      "title": "State-Space and Multi-Scale Convolutional Generative Adversarial Network for Traffic Flow Forecasting",
      "venue": "Systems",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/systems13110991?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/systems13110991, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2332363991",
          "name": "Wenxie Lin"
        },
        {
          "authorId": "2305458101",
          "name": "Zhe Zhang"
        },
        {
          "authorId": "2335408572",
          "name": "Yangzhen Zhao"
        },
        {
          "authorId": "2391834906",
          "name": "Jinyu Zhang"
        },
        {
          "authorId": "1384373905",
          "name": "Gang Ren"
        }
      ],
      "abstract": "Long-sequence traffic flow forecasting plays a crucial role in intelligent transportation systems. However, existing Transformer-based approaches face a quadratic complexity bottleneck in computation and are prone to over-smoothing in deep architectures. This results in overly averaged predictions that fail to capture the peaks and troughs of traffic flow. To address these issues, we propose a State-Space Generative Adversarial Network (SSGAN) with a state-space generator and a multi-scale convolutional discriminator. Specifically, a bidirectional Mamba-2 model was designed as the generator to leverage the linear complexity and efficient forecasting capability of state-space models for long-sequence modeling. Meanwhile, the discriminator incorporates a multi-scale convolutional structure to extract traffic features from the frequency domain, thereby capturing flow patterns across different scales, alleviating the over-smoothing issue and enhancing discriminative ability. Through adversarial training, the model is able to better approximate the true distribution of traffic flow. Experiments conducted on four real-world public traffic flow datasets demonstrate that the proposed method outperformed the baselines in both forecasting accuracy and computational efficiency."
    },
    {
      "paperId": "0cdc78f8543e02202e0c61da0c53a96926c17a95",
      "externalIds": {
        "DOI": "10.1109/TIP.2025.3627408",
        "CorpusId": 282810868,
        "PubMed": "41191463"
      },
      "corpusId": 282810868,
      "title": "Food3D: Text-Driven Customizable 3D Food Generation With Gaussian Splatting",
      "venue": "IEEE Transactions on Image Processing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIP.2025.3627408?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIP.2025.3627408, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2356372453",
          "name": "Dongjian Yu"
        },
        {
          "authorId": "2366119",
          "name": "Weiqing Min"
        },
        {
          "authorId": "2296029901",
          "name": "Xin Jin"
        },
        {
          "authorId": "2090288234",
          "name": "Qian Jiang"
        },
        {
          "authorId": "2077434734",
          "name": "Shao-qing Yao"
        },
        {
          "authorId": "2249173362",
          "name": "Shuqiang Jiang"
        }
      ],
      "abstract": "Realistic 3D food creation generation plays a critical role in applications such as nutritional assessment, advertising, and virtual content creation. The existing text-to-3D models typically begin by initializing a 3D representation, which is subsequently refined using supervision from a text-to-image model to obtain the final 3D output. In this work, we present Food3D, a novel framework for 3D food generation designed to address two main limitations of current models. First, the limitation of initialization in 3D generation: poor initialization can result in the generated 3D food lacking crucial details and realism, thereby reducing its quality. To address this issue, we propose a generalized method named Food3D-G, which uses Mamba-based initialization to improve the starting point of the initialization process, thereby enhancing the visual fidelity and quality of the generated 3D food. Second, the limitation of text-to-image models: current text-to-3D models often rely on text-to-image models for supervision. However, a considerable gap persists between the generated images and real-world visuals, particularly when modeling complex food structures. These models fail to accurately capture the fine details and textures, which negatively impacts the quality and realism of the generated 3D food models. To address this limitation, we propose a customizable method for personalized 3D food generation, termed Food3D-C. This method employs a dual-branch diffusion model that effectively captures intricate details, particularly in complex food structures. Within the Food3D framework, both proposed methods incorporate 3D Gaussian splatting (3D GS) and a schedulable interval score matching (S-ISM) algorithm to enhance shape and texture generation. Extensive experiments demonstrate that Food3D achieves state-of-the-art performance, with substantial improvements in detail, shape accuracy, and overall visual realism. Project page and source codes: https://yudongjian.github.io/Food3D/"
    },
    {
      "paperId": "4be429085aecc557ee7d2c09425e7f5b78a8e0e9",
      "externalIds": {
        "DOI": "10.3390/rs17213650",
        "CorpusId": 282934628
      },
      "corpusId": 282934628,
      "title": "Individual Planted Tree Seedling Detection from UAV Multimodal Data with the Alternate Scanning Fusion Method",
      "venue": "Remote Sensing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/rs17213650?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/rs17213650, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2319886431",
          "name": "Taoming Qi"
        },
        {
          "authorId": "2269517473",
          "name": "Yaokai Liu"
        },
        {
          "authorId": "2367558309",
          "name": "Junxiang Tan"
        },
        {
          "authorId": "2391846901",
          "name": "Pengyu Yin"
        },
        {
          "authorId": "2387788699",
          "name": "Changping Huang"
        },
        {
          "authorId": "2339251472",
          "name": "Zengguang Zhou"
        },
        {
          "authorId": "2307328563",
          "name": "Ziyang Li"
        }
      ],
      "abstract": "Detection of planted tree seedlings at the individual level is crucial for monitoring forest ecosystems and supporting silvicultural management. The combination of deep learning (DL) object detection algorithms and remote sensing (RS) data from unmanned aerial vehicles (UAVs) offers efficient and cost-effective solutions. However, current methods predominantly rely on unimodal RS data sources, overlooking the multi-source nature of RS data, which may result in an insufficient representation of target features. Moreover, there is a lack of multimodal frameworks tailored explicitly for detecting planted tree seedlings. Consequently, we propose a multimodal object detection framework for this task by integrating texture information from digital orthophoto maps (DOMs) and geometric information from digital surface models (DSMs). We introduce alternate scanning fusion (ASF), a novel multimodal fusion module based on state space models (SSMs). The ASF can achieve global feature fusion while maintaining linear computational complexity. We embed ASF modules into a dual-backbone YOLOv5 object detection framework, enabling feature-level fusion between DOM and DSM for end-to-end detection. To train and evaluate the proposed framework, we establish the planted tree seedling (PTS) dataset. On the PTS dataset, our method achieves an AP50 of 72.6% for detecting planted tree seedlings, significantly outperforming the original YOLOv5 on unimodal data: 63.5% on DOM and 55.9% on DSM. Within the YOLOv5 framework, comparative experiments on both our PTS dataset and the public VEDAI benchmark demonstrate that the ASF surpasses representative fusion methods in multimodal detection accuracy while maintaining relatively low computational cost."
    },
    {
      "paperId": "9bc28550d98fb9778821563fff2e29f2837cb898",
      "externalIds": {
        "ArXiv": "2511.02454",
        "DBLP": "journals/corr/abs-2511-02454",
        "DOI": "10.48550/arXiv.2511.02454",
        "CorpusId": 282749465
      },
      "corpusId": 282749465,
      "title": "Improving DF-Conformer Using Hydra For High-Fidelity Generative Speech Enhancement on Discrete Codec Token",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.02454, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "18477793",
          "name": "Shogo Seki"
        },
        {
          "authorId": "2390495768",
          "name": "Shaoxiang Dang"
        },
        {
          "authorId": "2276678060",
          "name": "Li Li"
        }
      ],
      "abstract": "The Dilated FAVOR Conformer (DF-Conformer) is an efficient variant of the Conformer architecture designed for speech enhancement (SE). It employs fast attention through positive orthogonal random features (FAVOR+) to mitigate the quadratic complexity associated with self-attention, while utilizing dilated convolution to expand the receptive field. This combination results in impressive performance across various SE models. In this paper, we propose replacing FAVOR+ with bidirectional selective structured state-space sequence models to achieve two main objectives:(1) enhancing global sequential modeling by eliminating the approximations inherent in FAVOR+, and (2) maintaining linear complexity relative to the sequence length. Specifically, we utilize Hydra, a bidirectional extension of Mamba, framed within the structured matrix mixer framework. Experiments conducted using a generative SE model on discrete codec tokens, known as Genhancer, demonstrate that the proposed method surpasses the performance of the DF-Conformer."
    },
    {
      "paperId": "b8b3079387d025e351055ef88551378c07b01c63",
      "externalIds": {
        "ArXiv": "2511.02193",
        "DBLP": "journals/corr/abs-2511-02193",
        "DOI": "10.48550/arXiv.2511.02193",
        "CorpusId": 282748993
      },
      "corpusId": 282748993,
      "title": "MM-UNet: Morph Mamba U-shaped Convolutional Networks for Retinal Vessel Segmentation",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.02193, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2381390235",
          "name": "Jiawen Liu"
        },
        {
          "authorId": "2390522639",
          "name": "Yuanbo Zeng"
        },
        {
          "authorId": "2362075766",
          "name": "Jiaming Liang"
        },
        {
          "authorId": "2390560606",
          "name": "Yizhen Yang"
        },
        {
          "authorId": "2381410471",
          "name": "Yiheng Zhang"
        },
        {
          "authorId": "2390498906",
          "name": "Enhui Cai"
        },
        {
          "authorId": "2361506244",
          "name": "Xiaoqi Sheng"
        },
        {
          "authorId": "2390206713",
          "name": "Hongmin Cai"
        }
      ],
      "abstract": "Accurate detection of retinal vessels plays a critical role in reflecting a wide range of health status indicators in the clinical diagnosis of ocular diseases. Recently, advances in deep learning have led to a surge in retinal vessel segmentation methods, which have significantly contributed to the quantitative analysis of vascular morphology. However, retinal vasculature differs significantly from conventional segmentation targets in that it consists of extremely thin and branching structures, whose global morphology varies greatly across images. These characteristics continue to pose challenges to segmentation precision and robustness. To address these issues, we propose MM-UNet, a novel architecture tailored for efficient retinal vessel segmentation. The model incorporates Morph Mamba Convolution layers, which replace pointwise convolutions to enhance branching topological perception through morph, state-aware feature sampling. Additionally, Reverse Selective State Guidance modules integrate reverse guidance theory with state-space modeling to improve geometric boundary awareness and decoding efficiency. Extensive experiments conducted on two public retinal vessel segmentation datasets demonstrate the superior performance of the proposed method in segmentation accuracy. Compared to the existing approaches, MM-UNet achieves F1-score gains of 1.64 % on DRIVE and 1.25 % on STARE, demonstrating its effectiveness and advancement. The project code is public via https://github.com/liujiawen-jpg/MM-UNet."
    },
    {
      "paperId": "da649d7ca5cb629243328400f8845e4186537f92",
      "externalIds": {
        "ArXiv": "2511.02651",
        "DBLP": "journals/corr/abs-2511-02651",
        "DOI": "10.48550/arXiv.2511.02651",
        "CorpusId": 282749433
      },
      "corpusId": 282749433,
      "title": "Apriel-H1: Towards Efficient Enterprise Reasoning Models",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.02651, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "145191120",
          "name": "O. Ostapenko"
        },
        {
          "authorId": "2374123253",
          "name": "Luke Kumar"
        },
        {
          "authorId": "2374075946",
          "name": "Raymond Li"
        },
        {
          "authorId": "2192609008",
          "name": "Denis Kocetkov"
        },
        {
          "authorId": "1404999623",
          "name": "J. Lamy-Poirier"
        },
        {
          "authorId": "2308458399",
          "name": "Shruthan Radhakrishna"
        },
        {
          "authorId": "51056069",
          "name": "S. Parikh"
        },
        {
          "authorId": "2391905422",
          "name": "Shambhavi Mishra"
        },
        {
          "authorId": "2272569243",
          "name": "S\u00e9bastien Paquet"
        },
        {
          "authorId": "31801337",
          "name": "Srinivas Sunkara"
        },
        {
          "authorId": "2390494861",
          "name": "Val'erie B'ecaert"
        },
        {
          "authorId": "100519532",
          "name": "Sathwik Tejaswi Madhusudhan"
        },
        {
          "authorId": "2294607304",
          "name": "Torsten Scholak"
        }
      ],
      "abstract": "Large Language Models (LLMs) achieve remarkable reasoning capabilities through transformer architectures with attention mechanisms. However, transformers suffer from quadratic time and memory complexity in the attention module (MHA) and require caching key-value states during inference, which severely limits throughput and scalability. High inference throughput is critical for agentic tasks, long-context reasoning, efficient deployment under high request loads, and more efficient test-time compute scaling. State Space Models (SSMs) such as Mamba offer a promising alternative with linear inference complexity and a constant memory footprint via recurrent computation with fixed-size hidden states. In this technical report we introduce the Apriel-H1 family of hybrid LLMs that combine transformer attention and SSM sequence mixers for efficient reasoning at 15B model size. These models are obtained through incremental distillation from a pretrained reasoning transformer, Apriel-Nemotron-15B-Thinker, progressively replacing less critical attention layers with linear Mamba blocks. We release multiple post-distillation variants of Apriel-H1-15B-Thinker with different SSM-to-MHA ratios and analyse how reasoning performance degrades as more Mamba layers replace MHA. Additionally, we release a 30/50 hybrid variant of Apriel-H1, further fine-tuned on a supervised dataset of reasoning traces, achieving over 2x higher inference throughput when deployed in the production-ready vLLM environment, with minimal degradation in reasoning performance. This shows that distilled hybrid SSM-Transformer architectures can deliver substantial efficiency gains over the pretrained transformer equivalent without substantially compromising the reasoning quality."
    },
    {
      "paperId": "0f9017e1fe3b19b64a0db45d5fca4dc6c80d401a",
      "externalIds": {
        "DBLP": "journals/corr/abs-2511-02230",
        "ArXiv": "2511.02230",
        "DOI": "10.48550/arXiv.2511.02230",
        "CorpusId": 282749552
      },
      "corpusId": 282749552,
      "title": "Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV Cache Time-to-Live",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.02230, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2384921668",
          "name": "Hanchen Li"
        },
        {
          "authorId": "2385785771",
          "name": "Qiuyang Mang"
        },
        {
          "authorId": "2385464968",
          "name": "Runyuan He"
        },
        {
          "authorId": "2112197220",
          "name": "Qizheng Zhang"
        },
        {
          "authorId": "2345007132",
          "name": "Huanzhi Mao"
        },
        {
          "authorId": "2390523235",
          "name": "Xiaokun Chen"
        },
        {
          "authorId": "2238575471",
          "name": "Alvin Cheung"
        },
        {
          "authorId": "2375864666",
          "name": "Joseph Gonzalez"
        },
        {
          "authorId": "2055174324",
          "name": "Ion Stoica"
        }
      ],
      "abstract": "Agentic LLM applications interleave LLM generation requests with tool calls. These tool calls break the continuity of the workflow by creating pauses between LLM requests, bringing many challenges for the serving system, especially under multi-turn scenarios. Each pause potentially causes KV cache eviction and extra waiting time before entering the continuous batch for the following LLM request. Since these pauses happen for each call, this problem becomes increasingly severe as turn number grow for agentic programs. Previous works either fail to incorporate information from the tool call, evicting KV cache that leads to repetitive prefill or loading, or ignore the continuity of a multi-turn program, creating waiting time between turns that increases per-request latency. We present Continuum, a serving system to optimize job completion time for multi-turn agent workloads by combining tool-aware KV cache timeout with program-level scheduling. By predicting tool call durations in agentic workflows, Continuum selectively pins the KV cache in GPU memory with a time-to-live value based on total turn number. When combined with program-level first-come-first-serve, Continuum prevents scheduling bubbles, preserves multi-turn continuity, and optimizes for throughput for complex agentic workflows. By modeling the variability of tool call and agent program continuity, Continuum outperforms state-of-the-art baselines. Our evaluation on real-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B models shows that Continuum significantly improves the average job completion times, and remains performant across different hardware setups and DRAM offloading schemes. Preview code is available at: https://github.com/Hanchenli/vllm-continuum"
    },
    {
      "paperId": "13b1c6afeebcd71d7a276e0807741682eec90900",
      "externalIds": {
        "DBLP": "journals/corr/abs-2511-05564",
        "ArXiv": "2511.05564",
        "DOI": "10.48550/arXiv.2511.05564",
        "CorpusId": 282911306
      },
      "corpusId": 282911306,
      "title": "M2S2L: Mamba-based Multi-Scale Spatial-temporal Learning for Video Anomaly Detection",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.05564, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2388919079",
          "name": "Yang Liu"
        },
        {
          "authorId": "2389019413",
          "name": "Boan Chen"
        },
        {
          "authorId": "2331876324",
          "name": "Xiaoguang Zhu"
        },
        {
          "authorId": "2153467142",
          "name": "Jing Liu"
        },
        {
          "authorId": "2284338434",
          "name": "Peng Sun"
        },
        {
          "authorId": "2260279361",
          "name": "Wei Zhou"
        }
      ],
      "abstract": "Video anomaly detection (VAD) is an essential task in the image processing community with prospects in video surveillance, which faces fundamental challenges in balancing detection accuracy with computational efficiency. As video content becomes increasingly complex with diverse behavioral patterns and contextual scenarios, traditional VAD approaches struggle to provide robust assessment for modern surveillance systems. Existing methods either lack comprehensive spatial-temporal modeling or require excessive computational resources for real-time applications. In this regard, we present a Mamba-based multi-scale spatial-temporal learning (M2S2L) framework in this paper. The proposed method employs hierarchical spatial encoders operating at multiple granularities and multi-temporal encoders capturing motion dynamics across different time scales. We also introduce a feature decomposition mechanism to enable task-specific optimization for appearance and motion reconstruction, facilitating more nuanced behavioral modeling and quality-aware anomaly assessment. Experiments on three benchmark datasets demonstrate that M2S2L framework achieves 98.5%, 92.1%, and 77.9% frame-level AUCs on UCSD Ped2, CUHK Avenue, and ShanghaiTech respectively, while maintaining efficiency with 20.1G FLOPs and 45 FPS inference speed, making it suitable for practical surveillance deployment."
    },
    {
      "paperId": "2e7b6377ee20b8c2f5979ed404db49f97babead0",
      "externalIds": {
        "ArXiv": "2511.05560",
        "DBLP": "journals/corr/abs-2511-05560",
        "DOI": "10.18653/v1/2025.babylm-main.14",
        "CorpusId": 282897005
      },
      "corpusId": 282897005,
      "title": "Sample-Efficient Language Modeling with Linear Attention and Lightweight Enhancements",
      "venue": "Proceedings of the First BabyLM Workshop",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.05560, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2298902857",
          "name": "Patrick Haller"
        },
        {
          "authorId": "144983077",
          "name": "Jonas Golde"
        },
        {
          "authorId": "2291365436",
          "name": "Alan Akbik"
        }
      ],
      "abstract": "We study architectural and optimization tech- niques for sample-efficient language modeling under the constraints of the BabyLM 2025 shared task. Our model, BLaLM, replaces self-attention with a linear-time mLSTM to- ken mixer and explores lightweight enhance- ments, including short convolutions, sliding window attention with dynamic modulation, and Hedgehog feature maps. To support train- ing in low-resource settings, we curate a high- quality corpus emphasizing readability and ped- agogical structure. Experiments across both STRICT and STRICT-SMALL tracks show that (1) linear attention combined with sliding win- dow attention consistently improves zero-shot performance, and (2) the Muon optimizer stabi- lizes convergence and reduces perplexity over AdamW. These results highlight effective strate- gies for efficient language modeling without relying on scale."
    },
    {
      "paperId": "cc25b38196ff50d4c8ddbaba3a2de051a812a682",
      "externalIds": {
        "DOI": "10.1117/12.3086853",
        "CorpusId": 282800623
      },
      "corpusId": 282800623,
      "title": "AMM-YOLOv8: lightweight pine wilt disease detection network",
      "venue": "International Conference on Image Processing, Intelligent Control and Computer Engineering",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1117/12.3086853?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1117/12.3086853, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2390931423",
          "name": "Yong Wu"
        },
        {
          "authorId": "2390883914",
          "name": "Yanfei Yin"
        },
        {
          "authorId": "2390675080",
          "name": "Bin Dong"
        },
        {
          "authorId": "2390867186",
          "name": "Xidong Zhang"
        },
        {
          "authorId": "2390944425",
          "name": "Jingke Xu"
        }
      ],
      "abstract": "Pine Wilt Disease (PWD) monitoring suffers from slow detection, low small-object accuracy, high missed rates, and sample variance. To address these, we propose a lightweight detection network, AMM-YOLOv8. It uses multi-scale feature enhancement and efficient extraction to improve small-object detection. The core of EMM-YOLOv8 integrates three key modules. First, Adaptive Kernel Convolution (AKConv) enhances feature extraction capability. Second, the lightweight module MobileOne reduces parameter quantity. Finally, the MLLA module, based on a linear attention mechanism, improves occluded target detection. These three modules collectively enhance detection performance while reducing model complexity. Experiments on the custom forestry dataset show that compared with the baseline model, the accuracy of the model is 94.67% mAP@0.5, the recall rate is 93.15%, the detection speed is 117.65 FPS, the parameters are reduced by 26.4%, and the computational cost (FLOPs) is reduced by 74.3%. The results verify the effectiveness of the proposed framework in PWD real-time detection, which provides a practical solution for intelligent monitoring of forest pests with robustness to occlusion and different sample conditions."
    },
    {
      "paperId": "fab55e2a664e5f3e51d06da9bde5ecf848bf3b27",
      "externalIds": {
        "DBLP": "journals/corr/abs-2511-01315",
        "ArXiv": "2511.01315",
        "DOI": "10.48550/arXiv.2511.01315",
        "CorpusId": 282739818
      },
      "corpusId": 282739818,
      "title": "MVSMamba: Multi-View Stereo with State Space Model",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.01315, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2292528691",
          "name": "Jianfei Jiang"
        },
        {
          "authorId": "2374347194",
          "name": "Qiankun Liu"
        },
        {
          "authorId": "2355230870",
          "name": "Hongyuan Liu"
        },
        {
          "authorId": "2333961739",
          "name": "Haochen Yu"
        },
        {
          "authorId": "2355238774",
          "name": "Liyong Wang"
        },
        {
          "authorId": "2164324897",
          "name": "Jiansheng Chen"
        },
        {
          "authorId": "2383935490",
          "name": "Huimin Ma"
        }
      ],
      "abstract": "Robust feature representations are essential for learning-based Multi-View Stereo (MVS), which relies on accurate feature matching. Recent MVS methods leverage Transformers to capture long-range dependencies based on local features extracted by conventional feature pyramid networks. However, the quadratic complexity of Transformer-based MVS methods poses challenges to balance performance and efficiency. Motivated by the global modeling capability and linear complexity of the Mamba architecture, we propose MVSMamba, the first Mamba-based MVS network. MVSMamba enables efficient global feature aggregation with minimal computational overhead. To fully exploit Mamba's potential in MVS, we propose a Dynamic Mamba module (DM-module) based on a novel reference-centered dynamic scanning strategy, which enables: (1) Efficient intra- and inter-view feature interaction from the reference to source views, (2) Omnidirectional multi-view feature representations, and (3) Multi-scale global feature aggregation. Extensive experimental results demonstrate MVSMamba outperforms state-of-the-art MVS methods on the DTU dataset and the Tanks-and-Temples benchmark with both superior performance and efficiency. The source code is available at https://github.com/JianfeiJ/MVSMamba."
    },
    {
      "paperId": "a5a57224c2adf98b334e95abd7259d4af05b1f72",
      "externalIds": {
        "DBLP": "journals/corr/abs-2511-01357",
        "ArXiv": "2511.01357",
        "DOI": "10.2312/pg.20251303",
        "CorpusId": 282740241
      },
      "corpusId": 282740241,
      "title": "CMI-MTL: Cross-Mamba interaction based multi-task learning for medical visual question answering",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.01357, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "24921532",
          "name": "Qiangguo Jin"
        },
        {
          "authorId": "2390594420",
          "name": "Xianyao Zheng"
        },
        {
          "authorId": "2254262205",
          "name": "Hui Cui"
        },
        {
          "authorId": "2255321824",
          "name": "Changming Sun"
        },
        {
          "authorId": "2372210280",
          "name": "Yuqi Fang"
        },
        {
          "authorId": "2365774629",
          "name": "Cong Cong"
        },
        {
          "authorId": "2254308815",
          "name": "Ran Su"
        },
        {
          "authorId": "2254904782",
          "name": "Leyi Wei"
        },
        {
          "authorId": "2066135805",
          "name": "Ping Xuan"
        },
        {
          "authorId": "2365970328",
          "name": "Junbo Wang"
        }
      ],
      "abstract": "Medical visual question answering (Med-VQA) is a crucial multimodal task in clinical decision support and telemedicine. Recent self-attention based methods struggle to effectively handle cross-modal semantic alignments between vision and language. Moreover, classification-based methods rely on predefined answer sets. Treating this task as a simple classification problem may make it unable to adapt to the diversity of free-form answers and overlook the detailed semantic information of free-form answers. In order to tackle these challenges, we introduce a Cross-Mamba Interaction based Multi-Task Learning (CMI-MTL) framework that learns cross-modal feature representations from images and texts. CMI-MTL comprises three key modules: fine-grained visual-text feature alignment (FVTA), cross-modal interleaved feature representation (CIFR), and free-form answer-enhanced multi-task learning (FFAE). FVTA extracts the most relevant regions in image-text pairs through fine-grained visual-text feature alignment. CIFR captures cross-modal sequential interactions via cross-modal interleaved feature representation. FFAE leverages auxiliary knowledge from open-ended questions through free-form answer-enhanced multi-task learning, improving the model's capability for open-ended Med-VQA. Experimental results show that CMI-MTL outperforms the existing state-of-the-art methods on three Med-VQA datasets: VQA-RAD, SLAKE, and OVQA. Furthermore, we conduct more interpretability experiments to prove the effectiveness. The code is publicly available at https://github.com/BioMedIA-repo/CMI-MTL."
    },
    {
      "paperId": "6404f8ec8e4821f40d5c4944c9ebb20a0bd67bfc",
      "externalIds": {
        "ArXiv": "2511.01243",
        "DBLP": "journals/corr/abs-2511-01243",
        "DOI": "10.48550/arXiv.2511.01243",
        "CorpusId": 282738929
      },
      "corpusId": 282738929,
      "title": "CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 8,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.01243, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2371374657",
          "name": "Yu Tian"
        },
        {
          "authorId": "2391721120",
          "name": "Zhongheng Yang"
        },
        {
          "authorId": "2390436775",
          "name": "Chenshi Liu"
        },
        {
          "authorId": "2390398013",
          "name": "Yiyun Su"
        },
        {
          "authorId": "2390518386",
          "name": "Ziwei Hong"
        },
        {
          "authorId": "2391108117",
          "name": "Zexi Gong"
        },
        {
          "authorId": "2390944401",
          "name": "Jingyuan Xu"
        }
      ],
      "abstract": "Brain lesion segmentation remains challenging due to small, low-contrast lesions, anisotropic sampling, and cross-slice discontinuities. We propose CenterMamba-SAM, an end-to-end framework that freezes a pretrained backbone and trains only lightweight adapters for efficient fine-tuning. At its core is the CenterMamba encoder, which employs a novel 3x3 corner-axis-center short-sequence scanning strategy to enable center-prioritized, axis-reinforced, and diagonally compensated information aggregation. This design enhances sensitivity to weak boundaries and tiny foci while maintaining sparse yet effective feature representation. A memory-driven structural prompt generator maintains a prototype bank across neighboring slices, enabling automatic synthesis of reliable prompts without user interaction, thereby improving inter-slice coherence. The memory-augmented multi-scale decoder integrates memory attention modules at multiple levels, combining deep supervision with progressive refinement to restore fine details while preserving global consistency. Extensive experiments on public benchmarks demonstrate that CenterMamba-SAM achieves state-of-the-art performance in brain lesion segmentation."
    },
    {
      "paperId": "24232791cbf7c190487a81416effc9d92086f9a2",
      "externalIds": {
        "ArXiv": "2511.01202",
        "DBLP": "journals/corr/abs-2511-01202",
        "DOI": "10.48550/arXiv.2511.01202",
        "CorpusId": 282739007
      },
      "corpusId": 282739007,
      "title": "Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.01202, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2390401403",
          "name": "Bo Bai"
        }
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in numerous real- world applications. While the vast majority of research conducted from an experimental perspective is progressing rapidly, it demands substantial computational power, data, and other resources. Therefore, how to open the black-box of LLMs from a theoretical standpoint has become a critical challenge. This paper takes the theory of rate-distortion function, directed information, and Granger causality as its starting point to investigate the information-theoretic principles behind LLMs, leading to the development of semantic information theory for LLMs, where the fundamental unit is token, rather than bits that lacks any semantic meaning. By defining the probabilistic model of LLMs, we discuss structure-agnostic information-theoretic measures, such as the directed rate- distortion function in pre-training, the directed rate-reward function in post-training, and the semantic information flow in inference phase. This paper also delves deeply into the theory of token-level semantic embedding and the information-theoretically optimal vectorization method. Thereafter, we propose a general definition of autoregression LLM, where the Transformer architecture and its performance such as ELBO, generalization error bound, memory capacity, and semantic information measures can be derived theoretically. Other architectures, such as Mamba/Mamba2 and LLaDA, are also discussed in our framework. Consequently, this paper provides a theoretical framework for understanding LLMs from the perspective of semantic information theory, which also offers the necessary theoretical tools for further in-depth research."
    },
    {
      "paperId": "c5615ac883cd1da4fa4dd748319c261059db311c",
      "externalIds": {
        "ArXiv": "2511.01299",
        "CorpusId": 282739392
      },
      "corpusId": 282739392,
      "title": "Towards General Auditory Intelligence: Large Multimodal Models for Machine Listening and Speaking",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.01299, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2239283908",
          "name": "Siyin Wang"
        },
        {
          "authorId": "2385457957",
          "name": "Zengrui Jin"
        },
        {
          "authorId": "2247237695",
          "name": "Changli Tang"
        },
        {
          "authorId": "2373501095",
          "name": "Qiujia Li"
        },
        {
          "authorId": "2324809463",
          "name": "Bo Li"
        },
        {
          "authorId": "2390875613",
          "name": "Chen Chen"
        },
        {
          "authorId": "2223605554",
          "name": "Yuchen Hu"
        },
        {
          "authorId": "2257283478",
          "name": "Wenyi Yu"
        },
        {
          "authorId": "2322710992",
          "name": "Yixuan Li"
        },
        {
          "authorId": "2322803697",
          "name": "Jimin Zhuang"
        },
        {
          "authorId": "2321417870",
          "name": "Yudong Yang"
        },
        {
          "authorId": "2249764807",
          "name": "Mingqiu Wang"
        },
        {
          "authorId": "2374425196",
          "name": "Michael Han"
        },
        {
          "authorId": "2290634593",
          "name": "Yifan Ding"
        },
        {
          "authorId": "2327820343",
          "name": "Ju Bai"
        },
        {
          "authorId": "2372299394",
          "name": "Tom Ouyang"
        },
        {
          "authorId": "2275193337",
          "name": "Shuo-Yiin Chang"
        },
        {
          "authorId": "2135092817",
          "name": "Xianzhao Chen"
        },
        {
          "authorId": "2323002776",
          "name": "Xiaohai Tian"
        },
        {
          "authorId": "2323523073",
          "name": "Jun Zhang"
        },
        {
          "authorId": "2257383962",
          "name": "Lu Lu"
        },
        {
          "authorId": "2107310187",
          "name": "Guangzhi Sun"
        },
        {
          "authorId": "2390401093",
          "name": "Zhehuai Chen"
        },
        {
          "authorId": "2268630085",
          "name": "Ji Wu"
        },
        {
          "authorId": "2340938925",
          "name": "Bowen Zhou"
        },
        {
          "authorId": "2373973958",
          "name": "Yuxuan Wang"
        },
        {
          "authorId": "2279918122",
          "name": "Tara N. Sainath"
        },
        {
          "authorId": "2374272851",
          "name": "Yonghui Wu"
        },
        {
          "authorId": "2305964831",
          "name": "Chao Zhang"
        }
      ],
      "abstract": "In the era of large language models (LLMs) and artificial general intelligence (AGI), computer audition must evolve beyond traditional paradigms to fully leverage the capabilities of foundation models, towards more comprehensive understanding, more natural generation and more human-like interaction. Audio, as a modality rich in semantic, emotional, and contextual cues, plays a vital role in achieving naturalistic and embodied machine intelligence. This survey provides a comprehensive review of recent progress in integrating audio into LLMs, with a focus on four key areas: audio comprehension, audio generation, speech-based interaction, and audio-visual understanding. We analyze how LLMs are reshaping audio perception and reasoning, enabling systems to understand sound at a deeper semantic level, generate expressive audio outputs, and engage in human-like spoken interaction. Furthermore, we explore how the fusion of audio and visual modalities enhances situational awareness and cross-modal reasoning, pushing the boundaries of multimodal intelligence. This survey not only synthesizes existing research but also identifies critical challenges and future directions for building audio-native AGI systems capable of perceiving, understanding, and interacting through sound as naturally as humans do."
    },
    {
      "paperId": "946421b79602094406f9a0944d9e31997e7893fd",
      "externalIds": {
        "DBLP": "journals/corr/abs-2511-01768",
        "ArXiv": "2511.01768",
        "DOI": "10.48550/arXiv.2511.01768",
        "CorpusId": 282739560
      },
      "corpusId": 282739560,
      "title": "UniLION: Towards Unified Autonomous Driving Model with Linear Group RNNs",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.01768, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2311450810",
          "name": "Zhe Liu"
        },
        {
          "authorId": "2239071867",
          "name": "Jinghua Hou"
        },
        {
          "authorId": "2284698331",
          "name": "Xiaoqing Ye"
        },
        {
          "authorId": "2297821328",
          "name": "Jingdong Wang"
        },
        {
          "authorId": "2310758544",
          "name": "Hengshuang Zhao"
        },
        {
          "authorId": "2311690017",
          "name": "Xiang Bai"
        }
      ],
      "abstract": "Although transformers have demonstrated remarkable capabilities across various domains, their quadratic attention mechanisms introduce significant computational overhead when processing long-sequence data. In this paper, we present a unified autonomous driving model, UniLION, which efficiently handles large-scale LiDAR point clouds, high-resolution multi-view images, and even temporal sequences based on the linear group RNN operator (i.e., performs linear RNN for grouped features). Remarkably, UniLION serves as a single versatile architecture that can seamlessly support multiple specialized variants (i.e., LiDAR-only, temporal LiDAR, multi-modal, and multi-modal temporal fusion configurations) without requiring explicit temporal or multi-modal fusion modules. Moreover, UniLION consistently delivers competitive and even state-of-the-art performance across a wide range of core tasks, including 3D perception (e.g., 3D object detection, 3D object tracking, 3D occupancy prediction, BEV map segmentation), prediction (e.g., motion prediction), and planning (e.g., end-to-end planning). This unified paradigm naturally simplifies the design of multi-modal and multi-task autonomous driving systems while maintaining superior performance. Ultimately, we hope UniLION offers a fresh perspective on the development of 3D foundation models in autonomous driving. Code is available at https://github.com/happinesslz/UniLION"
    },
    {
      "paperId": "9fbd017b6d81dda13a5ce728bb2bf7f1fb613c5d",
      "externalIds": {
        "ArXiv": "2511.01581",
        "DBLP": "journals/corr/abs-2511-01581",
        "DOI": "10.48550/arXiv.2511.01581",
        "CorpusId": 282739067
      },
      "corpusId": 282739067,
      "title": "ExplicitLM: Decoupling Knowledge from Parameters via Explicit Memory Banks",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.01581, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2338906608",
          "name": "Chengzhang Yu"
        },
        {
          "authorId": "2390433179",
          "name": "Zening Lu"
        },
        {
          "authorId": "2391887043",
          "name": "Chenyang Zheng"
        },
        {
          "authorId": "2374168650",
          "name": "Chiyue Wang"
        },
        {
          "authorId": "2390671019",
          "name": "Yiming Zhang"
        },
        {
          "authorId": "2365471509",
          "name": "Zhanpeng Jin"
        }
      ],
      "abstract": "Large language models suffer from knowledge staleness and lack of interpretability due to implicit knowledge storage across entangled network parameters, preventing targeted updates and reasoning transparency. We propose ExplicitLM, a novel architecture featuring a million-scale external memory bank storing human-readable knowledge as token sequences, enabling direct inspection and modification. We design a differentiable two-stage retrieval mechanism with efficient coarse-grained filtering via product key decomposition (reducing complexity from $\\mathcal{O}(N \\cdot |I|)$ to $\\mathcal{O}(\\sqrt{N} \\cdot |I|)$) and fine-grained Gumbel-Softmax matching for end-to-end training. Inspired by dual-system cognitive theory, we partition knowledge into frozen explicit facts (20%) and learnable implicit patterns (80%), maintained through Exponential Moving Average updates for stability. ExplicitLM achieves up to 43.67% improvement on knowledge-intensive tasks versus standard Transformers, with 3.62$\\times$ gains in low-data regimes (10k samples). Analysis shows strong correlations between memory retrieval and performance, with correct predictions achieving 49% higher hit rates. Unlike RAG systems with frozen retrieval, our jointly optimized architecture demonstrates that interpretable, updatable models can maintain competitive performance while providing unprecedented knowledge transparency."
    },
    {
      "paperId": "f530c8184dd1926907fe3a845d715c53a0c8882b",
      "externalIds": {
        "DOI": "10.1145/3680207.3765693",
        "CorpusId": 283266006
      },
      "corpusId": 283266006,
      "title": "Poster: Home-based, On-Device, Non-contact Sleep Staging with Infrared Video",
      "venue": "Proceedings of the 31st Annual International Conference on Mobile Computing and Networking",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3680207.3765693?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3680207.3765693, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2394356039",
          "name": "Kunmin Jang"
        },
        {
          "authorId": "2238012639",
          "name": "You Rim Choi"
        },
        {
          "authorId": "2395855008",
          "name": "Dongik Park"
        },
        {
          "authorId": "2238163611",
          "name": "Hyunwoo Shin"
        },
        {
          "authorId": "2238150212",
          "name": "Hyung-Sin Kim"
        }
      ],
      "abstract": "Sleep is essential for health and well-being, yet the gold-standard method for sleep stage assessment, polysomnography (PSG), requires overnight monitoring with numerous wired sensors and manual scoring, limiting its scalability and comfort in real-world settings. We present ViSS, the first end-to-end deep learning framework for fully non-contact sleep staging directly from raw infrared video. To capture the dual temporal structure of sleep, ViSS integrates per-epoch motion encoding and long-range sequence modeling in a compact hierarchical architecture, without relying on intermediate physiological signals. Evaluated on the largest infrared video dataset to date, including both healthy individuals and patients, ViSS achieves 80% accuracy and 0.78 macro F1-score using fewer than 7 million parameters. These results establish infrared video as a viable modality for automated sleep staging and highlight the potential of ViSS for scalable, privacy-preserving home-based assessment."
    },
    {
      "paperId": "f729b0265f7f9c58e94208ae6e93049c0b5ce797",
      "externalIds": {
        "DOI": "10.1145/3748636.3766534",
        "CorpusId": 283747244
      },
      "corpusId": 283747244,
      "title": "MambaLoc: Lightweight Indoor Localization For Unmanned Vehicles Using Cross-Modal Knowledge Distillation",
      "venue": "Proceedings of the 33rd ACM International Conference on Advances in Geographic Information Systems",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3748636.3766534?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3748636.3766534, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2397879166",
          "name": "Mohab Bahnassy"
        },
        {
          "authorId": "2397880813",
          "name": "Omar Saqr"
        },
        {
          "authorId": "48800268",
          "name": "Hamada Rizk"
        },
        {
          "authorId": "2266787664",
          "name": "Moustafa Youssef"
        }
      ],
      "abstract": "Indoor localization of Unmanned Vehicles is critical for tasks requiring low latency and low power consumption. Channel State Information (CSI) systems provide a promising approach by capturing fine-grained signal features, but often suffer from limited accuracy in complex environments or high computational cost. To address this, we propose MambaLoc, an indoor localization framework that balances efficiency and precision by leveraging Mamba architectures with cross-modal knowledge distillation. In offline training, a Transformer based on Ultra-Wideband (UWB) data acts as a teacher to guide the CSI-based Mamba student, improving robustness to multipath effects without relying on UWB during inference. The distillation process is further enhanced using a Gaussian Mixture Model (GMM) in both teacher and student, enabling effective transfer of UWB-derived spatial representations. Evaluations on real-world environments show that MambaLoc reduces MAE by 22.53% over a baseline Mamba, while cutting FLOPs by 65.1% and inference time by 58% compared to a Transformer model, making it a practical and deployable solution for UV localization."
    },
    {
      "paperId": "9043376f111c20dfa097f3f8ba67f16533f81df0",
      "externalIds": {
        "DOI": "10.1109/ICTAI66417.2025.00080",
        "CorpusId": 283923054
      },
      "corpusId": 283923054,
      "title": "MambaPan3D: Mamba-Transformer for 3D LiDAR Panoptic Segmentation with Adaptive Coordinate Fusion",
      "venue": "IEEE International Conference on Tools with Artificial Intelligence",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICTAI66417.2025.00080?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICTAI66417.2025.00080, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2399115808",
          "name": "RuiShen Zhou"
        },
        {
          "authorId": "2399056757",
          "name": "K. L. E. Law"
        }
      ],
      "abstract": "With the advent of autonomous intelligent systems, such as humanoid robots, environmental perception should require rapid and accurate real-time 3D scene interpretation. LiDAR sensors are core and accurate distance measuring components, but it is complicated to process the unstructured, sparse, and unevenly distributed nature of LiDAR point cloud data while meeting the real-time object classification needs. To address the limitations of current 3D LiDAR-based panoptic segmentation methods, we propose a MambaPan3D design. It is a hybrid architecture that integrates Mamba and Transformer models for efficient and accurate 3D point cloud understanding. Our framework solves two key challenges: 1) geometric ambiguity caused by sparse and irregular LiDAR point cloud distributions, and 2) inefficient long-range dependency modeling in largescale scenes. Specifically, CartPolar-KAN embedding, a novel positional encoding strategy, is introduced to interpret between Cartesian and polar coordinates by adding a Kolmogorov-Arnold network (KAN) with learnable B-spline basis functions. The module dynamically fuses multi-coordinate features to overcome the limitations of fixed Bird's-Eye View (BEV) quantization. Additionally, our Mamba-Transformer Decoder combines the global attention capabilities of the Transformer and the linear computational efficiency of the Mamba state-space model to achieve real-time inference while maintaining the global receptive field. Extensive experiments on SemanticKITTI dataset demonstrated state-of-the-art performance. The panoptic quality (PQ) could reach 63.3 % in complex urban scenes, i.e., 1.3 % higher than the current optimal baseline method. The proposed framework provides a powerful solution for real-time situational awareness in autonomous driving systems, balancing accuracy, efficiency, and scalability. Our MambaPan3D model offers a robust solution for real-time situational awareness in autonomous systems through balancing accuracy, efficiency, and scalability."
    },
    {
      "paperId": "aa74446324246e0e0cfe8f2f7c0aeed748134075",
      "externalIds": {
        "ArXiv": "2511.01096",
        "CorpusId": 282738980
      },
      "corpusId": 282738980,
      "title": "Hyper Hawkes Processes: Interpretable Models of Marked Temporal Point Processes",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.01096, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2363495449",
          "name": "Alex Boyd"
        },
        {
          "authorId": "2337685943",
          "name": "Andrew Warrington"
        },
        {
          "authorId": "1405125430",
          "name": "Taha A. Kass-Hout"
        },
        {
          "authorId": "2303385525",
          "name": "Parminder Bhatia"
        },
        {
          "authorId": "2390401444",
          "name": "Danica Xiao"
        }
      ],
      "abstract": "Foundational marked temporal point process (MTPP) models, such as the Hawkes process, often use inexpressive model families in order to offer interpretable parameterizations of event data. On the other hand, neural MTPPs models forego this interpretability in favor of absolute predictive performance. In this work, we present a new family MTPP models: the hyper Hawkes process (HHP), which aims to be as flexible and performant as neural MTPPs, while retaining interpretable aspects. To achieve this, the HHP extends the classical Hawkes process to increase its expressivity by first expanding the dimension of the process into a latent space, and then introducing a hypernetwork to allow time- and data-dependent dynamics. These extensions define a highly performant MTPP family, achieving state-of-the-art performance across a range of benchmark tasks and metrics. Furthermore, by retaining the linearity of the recurrence, albeit now piecewise and conditionally linear, the HHP also retains much of the structure of the original Hawkes process, which we exploit to create direct probes into how the model creates predictions. HHP models therefore offer both state-of-the-art predictions, while also providing an opportunity to ``open the box''and inspect how predictions were generated."
    },
    {
      "paperId": "f00608d498b34237fe54a5a7b282c5f77cd68ea2",
      "externalIds": {
        "DBLP": "journals/jcphy/YeoSKC26",
        "DOI": "10.1016/j.jcp.2025.114474",
        "CorpusId": 282725848
      },
      "corpusId": 282725848,
      "title": "Model-free learning of random dynamical system from noisy observations",
      "venue": "Journal of Computational Physics",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.jcp.2025.114474?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.jcp.2025.114474, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2382056627",
          "name": "Kyongmin Yeo"
        },
        {
          "authorId": "2215721525",
          "name": "Hyomin Shin"
        },
        {
          "authorId": "2301540369",
          "name": "Heechang Kim"
        },
        {
          "authorId": "2283111694",
          "name": "Minseok Choi"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "6f2a9d5c0c3a6d7c355060c177972e9764a3ed46",
      "externalIds": {
        "PubMedCentral": "12653453",
        "DOI": "10.3390/life15111706",
        "CorpusId": 282764327,
        "PubMed": "41302130"
      },
      "corpusId": 282764327,
      "title": "PICU Face and Thoracoabdominal Detection Using Self-Supervised Divided Space\u2013Time Mamba",
      "venue": "Life",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12653453, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2351280474",
          "name": "Mohamed Khalil Ben Salah"
        },
        {
          "authorId": "2244955595",
          "name": "P. Jouvet"
        },
        {
          "authorId": "2479033",
          "name": "R. Noumeir"
        }
      ],
      "abstract": "Non-contact vital sign monitoring in Pediatric Intensive Care Units is challenged by frequent occlusions, data scarcity, and the need for temporally stable anatomical tracking to extract reliable physiological signals. Traditional detectors produce unstable tracking, while video transformers are too computationally intensive for deployment on resource-limited clinical hardware. We introduce Divided Space\u2013Time Mamba, an architecture that decouples spatial and temporal feature learning using State Space Models to achieve linear-time complexity, over 92% lower than standard transformers. To handle data scarcity, we employ self-supervised pre-training with masked autoencoders on over 50 k domain-specific video clips and further enhance robustness with multimodal RGB-D input. Our model demonstrates superior performance, achieving 0.96 mAP@0.5, 0.62 mAP50-95, and 0.95 rotated IoU. Operating at 23 FPS (43 ms latency), our method is approximately 1.9\u00d7 faster than VideoMAE and 5.7\u00d7 faster than frame-wise YOLOv8, demonstrating its suitability for real-time clinical monitoring."
    },
    {
      "paperId": "67fc5b628c741d4f5ad83b7d96461bcf22cb9d45",
      "externalIds": {
        "DOI": "10.1002/mp.70117",
        "CorpusId": 282757111,
        "PubMed": "41188014"
      },
      "corpusId": 282757111,
      "title": "Multimodal synthetic CT generation in tumor radiotherapy",
      "venue": "Medical Physics (Lancaster)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1002/mp.70117?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1002/mp.70117, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2391391936",
          "name": "Xue Li"
        },
        {
          "authorId": "2390594872",
          "name": "Rongli Ran"
        },
        {
          "authorId": "2382081160",
          "name": "Liang Wu"
        },
        {
          "authorId": "2352888743",
          "name": "Jianfeng Qiu"
        }
      ],
      "abstract": "The use of MRI\u2010guided radiation therapy (MRIgRT) has shown considerable advantages. However, the acquisition of electron density information still relies on Computed tomography (CT) images. In the context of medical image synthesis, achieving high global and local accuracy, as well as efficient synthetic CT (sCT) generation, remains a key challenge."
    },
    {
      "paperId": "8d2d817163304365614cf16ffcadd0a562ed24c9",
      "externalIds": {
        "DOI": "10.1016/j.rse.2025.114950",
        "CorpusId": 280909702
      },
      "corpusId": 280909702,
      "title": "Robust and timely within-season conterminous United States crop type mapping using Landsat Sentinel-2 time series and the transformer architecture",
      "venue": "Remote Sensing of Environment",
      "year": 2025,
      "citationCount": 5,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.rse.2025.114950?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.rse.2025.114950, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2268645408",
          "name": "Hankui K. Zhang"
        },
        {
          "authorId": "2117689136",
          "name": "Yu Shen"
        },
        {
          "authorId": "2258309890",
          "name": "Xiaoyang Zhang"
        },
        {
          "authorId": "2377511138",
          "name": "Junjie Li"
        },
        {
          "authorId": "2367249544",
          "name": "Zhengwei Yang"
        },
        {
          "authorId": "2278200510",
          "name": "Yijia Xu"
        },
        {
          "authorId": "2377839198",
          "name": "Chen Zhang"
        },
        {
          "authorId": "2367155109",
          "name": "Liping Di"
        },
        {
          "authorId": "2242861204",
          "name": "David P. Roy"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "969e4d1d98a1a4fbb06d088d72bc2ef92396aa56",
      "externalIds": {
        "DOI": "10.1109/TCSVT.2025.3576772",
        "CorpusId": 279180274
      },
      "corpusId": 279180274,
      "title": "Tex2Sem: Learning From Textures to Semantics for Robust Semantic Correspondence",
      "venue": "IEEE transactions on circuits and systems for video technology (Print)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2025.3576772?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2025.3576772, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2338652198",
          "name": "Zenghui Wang"
        },
        {
          "authorId": "2089098780",
          "name": "Songlin Du"
        },
        {
          "authorId": "47971248",
          "name": "Y. Yan"
        },
        {
          "authorId": "2275617003",
          "name": "Guobao Xiao"
        },
        {
          "authorId": "2156553353",
          "name": "Xiaobo Lu"
        }
      ],
      "abstract": "Recent advances in semantic correspondence have witnessed growing interest in vision foundation models, particularly stable diffusion (SD) and self-distillation with no labels (DINO). However, existing methods underutilize the matching potential of SD and DINOv2 features and show similar background interference patterns. They lack texture-to-semantic learning and intra- and inter-image feature interaction. This study proposes Tex2Sem, a framework learning from textures to semantics, to address the two problems. For the first problem, we propose a texture-to-semantic learning paradigm that achieves texture-semantic trade-offs on features and correlation maps, including progressive fusion and correlation map computation. The SD and DINOv2 features are aggregated from textures to semantics to produce multi-stage progressive fusion features. The resulting multi-stage progressive fusion correlation maps improve semantic correspondence significantly. For the second problem, MamFormer, a hybrid architecture of Mamba-2 and Transformer, is proposed to improve intra- and inter-image feature aggregation and interaction. It enhances foreground focus and background suppression. Given the high computational cost of processing all-stage progressive fusion features, the terminal-stage aggregation and interaction mechanism (TAIM) is proposed to enhance feature learning efficiency. Experiments demonstrate that Tex2Sem achieves state-of-the-art performance on SPair-71k, AP-10K, and PF-PASCAL. Furthermore, Tex2Sem shows remarkable generalization capabilities in cross-species, cross-family, and cross-dataset matching and demonstrates the potential for applications in video swap and human poseestimation. Code is available at https://github.com/wzhlearning/Tex2Sem."
    },
    {
      "paperId": "a8757bf40c6e427ec7b772c79cd0c126d6dbab6c",
      "externalIds": {
        "DOI": "10.1109/JSEN.2025.3609806",
        "CorpusId": 281445740
      },
      "corpusId": 281445740,
      "title": "CM2-Net: A Hybrid CNN\u2013Mamba2 Net for 3-D Electromagnetic Tomography Image Reconstruction",
      "venue": "IEEE Sensors Journal",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JSEN.2025.3609806?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JSEN.2025.3609806, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2381696602",
          "name": "Yuxiang Liu"
        },
        {
          "authorId": "2297725539",
          "name": "Ronghua Zhang"
        },
        {
          "authorId": "2319649152",
          "name": "Wenying Fu"
        },
        {
          "authorId": "2381610326",
          "name": "Jiale Chen"
        },
        {
          "authorId": "2381522033",
          "name": "Anan Dai"
        }
      ],
      "abstract": "Three-dimensional electromagnetic tomography (3D EMT) image reconstruction is characterized by larger data volumes, more complex structures, and stronger spatial correlations. These characteristics often result in insufficient reconstruction accuracy when using traditional imaging algorithms. To address this challenge, we propose a deep learning-based reconstruction algorithm, hybrid CNN\u2013Mamba2-Net (CM2-Net), specifically designed for 3D-EMT image reconstruction. The architecture of the CM2-Net consists of three main components: an initial reconstruction module, a hybrid encoder, and a fusion decoder. The CNN pathway is capable of capturing fine-grained local features, while the Mamba-2 pathway, equipped with a linear attention mechanism, excels at learning and representing the 3-D spatial structure of the input data. Features from the CNN and Mamba2 pathways are fused at three different scales, summed point-wise, and restored to the original spatial resolution through the decoder. This design enables more accurate feature extraction and representation. The proposed model outperforms the baseline across five quantitative evaluation metrics, achieving a Dice score of 0.718, accuracy of 0.943, precision of 0.657, recall of 0.794, and an IoU of 0.561, demonstrating strong performance in the 3D-EMT reconstruction task."
    },
    {
      "paperId": "55de2aa69a45d0715d42394c22b018cfe30b1050",
      "externalIds": {
        "DOI": "10.1109/TCSVT.2025.3574657",
        "CorpusId": 278983205
      },
      "corpusId": 278983205,
      "title": "MaDiNet: Mamba Diffusion Network for SAR Target Detection",
      "venue": "IEEE transactions on circuits and systems for video technology (Print)",
      "year": 2025,
      "citationCount": 7,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2025.3574657?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2025.3574657, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2284955538",
          "name": "Jie Zhou"
        },
        {
          "authorId": "2239168580",
          "name": "Yongxiang Liu"
        },
        {
          "authorId": "2275585769",
          "name": "Bowen Peng"
        },
        {
          "authorId": "2150980738",
          "name": "Li Liu"
        },
        {
          "authorId": "2144439166",
          "name": "Xiang Li"
        }
      ],
      "abstract": "The fundamental challenge in SAR target detection lies in developing discriminative, efficient, and robust representations of target characteristics within intricate non-cooperative environments. However, accurate target detection is impeded by factors including the sparse distribution and discrete features of the targets, as well as complex background interference. In this study, we propose a Gamma Diffusion Model Network with MambaSAR module (MaDiNet) for SAR target detection. Specifically, MaDiNet leverages the Gamma distribution to model the statistical characteristics of SAR images, and conceptulizes SAR target detection as the task of generating target bounding boxes in the image space. Furthermore, we design a MambaSAR module to capture intricate spatial structural information of targets and enhance the capability of the model to differentiate between targets and complex backgrounds. The experimental results on multi-class target detection datasets have all achieved SOTA, with a particularly notable improvement of 6.7% in mAP50 on the ODSOG-1.0 dataset, proving the effectiveness of the proposed network. Code is available at https://github.com/JoyeZLearning/MaDiNet"
    },
    {
      "paperId": "69609c726d6db1dfe6b5a9d279fd4025b3e4ee47",
      "externalIds": {
        "DBLP": "journals/tjs/ZhengLZFLLL25",
        "DOI": "10.1007/s11227-025-07981-6",
        "CorpusId": 282737327
      },
      "corpusId": 282737327,
      "title": "CDMM: conditional diffusion model with mamba for low-light underwater image enhancement",
      "venue": "Journal of Supercomputing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11227-025-07981-6?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11227-025-07981-6, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2261249363",
          "name": "Jianhua Zheng"
        },
        {
          "authorId": "2260919284",
          "name": "Junde Lu"
        },
        {
          "authorId": "2260837987",
          "name": "Ruolin Zhao"
        },
        {
          "authorId": "2260831797",
          "name": "Yusha Fu"
        },
        {
          "authorId": "2378267278",
          "name": "Jinfang Liu"
        },
        {
          "authorId": "2387140752",
          "name": "Zhaoxi Luo"
        },
        {
          "authorId": "2390412981",
          "name": "Xiaomin Li"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "131a6031f2eaa21fa58fb8f42e3cc2cde005e20d",
      "externalIds": {
        "ArXiv": "2511.00576",
        "DBLP": "journals/corr/abs-2511-00576",
        "DOI": "10.48550/arXiv.2511.00576",
        "CorpusId": 282739409
      },
      "corpusId": 282739409,
      "title": "FlashEVA: Accelerating LLM inference via Efficient Attention",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.00576, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2390403046",
          "name": "Juan Gabriel Kostelec"
        },
        {
          "authorId": "2391921851",
          "name": "Qinghai Guo"
        }
      ],
      "abstract": "Transformer models have revolutionized natural language processing, achieving state-of-the-art performance and demonstrating remarkable scalability. However, their memory demands, particularly due to maintaining full context in memory, pose significant challenges for inference. In this paper, we present FlashEVA, an efficient implementation of EVA (Efficient Attention via Control Variates), and demonstrate how to finetune transformers to adapt to FlashEVA attention. Our method enables fine-tuning of Transformer models with as few as 1.5B tokens while preserving effectiveness across various downstream tasks. Notably, FlashEVA achieves up to 6.7x higher throughput and 5x lower peak GPU memory usage during inference compared to standard Transformer implementations. Despite these improvements, we observe limitations in retrieval-focused tasks. Our implementation offers control over the trade-off between throughput and accuracy through adjustable hyperparameters, providing flexibility for diverse use cases. This work represents a significant step towards more efficient and adaptable Transformer-based models for inference."
    },
    {
      "paperId": "4d3e638685da22b7301b73d517cf4511ee4ccc42",
      "externalIds": {
        "ArXiv": "2511.00443",
        "DBLP": "journals/corr/abs-2511-00443",
        "DOI": "10.48550/arXiv.2511.00443",
        "CorpusId": 282738906
      },
      "corpusId": 282738906,
      "title": "Region-Aware Reconstruction Strategy for Pre-training fMRI Foundation Model",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.00443, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2390395979",
          "name": "Ruthwik Reddy Doodipala"
        },
        {
          "authorId": "2345208006",
          "name": "Pankaj Pandey"
        },
        {
          "authorId": "2390396740",
          "name": "Carolina Torres Rojas"
        },
        {
          "authorId": "3061540",
          "name": "Manob Saikia"
        },
        {
          "authorId": "2273121628",
          "name": "Ranganatha Sitaram"
        }
      ],
      "abstract": "The emergence of foundation models in neuroimaging is driven by the increasing availability of large-scale and heterogeneous brain imaging datasets. Recent advances in self-supervised learning, particularly reconstruction-based objectives, have demonstrated strong potential for pretraining models that generalize effectively across diverse downstream functional MRI (fMRI) tasks. In this study, we explore region-aware reconstruction strategies for a foundation model in resting-state fMRI, moving beyond approaches that rely on random region masking. Specifically, we introduce an ROI-guided masking strategy using the Automated Anatomical Labelling Atlas (AAL3), applied directly to full 4D fMRI volumes to selectively mask semantically coherent brain regions during self-supervised pretraining. Using the ADHD-200 dataset comprising 973 subjects with resting-state fMRI scans, we show that our method achieves a 4.23% improvement in classification accuracy for distinguishing healthy controls from individuals diagnosed with ADHD, compared to conventional random masking. Region-level attribution analysis reveals that brain volumes within the limbic region and cerebellum contribute most significantly to reconstruction fidelity and model representation. Our results demonstrate that masking anatomical regions during model pretraining not only enhances interpretability but also yields more robust and discriminative representations. In future work, we plan to extend this approach by evaluating it on additional neuroimaging datasets, and developing new loss functions explicitly derived from region-aware reconstruction objectives. These directions aim to further improve the robustness and interpretability of foundation models for functional neuroimaging."
    },
    {
      "paperId": "13a147f17263993ac1f61fd07060dee66a273ab3",
      "externalIds": {
        "DOI": "10.1016/j.neunet.2025.108281",
        "CorpusId": 282785181,
        "PubMed": "41232223"
      },
      "corpusId": 282785181,
      "title": "Open-world surgical video generation via dual-visual diffusion and dual-annealed generation.",
      "venue": "Neural Networks",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.neunet.2025.108281?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.neunet.2025.108281, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2391092942",
          "name": "Ning Ma"
        },
        {
          "authorId": "2391327070",
          "name": "Shu Yang"
        },
        {
          "authorId": "2390875083",
          "name": "Yizhao Zhou"
        },
        {
          "authorId": "2222840428",
          "name": "Chao Zhang"
        },
        {
          "authorId": "2391067025",
          "name": "Jian Chen"
        },
        {
          "authorId": "2390692790",
          "name": "Xiaoman He"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "5c864d554e2be56b6bad7b5a88c06a231f6ee297",
      "externalIds": {
        "DOI": "10.1016/j.patcog.2025.112664",
        "CorpusId": 282870577
      },
      "corpusId": 282870577,
      "title": "DAF-Mamba: Dynamic Selective and Adaptive Fused Mamba for Cardiac Image Segmentation",
      "venue": "Pattern Recognition",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.patcog.2025.112664?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.patcog.2025.112664, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2359504457",
          "name": "Yonglin Chen"
        },
        {
          "authorId": "2391289665",
          "name": "Yixiang Wang"
        },
        {
          "authorId": "2392671902",
          "name": "Zhongyuan Liu"
        },
        {
          "authorId": "2391266373",
          "name": "Yuyan Weng"
        },
        {
          "authorId": "2342267465",
          "name": "Chihui Long"
        },
        {
          "authorId": "2391360384",
          "name": "Yalong Yang"
        },
        {
          "authorId": "2192672187",
          "name": "Jinhui Tang"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "932cf6093ebd011d9fbb4671d7919a81a372260c",
      "externalIds": {
        "DOI": "10.1016/j.eswa.2025.130085",
        "CorpusId": 282900524
      },
      "corpusId": 282900524,
      "title": "Costal Cartilage Segmentation with Topology Guided Deformable Mamba: Method and Benchmark",
      "venue": "Expert systems with applications",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.eswa.2025.130085?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.eswa.2025.130085, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2209585591",
          "name": "Senmao Wang"
        },
        {
          "authorId": "2277609457",
          "name": "Haifan Gong"
        },
        {
          "authorId": "2300166537",
          "name": "Runmeng Cui"
        },
        {
          "authorId": "2277607346",
          "name": "Boyao Wan"
        },
        {
          "authorId": "2279961525",
          "name": "Zhonglin Hu"
        },
        {
          "authorId": "2316103121",
          "name": "Haiqing Yang"
        },
        {
          "authorId": "2218078021",
          "name": "Jingyang Zhou"
        },
        {
          "authorId": "2391691997",
          "name": "Haiyue Jiang"
        },
        {
          "authorId": "2218281146",
          "name": "Lin Lin"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "922ca758d2867090bf5b8e7bd41da1a47aaf0ec2",
      "externalIds": {
        "DBLP": "journals/tits/ZhangCSLBZZ25",
        "DOI": "10.1109/TITS.2025.3590177",
        "CorpusId": 280371624
      },
      "corpusId": 280371624,
      "title": "SLPDR: A Benchmark for Ship License Plate Detection and Recognition",
      "venue": "IEEE transactions on intelligent transportation systems (Print)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TITS.2025.3590177?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TITS.2025.3590177, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2155336587",
          "name": "Youmei Zhang"
        },
        {
          "authorId": "2374000597",
          "name": "Junyu Chen"
        },
        {
          "authorId": "2278049280",
          "name": "Ran Song"
        },
        {
          "authorId": "2261681199",
          "name": "Yonghuai Liu"
        },
        {
          "authorId": "2321455697",
          "name": "Ardhendu Behera"
        },
        {
          "authorId": "2155080480",
          "name": "Mingxin Zhang"
        },
        {
          "authorId": "2256596531",
          "name": "Wei Zhang"
        }
      ],
      "abstract": "Ship identification is a prerequisite for the intelligent management of maritime transportation, yet existing research is confined to broad ship detection and categorization, which only provides the ship\u2019s location or type instead of its identification. Inspired by the research on the Car License Plate (CLP), we make the first attempt to propose the concept of the Ship License Plate (SLP). In addition, the limited data hinders research on ship identification. To overcome this obstacle, we construct the first large-scale Ship License Plate Detection and Recognition (SLPDR) dataset, which contains 1,472 ship identities and 88,862 images. In addition, this paper proposes an SLP detection model named YOLO-SSA and evaluates this model as well as typical detection methods on the SLPDR dataset. The experimental results demonstrate that the proposed YOLO-SSA achieves better SLP detection performance by enhancing the features where ships and SLPs are located. Furthermore, we explore the prospective applications of SLPs in intelligent maritime transportation, including ship monitoring and berth management. Project web page: https://vsislab.github.io/SLPDR/"
    },
    {
      "paperId": "edb5eb1d5a9cd3a79904c8b308a709f98d5e7605",
      "externalIds": {
        "DBLP": "journals/tjs/ZhangXWDW25",
        "DOI": "10.1007/s11227-025-08012-0",
        "CorpusId": 282944388
      },
      "corpusId": 282944388,
      "title": "Enhancing dangerous scene classification with multimodal LLMs and attention mechanisms",
      "venue": "Journal of Supercomputing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11227-025-08012-0?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11227-025-08012-0, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2391983189",
          "name": "Dong Zhang"
        },
        {
          "authorId": "2393935030",
          "name": "Tian Xie"
        },
        {
          "authorId": "2392551932",
          "name": "Shuai Wu"
        },
        {
          "authorId": "2191119244",
          "name": "Shukai Duan"
        },
        {
          "authorId": "1485125863",
          "name": "Lidan Wang"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "58b1c4b2020c08c47d6194334cd839160825bac8",
      "externalIds": {
        "DBLP": "journals/tcom/LiaoWZZ25",
        "DOI": "10.1109/TCOMM.2025.3593643",
        "CorpusId": 280503687
      },
      "corpusId": 280503687,
      "title": "Cross-Modal Semantic Transmission Strategy for Mobile Scenarios",
      "venue": "IEEE Transactions on Communications",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCOMM.2025.3593643?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCOMM.2025.3593643, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "145113878",
          "name": "Junqing Liao"
        },
        {
          "authorId": "153664275",
          "name": "Xinpo Wei"
        },
        {
          "authorId": "2156266446",
          "name": "Liang Zhou"
        },
        {
          "authorId": "2265756203",
          "name": "Weihua Zhuang"
        }
      ],
      "abstract": "To fulfill the demands of emerging multi-modal services, the cross-modal semantic communication paradigm comes into being. It fully utilizes potential semantic correlations among modalities to address polysemy and ambiguity issues, enhancing transmission reliability. However, applying cross-modal semantic communication in resource-constrained mobile scenarios introduces new challenges, including radio spectrum bandwidth limitations and fluctuations for the transmitter, and computing resource constraints for the receiver, which leads to potential transmission failures. To bridge this gap, this paper proposes a cross-modal semantic transmission strategy for mobile scenarios (MobileCMST). We first construct the framework for MobileCMST. Within this framework, a semantic encoder is designed to achieve redundancy elimination for visual and haptic signals. Then, a semantic delivery approach is developed to cope with bandwidth fluctuations and multipath fading channels. Finally, an efficient semantic decoder based on a visual-haptic semantic-integrated diffusion model is proposed. It employs the Mamba backbone to reconstruct high-quality signals with lightweight computational complexity. Extensive experiments demonstrate the excellent performance of the proposed MobileCMST strategy in resource-constrained mobile scenarios."
    },
    {
      "paperId": "86ce032a3f79956b62827ecdde0de4a34d94dfea",
      "externalIds": {
        "DOI": "10.1016/j.inffus.2025.103967",
        "CorpusId": 283172956
      },
      "corpusId": 283172956,
      "title": "PAREformer: Positional Adaptive and Recurrent Enhanced Transformer for Time Series Forecasting",
      "venue": "Information Fusion",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.inffus.2025.103967?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.inffus.2025.103967, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2146518951",
          "name": "Suxin Tong"
        },
        {
          "authorId": "2256778638",
          "name": "Jingling Yuan"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "3352060315590e956946c6ec523888b874202ab1",
      "externalIds": {
        "DOI": "10.1007/s11227-025-08077-x",
        "CorpusId": 283253168
      },
      "corpusId": 283253168,
      "title": "AGBi-Mamba: a traffic flow prediction model based on adaptive graph convolution and bidirectional Mamba networks",
      "venue": "Journal of Supercomputing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11227-025-08077-x?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11227-025-08077-x, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2394206812",
          "name": "Guoyan Li"
        },
        {
          "authorId": "2394352653",
          "name": "Xue Zhang"
        },
        {
          "authorId": null,
          "name": "Tianying Gao"
        },
        {
          "authorId": "2395409693",
          "name": "Yang Cao"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "12bc88c9a4fca6e4e99628d09852aa5e20cda6d3",
      "externalIds": {
        "PubMedCentral": "12656178",
        "DOI": "10.3390/s25226856",
        "CorpusId": 282924989,
        "PubMed": "41305066"
      },
      "corpusId": 282924989,
      "title": "GMG-LDefmamba-YOLO: An Improved YOLOv11 Algorithm Based on Gear-Shaped Convolution and a Linear-Deformable Mamba Model for Small Object Detection in UAV Images",
      "venue": "Italian National Conference on Sensors",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12656178, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2391871173",
          "name": "Yiming Yang"
        },
        {
          "authorId": "2292994104",
          "name": "Lingyu Yan"
        },
        {
          "authorId": "2350435553",
          "name": "Jing Wang"
        },
        {
          "authorId": "2253859354",
          "name": "Jinhang Liu"
        },
        {
          "authorId": "2319631089",
          "name": "Xing Tang"
        }
      ],
      "abstract": "Object detection plays a crucial role in remote sensing and UAV image technology, but it faces the challenge of speed and accuracy in multi-scale dense small target mission detection scenarios and is susceptible to noise interference, such as weather conditions, lighting changes, and occluded backgrounds in complex backgrounds. In recent years, Mamba-based methods have become hot in the field of object detection, showing great potential in capturing remote dependencies with linear complexity but lacking deep customization of remote sensing targets. Based on this, we propose GMG-LDefmamba-YOLO, which contains two core modules: the Gaussian mask gear convolution module forms a gear-shaped receptive field through improved convolutional splicing to enhance the extraction of small target features and combines the Gaussian mask mechanism to dynamically modulate the feature weights to suppress complex background interference. The linear deformable Mamba module integrates linear deformable sampling, a spatial state dual model, and residual gating MLP components, integrating the advantages of flexible capture of local features and efficient modeling of global dependence, dynamically adapting to target scale changes and spatial distribution, and reducing computational costs. Experiments on DOTA-v1.0, VEDAI, and USOD datasets show that the mAP50 of the model reaches 70.91%, 77.94%, and 90.28%, respectively, which is better than the baseline and mainstream methods, and maintains the lightweight characteristics, providing efficient technical support for remote sensing monitoring, UAV inspection, and other fields."
    },
    {
      "paperId": "3ab596f5f3ddc5208fcb73b733399086074d001b",
      "externalIds": {
        "PubMedCentral": "12656002",
        "DOI": "10.3390/s25227053",
        "CorpusId": 283124311,
        "PubMed": "41305260"
      },
      "corpusId": 283124311,
      "title": "DPM-UNet: A Mamba-Based Network with Dynamic Perception Feature Enhancement for Medical Image Segmentation",
      "venue": "Italian National Conference on Sensors",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12656002, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2393368901",
          "name": "Shangyu Xu"
        },
        {
          "authorId": "2393250309",
          "name": "Xiaohang Liu"
        },
        {
          "authorId": "2393398125",
          "name": "Hongsheng Lei"
        },
        {
          "authorId": "2295811143",
          "name": "Bin Hui"
        }
      ],
      "abstract": "In medical image segmentation, effective integration of global and local features is crucial. Current methods struggle to simultaneously model long-range dependencies and fine local details. Convolutional Neural Networks (CNNs) excel at extracting local features but are limited by their local receptive fields for capturing long-range dependencies. While global self-attention mechanisms (e.g., in Transformers) can capture long-range spatial relationships, their quadratic computational complexity incurs high costs for high-resolution medical images. To address these limitations, State Space Models (SSMs), which maintain linear complexity while effectively establishing long-range dependencies, have been introduced to visual tasks. Leveraging the advantages of SSMs, this paper proposes DPM-UNet. The network employs a Dual-path Residual Fusion Module (DRFM) at shallow layers to extract local detailed features and a DPMamba Module at deep layers to model global semantic information, achieving effective local global feature fusion. A Multi-scale Aggregation Attention Network (MAAN) is further incorporated to enhance multi-scale representations. The proposed method collaboratively captures local details, long-range dependencies, and multi-scale information in medical images. Experiments on three public datasets demonstrate that DPM-UNet outperforms existing methods across multiple evaluation metrics."
    },
    {
      "paperId": "c96b1fd8b5d89f0f0feebf71d69843017594a179",
      "externalIds": {
        "PubMedCentral": "12656456",
        "DOI": "10.3390/s25226815",
        "CorpusId": 282849885,
        "PubMed": "41305024"
      },
      "corpusId": 282849885,
      "title": "Enhanced Deep Neural Network for Prostate Segmentation in Micro-Ultrasound Images",
      "venue": "Italian National Conference on Sensors",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12656456, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "1441165776",
          "name": "Ahmed AL-Qurri"
        },
        {
          "authorId": "2386903801",
          "name": "Asem Thaher"
        },
        {
          "authorId": "9526710",
          "name": "M. Almekkawy"
        }
      ],
      "abstract": "Prostate cancer is a global health concern, and early diagnosis plays a vital role in improving the survival rate. Accurate segmentation is a key step in the automated diagnosis of prostate cancer; however, manual segmentation remains time-consuming and challenging. Micro-Ultrasound (US) is particularly well-suited for prostate cancer detection, offering real-time imaging with a resolution comparable to that of MRI. This enables improved spatial resolution and detailed visualization of small anatomical structures. With recent advances in deep learning for medical image segmentation, precise prostate segmentation has become critical for biopsy guidance, disease diagnosis, and follow-up. However, segmentation of the prostate in micro-US images remains challenging due to indistinct boundaries between the prostate and surrounding tissue. In this work, we propose a model for precise micro-ultrasound image segmentation. The model employs a dual-encoder architecture that integrates Convolutional Neural Networks (CNN) and Transformer-based encoders in parallel, combined with a fusion module to capture both global dependencies and low-level spatial details. More importantly, we introduce a decoder based on Mamba v2 to enhance segmentation accuracy. A Hypergraph Neural Network (HGNN) is employed as a bridge between the dual encoders and Mamba decoder to model correlations among non-pairwise connections. Experimental results on micro-US datasets demonstrated that our model achieved superior or comparable performance to state-of-the-art methods, with a Dice score of 0.9416 and an HD95 of 1.93."
    },
    {
      "paperId": "34039f1d728702f96c929db452bea14ecece12eb",
      "externalIds": {
        "PubMedCentral": "12655590",
        "DOI": "10.3390/plants14223434",
        "CorpusId": 282964098,
        "PubMed": "41304585"
      },
      "corpusId": 282964098,
      "title": "Depth Imaging-Based Framework for Efficient Phenotypic Recognition in Tomato Fruit",
      "venue": "Plants",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12655590, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2295082258",
          "name": "Junqing Li"
        },
        {
          "authorId": "2295018357",
          "name": "Guoao Dong"
        },
        {
          "authorId": "2393999915",
          "name": "Yuhang Liu"
        },
        {
          "authorId": "2395838396",
          "name": "Hua Yuan"
        },
        {
          "authorId": "2392303322",
          "name": "Zheng Xu"
        },
        {
          "authorId": "51011069",
          "name": "Wenfeng Nie"
        },
        {
          "authorId": "2336596961",
          "name": "Yan Zhang"
        },
        {
          "authorId": "2281765525",
          "name": "Qinghua Shi"
        }
      ],
      "abstract": "Tomato is a globally significant horticultural crop with substantial economic and nutritional value. High-precision phenotypic analysis of tomato fruit characteristics, enabled by computer vision and image-based phenotyping technologies, is essential for varietal selection and automated quality evaluation. An intelligent detection framework for phenomics analysis of tomato fruits was developed in this study, which combines image processing techniques with deep learning algorithms to automate the extraction and quantitative analysis of 12 phenotypic traits, including fruit morphology, structure, color and so on. First, a dataset of tomato fruit section images was developed using a depth camera. Second, the SegFormer model was improved by incorporating the MLLA linear attention mechanism, and a lightweight SegFormer-MLLA model for tomato fruit phenotype segmentation was proposed. Accurate segmentation of tomato fruit stem scars and locular structures was achieved, with significantly reduced computational cost by the proposed model. Finally, a Hybrid Depth Regression Model was designed to optimize the estimation of optimal depth. By fusing RGB and depth information, the framework enabled efficient detection of key phenotypic traits, including fruit longitudinal diameter, transverse diameter, mesocarp thickness, and depth and width of stem scar. Experimental results demonstrated a high correlation between the phenotypic parameters detected by the proposed model and the manually measured values, effectively validating the accuracy and feasibility of the model. Hence, we developed an equipment automatically phenotyping tomato fruits and the corresponding software system, providing reliable data support for precision tomato breeding and intelligent cultivation, as well as a reference methodology for phenotyping other fruit crops."
    },
    {
      "paperId": "8674d022f0c72caf9f05844b53834b4f0afb68a0",
      "externalIds": {
        "PubMedCentral": "12656524",
        "DOI": "10.3390/s25227007",
        "CorpusId": 283189140,
        "PubMed": "41305215"
      },
      "corpusId": 283189140,
      "title": "Path-Routing Convolution and Scalable Lightweight Networks for Robust Underwater Acoustic Target Recognition",
      "venue": "Italian National Conference on Sensors",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12656524, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2393667046",
          "name": "Yue Zhao"
        },
        {
          "authorId": "2376131408",
          "name": "Menghan Chen"
        },
        {
          "authorId": "2377233496",
          "name": "Yuchen Lu"
        },
        {
          "authorId": "2213709648",
          "name": "Liangliang Cheng"
        },
        {
          "authorId": "2392900719",
          "name": "Cheng Chen"
        },
        {
          "authorId": "2393799419",
          "name": "Yifei Li"
        },
        {
          "authorId": "17788186",
          "name": "N. F. Alkayem"
        }
      ],
      "abstract": "Highlights What are the main findings? A novel PR-Conv mechanism is proposed to adaptively extract multi-scale ship acoustic features. The model achieves 98.58% classification accuracy while using 94% fewer parameters than conventional architectures. What is the implication of the main findings? The lightweight design greatly reduces deployment cost and enables real-time inference. The model maintains a robust 77.8% accuracy under 10 dB SNR, demonstrating strong potential for real-world ship-radiated noise identification applications. Abstract Maritime traffic surveillance and ocean environmental protection urgently require the accurate identification of surface vessel types. Although deep learning methods have significantly improved the underwater acoustic target recognition performance, the existing models suffer from large parameter counts and fail to adapt to the multi-scale spectral features of radiated noise from different vessel types, restricting their practical deployment on power-constrained underwater sensors. To address these challenges, this paper proposes a novel path-routing convolution mechanism that achieves the discriminative extraction of cross-scale acoustic features through multi-dilation-rate parallel paths and an adaptive routing strategy and designs the MobilePR-ConvNet unified architecture that enables a single framework to automatically adapt to diverse hardware platforms through systematic width scaling. Experiments on the DeepShip and ShipsEar datasets demonstrate that the proposed method achieved 98.58% and 97.82% recognition accuracies, respectively, while maintaining a 77.8% robust performance under 10 dB low-signal-to-noise-ratio conditions, validating the cross-dataset generalization capability in complex marine environments and providing an effective solution for intelligent deployment on resource-constrained underwater devices."
    },
    {
      "paperId": "99f8677d071d343cc0f0a86d778eabaac7154ea0",
      "externalIds": {
        "PubMedCentral": "12656390",
        "DOI": "10.3390/s25226865",
        "CorpusId": 282979471,
        "PubMed": "41305072"
      },
      "corpusId": 282979471,
      "title": "CONTI-CrackNet: A Continuity-Aware State-Space Network for Crack Segmentation",
      "venue": "Italian National Conference on Sensors",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12656390, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2392210551",
          "name": "Wenjie Song"
        },
        {
          "authorId": "2393149812",
          "name": "Min Zhao"
        },
        {
          "authorId": "2392677770",
          "name": "Xunqian Xu"
        }
      ],
      "abstract": "Crack segmentation in cluttered scenes with slender and irregular patterns remains difficult, and practical systems must balance accuracy and efficiency. We present CONTI-CrackNet, which is a lightweight visual state-space network that integrates a Multi-Directional Selective Scanning Strategy (MD3S). MD3S performs bidirectional scanning along the horizontal, vertical, and diagonal directions, and it fuses the complementary paths with a Bidirectional Gated Fusion (BiGF) module to strengthen global continuity. To preserve fine details while completing global texture, we propose a Dual-Branch Pixel-Level Global\u2013Local Fusion (DBPGL) module that incorporates a Pixel-Adaptive Pooling (PAP) mechanism to dynamically weight max-pooled responses and average-pooled responses. Evaluated on two public benchmarks, the proposed method achieves an F1 score (F1) of 0.8332 and a mean Intersection over Union (mIoU) of 0.8436 on the TUT dataset, and it achieves an mIoU of 0.7760 on the CRACK500 dataset, surpassing competitive Convolutional Neural Network (CNN), Transformer, and Mamba baselines. With 512 \u00d7 512 input, the model requires 24.22 G floating point operations (GFLOPs), 6.01 M parameters (Params), and operates at 42 frames per second (FPS) on an RTX 3090 GPU, delivering a favorable accuracy\u2013efficiency balance. These results show that CONTI-CrackNet improves continuity and edge recovery for thin cracks while keeping computational cost low, and it is lightweight in terms of parameter count and computational cost."
    },
    {
      "paperId": "89582d7ab684adc4d8663cebad074504d6aa58ab",
      "externalIds": {
        "DOI": "10.1109/SiPS66314.2025.11261275",
        "CorpusId": 283356598
      },
      "corpusId": 283356598,
      "title": "A Lightweight and Efficient Framework for Wiring Inspection in Electric Power Systems",
      "venue": "IEEE Workshop on Signal Processing Systems",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/SiPS66314.2025.11261275?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/SiPS66314.2025.11261275, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2396750717",
          "name": "Haitian Yang"
        },
        {
          "authorId": "2152099639",
          "name": "Yaohua Liao"
        },
        {
          "authorId": "2274899635",
          "name": "Mengmeng Zhu"
        },
        {
          "authorId": "2395355656",
          "name": "Yuming Liu"
        },
        {
          "authorId": "2395002693",
          "name": "Dezhi Ji"
        },
        {
          "authorId": "2395007430",
          "name": "Xin Lou"
        },
        {
          "authorId": "2395741248",
          "name": "Wei Zhou"
        }
      ],
      "abstract": "Accurate electric power metering is critical for ensuring fairness in market transactions, with metering junction boxes playing a pivotal role. This paper introduces a novel metering junction box wiring inspection system based on a lightweight and efficient object detection framework, MambaYOLO. By introducing the Cross-Mamba Feature Pyramid Network (CM-FPN), this framework enhances multiscale feature fusion with linear complexity. Meanwhile, inspired by neural collaboration mechanisms between the prefrontal, parietal, and occipital regions, we propose a Fusion Coordinate Attention Network (FusCAN) to serve as the detection head. Additionally, we also contribute a dedicated dataset for electric power wiring inspection tasks. Experimental results show that the proposed framework outperforms conventional YOLO variants, setting a new state-of-the-art (SOTA) performance. Specifically, it achieves a $0.9 \\%$ improvement in mean Average Precision (mAP) and a $\\mathbf{1. 0 \\%}$ increase in recall, while simultaneously reducing the number of parameters by $\\mathbf{7 5. 5 \\%}$, computational load by $\\mathbf{6 3. 9 \\%}$, and inference time by $45.1 \\%$. Real-world deployment verification on the NVIDIA Jetson Xavier platform yields subsecond feedback in actual electric power scenarios, confirming the system\u2019s strong deployment capability on resource-constrained edge devices."
    },
    {
      "paperId": "d64aa2afbe46ed9fe1ea0de066af129021fed903",
      "externalIds": {
        "DOI": "10.1016/j.imavis.2025.105850",
        "CorpusId": 283527968
      },
      "corpusId": 283527968,
      "title": "Combining short-term and long-term memory for robust visual tracking",
      "venue": "Image and Vision Computing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.imavis.2025.105850?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.imavis.2025.105850, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2379383196",
          "name": "Zifan Rui"
        },
        {
          "authorId": "2395995760",
          "name": "Xiaoxiao Wang"
        },
        {
          "authorId": "2396200652",
          "name": "Yiteng Yang"
        },
        {
          "authorId": "2397170444",
          "name": "Guang Han"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "ddfac28a6c1e7b8e35f6451e338e211ffe95b923",
      "externalIds": {
        "DOI": "10.1109/TCE.2025.3614990",
        "CorpusId": 281621154
      },
      "corpusId": 281621154,
      "title": "Mamba-Enhanced Emotion Analysis TinyML Models for Embedded Devices Deployment",
      "venue": "IEEE transactions on consumer electronics",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCE.2025.3614990?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCE.2025.3614990, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2374038822",
          "name": "Xingchen Jin"
        },
        {
          "authorId": "2152490091",
          "name": "Shakir Khan"
        },
        {
          "authorId": "2361644558",
          "name": "Mehdi Hosseinzadeh"
        },
        {
          "authorId": "2283749611",
          "name": "Neeraj Kumar"
        },
        {
          "authorId": "2269714428",
          "name": "Xiyin Wu"
        }
      ],
      "abstract": "The accuracy of emotion analysis has rapidly improved thanks to the breakthroughs of convolutional neural networks (CNNs) and Transformers. Moreover, the multi-head self-attention (MSA) mechanism of Transformer perfectly fits the modeling of the dependency relationship between expressions and different facial regions. However, CNNs struggle to capture global dependencies, and Transformers\u2019 quadratic complexity poses a big challenge to deploy on low-power devices. To resolve these issues, we design a robust and efficient hybrid Tiny machine learning (TinyML) model named HCMTMM for emotion recognition in ultra-low-power embedded devices. Specifically, we propose a hybrid deep model by combining a CNN and Mamba module, which relies on the state space models (SSM) framework, which can effectively exploit the local and global dependencies of different facial regions to enhance emotional recognition performance with linear computational complexity. Moreover, we leverage multi-loss distillation learning to enhance recognition performance. We conducted extensive comparative experiments on four publicly available datasets, and the experimental results showed that when running the family of CNNs, our proposed solution outperforms any other implementation in terms of accuracy and model size. Moreover, we port and test the proposed model on the embedded device ESP32 Cam platform. Our proposed model achieves remarkable results in inference speed."
    },
    {
      "paperId": "e8f9bc380d15ba72cb63e7f7bfcb65860f52db14",
      "externalIds": {
        "DOI": "10.1109/TCE.2025.3626474",
        "CorpusId": 282651727
      },
      "corpusId": 282651727,
      "title": "HTI-Hand: A Hybrid Token Initialization Framework With Relative Encoding and Mask-Guided Priors for 3D Hand Reconstruction",
      "venue": "IEEE transactions on consumer electronics",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCE.2025.3626474?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCE.2025.3626474, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2349265948",
          "name": "Shufang Zhang"
        },
        {
          "authorId": "2349625805",
          "name": "Ziyang Zhang"
        },
        {
          "authorId": "2349428410",
          "name": "Ziyu Meng"
        },
        {
          "authorId": "2349299406",
          "name": "Jiazheng Wu"
        },
        {
          "authorId": "2389629317",
          "name": "Shouzheng Wang"
        },
        {
          "authorId": "2349398334",
          "name": "Tao Jiang"
        },
        {
          "authorId": "2349571240",
          "name": "Shan An"
        }
      ],
      "abstract": "Reconstructing 3D hand mesh from monocular image has witnessed remarkable advancements with the advent of transformer-based architectures. However, existing parametric methods tend to exhibit relatively slow and unstable convergence during the training phase. To address this challenge, we introduce different tokens for hand model parameters, which allow semantically independent representations of hand pose and shape. Building on this design, we propose a structured token initialization strategy that incorporates semantic cues into each token type. Specifically, we leverage a lightweight regression network to obtain rough 2D keypoints and encode their relative spatial layout to initialize the pose token with position-aware priors. In parallel, a low-dimensional hand segmentation is predicted under explicit supervision, and its intermediate features are utilized to initialize the shape token with geometry-aware priors, enhancing robustness to occlusion. Finally, these well-initialized tokens are further refined through multi-token attention, where structured interactions across pose and shape token facilitate more accurate and stable mesh reconstruction. Experimental results on the FreiHAND and HO3Dv2 datasets demonstrate that our approach outperforms most mainstream methods in both convergence speed and final accuracy."
    },
    {
      "paperId": "c2adde8f912e4d38c3a84e283cabb6ae03d9c056",
      "externalIds": {
        "ArXiv": "2510.27155",
        "DBLP": "journals/corr/abs-2510-27155",
        "DOI": "10.48550/arXiv.2510.27155",
        "CorpusId": 282719140
      },
      "corpusId": 282719140,
      "title": "AFM-Net: Advanced Fusing Hierarchical CNN Visual Priors with Global Sequence Modeling for Remote Sensing Image Scene Classification",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.27155, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2391372403",
          "name": "Yuanhao Tang"
        },
        {
          "authorId": "2237425470",
          "name": "Xuechao Zou"
        },
        {
          "authorId": "2390553236",
          "name": "Zhengpei Hu"
        },
        {
          "authorId": "2248083639",
          "name": "Junliang Xing"
        },
        {
          "authorId": "2390937425",
          "name": "Chengkun Zhang"
        },
        {
          "authorId": "2391152476",
          "name": "Jianqiang Huang"
        }
      ],
      "abstract": "Remote sensing image scene classification remains a challenging task, primarily due to the complex spatial structures and multi-scale characteristics of ground objects. Existing approaches see CNNs excel at modeling local textures, while Transformers excel at capturing global context. However, efficiently integrating them remains a bottleneck due to the high computational cost of Transformers. To tackle this, we propose AFM-Net, a novel Advanced Hierarchical Fusing framework that achieves effective local and global co-representation through two pathways: a CNN branch for extracting hierarchical visual priors, and a Mamba branch for efficient global sequence modeling. The core innovation of AFM-Net lies in its Hierarchical Fusion Mechanism, which progressively aggregates multi-scale features from both pathways, enabling dynamic cross-level feature interaction and contextual reconstruction to produce highly discriminative representations. These fused features are then adaptively routed through a Mixture-of-Experts classifier module, which dispatches them to the most suitable experts for fine-grained scene recognition. Experiments on AID, NWPU-RESISC45, and UC Merced show that AFM-Net obtains 93.72, 95.54, and 96.92 percent accuracy, surpassing state-of-the-art methods with balanced performance and efficiency. Code is available at https://github.com/tangyuanhao-qhu/AFM-Net."
    },
    {
      "paperId": "53d4ee97d04dcd4c4f5688a98ab3acba20868cf9",
      "externalIds": {
        "DBLP": "journals/mms/QiaoGHLYY25",
        "DOI": "10.1007/s00530-025-02018-7",
        "CorpusId": 282616082
      },
      "corpusId": 282616082,
      "title": "Slfmamba:a state space based vision foundation models fine-tuning for domain generalized semantic segmentations",
      "venue": "Multimedia Systems",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s00530-025-02018-7?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s00530-025-02018-7, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2367667557",
          "name": "Yongchao Qiao"
        },
        {
          "authorId": "2257005689",
          "name": "Ya'nan Guan"
        },
        {
          "authorId": "2298202863",
          "name": "Qihan He"
        },
        {
          "authorId": "2298065723",
          "name": "Zhongxu Li"
        },
        {
          "authorId": "2375217326",
          "name": "Jingmin Yang"
        },
        {
          "authorId": "2256510064",
          "name": "Wenyuan Yang"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "c5764209fe280ae77512779d853e1075be89f312",
      "externalIds": {
        "PubMedCentral": "12578215",
        "DOI": "10.1371/journal.pone.0332873",
        "CorpusId": 282689700,
        "PubMed": "41171833"
      },
      "corpusId": 282689700,
      "title": "SR-LMamba: A lane detection model for complex scenes integrating curvelet transform with Mamba architecture",
      "venue": "PLoS ONE",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12578215, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2383377399",
          "name": "Mingliang Chen"
        },
        {
          "authorId": "2383362055",
          "name": "Qinhao Jia"
        },
        {
          "authorId": "2383855161",
          "name": "Jing Yang"
        },
        {
          "authorId": "2383360946",
          "name": "Shuxian Liu"
        }
      ],
      "abstract": "Lane detection seeks to accurately identify the position and geometry of lane markings in real-world driving environments. However, existing models often struggle with robustness and accuracy due to insufficient integration of high-level semantic understanding and low-level geometric features. To tackle these limitations, we propose SR-LMamba, a novel lane detection framework built upon the Sketch-and-Refine paradigm of SRLane. At the core of our approach lies LMamba, a lightweight three-stage backbone network that accelerates inference while effectively capturing both geometric structures and sequential patterns through a synergistic combination of curvelet transform and the Mamba architecture. In the Refine stage, we introduce the Criss-Cross Lane Association Module (CLAM), which employs a multi-lane criss-cross attention mechanism to enhance feature interactions and applies polynomial regression to refine lane curve fitting. To further boost performance, we design tailored loss functions\u2014angle loss and criss-cross attention loss\u2014aligned with the model architecture. Experimental results show that SR-LMamba achieves an F1 score of 80.04%, outperforming current state-of-the-art models with similar parameter sizes, and demonstrating superior robustness across four challenging driving scenarios. In addition, we publicly release our code and models at https://github.com/chenml1/SR-LMamba."
    },
    {
      "paperId": "489e0540967bcad6f60a0f811953332e2c952f00",
      "externalIds": {
        "DOI": "10.1016/j.compbiomed.2025.111263",
        "CorpusId": 282708480,
        "PubMed": "41175715"
      },
      "corpusId": 282708480,
      "title": "A 3D self-configuring hybrid transformer with multi-task learning for 3D automated breast ultrasound segmentation.",
      "venue": "Computers in Biology and Medicine",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.compbiomed.2025.111263?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.compbiomed.2025.111263, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2293840072",
          "name": "Hyunsu Jeong"
        },
        {
          "authorId": "2268342088",
          "name": "Chiho Yoon"
        },
        {
          "authorId": "2299945122",
          "name": "Hyunseok Lim"
        },
        {
          "authorId": "2186306350",
          "name": "Jongjun Won"
        },
        {
          "authorId": "2156002978",
          "name": "Kiduk Kim"
        },
        {
          "authorId": "2344642267",
          "name": "Gongning Luo"
        },
        {
          "authorId": "2199851055",
          "name": "Mingwang Xu"
        },
        {
          "authorId": "2288676544",
          "name": "Namkug Kim"
        },
        {
          "authorId": "2354185767",
          "name": "Chulhong Kim"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "9bc0a6784387173cef1453c2d5c5a0f177c7099d",
      "externalIds": {
        "ArXiv": "2510.27296",
        "DBLP": "journals/corr/abs-2510-27296",
        "DOI": "10.48550/arXiv.2510.27296",
        "CorpusId": 282719082
      },
      "corpusId": 282719082,
      "title": "Versatile and Efficient Medical Image Super-Resolution Via Frequency-Gated Mamba",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.27296, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2158105217",
          "name": "Wenfeng Huang"
        },
        {
          "authorId": null,
          "name": "Xiangyun Liao"
        },
        {
          "authorId": "2344609300",
          "name": "Wei Cao"
        },
        {
          "authorId": "2238952299",
          "name": "Wenjing Jia"
        },
        {
          "authorId": "2364583171",
          "name": "Weixin Si"
        }
      ],
      "abstract": "Medical image super-resolution (SR) is essential for enhancing diagnostic accuracy while reducing acquisition cost and scanning time. However, modeling both long-range anatomical structures and fine-grained frequency details with low computational overhead remains challenging. We propose FGMamba, a novel frequency-aware gated state-space model that unifies global dependency modeling and fine-detail enhancement into a lightweight architecture. Our method introduces two key innovations: a Gated Attention-enhanced State-Space Module (GASM) that integrates efficient state-space modeling with dual-branch spatial and channel attention, and a Pyramid Frequency Fusion Module (PFFM) that captures high-frequency details across multiple resolutions via FFT-guided fusion. Extensive evaluations across five medical imaging modalities (Ultrasound, OCT, MRI, CT, and Endoscopic) demonstrate that FGMamba achieves superior PSNR/SSIM while maintaining a compact parameter footprint ($<$0.75M), outperforming CNN-based and Transformer-based SOTAs. Our results validate the effectiveness of frequency-aware state-space modeling for scalable and accurate medical image enhancement."
    },
    {
      "paperId": "7058995c387bd2d1f3c7e6437c2967e03e25620f",
      "externalIds": {
        "ArXiv": "2510.27258",
        "DBLP": "journals/corr/abs-2510-27258",
        "DOI": "10.48550/arXiv.2510.27258",
        "CorpusId": 282719392
      },
      "corpusId": 282719392,
      "title": "Higher-order Linear Attention",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.27258, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2333528944",
          "name": "Yifan Zhang"
        },
        {
          "authorId": "2342254044",
          "name": "Zhen Qin"
        },
        {
          "authorId": "2329737857",
          "name": "Quanquan Gu"
        }
      ],
      "abstract": "The quadratic cost of scaled dot-product attention is a central obstacle to scaling autoregressive language models to long contexts. Linear-time attention and State Space Models (SSMs) provide scalable alternatives but are typically restricted to first-order or kernel-based approximations, which can limit expressivity. We introduce Higher-order Linear Attention (HLA), a causal, streaming mechanism that realizes higher interactions via compact prefix sufficient statistics. In the second-order case, HLA maintains a constant-size state and computes per-token outputs in linear time without materializing any $n \\times n$ matrices. We give closed-form streaming identities, a strictly causal masked variant using two additional summaries, and a chunk-parallel training scheme based on associative scans that reproduces the activations of a serial recurrence exactly. We further outline extensions to third and higher orders. Collectively, these results position HLA as a principled, scalable building block that combines attention-like, data-dependent mixing with the efficiency of modern recurrent architectures. Project Page: https://github.com/yifanzhang-pro/HLA."
    },
    {
      "paperId": "67c22adac7d46d7f39fea8a1b38fb1aadba957c8",
      "externalIds": {
        "ArXiv": "2511.00260",
        "DBLP": "journals/corr/abs-2511-00260",
        "DOI": "10.48550/arXiv.2511.00260",
        "CorpusId": 282738439
      },
      "corpusId": 282738439,
      "title": "MambaNetLK: Enhancing Colonoscopy Point Cloud Registration with Mamba",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.00260, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2391075243",
          "name": "Linzhe Jiang"
        },
        {
          "authorId": "2349762643",
          "name": "Jiayuan Huang"
        },
        {
          "authorId": "2297769993",
          "name": "Sophia Bano"
        },
        {
          "authorId": "2244269282",
          "name": "Matthew J. Clarkson"
        },
        {
          "authorId": "2293385839",
          "name": "Zhehua Mao"
        },
        {
          "authorId": "2372202808",
          "name": "Mobarak I. Hoque"
        }
      ],
      "abstract": "Accurate 3D point cloud registration underpins reliable image-guided colonoscopy, directly affecting lesion localization, margin assessment, and navigation safety. However, biological tissue exhibits repetitive textures and locally homogeneous geometry that cause feature degeneracy, while substantial domain shifts between pre-operative anatomy and intra-operative observations further degrade alignment stability. To address these clinically critical challenges, we introduce a novel 3D registration method tailored for endoscopic navigation and a high-quality, clinically grounded dataset to support rigorous and reproducible benchmarking. We introduce C3VD-Raycasting-10k, a large-scale benchmark dataset with 10,014 geometrically aligned point cloud pairs derived from clinical CT data. We propose MambaNetLK, a novel correspondence-free registration framework, which enhances the PointNetLK architecture by integrating a Mamba State Space Model (SSM) as a cross-modal feature extractor. As a result, the proposed framework efficiently captures long-range dependencies with linear-time complexity. The alignment is achieved iteratively using the Lucas-Kanade algorithm. On the clinical dataset, C3VD-Raycasting-10k, MambaNetLK achieves the best performance compared with the state-of-the-art methods, reducing median rotation error by 56.04% and RMSE translation error by 26.19% over the second-best method. The model also demonstrates strong generalization on ModelNet40 and superior robustness to initial pose perturbations. MambaNetLK provides a robust foundation for 3D registration in surgical navigation. The combination of a globally expressive SSM-based feature extractor and a large-scale clinical dataset enables more accurate and reliable guidance systems in minimally invasive procedures like colonoscopy."
    },
    {
      "paperId": "78ecd342192fb4c60cf6c4875629bf906fcb2e93",
      "externalIds": {
        "PubMedCentral": "12610691",
        "DOI": "10.3390/s25216652",
        "CorpusId": 282699614,
        "PubMed": "41228875"
      },
      "corpusId": 282699614,
      "title": "Fault Diagnosis of Rolling Bearings Using Denoising Multi-Channel Mixture of CNN and Mamba-Enhanced Adaptive Self-Attention LSTM",
      "venue": "Italian National Conference on Sensors",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12610691, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2332539119",
          "name": "Songjiang Lai"
        },
        {
          "authorId": "2048017816",
          "name": "Tsun-hin Cheung"
        },
        {
          "authorId": "2291765616",
          "name": "K. Fung"
        },
        {
          "authorId": "2291590000",
          "name": "Kaiwen Xue"
        },
        {
          "authorId": "2323520400",
          "name": "Jiayi Zhao"
        },
        {
          "authorId": "2106537472",
          "name": "H. L. Goshu"
        },
        {
          "authorId": "2321589106",
          "name": "Zihang Lyu"
        },
        {
          "authorId": "2291516173",
          "name": "Kin-Man Lam"
        }
      ],
      "abstract": "Recent advancements in deep learning have significantly improved fault diagnosis methods. However, challenges such as insufficient feature extraction, limited long-range dependency modeling, and environmental noise continue to hinder their effectiveness. This paper presents a novel mixture of multi-view convolutional (MOM-Conv) layers integrating the Mixture of Experts (MOE) mechanism. This design effectively captures and fuses both local and contextual information, thereby enhancing feature extraction and representation. This proposed approach aims to improve prediction accuracy under varying noise conditions, particularly in rolling ball bearing systems characterized by noisy signals. Additionally, we propose the Mamba-enhanced adaptive self-attention long short-term memory (MASA-LSTM) model, which effectively captures both global and local dependencies in ultra-long time series data. This model addresses the limitations of traditional models in extracting long-range dependencies from such signals. The architecture also integrates a multi-step temporal state fusion mechanism to optimize information flow and incorporates adaptive parameter tuning, thereby improving dynamic adaptability within the LSTM framework. To further mitigate the impact of noise, we transform vibration signals into denoised multi-channel representations, enhancing model stability in noisy environments. Experimental results show that our proposed model outperforms existing state-of-the-art approaches on both the Paderborn and Case Western Reserve University bearing datasets, demonstrating remarkable robustness and effectiveness across various noise levels."
    },
    {
      "paperId": "fe76958d1eee073125893817bf55504aeddcb4cb",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-26732",
        "ArXiv": "2510.26732",
        "DOI": "10.48550/arXiv.2510.26732",
        "CorpusId": 282598222
      },
      "corpusId": 282598222,
      "title": "Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.26732, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "40978371",
          "name": "J. Curt\u00f2"
        },
        {
          "authorId": "2157860770",
          "name": "I. D. Zarz\u00e0"
        },
        {
          "authorId": "2242243232",
          "name": "Pablo Garc'ia"
        },
        {
          "authorId": "2388920978",
          "name": "Jordi Cabot"
        }
      ],
      "abstract": "This paper presents a comprehensive cross-platform evaluation of reasoning capabilities in contemporary foundation models, establishing an infrastructure-agnostic benchmark across three computational paradigms: HPC supercomputing (MareNostrum 5), cloud platforms (Nebius AI Studio), and university clusters (a node with eight H200 GPUs). We evaluate 15 foundation models across 79 problems spanning eight academic domains (Physics, Mathematics, Chemistry, Economics, Biology, Statistics, Calculus, and Optimization) through three experimental phases: (1) Baseline establishment: Six models (Mixtral-8x7B, Phi-3, LLaMA 3.1-8B, Gemma-2-9b, Mistral-7B, OLMo-7B) evaluated on 19 problems using MareNostrum 5, establishing methodology and reference performance; (2) Infrastructure validation: The 19-problem benchmark repeated on university cluster (seven models including Falcon-Mamba state-space architecture) and Nebius AI Studio (nine state-of-the-art models: Hermes-4 70B/405B, LLaMA 3.1-405B/3.3-70B, Qwen3 30B/235B, DeepSeek-R1, GPT-OSS 20B/120B) to confirm infrastructure-agnostic reproducibility; (3) Extended evaluation: Full 79-problem assessment on both university cluster and Nebius platforms, probing generalization at scale across architectural diversity. The findings challenge conventional scaling assumptions, establish training data quality as more critical than model size, and provide actionable guidelines for model selection across educational, production, and research contexts. The tri-infrastructure methodology and 79-problem benchmark enable longitudinal tracking of reasoning capabilities as foundation models evolve."
    },
    {
      "paperId": "d963d02f7528354f31f6e59f858bb9af05bbdb5d",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-26182",
        "ArXiv": "2510.26182",
        "DOI": "10.48550/arXiv.2510.26182",
        "CorpusId": 282592643
      },
      "corpusId": 282592643,
      "title": "MossNet: Mixture of State-Space Experts is a Multi-Head Attention",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.26182, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "52089303",
          "name": "Shikhar Tuli"
        },
        {
          "authorId": "2313592398",
          "name": "J. Smith"
        },
        {
          "authorId": "2342274567",
          "name": "Haris Jeelani"
        },
        {
          "authorId": "2389254905",
          "name": "Chi-Heng Lin"
        },
        {
          "authorId": "2115715019",
          "name": "Abhishek Patel"
        },
        {
          "authorId": "2018561",
          "name": "Vasili Ramanishka"
        },
        {
          "authorId": "2343710374",
          "name": "Yen-Chang Hsu"
        },
        {
          "authorId": "2389364277",
          "name": "Hongxia Jin"
        }
      ],
      "abstract": "Large language models (LLMs) have significantly advanced generative applications in natural language processing (NLP). Recent trends in model architectures revolve around efficient variants of transformers or state-space/gated-recurrent models (SSMs, GRMs). However, prevailing SSM/GRM-based methods often emulate only a single attention head, potentially limiting their expressiveness. In this work, we propose MossNet, a novel mixture-of-state-space-experts architecture that emulates a linear multi-head attention (MHA). MossNet leverages a mixture-of-experts (MoE) implementation not only in channel-mixing multi-layered perceptron (MLP) blocks but also in the time-mixing SSM kernels to realize multiple\"attention heads.\"Extensive experiments on language modeling and downstream evaluations show that MossNet outperforms both transformer- and SSM-based architectures of similar model size and data budgets. Larger variants of MossNet, trained on trillions of tokens, further confirm its scalability and superior performance. In addition, real-device profiling on a Samsung Galaxy S24 Ultra and an Nvidia A100 GPU demonstrate favorable runtime speed and resource usage compared to similarly sized baselines. Our results suggest that MossNet is a compelling new direction for efficient, high-performing recurrent LLM architectures."
    },
    {
      "paperId": "38254573109c09a812a3fd3ab11e359025e885e0",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-26083",
        "ArXiv": "2510.26083",
        "DOI": "10.48550/arXiv.2510.26083",
        "CorpusId": 282592609
      },
      "corpusId": 282592609,
      "title": "Nirvana: A Specialized Generalist Model With Task-Aware Memory Mechanism",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.26083, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2345305718",
          "name": "Yuhua Jiang"
        },
        {
          "authorId": "2385487863",
          "name": "Shuang Cheng"
        },
        {
          "authorId": "2361197049",
          "name": "Yihao Liu"
        },
        {
          "authorId": "2290020564",
          "name": "Ermo Hua"
        },
        {
          "authorId": "2294321480",
          "name": "Che Jiang"
        },
        {
          "authorId": "2346291295",
          "name": "Weigao Sun"
        },
        {
          "authorId": "2389119410",
          "name": "Yu Cheng"
        },
        {
          "authorId": "2390234953",
          "name": "Feifei Gao"
        },
        {
          "authorId": "2363582001",
          "name": "Biqing Qi"
        },
        {
          "authorId": "2218723159",
          "name": "Bowen Zhou"
        }
      ],
      "abstract": "Specialized Generalist Models (SGMs) aim to preserve broad capabilities while achieving expert-level performance in target domains. However, traditional LLM structures including Transformer, Linear Attention, and hybrid models do not employ specialized memory mechanism guided by task information. In this paper, we present Nirvana, an SGM with specialized memory mechanism, linear time complexity, and test-time task information extraction. Besides, we propose the Task-Aware Memory Trigger ($\\textit{Trigger}$) that flexibly adjusts memory mechanism based on the current task's requirements. In Trigger, each incoming sample is treated as a self-supervised fine-tuning task, enabling Nirvana to adapt its task-related parameters on the fly to domain shifts. We also design the Specialized Memory Updater ($\\textit{Updater}$) that dynamically memorizes the context guided by Trigger. We conduct experiments on both general language tasks and specialized medical tasks. On a variety of natural language modeling benchmarks, Nirvana achieves competitive or superior results compared to the existing LLM structures. To prove the effectiveness of Trigger on specialized tasks, we test Nirvana's performance on a challenging medical task, i.e., Magnetic Resonance Imaging (MRI). We post-train frozen Nirvana backbone with lightweight codecs on paired electromagnetic signals and MRI images. Despite the frozen Nirvana backbone, Trigger guides the model to adapt to the MRI domain with the change of task-related parameters. Nirvana achieves higher-quality MRI reconstruction compared to conventional MRI models as well as the models with traditional LLMs'backbone, and can also generate accurate preliminary clinical reports accordingly."
    },
    {
      "paperId": "606a57fb0ee1f8179a8dc24c09b98189b083fcd4",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-26493",
        "ArXiv": "2510.26493",
        "DOI": "10.48550/arXiv.2510.26493",
        "CorpusId": 282592567
      },
      "corpusId": 282592567,
      "title": "Context Engineering 2.0: The Context of Context Engineering",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.26493, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2381361668",
          "name": "Qishuo Hua"
        },
        {
          "authorId": "2329324157",
          "name": "Lyumanshan Ye"
        },
        {
          "authorId": "2353950754",
          "name": "Dayuan Fu"
        },
        {
          "authorId": "2307314606",
          "name": "Yang Xiao"
        },
        {
          "authorId": "2354110827",
          "name": "Xiaojie Cai"
        },
        {
          "authorId": "2374231057",
          "name": "Yunze Wu"
        },
        {
          "authorId": "2316012256",
          "name": "Jifan Lin"
        },
        {
          "authorId": "2372658753",
          "name": "Junfei Wang"
        },
        {
          "authorId": "2327103110",
          "name": "Pengfei Liu"
        }
      ],
      "abstract": "Karl Marx once wrote that ``the human essence is the ensemble of social relations'', suggesting that individuals are not isolated entities but are fundamentally shaped by their interactions with other entities, within which contexts play a constitutive and essential role. With the advent of computers and artificial intelligence, these contexts are no longer limited to purely human--human interactions: human--machine interactions are included as well. Then a central question emerges: How can machines better understand our situations and purposes? To address this challenge, researchers have recently introduced the concept of context engineering. Although it is often regarded as a recent innovation of the agent era, we argue that related practices can be traced back more than twenty years. Since the early 1990s, the field has evolved through distinct historical phases, each shaped by the intelligence level of machines: from early human--computer interaction frameworks built around primitive computers, to today's human--agent interaction paradigms driven by intelligent agents, and potentially to human--level or superhuman intelligence in the future. In this paper, we situate context engineering, provide a systematic definition, outline its historical and conceptual landscape, and examine key design considerations for practice. By addressing these questions, we aim to offer a conceptual foundation for context engineering and sketch its promising future. This paper is a stepping stone for a broader community effort toward systematic context engineering in AI systems."
    },
    {
      "paperId": "fc6412d9ec7a6a07ce9ef15273279a0021d09422",
      "externalIds": {
        "ArXiv": "2510.26692",
        "DBLP": "journals/corr/abs-2510-26692",
        "DOI": "10.48550/arXiv.2510.26692",
        "CorpusId": 282592221
      },
      "corpusId": 282592221,
      "title": "Kimi Linear: An Expressive, Efficient Attention Architecture",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 5,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.26692, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "49890808",
          "name": "Yu Zhang"
        },
        {
          "authorId": "2294515390",
          "name": "Zongyu Lin"
        },
        {
          "authorId": "2346980722",
          "name": "Xingcheng Yao"
        },
        {
          "authorId": "2390286041",
          "name": "Jiaxi Hu"
        },
        {
          "authorId": "2396642649",
          "name": "Fanqing Meng"
        },
        {
          "authorId": "2291312504",
          "name": "Chengyin Liu"
        },
        {
          "authorId": "2242124145",
          "name": "Xin Men"
        },
        {
          "authorId": "2320707621",
          "name": "Songlin Yang"
        },
        {
          "authorId": "2389423298",
          "name": "Zhiyuan Li"
        },
        {
          "authorId": "2321410487",
          "name": "Wentao Li"
        },
        {
          "authorId": "2341538580",
          "name": "Enzhe Lu"
        },
        {
          "authorId": "2373547180",
          "name": "Weizhou Liu"
        },
        {
          "authorId": "2346282315",
          "name": "Yanru Chen"
        },
        {
          "authorId": "2347179648",
          "name": "Weixin Xu"
        },
        {
          "authorId": "2323432282",
          "name": "Long Yu"
        },
        {
          "authorId": "2279312203",
          "name": "Ye-Jia Wang"
        },
        {
          "authorId": "2373687267",
          "name": "Yu Fan"
        },
        {
          "authorId": "2390908669",
          "name": "Longguang Zhong"
        },
        {
          "authorId": "2341538388",
          "name": "Enming Yuan"
        },
        {
          "authorId": "2302372580",
          "name": "Dehao Zhang"
        },
        {
          "authorId": "2387617285",
          "name": "Yizhi Zhang"
        },
        {
          "authorId": "2373576123",
          "name": "T. Y. Liu"
        },
        {
          "authorId": "2389409603",
          "name": "Haiming Wang"
        },
        {
          "authorId": "2390899326",
          "name": "Shengjun Fang"
        },
        {
          "authorId": "2309423875",
          "name": "Weiran He"
        },
        {
          "authorId": "2341613320",
          "name": "Shaowei Liu"
        },
        {
          "authorId": "2389108359",
          "name": "Yiwei Li"
        },
        {
          "authorId": "2277159507",
          "name": "Jianling Su"
        },
        {
          "authorId": "2346807348",
          "name": "Jiezhong Qiu"
        },
        {
          "authorId": "2375413391",
          "name": "Bo Pang"
        },
        {
          "authorId": "2341660422",
          "name": "Junjie Yan"
        },
        {
          "authorId": "2346267080",
          "name": "Zhejun Jiang"
        },
        {
          "authorId": "2341676205",
          "name": "Weixiao Huang"
        },
        {
          "authorId": "2282949692",
          "name": "Bo Yin"
        },
        {
          "authorId": "2389562088",
          "name": "Jiacheng You"
        },
        {
          "authorId": "2355053769",
          "name": "Chu Wei"
        },
        {
          "authorId": "2351543321",
          "name": "Zhengtao Wang"
        },
        {
          "authorId": "2349770059",
          "name": "Chao Hong"
        },
        {
          "authorId": "2373609538",
          "name": "Yutian Chen"
        },
        {
          "authorId": "2373873104",
          "name": "Guanduo Chen"
        },
        {
          "authorId": "2389428625",
          "name": "Yucheng Wang"
        },
        {
          "authorId": "2347403465",
          "name": "Hua Zheng"
        },
        {
          "authorId": "2373662896",
          "name": "Feng Wang"
        },
        {
          "authorId": "2341669617",
          "name": "Yibo Liu"
        },
        {
          "authorId": "2323559064",
          "name": "Meng-xiao Dong"
        },
        {
          "authorId": "2375780543",
          "name": "Zheng Zhang"
        },
        {
          "authorId": "2374104177",
          "name": "Siyuan Pan"
        },
        {
          "authorId": "2335072605",
          "name": "Wenhao Wu"
        },
        {
          "authorId": "2389290738",
          "name": "Yuhao Wu"
        },
        {
          "authorId": "2373532086",
          "name": "Longyu Guan"
        },
        {
          "authorId": "2321934353",
          "name": "Ji-Hua Tao"
        },
        {
          "authorId": "2257348619",
          "name": "Guohong Fu"
        },
        {
          "authorId": "2309706542",
          "name": "Xinran Xu"
        },
        {
          "authorId": "2341669353",
          "name": "Yuzhi Wang"
        },
        {
          "authorId": "2341540112",
          "name": "Guokun Lai"
        },
        {
          "authorId": "2145094614",
          "name": "Yuxin Wu"
        },
        {
          "authorId": "2271357239",
          "name": "Xinyue Zhou"
        },
        {
          "authorId": "2346584743",
          "name": "Zhilin Yang"
        },
        {
          "authorId": "2318984243",
          "name": "Yulun Du"
        }
      ],
      "abstract": "We introduce Kimi Linear, a hybrid linear attention architecture that, for the first time, outperforms full attention under fair comparisons across various scenarios -- including short-context, long-context, and reinforcement learning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an expressive linear attention module that extends Gated DeltaNet with a finer-grained gating mechanism, enabling more effective use of limited finite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware efficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR) transition matrices, which substantially reduces computation compared to the general DPLR formulation while remaining more consistent with the classical delta rule. We pretrain a Kimi Linear model with 3B activated parameters and 48B total parameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention (MLA). Our experiments show that with an identical training recipe, Kimi Linear outperforms full MLA with a sizeable margin across all evaluated tasks, while reducing KV cache usage by up to 75% and achieving up to 6 times decoding throughput for a 1M context. These results demonstrate that Kimi Linear can be a drop-in replacement for full attention architectures with superior performance and efficiency, including tasks with longer input and output lengths. To support further research, we open-source the KDA kernel and vLLM implementations, and release the pre-trained and instruction-tuned model checkpoints."
    },
    {
      "paperId": "b155499221c6f826068eddb230915596d766e74a",
      "externalIds": {
        "PubMedCentral": "12575864",
        "DOI": "10.1038/s41598-025-21756-2",
        "CorpusId": 282597317,
        "PubMed": "41168265"
      },
      "corpusId": 282597317,
      "title": "DC-Mamba for surface micro defect classification on large aperture optics with multi axis attention",
      "venue": "Scientific Reports",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBYNCND",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12575864, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2309663043",
          "name": "Dejin Zhao"
        },
        {
          "authorId": "2325213980",
          "name": "Rui Sun"
        },
        {
          "authorId": "2325067928",
          "name": "Tong Tong"
        },
        {
          "authorId": "2310257895",
          "name": "Yunjie Ma"
        },
        {
          "authorId": "2249081942",
          "name": "Xiaolong Yuan"
        },
        {
          "authorId": "2361946771",
          "name": "Dexin Kong"
        },
        {
          "authorId": "2355029020",
          "name": "Bo Li"
        },
        {
          "authorId": "2344270239",
          "name": "Lili Cheng"
        },
        {
          "authorId": "2226520248",
          "name": "Jiajian Meng"
        },
        {
          "authorId": "2248773591",
          "name": "Jianhai Zhang"
        }
      ],
      "abstract": "Large-aperture optics play a pivotal role in high-performance optical systems, and the presence of micro-defects on their surfaces can significantly degrade system performance and reliability. Traditional methods for detecting these defects face challenges due to their small size, multiplicity, and complexity. This paper proposes an improved Mamba-based defect classification method, DC-Mamba, specifically designed for detecting surface micro-defects on large-aperture optics. DC-Mamba replaces the original model\u2019s scanning mechanism and neck structure with a multi-axis interactive 2D Selective Scan (MISS2D) and a Multi-axis Interactive Feature Pyramid Network (MIFPN), achieving coordinated modeling of positional information, hierarchical structures, spatial relationships, and channel-wise features in the input feature maps. Evaluation on the NEU-DET dataset demonstrates that DC-Mamba, with MACSA, increases the AP from 41.7% to 45.7% and AP50 from 72.2% to 74.7%, compared to the original VMamba model. Furthermore, DC-Mamba achieves an AP of 64.3% on our self-made optic surface micro-defect (OSMD) dataset. By effectively distinguishing micro-defects from interference points, DC-Mamba provides a robust solution for intelligent defect detection on large-aperture optics surfaces."
    },
    {
      "paperId": "c6b95bda66e5897adf380e35c905372dc20e3e95",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-26912",
        "ArXiv": "2510.26912",
        "DOI": "10.18653/v1/2025.babylm-main.27",
        "CorpusId": 282719172
      },
      "corpusId": 282719172,
      "title": "Understanding and Enhancing Mamba-Transformer Hybrids for Memory Recall and Language Modeling",
      "venue": "Proceedings of the First BabyLM Workshop",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.26912, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2140191673",
          "name": "Hyunji Lee"
        },
        {
          "authorId": "2265843326",
          "name": "Wenhao Yu"
        },
        {
          "authorId": "2254831297",
          "name": "Hongming Zhang"
        },
        {
          "authorId": "2311458574",
          "name": "Kaixin Ma"
        },
        {
          "authorId": "2323825389",
          "name": "Jiyeon Kim"
        },
        {
          "authorId": "2303999735",
          "name": "Dong Yu"
        },
        {
          "authorId": "2266468609",
          "name": "Minjoon Seo"
        }
      ],
      "abstract": "Hybrid models that combine state space models (SSMs) with attention mechanisms have shown strong performance by leveraging the efficiency of SSMs and the high recall ability of attention. However, the architectural design choices behind these hybrid models remain insufficiently understood. In this work, we analyze hybrid architectures through the lens of memory utilization and overall performance, and propose a complementary method to further enhance their effectiveness. We first examine the distinction between sequential and parallel integration of SSM and attention layers. Our analysis reveals several interesting findings, including that sequential hybrids perform better on shorter contexts, whereas parallel hybrids are more effective for longer contexts. We also introduce a data-centric approach of continually training on datasets augmented with paraphrases, which further enhances recall while preserving other capabilities. It generalizes well across different base models and outperforms architectural modifications aimed at enhancing recall. Our findings provide a deeper understanding of hybrid SSM-attention models and offer practical guidance for designing architectures tailored to various use cases. Our findings provide a deeper understanding of hybrid SSM-attention models and offer practical guidance for designing architectures tailored to various use cases."
    },
    {
      "paperId": "976c850636176addd29ee604868ddf657d8c7227",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-26001",
        "ArXiv": "2510.26001",
        "DOI": "10.48550/arXiv.2510.26001",
        "CorpusId": 282591906
      },
      "corpusId": 282591906,
      "title": "Larger Hausdorff Dimension in Scanning Pattern Facilitates Mamba-Based Methods in Low-Light Image Enhancement",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.26001, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2389126014",
          "name": "Xinhua Wang"
        },
        {
          "authorId": "2365053308",
          "name": "Caibo Feng"
        },
        {
          "authorId": "2391969385",
          "name": "Xiangjun Fu"
        },
        {
          "authorId": "2389842174",
          "name": "Chunxiao Liu"
        }
      ],
      "abstract": "We propose an innovative enhancement to the Mamba framework by increasing the Hausdorff dimension of its scanning pattern through a novel Hilbert Selective Scan mechanism. This mechanism explores the feature space more effectively, capturing intricate fine-scale details and improving overall coverage. As a result, it mitigates information inconsistencies while refining spatial locality to better capture subtle local interactions without sacrificing the model's ability to handle long-range dependencies. Extensive experiments on publicly available benchmarks demonstrate that our approach significantly improves both the quantitative metrics and qualitative visual fidelity of existing Mamba-based low-light image enhancement methods, all while reducing computational resource consumption and shortening inference time. We believe that this refined strategy not only advances the state-of-the-art in low-light image enhancement but also holds promise for broader applications in fields that leverage Mamba-based techniques."
    },
    {
      "paperId": "87bf6b846c8115f00d7d38d465cceef251833752",
      "externalIds": {
        "ArXiv": "2510.25502",
        "DBLP": "journals/corr/abs-2510-25502",
        "DOI": "10.48550/arXiv.2510.25502",
        "CorpusId": 282574554
      },
      "corpusId": 282574554,
      "title": "TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time Series Forecasting",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.25502, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2384445337",
          "name": "Vladyslav Moroshan"
        },
        {
          "authorId": "81742501",
          "name": "Julien N. Siems"
        },
        {
          "authorId": "51109984",
          "name": "Arber Zela"
        },
        {
          "authorId": "2345692748",
          "name": "Timur Carstensen"
        },
        {
          "authorId": "2282539872",
          "name": "Frank Hutter"
        }
      ],
      "abstract": "Foundation models for zero-shot time series forecasting face challenges in efficient long-horizon prediction and reproducibility, with existing synthetic-only approaches underperforming on challenging benchmarks. This paper presents TempoPFN, a univariate time series foundation model based on linear Recurrent Neural Networks (RNNs) pre-trained exclusively on synthetic data. The model uses a GatedDeltaProduct architecture with state-weaving for fully parallelizable training across sequence lengths, eliminating the need for windowing or summarization techniques while maintaining robust temporal state-tracking. Our comprehensive synthetic data pipeline unifies diverse generators, including stochastic differential equations, Gaussian processes, and audio synthesis, with novel augmentations. In zero-shot evaluations on the Gift-Eval, fev-bench and Chronos-ZS benchmarks, TempoPFN achieves top-tier competitive performance, outperforming all existing synthetic-only approaches and surpassing the majority of models trained on real-world data, while being more efficient than existing baselines by leveraging fully parallelizable training and inference. We open-source our complete data generation pipeline and training code, providing a reproducible foundation for future research."
    },
    {
      "paperId": "3be4ba3021f1624020088ada997e960986701302",
      "externalIds": {
        "DBLP": "journals/dai/LiuY25",
        "DOI": "10.1007/s44163-025-00561-w",
        "CorpusId": 282546250
      },
      "corpusId": 282546250,
      "title": "Artificial neural network and the prospect of AGI: an argument from architecture",
      "venue": "Discover Artificial Intelligence",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s44163-025-00561-w?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s44163-025-00561-w, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2388990636",
          "name": "Chang Liu"
        },
        {
          "authorId": "2388747772",
          "name": "Bin Ye"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "a28d45d28019a3250fb73c301c55f95bf6d5ad2f",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-25259",
        "ArXiv": "2510.25259",
        "DOI": "10.48550/arXiv.2510.25259",
        "CorpusId": 282574604
      },
      "corpusId": 282574604,
      "title": "TV-Rec: Time-Variant Convolutional Filter for Sequential Recommendation",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.25259, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2171121134",
          "name": "Yehjin Shin"
        },
        {
          "authorId": "67027632",
          "name": "Jeongwhan Choi"
        },
        {
          "authorId": "2265723597",
          "name": "Seojin Kim"
        },
        {
          "authorId": "2265649308",
          "name": "Noseong Park"
        }
      ],
      "abstract": "Recently, convolutional filters have been increasingly adopted in sequential recommendation for their ability to capture local sequential patterns. However, most of these models complement convolutional filters with self-attention. This is because convolutional filters alone, generally fixed filters, struggle to capture global interactions necessary for accurate recommendation. We propose Time-Variant Convolutional Filters for Sequential Recommendation (TV-Rec), a model inspired by graph signal processing, where time-variant graph filters capture position-dependent temporal variations in user sequences. By replacing both fixed kernels and self-attention with time-variant filters, TV-Rec achieves higher expressive power and better captures complex interaction patterns in user behavior. This design not only eliminates the need for self-attention but also reduces computation while accelerating inference. Extensive experiments on six public benchmarks show that TV-Rec outperforms state-of-the-art baselines by an average of 7.49%."
    },
    {
      "paperId": "669558f00a2ed405ad2f51dcffacce058152ecaa",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-25193",
        "ArXiv": "2510.25193",
        "DOI": "10.48550/arXiv.2510.25193",
        "CorpusId": 282574846
      },
      "corpusId": 282574846,
      "title": "State Space and Self-Attention Collaborative Network with Feature Aggregation for DOA Estimation",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.25193, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2389327457",
          "name": "Qi You"
        },
        {
          "authorId": "2300645867",
          "name": "Qinghua Huang"
        },
        {
          "authorId": "2388927015",
          "name": "Yi-Cheng Lin"
        }
      ],
      "abstract": "Accurate direction-of-arrival (DOA) estimation for sound sources is challenging due to the continuous changes in acoustic characteristics across time and frequency. In such scenarios, accurate localization relies on the ability to aggregate relevant features and model temporal dependencies effectively. In time series modeling, achieving a balance between model performance and computational efficiency remains a significant challenge. To address this, we propose FA-Stateformer, a state space and self-attention collaborative network with feature aggregation. The proposed network first employs a feature aggregation module to enhance informative features across both temporal and spectral dimensions. This is followed by a lightweight Conformer architecture inspired by the squeeze-and-excitation mechanism, where the feedforward layers are compressed to reduce redundancy and parameter overhead. Additionally, a temporal shift mechanism is incorporated to expand the receptive field of convolutional layers while maintaining a compact kernel size. To further enhance sequence modeling capabilities, a bidirectional Mamba module is introduced, enabling efficient state-space-based representation of temporal dependencies in both forward and backward directions. The remaining self-attention layers are combined with the Mamba blocks, forming a collaborative modeling framework that achieves a balance between representation capacity and computational efficiency. Extensive experiments demonstrate that FA-Stateformer achieves superior performance and efficiency compared to conventional architectures."
    },
    {
      "paperId": "f9d4e8e785e2cceb26e78ec31086b0f91d17da95",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-25138",
        "ArXiv": "2510.25138",
        "DOI": "10.48550/arXiv.2510.25138",
        "CorpusId": 282574893
      },
      "corpusId": 282574893,
      "title": "Learning Spatial-Aware Manipulation Ordering",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.25138, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2244779142",
          "name": "Yuxiang Yan"
        },
        {
          "authorId": "2390218578",
          "name": "Zhiyuan Zhou"
        },
        {
          "authorId": "2345909388",
          "name": "Xin Gao"
        },
        {
          "authorId": "2291155053",
          "name": "Guanghao Li"
        },
        {
          "authorId": "2389943091",
          "name": "Shenglin Li"
        },
        {
          "authorId": "2317995958",
          "name": "Jiaqi Chen"
        },
        {
          "authorId": "2388810824",
          "name": "Qunyan Pu"
        },
        {
          "authorId": "2345820037",
          "name": "Jian Pu"
        }
      ],
      "abstract": "Manipulation in cluttered environments is challenging due to spatial dependencies among objects, where an improper manipulation order can cause collisions or blocked access. Existing approaches often overlook these spatial relationships, limiting their flexibility and scalability. To address these limitations, we propose OrderMind, a unified spatial-aware manipulation ordering framework that directly learns object manipulation priorities based on spatial context. Our architecture integrates a spatial context encoder with a temporal priority structuring module. We construct a spatial graph using k-Nearest Neighbors to aggregate geometric information from the local layout and encode both object-object and object-manipulator interactions to support accurate manipulation ordering in real-time. To generate physically and semantically plausible supervision signals, we introduce a spatial prior labeling method that guides a vision-language model to produce reasonable manipulation orders for distillation. We evaluate OrderMind on our Manipulation Ordering Benchmark, comprising 163,222 samples of varying difficulty. Extensive experiments in both simulation and real-world environments demonstrate that our method significantly outperforms prior approaches in effectiveness and efficiency, enabling robust manipulation in cluttered scenes."
    },
    {
      "paperId": "fb356391908cadb1cb56ea82fd5779033dd2d9c9",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-25517",
        "ArXiv": "2510.25517",
        "DOI": "10.48550/arXiv.2510.25517",
        "CorpusId": 282579209
      },
      "corpusId": 282579209,
      "title": "Predicate Renaming via Large Language Models",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.25517, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2221857852",
          "name": "Elisabetta Gentili"
        },
        {
          "authorId": "143681572",
          "name": "Tony Ribeiro"
        },
        {
          "authorId": "2389340429",
          "name": "Fabrizio Riguzzi"
        },
        {
          "authorId": "2372750882",
          "name": "Katsumi Inoue"
        }
      ],
      "abstract": "In this paper, we address the problem of giving names to predicates in logic rules using Large Language Models (LLMs). In the context of Inductive Logic Programming, various rule generation methods produce rules containing unnamed predicates, with Predicate Invention being a key example. This hinders the readability, interpretability, and reusability of the logic theory. Leveraging recent advancements in LLMs development, we explore their ability to process natural language and code to provide semantically meaningful suggestions for giving a name to unnamed predicates. The evaluation of our approach on some hand-crafted logic rules indicates that LLMs hold potential for this task."
    },
    {
      "paperId": "5e43dbe739424aff8ab9f91016daa062645aeb5d",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-25557",
        "ArXiv": "2510.25557",
        "DOI": "10.48550/arXiv.2510.25557",
        "CorpusId": 282574809
      },
      "corpusId": 282574809,
      "title": "Hybrid Quantum-Classical Recurrent Neural Networks",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.25557, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2389027621",
          "name": "Wenduan Xu"
        }
      ],
      "abstract": "We present a hybrid quantum-classical recurrent neural network (QRNN) architecture in which the recurrent core is realized as a parametrized quantum circuit (PQC) controlled by a classical feedforward network. The hidden state is the quantum state of an $n$-qubit PQC in an exponentially large Hilbert space $\\mathbb{C}^{2^n}$, which serves as a coherent recurrent quantum memory. The PQC is unitary by construction, making the hidden-state evolution norm-preserving without external constraints. At each timestep, mid-circuit Pauli expectation-value readouts are combined with the input embedding and processed by the feedforward network, which provides explicit classical nonlinearity. The outputs parametrize the PQC, which updates the hidden state via unitary dynamics. The QRNN is compact and physically consistent, and it unifies (i) unitary recurrence as a high-capacity memory, (ii) partial observation via mid-circuit readouts, and (iii) nonlinear classical control for input-conditioned parametrization. We evaluate the model in simulation with up to 14 qubits on sentiment analysis, MNIST, permuted MNIST, copying memory, and language modeling. For sequence-to-sequence learning, we further devise a soft attention mechanism over the mid-circuit readouts and show its effectiveness for machine translation. To our knowledge, this is the first model (RNN or otherwise) grounded in quantum operations to achieve competitive performance against strong classical baselines across a broad class of sequence-learning tasks."
    },
    {
      "paperId": "42ef49a88e6f8fe2bbd03261c717ca61d06f6769",
      "externalIds": {
        "ArXiv": "2511.00085",
        "DBLP": "journals/corr/abs-2511-00085",
        "DOI": "10.48550/arXiv.2511.00085",
        "CorpusId": 282738864
      },
      "corpusId": 282738864,
      "title": "MaGNet: A Mamba Dual-Hypergraph Network for Stock Prediction via Temporal-Causal and Global Relational Learning",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.00085, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2388838997",
          "name": "Peilin Tan"
        },
        {
          "authorId": "2392818248",
          "name": "Chuanqi Shi"
        },
        {
          "authorId": "2389075982",
          "name": "Dian Tu"
        },
        {
          "authorId": "2390573502",
          "name": "Liang Xie"
        }
      ],
      "abstract": "Stock trend prediction is crucial for profitable trading strategies and portfolio management yet remains challenging due to market volatility, complex temporal dynamics and multifaceted inter-stock relationships. Existing methods struggle to effectively capture temporal dependencies and dynamic inter-stock interactions, often neglecting cross-sectional market influences, relying on static correlations, employing uniform treatments of nodes and edges, and conflating diverse relationships. This work introduces MaGNet, a novel Mamba dual-hyperGraph Network for stock prediction, integrating three key innovations: (1) a MAGE block, which leverages bidirectional Mamba with adaptive gating mechanisms for contextual temporal modeling and integrates a sparse Mixture-of-Experts layer to enable dynamic adaptation to diverse market conditions, alongside multi-head attention for capturing global dependencies; (2) Feature-wise and Stock-wise 2D Spatiotemporal Attention modules enable precise fusion of multivariate features and cross-stock dependencies, effectively enhancing informativeness while preserving intrinsic data structures, bridging temporal modeling with relational reasoning; and (3) a dual hypergraph framework consisting of the Temporal-Causal Hypergraph (TCH) that captures fine-grained causal dependencies with temporal constraints, and Global Probabilistic Hypergraph (GPH) that models market-wide patterns through soft hyperedge assignments and Jensen-Shannon Divergence weighting mechanism, jointly disentangling localized temporal influences from instantaneous global structures for multi-scale relational learning. Extensive experiments on six major stock indices demonstrate MaGNet outperforms state-of-the-art methods in both superior predictive performance and exceptional investment returns with robust risk management capabilities. Codes available at: https://github.com/PeilinTime/MaGNet."
    },
    {
      "paperId": "fcb97f2781a12bde83c071eafbfa9220dda6f964",
      "externalIds": {
        "DOI": "10.1109/ISPA66905.2025.11259472",
        "CorpusId": 283470011
      },
      "corpusId": 283470011,
      "title": "ESTER-VAR: Enhancing Semantic and Temporal Events Representation for Video Anomaly Recognition",
      "venue": "International Symposium on Image and Signal Processing and Analysis",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ISPA66905.2025.11259472?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ISPA66905.2025.11259472, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2395923509",
          "name": "Gon\u00e7alo Nereu Figueiredo"
        },
        {
          "authorId": "2395919274",
          "name": "Jorge Batista"
        }
      ],
      "abstract": "Recent Weakly-Supervised Video Anomaly Recognition approaches still face critical limitations, mainly due to two reasons: (1) Existing methods often struggle to capture both subtle local changes and long-range temporal dependencies, which are essential to understand complex anomaly dynamics; (2) The semantic information is constrained to class names or a small number of learnable tokens, which is not discriminative enough. To address these issues, we propose ESTER-VAR, a novel architecture composed of two main modules: The Temporal Adapter that captures both short- and long-range temporal dependencies through local and global patterns, sequence modelling, and multiscale temporal information; and the Textual Module, which combines LLM-generated anomaly descriptions with learnable prompts to produce more discriminative anomaly information. Experiments conducted on the XD-Violence and UCF-Crime datasets demonstrate state-of-the-art performance in fine-grained anomaly recognition and competitive results in coarse-grained anomaly detection. Specifically, our framework achieves 84.42% AP and 26.55% AVG mAP on XD-Violence, and 88.06% AUC and 7.49% AVG mAP on UCF-Crime."
    },
    {
      "paperId": "0d7fca53115b95b023782b7afef994bf6bc36b21",
      "externalIds": {
        "DOI": "10.1109/ISPA66905.2025.11259458",
        "CorpusId": 283470967
      },
      "corpusId": 283470967,
      "title": "Cross-Modal Fusion Mamba for Multimodal Depression Detection",
      "venue": "International Symposium on Image and Signal Processing and Analysis",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ISPA66905.2025.11259458?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ISPA66905.2025.11259458, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2398003625",
          "name": "Bowen Zhou"
        },
        {
          "authorId": "1831028182",
          "name": "Marc-Andr\u00e9 Fiedler"
        },
        {
          "authorId": "2351056723",
          "name": "A. Al-Hamadi"
        }
      ],
      "abstract": "Depression detection using multimodal signals has garnered growing attention due to its potential for early warning. However, existing approaches often rely on limited visual features or computationally intensive fusion mechanisms. In this study, we present a novel framework based on the Mamba structure to address these challenges. Our method fuses audio features with enriched visual representations by combining facial landmarks and action units (AUs), enhancing the expressiveness of visual cues. To capture intramodal information, we propose the Audio Mamba Encoder (AME) for audio modality, and the Vision CrossMamba (VCM) module for visual feature fusion. Furthermore, the Audio-Vision CrossMamba (AVCM) module is designed for intermodal interactions. Experimental results demonstrate superior performance over several baselines, highlighting the effectiveness of the proposed framework in detecting depression from multimodal data."
    },
    {
      "paperId": "37f8ccf7ab5cecba9e09f60b0e949a5bf7f76bb0",
      "externalIds": {
        "ArXiv": "2510.24260",
        "DBLP": "journals/corr/abs-2510-24260",
        "DOI": "10.48550/arXiv.2510.24260",
        "CorpusId": 282401039
      },
      "corpusId": 282401039,
      "title": "DeshadowMamba: Deshadowing as 1D Sequential Similarity",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.24260, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2312332049",
          "name": "Zhaotong Yang"
        },
        {
          "authorId": "2388038346",
          "name": "Yi Chen"
        },
        {
          "authorId": "2388119728",
          "name": "Yanying Li"
        },
        {
          "authorId": "2267886539",
          "name": "Shengfeng He"
        },
        {
          "authorId": "2154894041",
          "name": "Yangyang Xu"
        },
        {
          "authorId": "2211065645",
          "name": "Junyu Dong"
        },
        {
          "authorId": "2388902125",
          "name": "Jian Yang"
        },
        {
          "authorId": "2108867185",
          "name": "Yong Du"
        }
      ],
      "abstract": "Recent deep models for image shadow removal often rely on attention-based architectures to capture long-range dependencies. However, their fixed attention patterns tend to mix illumination cues from irrelevant regions, leading to distorted structures and inconsistent colors. In this work, we revisit shadow removal from a sequence modeling perspective and explore the use of Mamba, a selective state space model that propagates global context through directional state transitions. These transitions yield an efficient global receptive field while preserving positional continuity. Despite its potential, directly applying Mamba to image data is suboptimal, since it lacks awareness of shadow-non-shadow semantics and remains susceptible to color interference from nearby regions. To address these limitations, we propose CrossGate, a directional modulation mechanism that injects shadow-aware similarity into Mamba's input gate, allowing selective integration of relevant context along transition axes. To further ensure appearance fidelity, we introduce ColorShift regularization, a contrastive learning objective driven by global color statistics. By synthesizing structured informative negatives, it guides the model to suppress color contamination and achieve robust color restoration. Together, these components adapt sequence modeling to the structural integrity and chromatic consistency required for shadow removal. Extensive experiments on public benchmarks demonstrate that DeshadowMamba achieves state-of-the-art visual quality and strong quantitative performance."
    },
    {
      "paperId": "58549bbb5bffab9c286694963639586c5388313f",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-25791",
        "ArXiv": "2510.25791",
        "DOI": "10.48550/arXiv.2510.25791",
        "CorpusId": 282592395
      },
      "corpusId": 282592395,
      "title": "The Kinetics of Reasoning: How Chain-of-Thought Shapes Learning in Transformers?",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.25791, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2153054899",
          "name": "Zihan Pengmei"
        },
        {
          "authorId": "2336865777",
          "name": "C. Mavromatis"
        },
        {
          "authorId": "2223163421",
          "name": "Zhengyuan Shen"
        },
        {
          "authorId": "2389395113",
          "name": "Yunyi Zhang"
        },
        {
          "authorId": "40043851",
          "name": "V. Ioannidis"
        },
        {
          "authorId": "145344187",
          "name": "H. Rangwala"
        }
      ],
      "abstract": "Chain-of-thought (CoT) supervision can substantially improve transformer performance, yet the mechanisms by which models learn to follow and benefit from CoT remain poorly understood. We investigate these learning dynamics through the lens of grokking by pretraining transformers on symbolic reasoning tasks with tunable algorithmic complexity and controllable data composition to study their generalization. Models were trained under two settings: (i) producing only final answers, and (ii) emitting explicit CoT traces before answering. Our results show that while CoT generally improves task performance, its benefits depend on task complexity. To quantify these effects, we model the accuracy of the logarithmic training steps with a three-parameter logistic curve, revealing how the learning speed and shape vary with task complexity, data distribution, and the presence of CoT supervision. We also uncover a transient trace unfaithfulness phase: early in training, models often produce correct answers while skipping or contradicting CoT steps, before later aligning their reasoning traces with answers. Empirically, we (1) demonstrate that CoT accelerates generalization but does not overcome tasks with higher algorithmic complexity, such as finding list intersections; (2) introduce a kinetic modeling framework for understanding transformer learning; (3) characterize trace faithfulness as a dynamic property that emerges over training; and (4) show CoT alters internal transformer computation mechanistically."
    },
    {
      "paperId": "b14d5068188eb6882adeef0a7d1b85ae03ff9cd9",
      "externalIds": {
        "DOI": "10.1101/2025.10.27.684803",
        "CorpusId": 282593556
      },
      "corpusId": 282593556,
      "title": "The Human Brain as a Dynamic Mixture of Expert Models in Video Understanding",
      "venue": "bioRxiv",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1101/2025.10.27.684803?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1101/2025.10.27.684803, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2185315713",
          "name": "Christina Sartzetaki"
        },
        {
          "authorId": "2389404278",
          "name": "Anne W. Zonneveld"
        },
        {
          "authorId": "84527352",
          "name": "Pablo Oyarzo"
        },
        {
          "authorId": "2159349255",
          "name": "A. T. Gifford"
        },
        {
          "authorId": "115203789",
          "name": "R. Cichy"
        },
        {
          "authorId": "2329084188",
          "name": "Pascal Mettes"
        },
        {
          "authorId": "2313617148",
          "name": "I. Groen"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "4539aea748444a72055058a49821f935d337599d",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-24139",
        "ArXiv": "2510.24139",
        "DOI": "10.48550/arXiv.2510.24139",
        "CorpusId": 282401570
      },
      "corpusId": 282401570,
      "title": "Beyond Line-Level Filtering for the Pretraining Corpora of LLMs",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.24139, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2288710439",
          "name": "Chanwoo Park"
        },
        {
          "authorId": "2388242905",
          "name": "Suyoung Park"
        },
        {
          "authorId": "2374474863",
          "name": "Yelim Ahn"
        },
        {
          "authorId": "2371083694",
          "name": "Jongmin Kim"
        },
        {
          "authorId": "2388017221",
          "name": "Jongyeon Park"
        },
        {
          "authorId": "2374496711",
          "name": "Jaejin Lee"
        }
      ],
      "abstract": "While traditional line-level filtering techniques, such as line-level deduplication and trailing-punctuation filters, are commonly used, these basic methods can sometimes discard valuable content, negatively affecting downstream performance. In this paper, we introduce two methods-pattern-aware line-level deduplication (PLD) and pattern-aware trailing punctuation filtering (PTF)-by enhancing the conventional filtering techniques. Our approach not only considers line-level signals but also takes into account their sequential distribution across documents, enabling us to retain structurally important content that might otherwise be removed. We evaluate these proposed methods by training small language models (1 B parameters) in both English and Korean. The results demonstrate that our methods consistently improve performance on multiple-choice benchmarks and significantly enhance generative question-answering accuracy on both SQuAD v1 and KorQuAD v1."
    },
    {
      "paperId": "498f9861253bfc2d2d47013da1eb64ac2a120934",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-24125",
        "ArXiv": "2510.24125",
        "DOI": "10.48550/arXiv.2510.24125",
        "CorpusId": 282401638
      },
      "corpusId": 282401638,
      "title": "Causal Convolutional Neural Networks as Finite Impulse Response Filters",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.24125, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2133300239",
          "name": "Kiran Bacsa"
        },
        {
          "authorId": "2263281886",
          "name": "Wei Liu"
        },
        {
          "authorId": "2350087710",
          "name": "Xudong Jian"
        },
        {
          "authorId": "2327933975",
          "name": "Huangbin Liang"
        },
        {
          "authorId": "2327336204",
          "name": "Eleni N. Chatzi"
        }
      ],
      "abstract": "This study investigates the behavior of Causal Convolutional Neural Networks (CNNs) with quasi-linear activation functions when applied to time-series data characterized by multimodal frequency content. We demonstrate that, once trained, such networks exhibit properties analogous to Finite Impulse Response (FIR) filters, particularly when the convolutional kernels are of extended length exceeding those typically employed in standard CNN architectures. Causal CNNs are shown to capture spectral features both implicitly and explicitly, offering enhanced interpretability for tasks involving dynamic systems. Leveraging the associative property of convolution, we further show that the entire network can be reduced to an equivalent single-layer filter resembling an FIR filter optimized via least-squares criteria. This equivalence yields new insights into the spectral learning behavior of CNNs trained on signals with sparse frequency content. The approach is validated on both simulated beam dynamics and real-world bridge vibration datasets, underlining its relevance for modeling and identifying physical systems governed by dynamic responses."
    },
    {
      "paperId": "5d68bda617e3505fa14f57fab34e8179304a3a51",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-24963",
        "ArXiv": "2510.24963",
        "DOI": "10.48550/arXiv.2510.24963",
        "CorpusId": 282573786
      },
      "corpusId": 282573786,
      "title": "Language Model Behavioral Phases are Consistent Across Architecture, Training Data, and Scale",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.24963, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "118683123",
          "name": "James A. Michaelov"
        },
        {
          "authorId": "2388793486",
          "name": "Roger P. Levy"
        },
        {
          "authorId": "2257347492",
          "name": "Benjamin Bergen"
        }
      ],
      "abstract": "We show that across architecture (Transformer vs. Mamba vs. RWKV), training dataset (OpenWebText vs. The Pile), and scale (14 million parameters to 12 billion parameters), autoregressive language models exhibit highly consistent patterns of change in their behavior over the course of pretraining. Based on our analysis of over 1,400 language model checkpoints on over 110,000 tokens of English, we find that up to 98% of the variance in language model behavior at the word level can be explained by three simple heuristics: the unigram probability (frequency) of a given word, the $n$-gram probability of the word, and the semantic similarity between the word and its context. Furthermore, we see consistent behavioral phases in all language models, with their predicted probabilities for words overfitting to those words'$n$-gram probabilities for increasing $n$ over the course of training. Taken together, these results suggest that learning in neural language models may follow a similar trajectory irrespective of model details."
    },
    {
      "paperId": "fdccf8afa54e67a668d4016e8c2d3c3c4fbef29e",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-24966",
        "ArXiv": "2510.24966",
        "DOI": "10.48550/arXiv.2510.24966",
        "CorpusId": 282574834
      },
      "corpusId": 282574834,
      "title": "Sequences of Logits Reveal the Low Rank Structure of Language Models",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.24966, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "3348246",
          "name": "Noah Golowich"
        },
        {
          "authorId": "2391493970",
          "name": "Allen Liu"
        },
        {
          "authorId": "2281640205",
          "name": "Abhishek Shetty"
        }
      ],
      "abstract": "A major problem in the study of large language models is to understand their inherent low-dimensional structure. We introduce an approach to study the low-dimensional structure of language models at a model-agnostic level: as sequential probabilistic models. We first empirically demonstrate that a wide range of modern language models exhibit low-rank structure: in particular, matrices built from the model's logits for varying sets of prompts and responses have low approximate rank. We then show that this low-rank structure can be leveraged for generation -- in particular, we can generate a response to a target prompt using a linear combination of the model's outputs on unrelated, or even nonsensical prompts. On the theoretical front, we observe that studying the approximate rank of language models in the sense discussed above yields a simple universal abstraction whose theoretical predictions parallel our experiments. We then analyze the representation power of the abstraction and give provable learning guarantees."
    },
    {
      "paperId": "090bcc1a89f4c359b2fafeb9c1e841325db2f522",
      "externalIds": {
        "PubMedCentral": "12610613",
        "DOI": "10.3390/s25216614",
        "CorpusId": 282642079,
        "PubMed": "41228837"
      },
      "corpusId": 282642079,
      "title": "DB-YOLO: A Dual-Branch Parallel Industrial Defect Detection Network",
      "venue": "Italian National Conference on Sensors",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12610613, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2334523235",
          "name": "Ziling Fan"
        },
        {
          "authorId": "2332751471",
          "name": "Yan Zhao"
        },
        {
          "authorId": "2390435683",
          "name": "Chaofu Liu"
        },
        {
          "authorId": "2390570903",
          "name": "Jinliang Qiu"
        }
      ],
      "abstract": "Insulator defect detection in power inspection tasks faces significant challenges due to the large variations in defect sizes and complex backgrounds, which hinder the accurate identification of both small and large defects. To overcome these issues, we propose a novel dual-branch YOLO-based algorithm (DB-YOLO), built upon the YOLOv11 architecture. The model introduces two dedicated branches, each tailored for detecting large and small defects, respectively, thereby enhancing robustness and precision across multiple scales. To further strengthen global feature representation, the Mamba mechanism is integrated, improving the detection of large defects in cluttered scenes. An adaptive weighted CIoU loss function, designed based on defect size, is employed to refine localization during training. Additionally, ShuffleNetV2 is embedded as a lightweight backbone to boost inference speed without compromising accuracy. We evaluate DB-YOLO on the following three datasets: the open source CPLID, a self-built insulator defect dataset, and GC-10. Experimental results demonstrate that DB-YOLO achieves superior performance in both accuracy and real-time efficiency compared to existing state-of-the-art methods. These findings suggest that the proposed approach offers strong potential for practical deployment in real-world power inspection applications."
    },
    {
      "paperId": "b1ffdfe52e771219e45a210f7f9f362a8a7069d6",
      "externalIds": {
        "DBLP": "journals/sivp/LuoWLLCLZ25",
        "DOI": "10.1007/s11760-025-04914-x",
        "CorpusId": 282566383
      },
      "corpusId": 282566383,
      "title": "Efficient Echocardiogram Segmentation with Multi-Scale Residual State Space Modeling",
      "venue": "Signal, Image and Video Processing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11760-025-04914-x?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11760-025-04914-x, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2371157190",
          "name": "Zihe Luo"
        },
        {
          "authorId": "2369910644",
          "name": "Fufeng Wang"
        },
        {
          "authorId": null,
          "name": "Xinkui Liao"
        },
        {
          "authorId": "2280943747",
          "name": "Yuheng Liang"
        },
        {
          "authorId": "2280952574",
          "name": "Kaihua Che"
        },
        {
          "authorId": "2238056907",
          "name": "Wei Lv"
        },
        {
          "authorId": "2280956322",
          "name": "Xiaolin Zhu"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "142e264bd7e80ff4f3f81532ad4c0a18492ed7b2",
      "externalIds": {
        "DOI": "10.1117/12.3069876",
        "CorpusId": 282629822
      },
      "corpusId": 282629822,
      "title": "Enhanced-RSMamba: state space model for semantic segmentation of remote sensing images",
      "venue": "Environmental Remote Sensing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1117/12.3069876?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1117/12.3069876, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2389529335",
          "name": "Kangning Wang"
        },
        {
          "authorId": "2337102966",
          "name": "Xiao Wang"
        },
        {
          "authorId": "2389700018",
          "name": "Yongfei Xian"
        },
        {
          "authorId": "2271747878",
          "name": "Zhiguo Jiang"
        },
        {
          "authorId": "2000351915",
          "name": "Haopeng Zhang"
        }
      ],
      "abstract": "Semantic segmentation of remote sensing images is vital for land use analysis and environmental monitoring but faces challenges in capturing long-range dependencies and multiscale features in high-resolution scenes. We propose an optimized MambaVision architecture that integrates 2D selective scanning with adaptive dilated convolutions to effectively capture local and global features. Our novel Multi-Frequency Multi-Scale (MFMS) decoder fuses frequency-domain features, derived from Discrete Cosine Transform, with multiscale spatial information and orientation-aware attention, improving texture and boundary detection. As the first application of MambaVision to remote sensing segmentation, our model achieves an mIoU of 84.24% on the ISPRS Potsdam dataset, a 6.57% improvement over baselines. Ablation studies confirm the effectiveness of 2D scanning and orientation attention, making our approach efficient for resource-constrained applications."
    },
    {
      "paperId": "afc3eb214913580c67b4ed0d5539a0835e9fcbdf",
      "externalIds": {
        "DOI": "10.1109/ICCKE68588.2025.11273813",
        "CorpusId": 283726044
      },
      "corpusId": 283726044,
      "title": "MC-BioCLIPSR: A Mamba-CNN Hybrid Network with BioMedCLIP-Guided Loss for High-Resolution Brain MRI Reconstruction",
      "venue": "International Conference on Computer and Knowledge Engineering",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCKE68588.2025.11273813?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCKE68588.2025.11273813, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2346126164",
          "name": "Amin Kazempour"
        },
        {
          "authorId": "2371071209",
          "name": "Jafar Tanha"
        },
        {
          "authorId": "2267735033",
          "name": "S. Roshan"
        },
        {
          "authorId": "2291765827",
          "name": "Mahdi Zarrin"
        },
        {
          "authorId": "1658975121",
          "name": "Haniyeh Nikkhah"
        }
      ],
      "abstract": "High-quality images are vital in medical imaging, as they reveal fine anatomical details crucial for reliable diagnosis. Yet, obtaining high-resolution scans can be costly and time-intensive. An effective solution is to computationally enhance low-resolution scans to their high-resolution equivalents. In this work, we present a new encoder-decoder framework built upon deep learning principles, which combines convolutional neural networks (CNNs) with Mamba-based modules for accurate high-resolution image reconstruction. We further introduce a novel loss function inspired by the BioMedCLIP paradigm to improve training performance. To validate the effectiveness of our approach, we benchmark it against multiple state-of-the-art methods using the IXI dataset."
    },
    {
      "paperId": "2717a854428aece0fdac63f56cad14c2c7831597",
      "externalIds": {
        "DOI": "10.1145/3746027.3755380",
        "CorpusId": 282395064
      },
      "corpusId": 282395064,
      "title": "An Event-tailored State-Space Based Model for Pedestrian Detection",
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746027.3755380?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746027.3755380, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2387995479",
          "name": "Liuyi Li"
        },
        {
          "authorId": "2390187445",
          "name": "Feng Shi"
        },
        {
          "authorId": "2388277030",
          "name": "Jian Wang"
        },
        {
          "authorId": "2387963342",
          "name": "Jinjing Zhu"
        },
        {
          "authorId": "2308295167",
          "name": "Wenze Shao"
        }
      ],
      "abstract": "Event cameras, as emerging bio-inspired sensors, endow us with a unique scene perception capability with sub-millisecond latency in challenging environments, such as high-dynamic range and motion blur, as to which a plausible yet efficient exploration on spatiotemporal characteristics of the sparse, asynchronous event data remains an open problem. Event-based pedestrian detection, considered as a promising alternate for road safety in autonomous driving, is chosen as the testbed in this paper for pursuing a specific event-tailored spatiotemporal model. Note that, heterogeneous architectures are generally used in literature, such as building on a CNN/Transformer-style model for capturing the spatial features and a RNN/LSTM model for mining the temporal coherence, respectively. However, existing methods still face significant limitations, particularly as deployed in multi-rate dynamic environments, characterized by pronounced sparsity patterns in slow-motion or other scenarios. As such, a homogeneous neural network for robust pedestrian detection is proposed, with Event-tailored Recurrent Spatiotemporal State-Space Module (ERS3M) as the core innovation, for a joint meticulous modeling of spatiotemporal sparsity and dynamics over event data. On one hand, inspired by Vision Mamba, ERS3M is equipped with adaptive spatiotemporal state propagation as well as multi-directional compensatory scanning, enabling elaborate detection even as to observation intervals with extremely limited events triggered. On the other hand, ERS3 M is augmented with an additional block termed Temporal-Entropy Synergy, offering a collaborative spatiotemporal event purification mechanism, so as to enhance the probability credibility of event streams in visual semantics considering their complicated dynamics. Finally, ERS3M ends with an aliasing-alleviated S5 block to transit information between consecutive time steps, facilitating the temporal consistent pedestrian detection. Evaluations on the PEDRo dataset demonstrate that, the proposed detection method with ERS3 M as backbone has achieved a comparable or even superior performance to state-of-the-art approaches in terms of both accuracy and efficiency."
    },
    {
      "paperId": "5001209f5237470734c1ac948066706dd138ba8e",
      "externalIds": {
        "DOI": "10.1145/3728425.3759910",
        "CorpusId": 283268850
      },
      "corpusId": 283268850,
      "title": "Learning Multi-Domain Representations to Restore Degraded Surgical Videos",
      "venue": "Proceedings of the 1st International Workshop &amp; Challenge on Subtle Visual Computing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3728425.3759910?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3728425.3759910, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2355911341",
          "name": "Hongtao Wu"
        },
        {
          "authorId": "2274928570",
          "name": "Hongqiu Wang"
        },
        {
          "authorId": "2237943083",
          "name": "Yijun Yang"
        },
        {
          "authorId": "2395598046",
          "name": "Fengdi Jiang"
        },
        {
          "authorId": "153107262",
          "name": "Zhaohu Xing"
        },
        {
          "authorId": "2280201265",
          "name": "Lei Zhu"
        }
      ],
      "abstract": "Laparoscopic surgical videos frequently exhibit subtle yet impactful visual degradations caused by the concurrent presence of surgical smoke and fog. These degradations often manifest as low-contrast occlusions and fine-grained scattering patterns that are challenging to detect and disentangle, yet critically impair the surgeon's situational awareness. Existing desmoking techniques predominantly target either smoke or fog in isolation, limiting their generalization and robustness when confronted with compounded degradations. In this work, we introduce a novel video-based surgical desmoking framework that explicitly models these subtle visual signals by jointly leveraging multi-domain representation learning. To address the complex temporal dynamics of degradations inherent in surgical video data, we propose a multi-scale Gaussian window constraint to effectively characterize inter-frame representation distributions while preserving fine structural details. Complementarily, we design a novel frequency restoration strategy that facilitates the recovery of subtle visual information attenuated across the spectral dimension. We validate our approach on both synthetic and real-world laparoscopic video benchmarks, demonstrating that it significantly outperforms state-of-the-art baselines in terms of perceptual quality and quantitative restoration metrics."
    },
    {
      "paperId": "2762a4dcd9ebb393feb5d0bf834883261740d4c4",
      "externalIds": {
        "DOI": "10.1145/3746027.3755306",
        "CorpusId": 282393718
      },
      "corpusId": 282393718,
      "title": "Probabilistic Mixture of Hyperbolic Mamba for Few-Shot Class-Incremental Learning",
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746027.3755306?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746027.3755306, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2366496568",
          "name": "Yawen Cui"
        },
        {
          "authorId": "2384446752",
          "name": "Wenbin Zou"
        },
        {
          "authorId": "2310349831",
          "name": "Huiping Zhuang"
        },
        {
          "authorId": "2374179631",
          "name": "Yi Wang"
        },
        {
          "authorId": "2075421795",
          "name": "L. Chau"
        }
      ],
      "abstract": "Few-shot class-incremental learning (FSCIL) grapples with the dual challenge of learning new classes from minimal labeled training data while alleviating catastrophic forgetting of previous learned classes. Compared with previous methods employing static adaptation on specific parameters, current works verify that dynamic weights and sequence modeling in Selective State Space Models (SSMs) can capture distinctive feature drifts in FSCIL. However, the flattening operation in SSMs fragments the latent semantic relationship, where the resulting task isolation and representation degeneration are detrimental to FSCIL. Toward this issue, this paper presents a novel framework named Probabilistic Mixture of Hyperbolic State Space Experts (PmH-SSE) for FSCIL. First, since SSMs rely on scanning as an alternative to self-attention, the Hyperbolic state space model with multi-scale hybrid scan is built to facilitate few-shot learning by providing an extra Hyperbolic geometry that encodes hierarchical relationships. Moreover, we propose the probabilistic mixture of Mamba to increase the model's flexibility in handling non-stationary data streams in FSCIL and enhance the stability of high-parameter models in few-shot conditions. Finally, under the same experimental conditions, the proposed PmH-SSE demonstrates superior performance in comprehensive experiments. The codes are available at https://github.com/yawencui/PmH-SSE."
    },
    {
      "paperId": "38e35762548601409c76191da5f4b8eade578643",
      "externalIds": {
        "DOI": "10.1145/3746027.3755055",
        "CorpusId": 282396036
      },
      "corpusId": 282396036,
      "title": "Balancing Cross-Modal Attention for Generalized Zero-Shot Learning",
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746027.3755055?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746027.3755055, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2268393647",
          "name": "Zhijie Rao"
        },
        {
          "authorId": "2319633611",
          "name": "Jingcai Guo"
        }
      ],
      "abstract": "Recent advances in Generalized Zero-Shot Learning have demonstrated promising results by leveraging cross-modal attention mechanisms to model the nuanced relationships between semantic attributes and visual regions. However, we find that inherent attribute imbalance leads to significant attention disparities, i.e., those attributes with less sample weights often exhibit lower attention confidence and poorer localization accuracy. This attention illusion obviously leads to potential degraded or even counterproductive contributions to category determination. Conventional rebalancing approaches like resampling or reweighting are ineffective for attention calibration due to the complex interdependencies among attributes in one object and the absence of region-level annotation guidance. In this paper, we propose a novel coarse-to-fine framework termed Hierarchical Progressive Attention Network (HPAN), which leverages latent attention consistency across attributes to rectify the focus deviation of minor attributes. HPAN comprises two synergistic components, Super-Attribute Guided Generable Attention (SGGA) and Rebalanced Attribute Attention Calibration (RAAC). SGGA employs super-attributes to establish unified attention regions for both major and minor attributes, subsequently propagating these attention masks to RAAC for calibration. RAAC specializes in capturing fine-grained attribute-region interaction relations, with its output attention masks serving as supervisory signals to iteratively optimize the coarse attention of SGGA. Extensive experiments on three benchmark datasets verify the effectiveness of the proposed method. Code is available at github.com/zjrao/HPAN."
    },
    {
      "paperId": "cdb6f8f8c24c23291fb724e1518454c5b0a7a60e",
      "externalIds": {
        "DOI": "10.1145/3746027.3755431",
        "CorpusId": 282395333
      },
      "corpusId": 282395333,
      "title": "Radar-Mamba: 4D Millimeter-Wave Point Cloud Enhancement via State Space Models",
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746027.3755431?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746027.3755431, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2366148763",
          "name": "Hong Gao"
        },
        {
          "authorId": "2365143957",
          "name": "Xiangkai Xu"
        },
        {
          "authorId": "2392602485",
          "name": "Tianqi Zhu"
        },
        {
          "authorId": "2358189468",
          "name": "Xiugang Dong"
        },
        {
          "authorId": "2356242232",
          "name": "Yiming Bao"
        },
        {
          "authorId": "2244265504",
          "name": "Min-Ling Zhang"
        }
      ],
      "abstract": "The 4D millimeter wave radar has gained increasing attention in autonomous driving due to its robustness against environmental interference compared to other perception devices such as cameras or LiDAR. However, the practical deployment of radar remains challenging due to the noise and high sparsity of radar point clouds, as well as the resource limitations of edge computing. To address these issues, we propose Radar-Mamba, a lightweight and efficient radar enhancement approach for boosting radar perception. The proposed approach contains three main components: cross-modal alignment, radar enhancement architecture based on the Mamba model, and Doppler feature fusion. Specifically, the point clouds of radar and LiDAR are first refined and aligned to build more dense and rich occupancies. The aligned 4D radar data is then enhanced by capturing both local and global spatial-temporal features, while integrating radar-specific velocity and elevation information for further denoising. Experiments on two open-source datasets demonstrate that our method achieves state-of-the-art performance and generates high-quality 4D point clouds with a density comparable to LiDAR while maintaining a low parameter count friendly for practical deployment."
    },
    {
      "paperId": "7513501b227f7a4e80cb4b69f8b31d22aed7d8e3",
      "externalIds": {
        "ArXiv": "2510.22951",
        "DBLP": "journals/corr/abs-2510-22951",
        "DOI": "10.48550/arXiv.2510.22951",
        "CorpusId": 282390196
      },
      "corpusId": 282390196,
      "title": "Hankel Singular Value Regularization for Highly Compressible State Space Models",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.22951, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "38981057",
          "name": "Paul Schwerdtner"
        },
        {
          "authorId": "2256992538",
          "name": "Jules Berman"
        },
        {
          "authorId": "2256992183",
          "name": "Benjamin Peherstorfer"
        }
      ],
      "abstract": "Deep neural networks using state space models as layers are well suited for long-range sequence tasks but can be challenging to compress after training. We use that regularizing the sum of Hankel singular values of state space models leads to a fast decay of these singular values and thus to compressible models. To make the proposed Hankel singular value regularization scalable, we develop an algorithm to efficiently compute the Hankel singular values during training iterations by exploiting the specific block-diagonal structure of the system matrices that we use in our state space model parametrization. Experiments on Long Range Arena benchmarks demonstrate that the regularized state space layers are up to 10$\\times$ more compressible than standard state space layers while maintaining high accuracy."
    },
    {
      "paperId": "5e2d644de989f373e821e91c55db8ee0714118d0",
      "externalIds": {
        "ArXiv": "2510.22942",
        "DBLP": "journals/corr/abs-2510-22942",
        "DOI": "10.48550/arXiv.2510.22942",
        "CorpusId": 282390292
      },
      "corpusId": 282390292,
      "title": "GTR-Mamba: Geometry-to-Tangent Routing for Hyperbolic POI Recommendation",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.22942, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2381106201",
          "name": "Zhuoxuan Li"
        },
        {
          "authorId": "2387871313",
          "name": "Jieyuan Pei"
        },
        {
          "authorId": "2215415965",
          "name": "Tangwei Ye"
        },
        {
          "authorId": "2327923557",
          "name": "Zhongyuan Lai"
        },
        {
          "authorId": "2388531588",
          "name": "Zihan Liu"
        },
        {
          "authorId": "2387969823",
          "name": "Fengyuan Xu"
        },
        {
          "authorId": "2389081614",
          "name": "Qi Zhang"
        },
        {
          "authorId": "2387960579",
          "name": "Liang Hu"
        }
      ],
      "abstract": "Next Point-of-Interest (POI) recommendation is a critical task in modern Location-Based Social Networks (LBSNs), aiming to model the complex decision-making process of human mobility to provide personalized recommendations for a user's next check-in location. Existing POI recommendation models, predominantly based on Graph Neural Networks and sequential models, have been extensively studied. However, these models face a fundamental limitation: they struggle to simultaneously capture the inherent hierarchical structure of spatial choices and the dynamics and irregular shifts of user-specific temporal contexts. To overcome this limitation, we propose GTR-Mamba, a novel framework for cross-manifold conditioning and routing. GTR-Mamba leverages the distinct advantages of different mathematical spaces for different tasks: it models the static, tree-like preference hierarchies in hyperbolic geometry, while routing the dynamic sequence updates to a novel Mamba layer in the computationally stable and efficient Euclidean tangent space. This process is coordinated by a cross-manifold channel that fuses spatio-temporal information to explicitly steer the State Space Model (SSM), enabling flexible adaptation to contextual changes. Extensive experiments on three real-world datasets demonstrate that GTR-Mamba consistently outperforms state-of-the-art baseline models in next POI recommendation."
    },
    {
      "paperId": "b0bbeb29d009aab1182525bdb7e64cbac2767f87",
      "externalIds": {
        "ArXiv": "2510.23043",
        "DBLP": "journals/corr/abs-2510-23043",
        "DOI": "10.48550/arXiv.2510.23043",
        "CorpusId": 282390001
      },
      "corpusId": 282390001,
      "title": "HieraMamba: Video Temporal Grounding via Hierarchical Anchor-Mamba Pooling",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.23043, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2333529956",
          "name": "Joungbin An"
        },
        {
          "authorId": "2285291382",
          "name": "Kristen Grauman"
        }
      ],
      "abstract": "Video temporal grounding, the task of localizing the start and end times of a natural language query in untrimmed video, requires capturing both global context and fine-grained temporal detail. This challenge is particularly pronounced in long videos, where existing methods often compromise temporal fidelity by over-downsampling or relying on fixed windows. We present HieraMamba, a hierarchical architecture that preserves temporal structure and semantic richness across scales. At its core are Anchor-MambaPooling (AMP) blocks, which utilize Mamba's selective scanning to produce compact anchor tokens that summarize video content at multiple granularities. Two complementary objectives, anchor-conditioned and segment-pooled contrastive losses, encourage anchors to retain local detail while remaining globally discriminative. HieraMamba sets a new state-of-the-art on Ego4D-NLQ, MAD, and TACoS, demonstrating precise, temporally faithful localization in long, untrimmed videos."
    },
    {
      "paperId": "a156c51adce1dd22eb700b9a3d8c948211ca9700",
      "externalIds": {
        "DOI": "10.1145/3746027.3754817",
        "CorpusId": 282393957
      },
      "corpusId": 282393957,
      "title": "DDSE: A Decoupled Dual-Stream Enhanced Framework for Multimodal Sentiment Analysis with Text-Centric SSM",
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746027.3754817?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746027.3754817, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2391047759",
          "name": "Shenjie Jiang"
        },
        {
          "authorId": "2387960379",
          "name": "Zhuoyu Wang"
        },
        {
          "authorId": "2261939375",
          "name": "Xuecheng Wu"
        },
        {
          "authorId": "2283746392",
          "name": "Hongru Ji"
        },
        {
          "authorId": "2342445681",
          "name": "Mingxin Li"
        },
        {
          "authorId": "48568389",
          "name": "Xianghua Li"
        },
        {
          "authorId": "2319196620",
          "name": "Chao Gao"
        }
      ],
      "abstract": "Multimodal Sentiment Analysis (MSA) aims to identify sentiment polarity and intensity in media. Current methods typically employ a two-stage pipeline: extracting features from each modality, then predicting sentiment based on fused representations. However, most fusion strategies align features from different modalities in a single step, leading to conflicts during cross-modal interactions and hindering the modeling of hierarchical sentiment dependencies. Additionally, existing methods often overlook the dominant role of textual modality in high level latent fusion space, causing explicit linguistic sentiment cues to be obscured by redundant information. To address these issues, DDSE (Decoupled Dual-Stream Enhanced framework) is proposed in this work, which decouples features into public and private representations for improved feature enhancement and cross-modal interaction. The proposed TC-Mamba module enables progressive cross-modal interactions within shared state transition matrices under a text-guided fusion paradigm, effectively preserving sentiment cues and minimizing redundancy. Additionally, DDSE adopts a multi-task learning strategy to further enhance overall performance. Extensive experiments on the MOSI and MOSEI datasets demonstrate that DDSE achieves state-of-the-art results, with Acc-5 improvements of 3.06% and 0.1%, respectively, underscoring its effectiveness in MSA. Ablation studies confirm the critical contributions of each component within the framework. Code is available at https://anonymous.4open.science/r/DDSE-76D6."
    },
    {
      "paperId": "a55db6bb521fddad52dd232a618f55e00ff37817",
      "externalIds": {
        "ArXiv": "2512.04112",
        "DOI": "10.1145/3746027.3758167",
        "CorpusId": 282394165
      },
      "corpusId": 282394165,
      "title": "MindFuse: Towards GenAI Explainability in Marketing Strategy Co-Creation",
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.04112, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2229491",
          "name": "Aleksandr Farseev"
        },
        {
          "authorId": "2209212166",
          "name": "Marlo Ongpin"
        },
        {
          "authorId": "2109122521",
          "name": "Qi Yang"
        },
        {
          "authorId": "2209214260",
          "name": "Ilia Gossoudarev"
        },
        {
          "authorId": "2285616237",
          "name": "Yu-Yi Chu-Farseeva"
        },
        {
          "authorId": "2311886360",
          "name": "Sergey Nikolenko"
        }
      ],
      "abstract": "The future of digital marketing lies in the convergence of human creativity and generative AI, where insight, strategy, and storytelling are co-authored by intelligent systems. We present MindFuse, a brave new explainable generative AI framework designed to act as a strategic partner in the marketing process. Unlike conventional LLM applications that stop at content generation, MindFuse fuses CTR-based content AI-guided co-creation with large language models to extract, interpret, and iterate on communication narratives grounded in real advertising data. MindFuse operates across the full marketing lifecycle: from distilling content pillars and customer personas from competitor campaigns to recommending in-flight optimizations based on live performance telemetry. It uses attention-based explainability to diagnose ad effectiveness and guide content iteration, while aligning messaging with strategic goals through dynamic narrative construction and storytelling. We introduce a new paradigm in GenAI for marketing, where LLMs not only generate content but reason through it, adapt campaigns in real time, and learn from audience engagement patterns. Our results, validated in agency deployments, demonstrate up to 12\u00d7 efficiency gains, setting the stage for future integration with empirical audience data (e.g., GWI, Nielsen) and full-funnel attribution modeling. MindFuse redefines AI not just as a tool, but as a collaborative agent in the creative and strategic fabric of modern marketing. In the end of the paper, we also provide a forward-looking forecast on how platforms like MindFuse and Google DeepMind's ACAI are likely to shape the future of marketing agencies. These systems will not only reset client expectations toward greater transparency, speed, and personalization, but will also redefine the skillsets demanded from the new generation of marketers. As hybrid agencies emerge-blending creative storytelling with data science and AI engineering-the competitive landscape will increasingly hinge on talent. In this new environment, professionals will be expected to pair human imagination with technical fluency, while agencies will need to reinvent themselves as AI facilitators and curators of brand authenticity amidst the ongoing talent war led by platforms such as Meta."
    },
    {
      "paperId": "c34a43adf0d769052f309a94eb54af4a315a09e2",
      "externalIds": {
        "DOI": "10.1145/3746027.3755373",
        "CorpusId": 282394416
      },
      "corpusId": 282394416,
      "title": "Geo-CF2Net: Geometry-Prior Cross-Frequency Interactive Fusion Network for 3D Human Action Recognition",
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746027.3755373?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746027.3755373, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2321492221",
          "name": "Zhaoyu Chen"
        },
        {
          "authorId": "2182284005",
          "name": "Qian Huang"
        },
        {
          "authorId": "2155446230",
          "name": "Xing Li"
        },
        {
          "authorId": "2277246238",
          "name": "Yunfei Zhang"
        },
        {
          "authorId": "2321715233",
          "name": "Shihao Han"
        },
        {
          "authorId": "2390567833",
          "name": "Ge Gao"
        },
        {
          "authorId": "2325484102",
          "name": "Yirui Wu"
        },
        {
          "authorId": "2256600455",
          "name": "Xin Li"
        },
        {
          "authorId": "2350532805",
          "name": "Ziyang Yin"
        }
      ],
      "abstract": "Dynamic point cloud-based human action recognition has garnered increasing attention due to its inherent advantages in privacy preservation and structural completeness. Current methods typically rely on nested point spatio-temporal convolutions to understand motion semantics in a bottom-up manner, which is intractable for capturing high-fidelity human dynamics disentangled from spatio-temporal interference. Motivated by this, designing a practical spatio-temporal factorization backbone is essential. However, the repeated coarsening of aggregated features along the spatial dimension often leads to the degradation of intrinsic geometric texture relations within point cloud data. Moreover, discretizing continuous visual data into isolated temporal hyperpoints significantly diminishes temporal continuity, resulting in the fragmentation of human action. To circumvent above limitations, we propose a novel Geometry-Prior Cross-Frequency Interactive Fusion Network (Geo-CF2Net). Specifically, we investigate a Spatial-Geometry Pose Prior (SGPP) module, which compensates for pose information loss during spatial downsampling by explicitly modeling geometric constraints among neighboring points. In addition, we elaborate on a Temporal Motion Unit Interactive Coordination (TMIC) module to track the interactive composite semantics of low-frequency steady-state venations and high-frequency transient-state details within a high-dimensional pose evolution flow. Extensive experiments on three public benchmarks substantiate the superiority of Geo-CF2Net over state-of-the-art methods."
    },
    {
      "paperId": "ab6b66d9f817af8098a534cd8cc15ff92da05494",
      "externalIds": {
        "DOI": "10.1145/3746027.3755721",
        "CorpusId": 282394485
      },
      "corpusId": 282394485,
      "title": "High-Performance Discriminative Tracking with Spatio-Temporal Template Fusion",
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746027.3755721?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746027.3755721, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2367091930",
          "name": "Xuedong He"
        },
        {
          "authorId": "46485362",
          "name": "Huiying Xu"
        },
        {
          "authorId": "2237741995",
          "name": "Xinzhong Zhu"
        },
        {
          "authorId": "2267543493",
          "name": "Hongbo Li"
        }
      ],
      "abstract": "The current one-stream tracking framework has received far-reaching attention for its significant improvement in tracking performance, yet it is essentially an extension of Siamese trackers. However, the one-stream framework of discriminative trackers has not been effectively exploited, still using separate feature extraction and model prediction. Therefore, this article aims to implement a one-stream learning strategy for feature extraction and model prediction under the discriminative tracking framework. To this end, we have leveraged the prevailing Vision Transformer and Vision Mamba backbones to achieve our motivation. Moreover, we innovatively combine templates with discriminative tracking methods to enhance the ability of target-aware feature learning, and further propose the attention fusion module to implement spatiotemporal template fusion, which can enhance the adaptability of the tracking model to dynamic changes of targets. The experiments on multiple popular tracking benchmarks have demonstrated that our proposed tracking architecture has superior tracking performance. Concisely, our tracker obtains an AUC of 73.3% on LaSOT dataset, and an AO of 78.2% on GOT-10k dataset. The code, raw results, and trained models are available at https://github.com/hexdjx/VisTrack."
    },
    {
      "paperId": "eef2930850059df4cc28d54deb2ee61880af497b",
      "externalIds": {
        "DOI": "10.1145/3746027.3755630",
        "CorpusId": 282394437
      },
      "corpusId": 282394437,
      "title": "Efficient Semantic Codec for Real-time Vibrotactile Transmission",
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746027.3755630?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746027.3755630, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2388079905",
          "name": "Runjie Wang"
        },
        {
          "authorId": "2292392466",
          "name": "Kemi Chen"
        },
        {
          "authorId": "2388027425",
          "name": "Shuijie Li"
        },
        {
          "authorId": "2288324312",
          "name": "Mingkai Chen"
        },
        {
          "authorId": "2153707464",
          "name": "Tiesong Zhao"
        }
      ],
      "abstract": "Nowadays, haptic data has gained a fast-growing volume with enormous interaction points during human-computer interaction and embodied AI. In the near future, the massive haptic signals -encompassing both kinesthetic and vibrotactile signals- will place significant demands on both communication and computing resources. To address this challenge, we propose the first task-oriented sematic codec of low-delay vibrotactile transmission, namely, vibrotactile semantic codec (VTSC). Specifically, we design a perception-based vibrotactile semantic extraction mechanism (PSEM) that considers the high and low thresholds of vibrotactile perception in effective semantic coding while adhering to the low delay constraint. Inspired by this principle, we then propose a vibrotactile semantic encoder (VSE) with local and global semantic extractors, which can efficiently extract and preserve semantic features within the short frame context. Besides, we present a semantic distribution loss function to enhance the learning of meaningful representations. Comprehensive experiments demonstrate the superiority of our VTSC, achieving significantly higher task accuracy than the state-of-the-art vibrotactile codecs at the same compression ratio (CR), e.g. 60% improvement when CR=256. When compared to transferred audio-visual sematic codecs, our VTSC also shows promising improvements, validating the effectiveness our approach."
    },
    {
      "paperId": "2d849d51d7df36c9365cf3fe2b624fdf9eaaae87",
      "externalIds": {
        "DOI": "10.1145/3746027.3755073",
        "CorpusId": 282394591
      },
      "corpusId": 282394591,
      "title": "Deep Multi-Level Contrastive Clustering for Multi-Modal Remote Sensing Images",
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746027.3755073?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746027.3755073, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2388103914",
          "name": "Weiqi Liu"
        },
        {
          "authorId": "2118068231",
          "name": "Yongshan Zhang"
        },
        {
          "authorId": "2108020959",
          "name": "Xinxin Wang"
        },
        {
          "authorId": "2274543587",
          "name": "Lefei Zhang"
        }
      ],
      "abstract": "Multi-modal remote sensing image clustering aims to group similar pixels into the same cluster and separate dissimilar ones by leveraging the consistency and complementary information across multiple modalities, without relying on label guidance. Most existing deep learning-based methods address this task through a two-stage pipeline of feature learning followed by clustering, or adopt simple instance-level contrastive learning frameworks. In this paper, we propose an end-to-end deep multi-level contrastive clustering (DMLCC) model for multi-modal remote sensing images. The proposed DMLCC consists of three key components. Specifically, modality-specific vision encoders are initially employed to extract preliminary feature representations tailored to the characteristics of each modality. Spatial-spectral cross-modal fusion is then performed by integrating dedicated spatial and spectral feature extractors alongside a cross-modal fusion block to effectively capture and align complementary information across modalities. Finally, multi-level contrastive learning is applied to enhance feature discriminability through instance-level contrastive learning, while simultaneously promoting cluster separability via cluster-level contrastive learning. The network is trained in an end-to-end manner by integrating the three components to directly yield the clustering results. Extensive experiments on three datasets demonstrate that the proposed DMLCC outperforms state-of-the-art methods. Our code is publicly available at https://github.com/ZhangYongshan/DMLCC."
    },
    {
      "paperId": "f4584a9bcca52564d979667aa5f1fcb9127b5c07",
      "externalIds": {
        "DOI": "10.1145/3746027.3755103",
        "CorpusId": 282395122
      },
      "corpusId": 282395122,
      "title": "Dynamic Beauty is Easy to Find: A Large-Scale Composition-Aware Dataset and an End-to-End Framework for Video Reframing",
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746027.3755103?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746027.3755103, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2388838188",
          "name": "Sitian Gu"
        },
        {
          "authorId": "2047424526",
          "name": "Zhiyu Pan"
        },
        {
          "authorId": "2047434854",
          "name": "Chaoyi Hong"
        },
        {
          "authorId": "1785399445",
          "name": "Chengxin Liu"
        },
        {
          "authorId": "2209279744",
          "name": "Zhiguo Cao"
        }
      ],
      "abstract": "Video reframing, which converts landscape-oriented (LO) to portrait-oriented (PO) video for some PO devices such as smartphones and tablets, faces challenges. Existing approaches mainly follow a multi-step pipeline to preserve video content that ignore composition quality due to lack of large-scale datasets. To address these challenges, we propose a fully automated composition-aware dataset using vision-language models and image composition assessment models, pairing LO videos with high-quality PO versions. We then propose an end-to-end model with an attention-aware backbone and a time-aware consistency module. Experiments show our approach outperforms others in efficiency and effectiveness, proving that composition awareness and end-to-end modeling are critical for video reframing."
    },
    {
      "paperId": "e8a22af496745e85c34ff551de9cb2a9f591a6ff",
      "externalIds": {
        "DOI": "10.1145/3746027.3754550",
        "CorpusId": 282394946
      },
      "corpusId": 282394946,
      "title": "Contextually-Guided State Space Fusion for Misaligned Multi-Spectral Object Detection",
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746027.3754550?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746027.3754550, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2390482143",
          "name": "Guyue Jin"
        },
        {
          "authorId": "2349568235",
          "name": "Tianming Zhao"
        },
        {
          "authorId": "2388597967",
          "name": "Jiacan Yan"
        },
        {
          "authorId": "2348968928",
          "name": "Tian Tian"
        }
      ],
      "abstract": "Multi-modal feature fusion under conditions of image misalignment remains a significant challenge in multispectral object detection. Existing approaches predominantly rely on cross-attention mechanisms; however, when local features are sparse, inadequate feature capture hinders accurate alignment and results in distorted fusion outcomes. To address this problem, we propose a novel multispectral fusion detection network, CSSFDet, which leverages the intrinsic correlations among image regions in visual recognition to dynamically enhance local features via global semantic constraints during the fusion process. Specifically, we introduce a Contextual Region Feature Fusion Module (CRFM) that regulates the fusion process through a selective state-space formulation, adaptively incorporating surrounding context to compensate for local feature degradation caused by misalignment. Moreover, we design a Complementary Enhancement Module (CoE) to ensure both distinctiveness and completeness of modality-specific features. Extensive experiments demonstrate that the proposed method achieves state-of-the-art performance across multiple datasets, attaining 84.1% mAP50 on the DroneVehicle dataset-a 20% improvement over the baseline. It also shows strong performance on the misaligned CVC-14 dataset, and sensitivity analysis on data shifts further underscores its robustness to misalignment."
    },
    {
      "paperId": "16118075ec9a0b9e1659cb0bbb30545f4bbf8780",
      "externalIds": {
        "DOI": "10.1145/3746027.3755677",
        "CorpusId": 282394968
      },
      "corpusId": 282394968,
      "title": "EDMG: Towards Efficient Long Dance Motion Generation with Fundamental Movements from Dance Genres",
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746027.3755677?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746027.3755677, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2388116432",
          "name": "Jinming Zhang"
        },
        {
          "authorId": "2268718291",
          "name": "Yunlian Sun"
        },
        {
          "authorId": "2334890630",
          "name": "Hongwen Zhang"
        },
        {
          "authorId": "2192672187",
          "name": "Jinhui Tang"
        }
      ],
      "abstract": "Dance is an important art form in human culture, but creating new dances can be both challenging and time-consuming. In this paper, we propose a novel dance choreography framework, EDMG, designed to efficiently generate creative and long-lasting dance sequences conditioning on music and dance descriptions. In the first stage, we propose a flexible dance diffusion method, combined with dance genre description and descriptions of fundamental movements to generate the dance sequences. To achieve high computational efficiency and inference speed, EDMG designs a lightweight denoising module by using selective parallel scanning algorithm from Mamba2. This Parallel Mamba Denoiser reduces significantly the number of parameters and accelerates remarkably both the learning and inference processes. In the second stage, by designing a smoothing module with a long receptive field, we mitigate joint error accumulation that causes jittering movements and foot sliding, thereby enhancing the fluency and visual appeal of the dance movements. Furthermore, we extend the AIST++ dataset by adding detailed descriptions of dance genres and fundamental movements, using the Large Language Model (LLM). These descriptions further improve the choreography generation. EDMG is validated through extensive experiments, demonstrating that our method can both effectively and efficiently generate long-term dances suitable for various dance genres. Project URL: https://github.com/neymar277/EDMG."
    },
    {
      "paperId": "c7ae36e7fa99d69bdfb72e91868f4387750291d3",
      "externalIds": {
        "DOI": "10.1145/3746027.3754572",
        "CorpusId": 282395329
      },
      "corpusId": 282395329,
      "title": "Spatial-Frequency Mamba Collaborative Learning Network for Infrared Small Target Detection",
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746027.3754572?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746027.3754572, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2301400990",
          "name": "Yongji Li"
        },
        {
          "authorId": "2301677653",
          "name": "Luping Wang"
        }
      ],
      "abstract": "Infrared (IR) search and track systems are widely applied in aerospace and defense fields. Infrared small target detection (IRSTD) in heavy clouds and chaotic terrestrial environments remains a challenging task. The semantic features of IR small targets are highly prone to vanishing with the addition of network layers. Transformer with quadratic computational complexity struggles for local feature refinement. To tackle this issue, we introduce a Mamba-driven approach dubbed Spatial-Frequency Mamba Collaborative Learning Network (SMCLNet). Specifically, the perspective transformation structures heterogeneous backgrounds. The reconstructed data couples Mamba's flattened multidirectional scanning mechanism. Given that small targets possess sparse and high-frequency properties, spatial Mamba and frequency Mamba collaboratively enrich the semantic features of small targets. The Texture Enhancement Module (TEM) effectively fuses spatial and frequency features to enhance the contrast information of small targets. To refine the features, the Fine-Grained Reinforcement Module (FRM) integrates multiple gradient operators to inscribe the intact small target profile. Both qualitative and quantitative experiments demonstrate that our proposed SMCLNet outperforms 14 recent benchmark algorithms on multiple public datasets."
    },
    {
      "paperId": "84a035d8cc9c8994309759dbbf9d1ed88fef977c",
      "externalIds": {
        "DOI": "10.1145/3746027.3754873",
        "CorpusId": 282395546
      },
      "corpusId": 282395546,
      "title": "UMSD:High Realism Motion Style Transfer via Unified Mamba-based Diffusion",
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746027.3754873?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746027.3754873, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2202592845",
          "name": "Ziyun Qian"
        },
        {
          "authorId": "2300151974",
          "name": "Zeyu Xiao"
        },
        {
          "authorId": "2240314827",
          "name": "Xingliang Jin"
        },
        {
          "authorId": "2295484495",
          "name": "Dingkang Yang"
        },
        {
          "authorId": "2188978724",
          "name": "Mingcheng Li"
        },
        {
          "authorId": "2300171891",
          "name": "Zhenyi Wu"
        },
        {
          "authorId": "2218981837",
          "name": "Dongliang Kou"
        },
        {
          "authorId": "2133273745",
          "name": "Peng Zhai"
        },
        {
          "authorId": "2274792031",
          "name": "Lihua Zhang"
        }
      ],
      "abstract": "Motion style transfer is a significant research area in computer vision, enabling the rapid switching of stylistic variations for the same motion in virtual digital humans. This dramatically enhances the richness and realism of motions, making it widely applicable in multimedia contexts such as film, gaming, and the Metaverse. However, most existing methods employ a two-stream structure, which often overlooks the intrinsic relationships between content and style motions, resulting in information loss and misalignment. Additionally, these methods struggle to capture temporal dependencies in long-range motion sequences, resulting in less natural outputs. To address these limitations, we propose a Unified Motion Style Diffusion (UMSD) Framework that simultaneously extracts features from content and style motions, achieving comprehensive information interaction. We also introduce the Motion Style Mamba (MSM) denoiser, which, for the first time in motion style transfer, leverages Mamba's powerful sequence modelling capability to produce more temporally coherent stylized motion sequences. Furthermore, we design a diffusion-based content consistency loss and a style consistency loss to ensure that the framework preserves content motion while effectively learning style motion features. Extensive experiments demonstrate that our approach outperforms State-Of-The-Art (SOTA) methods qualitatively and quantitatively, achieving more realistic and coherent motion style transfer."
    },
    {
      "paperId": "fcaad7d5a0b0bebe929833f093eefbe768f789d4",
      "externalIds": {
        "DOI": "10.1145/3746027.3754783",
        "CorpusId": 282395969
      },
      "corpusId": 282395969,
      "title": "SAMVSR: Leveraging Semantic Priors to Zone-Focused Mamba for Video Snow Removal",
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "year": 2025,
      "citationCount": 6,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746027.3754783?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746027.3754783, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2355911341",
          "name": "Hongtao Wu"
        },
        {
          "authorId": "2348304037",
          "name": "Yifeng Wu"
        },
        {
          "authorId": "2264412037",
          "name": "Jiaxuan Jiang"
        },
        {
          "authorId": "2390159148",
          "name": "Chengyu Wu"
        },
        {
          "authorId": "2243434085",
          "name": "Hong Wang"
        },
        {
          "authorId": "2179980880",
          "name": "Yefeng Zheng"
        }
      ],
      "abstract": "The outdoor vision systems are frequently degraded by snow particles, which obscure scene content and impair the performance of downstream vision tasks. While previous methods rely on physical priors, their performance often deteriorates under real-world conditions. Recently, semantic priors have proven effective in guiding image restoration, especially with the advent of the Segment Anything Model (SAM), which provides robust segmentation masks under adverse weather. However, leveraging SAM in video restoration remains underexplored due to the temporal inconsistency of inter-frame segmentation. In this work, we carefully construct the first framework to incorporate SAM-derived semantic priors into video snow removal, called SAMVSR. Specifically, to address temporal SAM label misalignment, we introduce an Entropy-wise Zone Propagation technique, which selects a reliable reference mask and semantically aligns instances across different frames via an entropy-guided label matching mechanism. Based on the aligned SAM semantic priors, we propose a Zone-Focused Mamba module, a novel Mamba-based architecture that restricts its scanning scope to semantically coherent zones, effectively mitigating irrelevant interactions and enhancing temporal-spatial consistency. Extensive experiments on both synthetic and real-world benchmarks finely validate the superiority of our proposed SAMVSR over existing state-of-the-art video desnowing techniques."
    },
    {
      "paperId": "2e0f1caf92de322b1cb2b7023e9d934347f43cb8",
      "externalIds": {
        "DOI": "10.1145/3746027.3755406",
        "CorpusId": 282395659
      },
      "corpusId": 282395659,
      "title": "Cycle-Consistent Mamba-Based Registration-Fusion Joint Network for Unregistered Hyperspectral Image Super-Resolution",
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746027.3755406?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746027.3755406, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2392822014",
          "name": "Quangui He"
        },
        {
          "authorId": "31123515",
          "name": "Jiahui Qu"
        },
        {
          "authorId": "2262111227",
          "name": "Wenqian Dong"
        },
        {
          "authorId": "2112870852",
          "name": "Song Xiao"
        },
        {
          "authorId": "2391126169",
          "name": "Qinghao Gao"
        }
      ],
      "abstract": "Hyperspectral image super-resolution (HSI-SR) has attracted significant attention in high-resolution HSI reconstruction. Most existing fusion-based HSI-SR methods assume that multi-source images are perfectly registered, which is impractical due to varying imaging conditions. Furthermore, methods that consider the registration issue typically treat registration and fusion as two separate steps, resulting in the accumulation of registration errors during the fusion process. To address these issues, we propose a Cycle-Consistent Mamba-Based Registration-Fusion Joint Network (CCM-RFJN), which step-wise optimizes the Registration-Fusion Unified Module (RFUM) through multiple cyclic iterative SR processes. Specifically, in each SR iteration, we map the super-resolved HR-HSI obtained through the RFUM back to the unregistered LR-HSI for the next SR, with cycle-consistency constraints imposed on both LR-HSI and HR-HSI to adaptively optimize the RFUM based on the reciprocal training strategy. In RFUM, we integrate the proposed Interactive Mamba Registration Module (IMR) and Dual-attention Mamba Fusion Module (DAMF), thereby achieving registration-fusion joint optimization. Specifically, IMR is developed to incorporate the interactive Mamba encoder into a pyramid architecture to facilitate multi-level information interactions, generating the deformation field to correct non-rigid misalignments. DAMF is designed to utilize the dual-attention Mamba mechanism to highlight and aggregate key features, thereby enhancing fusion performance. Experiments on three public datasets demonstrate that CCM-RFJN achieves the state-of-the-art performance. The code is available at https://github.com/Jiahuiqu/CCM-RFJN."
    },
    {
      "paperId": "c8ce40b796612d6e4de03ced89b1a74946735b47",
      "externalIds": {
        "DOI": "10.1145/3746027.3754876",
        "CorpusId": 282395804
      },
      "corpusId": 282395804,
      "title": "EchoVim: Making Vision Mamba Docile for Echocardiography Video Segmentation via Dynamic Interaction and Semantic Token-attentive Refinement",
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746027.3754876?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746027.3754876, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2387861818",
          "name": "Jingxing Guo"
        },
        {
          "authorId": "2372422008",
          "name": "Guilian Chen"
        },
        {
          "authorId": "2387815752",
          "name": "Yimu Sun"
        },
        {
          "authorId": "2261975948",
          "name": "Huisi Wu"
        },
        {
          "authorId": "2118629167",
          "name": "Jing Qin"
        }
      ],
      "abstract": "Automatic echocardiography video segmentation is a powerful tool for improving the accuracy of cardiovascular function assessment. However, it remains a challenging task owing to (1) extensive speckle noise and blurred boundaries, (2) dramatic shape variations of targeting structures across frames, and (3) limited labeled data due to the high cost of annotation. In this paper, we present a novel semi-supervised segmentation model based on Vision Mamba (Vim) to comprehensively tackle these challenges; we call it EchoVim. Our framework introduces three technical innovations: First, a bidirectional inference mechanism (BIM) which can propagate label information bidirectionally from end-diastolic (ED) and end-systolic (ES) frames to generate pseudo-labels, coupled with confidence-aware dynamic updating to progressively refine supervision signals. Second, a dynamic interaction temporal alignment (DITA) module that establishes anatomical correspondence across frames by adaptively enhancing features near temporally stable regions while suppressing motion-irrelevant artifacts, effectively addressing variations in cardiac shape. Third, a semantic token-attentive refinement (STR) module that constructs low-rank semantic tokens to encode cardiac structure priors, utilizing attention-guided nonlinear transformations to disentangle speckle noise from true anatomical patterns. We conduct extensive experiments on two benchmarking echocardiography video datasets: CAMUS and EchoNet-Dynamic, and the results demonstrate that our method outperforms existing state-of-the-art approaches with real-time inference. Codes are available at https://github.com/guojx2255/EchoVim."
    },
    {
      "paperId": "e7ec14229d0ccf436d2a09a58818512d0b60707c",
      "externalIds": {
        "DOI": "10.1145/3746027.3755847",
        "CorpusId": 282396174
      },
      "corpusId": 282396174,
      "title": "BiOMamba: Mamba-based Forward-Then-Backward Temporal Modeling for Online Action Detection and Anticipation",
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746027.3755847?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746027.3755847, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2361665036",
          "name": "Sensen Wang"
        },
        {
          "authorId": "2269548683",
          "name": "Yuehu Liu"
        },
        {
          "authorId": "2145178656",
          "name": "Chi Zhang"
        }
      ],
      "abstract": "Given that action evolution follows temporal progression, recent studies for Online Action Detection (OAD) and Online Action Anticipation (OAA) generally adopt forward temporal modeling to capture dependencies in observable video sequences. However, the strictly sequential nature of forward temporal modeling prevents subsequent frames from being used to enhance the earlier modeling process. In particular, the current frame, the last observable frame in the online video stream, serves as the direct visual cue for ongoing action recognition and the informative context for future action anticipation. As modeling errors accumulate over time, the resulting representations may progressively deviate from the actual semantics. Findings in cognitive neuroscience show that the hippocampus performs backward replay after observation to reinforce and correct the interpretation of previous observations. Inspired by this, we propose to incorporate backward temporal modeling following forward temporal modeling, enabling the model to leverage backward temporal modeling to enhance forward temporal modeling. Based on this idea, we propose a unified model for OAD and OAA, named Bidirectional Online Mamba (BiOMamba). Specifically, to address the excessive length and relevance imbalance in observable sequences, BiOMamba compresses distant long-term memory and preserves recent short-term memory. Then, BiOMamba sequentially model both forward and backward temporal dependencies in the whole memory. Finally, according to the temporal modeling result, BiOMamba generates representations for current and future actions. BiOMamba achieves state-of-the-art performance on THUMOS'14 (OAD: 73.3% mAP, OAA: 59.7% mAP) and TVSeries (OAD: 89.9% mcAP, OAA: 83.7% mcAP)."
    },
    {
      "paperId": "08da5b801db2b8c1010a163a9ea543a413076fd1",
      "externalIds": {
        "DOI": "10.1145/3746027.3754786",
        "CorpusId": 282395917
      },
      "corpusId": 282395917,
      "title": "Anatomical Region-Guided 3D PET/MR Tumor Segmentation via Medical Record",
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746027.3754786?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746027.3754786, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2323112373",
          "name": "Tianming Xu"
        },
        {
          "authorId": "2386948218",
          "name": "Tiantian Guo"
        },
        {
          "authorId": "2321400875",
          "name": "Youdan Feng"
        },
        {
          "authorId": "2389218877",
          "name": "Zihan Chen"
        },
        {
          "authorId": "2321406967",
          "name": "Qiaoyi Xue"
        },
        {
          "authorId": "2309680943",
          "name": "Lingzhi Hu"
        },
        {
          "authorId": "2185383249",
          "name": "Yuhang Shi"
        }
      ],
      "abstract": "Whole-body PET tumor segmentation remains challenging due to limited training data and substantial tumor heterogeneity, which impact the segmentation accuracy and the clinical utility. Tumor distribution information is usually contained in patient medical records and routinely utilized in medical image interpretation, which can also be used to improve the segmentation accuracy. This study introduces a novel 3D PET/MR tumor segmentation framework which integrates tumor distribution priors extracted from medical records. The proposed Tumor Localization Priors(TLP) are generated based on medical records using Large Language Models (LLMs) and organ localization based on MRI. Furthermore, the Region-Aware Fusion Module (RAFM) is designed to fuse TLP and encoded PET information through attention within the Anatomically-Consistent Multitask Model(ACMM). Moreover, Anatomical Consistency Loss(AC Loss) is introduced, integrating tumor localization and its anatomical distribution to enhance segmentation performance. Our method achieves an 9.30% Dice improvement over the baseline nnU-Net v2, with particularly notable 23.06% gains in precision while maintaining high recall (+6.19%). Clinical evaluations confirm superior detection of both primary and metastatic lesions, alongside reduced physiological uptake artifacts."
    },
    {
      "paperId": "aaa84077ff8144a48641451b03b7ad920fdf6349",
      "externalIds": {
        "DOI": "10.1145/3746027.3755521",
        "CorpusId": 282396014
      },
      "corpusId": 282396014,
      "title": "Freq-RWKV: Granularity-Aware Spatial-Frequency Synergy via Dual-Domain Recurrent Scanning for Pan-sharpening",
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746027.3755521?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746027.3755521, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2365985131",
          "name": "Xueheng Li"
        },
        {
          "authorId": "2216499293",
          "name": "Xuanhua He"
        },
        {
          "authorId": "2277795694",
          "name": "Tao Hu"
        },
        {
          "authorId": "2277846007",
          "name": "Jie Zhang"
        },
        {
          "authorId": "2275289990",
          "name": "Man Zhou"
        },
        {
          "authorId": "2269941210",
          "name": "Chengjun Xie"
        },
        {
          "authorId": "2312001381",
          "name": "Yingying Wang"
        },
        {
          "authorId": "2148375658",
          "name": "Bo Huang"
        }
      ],
      "abstract": "Pan-sharpening aims to improve the spatial resolution of low-resolution multispectral (LRMS) image by integrating high-frequency information from corresponding texture-rich panchromatic (PAN) image. While RWKV architecture has demonstrated remarkable global perception with linear computational efficiency in vision tasks, its inherent sequential scanning mechanism critically compromises local spatial coherence, hindering high-frequency reconstruction. To bridge this gap, we tailor Freq-RWKV, the first spatial-frequency adaptive RWKV featuring dual-domain scanning where wavelet-guided path selection dynamically modulates scanning granularity and orientation according to spectral-spatial information density distributions. Building upon this innovation, we architect the hierarchical U-shaped fusion network that strategically coordinates granularity-aware scanning across spatial and frequency domains, enabling adaptive trade-offs between performance and complexity. The U-shaped architecture implements coarse-to-fine enhancement: in the encoding stage, Coarse Structural Interaction (CSI-RWKV) module preserves geometric dependencies via window-constrained recurrent scanning while encoding structural priors into LRMS features; during decoding, the Fine-grained Frequency Interaction (FFI-RWKV) module performs edge-aware refinement through differentiable frequency-adaptive window partitioning, where multi-scale spectral wavelet attention prioritizes high-frequency PAN components extracted via discrete wavelet transform (DWT). This hybrid decomposition strategy maintains spectral integrity through approximation coefficients while detail coefficients regulate frequency-gated fusion thresholds. Extensive experiments on multiple satellite datasets validate the effectiveness of the proposed method."
    },
    {
      "paperId": "899ea5be8b10221f9cf8ee2aa919d930baacbf3f",
      "externalIds": {
        "DOI": "10.1145/3746027.3754914",
        "CorpusId": 282396165
      },
      "corpusId": 282396165,
      "title": "Efficient Trajectory Space-Time Super-Resolution for Fast Live-cell Imaging",
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746027.3754914?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746027.3754914, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2118180466",
          "name": "Ruian He"
        },
        {
          "authorId": "2362233993",
          "name": "Zixian Zhang"
        },
        {
          "authorId": "2146685919",
          "name": "Ri Cheng"
        },
        {
          "authorId": "2274926347",
          "name": "Weimin Tan"
        },
        {
          "authorId": "2303333514",
          "name": "Bo Yan"
        }
      ],
      "abstract": "Live-cell imaging is a powerful tool for studying dynamic subcellular processes by capturing the spatiotemporal organization of the biological microenvironment. However, limitations due to phototoxicity and photobleaching prevent microscopes from achieving high frame rates and high-quality images. Although current deep learning methods can enhance both frame rates and image resolution without compromising cell health, they often overlook the continuity of subcellular trajectories, which leads to discontinuous temporal modeling. It also incurs prohibitive computational costs due to exhaustive correlation computation that hinder real-time applications. To address these issues with high efficiency, we propose Trajectory Space-Time Super-Resolution (T-STSR), a method designed to boost frame rates and resolution in fast subcellular imaging while significantly reducing computational overhead. Our approach incorporates Spatial-Temporal Trajectory Modeling (STTM), which learns a state-space model over spatiotemporal slices to reconstruct particle trajectories at low cost. In addition, our novel Trajectory-Aware Loss randomly subsamples trajectory data during training, promoting continuous trajectory representation and mitigating noise with minimal additional computation. We validated T-STSR on both synthesized and real-world datasets with various particle types and noise conditions, demonstrating that our method achieves superior restoration results while saving 75% inference time compared to the previous SOTA model."
    },
    {
      "paperId": "bb40f61e6205e7fcc54a2070e278a85cf0ce9363",
      "externalIds": {
        "DOI": "10.1145/3746027.3758142",
        "CorpusId": 282396142
      },
      "corpusId": 282396142,
      "title": "A Novel Perspective on Low-Light Image Enhancement: Leveraging Artifact Regularization and Walsh-Hadamard Transform",
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746027.3758142?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746027.3758142, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2377903351",
          "name": "Weilin Wu"
        },
        {
          "authorId": "2389391519",
          "name": "Shifan Yang"
        },
        {
          "authorId": null,
          "name": "Qizhao Lin"
        },
        {
          "authorId": "2118654284",
          "name": "Xinghong Chen"
        },
        {
          "authorId": "2274753939",
          "name": "Kunping Yang"
        },
        {
          "authorId": "40470615",
          "name": "Jing Wang"
        },
        {
          "authorId": "2363602708",
          "name": "Guannan Chen"
        }
      ],
      "abstract": "Low-light image enhancement (LLIE) aims to restore low-light images to normal lighting conditions by improving their illumination and fine details, thereby facilitating efficient execution of downstream visual tasks. Traditional LLIE methods improve image quality but often introduce high-frequency artifacts, which are difficult to eliminate, hindering detail recovery and quality enhancement in LLIE. To solve this problem, we introduce a novel perspective: instead of traditional artifact suppression, sparsification-induced artifacts are repurposed as constructive regularization signals to guide detail recovery. By analyzing the impact of sparsified frequency components and their role in reconstruction artifacts, a detailed mathematical framework is presented. Specifically, we propose a novel loss function SASW-Loss which combining Sparse Artifact Similarity Loss (SAS-Loss) and Walsh-Hadamard Coefficient Loss (WHC-Loss). SAS-Loss mitigates the over-compensation of missing frequencies, helping the network recover structural details, while WHC-Loss optimizes the frequency-domain representation, restoring luminance, suppressing noise, and enhancing both structure and details. Extensive experiments show that our approach outperforms existing state-of-the-art methods, achieving superior performance in structural detail preservation and noise suppression. These results validate the effectiveness of our new perspective, which leverages sparsification artifacts to guide detail recovery, demonstrating significant improvements and robust performance across multiple models, and opening new avenues for future research. The code is available at https://github.com/werringwu/SASW.git."
    },
    {
      "paperId": "d515f6385bf8b887b64ca738089d858e086866cf",
      "externalIds": {
        "DOI": "10.1145/3746027.3754980",
        "CorpusId": 282396503
      },
      "corpusId": 282396503,
      "title": "CrosST: Cross Swin 4D Transformer for Multi-Modal Alzheimer's Detection",
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746027.3754980?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746027.3754980, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2349935712",
          "name": "Hao Wang"
        },
        {
          "authorId": "2363082357",
          "name": "Hanxiao Li"
        },
        {
          "authorId": "2183447948",
          "name": "Li Xu"
        }
      ],
      "abstract": "The task of spatiotemporal dynamic modeling of multi-modal high-dimensional neuroimaging data presents a significant challenge in the field of neuroscience. Recent works often integrate attention mechanisms for hierarchical modeling, but the gradual extraction of spatiotemporal features leads to feature isolation. Moreover, attention-based fusion mechanisms (such as cross attention) tend to focus on learning the self-similarity between different modalities, lacking sufficient exploration of the complementary information across modalities. To address these challenges, we propose the cross swin 4D transformer (CrosST), which can efficiently learn the spatiotemporal patterns of multi-modal high-dimensional neuroimaging data in an end-to-end manner. The unique diffusion cross attention fusion mechanism of CrosST connects features from different modalities through a diffusion strategy during the attention computation, enabling the transfer of differential information between modalities and achieving deep fusion of multi-modal coupled features. Additionally, a voxel interaction strategy is employed to alleviate the computational burden during the fusion process. Furthermore, CrosST utilizes a 4D shifted window technique to effectively combine local and global information, and introduces the innovative 4D-Mamba algorithm to enhance computational efficiency. We validate the model using a large-scale Alzheimer's disease dataset and design a multi-granularity cognitive stage task for evaluation. The results demonstrate the effectiveness of CrosST."
    },
    {
      "paperId": "b769c604b5d55baee8c3d303ae5b8a9157197f3a",
      "externalIds": {
        "DOI": "10.1145/3746027.3755486",
        "CorpusId": 282396461
      },
      "corpusId": 282396461,
      "title": "ESOD: Event-Based Small Object Detection",
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746027.3755486?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746027.3755486, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2380389866",
          "name": "Quanmin Liang"
        },
        {
          "authorId": "2388623243",
          "name": "Jinyi Lu"
        },
        {
          "authorId": "2387988236",
          "name": "Qiang Li"
        },
        {
          "authorId": "2279663389",
          "name": "Shuai Liu"
        },
        {
          "authorId": "47122416",
          "name": "Zhihao Zhao"
        },
        {
          "authorId": "2284605626",
          "name": "Yinzheng Zhao"
        },
        {
          "authorId": "2304009880",
          "name": "Wei Zhang"
        },
        {
          "authorId": "2193594535",
          "name": "Kai Huang"
        },
        {
          "authorId": "2237749443",
          "name": "Yonghong Tian"
        }
      ],
      "abstract": "Event-based object detection plays a crucial role in scenarios involving high-speed motion, extreme lighting conditions, and high-frequency detection. However, existing methods fail to address the challenges posed by small objects, including discriminative feature deficiency, the loss of critical information, and the inherent sparsity of event data. Moreover, the lack of benchmark datasets has significantly hindered progress in this field. To tackle these issues, we propose the Fully Deformable Detection Network (FDDNet), a lightweight framework that dynamically adapts to extract key features. First, we introduce a Long-Term Deformable Temporal Receptive Module (LDTR), which aligns critical features across consecutive event streams and leverages a State Space Model for long-range temporal modeling, enhancing the detection of high-speed small objects. Second, to address the sparsity of event data and the concentration of key features along object edges, we design a Sparse Feature Aggregation Block (SFAB) within the backbone and a coarse-to-fine deformable detection head, enabling hierarchical feature refinement from local to global, and improving the detection quality of sparse targets. Finally, to mitigate the lack of event-based small object datasets, we develop a high-quality, annotation-free data acquisition method and collect a real-world benchmark dataset for validation. Extensive experiments demonstrate that our approach achieves state-of-the-art (SOTA) performance on event-based small object detection tasks, with a mAP of 37.4% (+2.4%) on our benchmark and runs at 88 FPS, showcasing both accuracy and real-time capability. Our code and Supplement are available at https://github.com/Lqm26/ESOD."
    },
    {
      "paperId": "98a193592bacc7301c805223df29267825555735",
      "externalIds": {
        "DOI": "10.1145/3746027.3755658",
        "CorpusId": 282395829
      },
      "corpusId": 282395829,
      "title": "RWKV3D: An RWKV-Based Model with Multiple Training Strategies for Point Cloud Analysis",
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746027.3755658?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746027.3755658, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2359945500",
          "name": "Chenglong Sun"
        },
        {
          "authorId": "2387942017",
          "name": "Shijie Pang"
        },
        {
          "authorId": "2115862616",
          "name": "Yuzheng Wang"
        },
        {
          "authorId": "2283263817",
          "name": "Lizhe Qi"
        }
      ],
      "abstract": "Transformer-based models have achieved dominance in point cloud analysis, yet their quadratic computational complexity remains a fundamental limitation for practical applications. Recently, RWKV has emerged as a promising alternative for sequence modeling due to its linear computational complexity. However, it has yet to be effectively adapted to handle the unordered and sparse nature of point cloud data. In this paper, we propose RWKV3D, an innovative and computational framework tailored for point cloud analysis, which is adaptable to three training strategies: training from scratch, single-modal pre-training, and cross-modal pre-training. First, we replace the MLP layer with an advanced Local Feature Mixer (LFM), which not only enhances fine-grained feature extraction but also reduces the number of parameters. Second, we introduce a Bidirectional Multi-head Shift (BMS) mechanism to expand the receptive field, effectively capturing richer contextual information. Additionally, to enhance high-level feature processing, we strategically incorporate a Multi-head Self-Attention (MSA) block before the first RWKV3D block. Experimental results demonstrate that RWKV3D outperforms Transformer-based and Mamba-based methods while maintaining lower parameter counts and computational costs. Notably, it achieves several state-of-the-art results, including overall accuracies of 95.3% (training from scratch) and 95.9% (cross-modal pre-training) on the ModelNet40 dataset, as well as 95.28% (single-modal pre-training) on the ScanObjectNN (PB_T50_RS) dataset. These results underscore the superior efficacy of the RWKV architecture in 3D vision tasks and highlight its potential for broader multimodal learning scenarios."
    },
    {
      "paperId": "64af978ecdf73607431855a92f19d770a1fd716f",
      "externalIds": {
        "PubMedCentral": "12559288",
        "DOI": "10.1038/s41598-025-21243-8",
        "CorpusId": 282399513,
        "PubMed": "41145615"
      },
      "corpusId": 282399513,
      "title": "UAVs detect hazards with multi-directional Mamba on overhead transmission lines",
      "venue": "Scientific Reports",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBYNCND",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12559288, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2387995914",
          "name": "Cheng Xu"
        },
        {
          "authorId": "2232033125",
          "name": "Chunhou Zheng"
        },
        {
          "authorId": "2390915626",
          "name": "Jun Zhang"
        }
      ],
      "abstract": "Overhead transmission line hazard detection is related to the proper functioning of power communication systems and society. With the development of Unmanned Aerial Vehicles (UAVs) and deep learning, deep-learning-based hazard detection using UAVs has received extensive attention. Currently, research in this direction faces three main challenges: complex background interference, small-scale problems, and efficiency-performance balance. To address the above challenges, this study introduces Mamba based on State Space Models (SSMs) with linear complexity and proposes the UAV-MDMamba model for overhead transmission line hazard detection. We design a Multi-Directional Mamba (MDMamba) block to improve image spatial modeling and complex background suppression, which helps to capture hazardous areas in small-scale situations. Moreover, Patch-Level Inference Enhancement (PLIE) is designed to improve the detection accuracy of small targets in inference. Finally, we collect and label a dataset of overhead transmission line hazard detection for complex scenarios. Extensive experiments demonstrate that UAV-MDMamba performs excellently on the dataset. Therefore, this study improves the efficiency and accuracy of detecting hazards on overhead transmission lines."
    },
    {
      "paperId": "a7a55c6470e4fb4d4a6cc899f22ae871d6477fe0",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-24795",
        "ArXiv": "2510.24795",
        "DOI": "10.48550/arXiv.2510.24795",
        "CorpusId": 282574643
      },
      "corpusId": 282574643,
      "title": "A Survey on Efficient Vision-Language-Action Models",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 4,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.24795, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2389314045",
          "name": "Zhaoshu Yu"
        },
        {
          "authorId": "2390521084",
          "name": "Bo Wang"
        },
        {
          "authorId": "31081539",
          "name": "Pengpeng Zeng"
        },
        {
          "authorId": "2262205505",
          "name": "Haonan Zhang"
        },
        {
          "authorId": "2116921931",
          "name": "Ji Zhang"
        },
        {
          "authorId": "2671321",
          "name": "Lianli Gao"
        },
        {
          "authorId": "2268428960",
          "name": "Jingkuan Song"
        },
        {
          "authorId": "1429806753",
          "name": "N. Sebe"
        },
        {
          "authorId": "2334820484",
          "name": "Heng Tao Shen"
        }
      ],
      "abstract": "Vision-Language-Action models (VLAs) represent a significant frontier in embodied intelligence, aiming to bridge digital knowledge with physical-world interaction. While these models have demonstrated remarkable generalist capabilities, their deployment is severely hampered by the substantial computational and data requirements inherent to their underlying large-scale foundation models. Motivated by the urgent need to address these challenges, this survey presents the first comprehensive review of Efficient Vision-Language-Action models (Efficient VLAs) across the entire data-model-training process. Specifically, we introduce a unified taxonomy to systematically organize the disparate efforts in this domain, categorizing current techniques into three core pillars: (1) Efficient Model Design, focusing on efficient architectures and model compression; (2) Efficient Training, which reduces computational burdens during model learning; and (3) Efficient Data Collection, which addresses the bottlenecks in acquiring and utilizing robotic data. Through a critical review of state-of-the-art methods within this framework, this survey not only establishes a foundational reference for the community but also summarizes representative applications, delineates key challenges, and charts a roadmap for future research. We maintain a continuously updated project page to track our latest developments: https://evla-survey.github.io/"
    },
    {
      "paperId": "b77872cb90c09e72043cd421869c798fd2b46d3f",
      "externalIds": {
        "DOI": "10.1145/3728482.3757390",
        "CorpusId": 282735645
      },
      "corpusId": 282735645,
      "title": "Understanding Global Structure Relation via Reversible Visual State Space Model for Robust Cross-View Geo-Localization",
      "venue": "Proceedings of the 3rd International Workshop on UAVs in Multimedia: Capturing the World from a New Perspective",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3728482.3757390?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3728482.3757390, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2380424537",
          "name": "Peiyuan Ma"
        },
        {
          "authorId": "13686411",
          "name": "Yimin Fu"
        },
        {
          "authorId": "2317108409",
          "name": "Jialin Lyu"
        },
        {
          "authorId": "2144264532",
          "name": "Zhun-ga Liu"
        }
      ],
      "abstract": "Cross-view geo-localization aims to match images captured from different views over the same geographic region. Existing methods typically determine spatial correlations between cross-view images according to the similarity of representations extracted from salient areas. However, such local appearance representations fail to capture the underlying structural relationships among the corresponding regions, which severely undermines the reliability of localization results in complex scenes. To address this problem, we propose a reversible visual state-space model to enhance the understanding of global structural relations inherent in images captured from different views. Specifically, we design a progressive spatial analysis approach, which incrementally integrates geometric dependencies exploited at different levels to improve the understanding of the global structure. Moreover, we introduce a reversible rotational scanning mechanism based on the 2D-selective-scan (SS2D) module to facilitate the exploitation of geometric dependencies between cross-view images. Finally, we adopt the cross-dimension interaction strategy to enrich the informativeness of representations in the common space, thereby reinforcing the discriminability of cross-view representations between different regions. Extensive experiments on the University-1652 and University160k-WX datasets demonstrate that the proposed method achieves state-of-the-art performance while maintaining robustness under complex environmental conditions."
    },
    {
      "paperId": "01b14f71753b8fe4542433aee5b7e5867d42ac7d",
      "externalIds": {
        "DOI": "10.1109/WF-IoT64238.2025.11270783",
        "CorpusId": 283726023
      },
      "corpusId": 283726023,
      "title": "WiFi Signal-based Human Activity Recognition using Bidirectional Mamba Models",
      "venue": "World Forum on Internet of Things",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/WF-IoT64238.2025.11270783?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/WF-IoT64238.2025.11270783, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": null,
          "name": "Haiyang Tan"
        },
        {
          "authorId": "2397644933",
          "name": "Yi Dao"
        },
        {
          "authorId": "2340353917",
          "name": "Haiwei Zhang"
        },
        {
          "authorId": "2293396522",
          "name": "Tingting Guo"
        },
        {
          "authorId": "2282357900",
          "name": "Wenbo Wang"
        }
      ],
      "abstract": "WiFi signals, due to their non-contact, low-cost and privacy-preserving nature, have become an important data source for Human Activity Recognition (HAR). In this paper, we propose a state space model-based deep learning method for HAR using channel state information obtained from multi-antenna WiFi signals. The model is designed by combining depthwise separable convolution and a bidirectional Mamba model, aiming to extract the spatial and temporal features jointly along long sequential WiFi data frame. Experiments show that our proposed model achieves significant improvements in accuracy on three widely-used open datasets of NTU-Fi HAR, NTU-Fi Human-ID and UT-HAR, when compared to reference SOTA algorithms including BiLSTM, ProbSparse Transformer and ViT. Experimental results also show that our proposed model significantly reduces parameter size compared to reference models with comparable latent states while ensuring near-perfect test accuracy."
    },
    {
      "paperId": "fc90ca42f52c0635d35d07be8b299776c1654215",
      "externalIds": {
        "ArXiv": "2510.22752",
        "DBLP": "journals/corr/abs-2510-22752",
        "DOI": "10.48550/arXiv.2510.22752",
        "CorpusId": 282390025
      },
      "corpusId": 282390025,
      "title": "Beyond Semantics: How Temporal Biases Shape Retrieval in Transformer and State-Space Models",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.22752, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2344835310",
          "name": "Anooshka Bajaj"
        },
        {
          "authorId": "2229871471",
          "name": "Deven Mahesh Mistry"
        },
        {
          "authorId": "50841917",
          "name": "Sahaj Singh Maini"
        },
        {
          "authorId": "2344836025",
          "name": "Yash Aggarwal"
        },
        {
          "authorId": "2365267978",
          "name": "Zoran Tiganj"
        }
      ],
      "abstract": "In-context learning is governed by both temporal and semantic relationships, shaping how Large Language Models (LLMs) retrieve contextual information. Analogous to human episodic memory, where the retrieval of specific events is enabled by separating events that happened at different times, this work probes the ability of various pretrained LLMs, including transformer and state-space models, to differentiate and retrieve temporally separated events. Specifically, we prompted models with sequences containing multiple presentations of the same token, which reappears at the sequence end. By fixing the positions of these repeated tokens and permuting all others, we removed semantic confounds and isolated temporal effects on next-token prediction. Across diverse sequences, models consistently placed the highest probabilities on tokens following a repeated token, but with a notable bias for those nearest the beginning or end of the input. An ablation experiment linked this phenomenon in transformers to induction heads. Extending the analysis to unique semantic contexts with partial overlap further demonstrated that memories embedded in the middle of a prompt are retrieved less reliably. Despite architectural differences, state-space and transformer models showed comparable temporal biases. Our findings deepen the understanding of temporal biases in in-context learning and offer an illustration of how these biases can enable temporal separation and episodic retrieval."
    },
    {
      "paperId": "bf645679371a6068f6555bfc0e00cdcc2589be44",
      "externalIds": {
        "DOI": "10.1109/ICCAD66269.2025.11240963",
        "CorpusId": 283145234
      },
      "corpusId": 283145234,
      "title": "PAR-CIM: A Precise/Approximate Reconfigurable Digital CIM Macro with 0.35-4b Fractional Mixed-Bitwidth Quantization",
      "venue": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCAD66269.2025.11240963?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCAD66269.2025.11240963, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2392001656",
          "name": "Han Zhang"
        },
        {
          "authorId": "2391104991",
          "name": "Zhenyu Xue"
        },
        {
          "authorId": "2280487895",
          "name": "Wente Yi"
        },
        {
          "authorId": "2186493626",
          "name": "Tianshuo Bai"
        },
        {
          "authorId": "2264187783",
          "name": "Lehao Tan"
        },
        {
          "authorId": "2363910907",
          "name": "Jingcheng Gu"
        },
        {
          "authorId": "2395849434",
          "name": "Weijie Ding"
        },
        {
          "authorId": "2368644880",
          "name": "Wang Kang"
        },
        {
          "authorId": "2276789127",
          "name": "Biao Pan"
        }
      ],
      "abstract": "Digital computing-in-memory (DCIM) enables efficient deep neural networks (DNNs) acceleration but faces limitations in resource overhead, energy efficiency, and architectural flexibility. Existing approximate or reconfigurable DCIM solutions tackle these issues partially without achieving a holistic balance. To address this, we propose PAR-CIM, a highly energy-efficient reconfigurable CIM macro that integrates precise and approximate paradigm. First, we introduce layer/gate-level approximate computation (LGAC) into the adder tree (AT) of the DCIM core, achieving full operation with only 0.35\u00d7 the area of traditional implementations. Then, we develop a 0.35-4b fractional mixed-bitwidth quantization (FMBQ) algorithm, combining second-order Taylor sensitivity analysis with DoReFa-Net. This is complemented by a high-precision low-approximation (HPLA) mapping scheme to enhance energy efficiency. Additionally, a multi-bit reconfigurable computation mode (MBRM) strategy further improves architectural flexibility and enables the implementation of the proposed design. Under 40nm technology, PAR-CIM achieves 3048 TOPS/W at 1b/1b operations. With FMBQ, ResNet18 and our custom V-FuseMBA trained on CIFAR-10 achieve over 86.61% compression with accuracy loss under 0.74%, reaching classification accuracies of 93.67% and 92.86%, respectively."
    },
    {
      "paperId": "19911471482a3753e729ea3647432d04dc551078",
      "externalIds": {
        "ArXiv": "2510.22724",
        "DBLP": "journals/corr/abs-2510-22724",
        "DOI": "10.48550/arXiv.2510.22724",
        "CorpusId": 282388785
      },
      "corpusId": 282388785,
      "title": "Scalable Neural Decoders for Practical Real-Time Quantum Error Correction",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.22724, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2293780510",
          "name": "Changwon Lee"
        },
        {
          "authorId": "2122345305",
          "name": "Tak Hur"
        },
        {
          "authorId": "2300135804",
          "name": "Daniel K. Park"
        }
      ],
      "abstract": "Real-time, scalable, and accurate decoding is a critical component for realizing a fault-tolerant quantum computer. While Transformer-based neural decoders such as \\textit{AlphaQubit} have demonstrated high accuracy, the computational complexity of their core attention mechanism, which scales as $\\mathcal{O}(d^4)$ with code distance $d$, results in decoding speeds insufficient for practical real-time applications. In this work, we introduce and evaluate a \\textit{Mamba}-based decoder, a state-space model with $\\mathcal{O}(d^2)$ complexity. In memory experiments using Sycamore hardware data, our Mamba decoder matches the performance of its Transformer-based counterpart, providing that its superior efficiency does not come at the cost of performance. Crucially, in simulated real-time scenarios that account for decoder-induced noise, the Mamba decoder significantly outperforms the Transformer, exhibiting a higher error threshold of $0.0104$ compared to $0.0097$. These results demonstrate that Mamba decoders offer a compelling balance between speed and accuracy, making them a promising architecture for scalable, real-time quantum error correction."
    },
    {
      "paperId": "4c877ac8c40f5e6b05fecbb2594b73c47f625120",
      "externalIds": {
        "DOI": "10.36001/phmconf.2025.v17i1.4590",
        "CorpusId": 282468946
      },
      "corpusId": 282468946,
      "title": "Neural Counterfactual Reasoning for Interacting Systems: Bridging Physics-Informed Learning and Reasoning for PHM",
      "venue": "Annual Conference of the PHM Society",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.36001/phmconf.2025.v17i1.4590?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.36001/phmconf.2025.v17i1.4590, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2331329255",
          "name": "Amaury Wei"
        },
        {
          "authorId": "2319401178",
          "name": "Olga Fink"
        }
      ],
      "abstract": "Over the past decade, advances in sensing and information technologies have enabled industries to collect large amounts of data. Yet, decision-making often remains driven by the intuition of domain experts who rely on simplistic analyses and short-term considerations. This frequently leads to suboptimal decisions that fail to account for long-term effects, particularly in complex, interconnected systems. Current data-driven strategies typically focus on immediate objectives, overlooking relational structures and longer-term impacts. There is a growing need for more transparent, generalizable models that can simulate system behavior, reason about alternative future scenarios, and extrapolate to unseen conditions\u2014capabilities that are essential for decision-making in Prognostics andHealth Management (PHM). This research aims to advance reasoning and decision support in PHM through three novel contributions: (1) a physics-informed surrogate model for simulating rigid body interactions, enabling the exploration of \u201dwhat-if\u201d scenarios, (2) an object-centric visual reasoning model for dynamics prediction in sensor-limited environments, supporting visual inspection tasks, and (3) a neuro-symbolic framework for interpretable root-cause analysis in time series, improving diagnostic transparency and providing actionable insights."
    },
    {
      "paperId": "2c176961f26529a9e5d0c8f327ff8aade45f5e6c",
      "externalIds": {
        "DOI": "10.1109/ICCAD66269.2025.11240851",
        "CorpusId": 283145284
      },
      "corpusId": 283145284,
      "title": "How Do Errors Impact NN Accuracy on Non-Ideal Analog PIM? Fast Evaluation via an Error-Injected Robustness Metric",
      "venue": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCAD66269.2025.11240851?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCAD66269.2025.11240851, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2223374285",
          "name": "Lidong Guo"
        },
        {
          "authorId": "2135206587",
          "name": "Zhenhua Zhu"
        },
        {
          "authorId": null,
          "name": "Qiushi Lin"
        },
        {
          "authorId": "2395542392",
          "name": "Yuan Xie"
        },
        {
          "authorId": "2177314863",
          "name": "Huazhong Yang"
        },
        {
          "authorId": "2397778265",
          "name": "Wangyang Fu"
        },
        {
          "authorId": "2339563033",
          "name": "Yu Wang"
        }
      ],
      "abstract": "The emerging analog Processing-in-Memory (PIM) architectures have shown great potential to overcome the memory wall problem and accelerate neural network (NN) inference. However, different from digital architectures, the computation accuracy of analog PIM architectures is directly impacted by various errors, which are related to both software and hardware parameters. Existing PIM simulators mainly adopt the bit-and-crossbar slicing paradigm to evaluate the accuracy under various errors. Each MVM operation is performed bit by bit and crossbar by crossbar, which is extremely time-consuming, especially for models with a larger number of parameters, such as large language models (LLMs).In this work, we propose an error-injected robustness metric, unifying various errors into the weight dimension and facilitating joint error analysis. Based on the error-injected robustness metric, we propose a Non-Ideal PIM Accuracy (NIPA) evaluation model for relative accuracy evaluation, considering the coupling effect (i.e., various errors can be affected by the same factor) among various errors using NN\u2019s prior information. We further propose a non-slicing absolute accuracy evaluation method, eliminating the need for the time-consuming bit-and-crossbar slicing process. Extensive experiments on CNNs and LLMs validate that the proposed NIPA evaluation model achieves high correlations of up to 0.91 with the absolute accuracy evaluated by DNN+NeuroSim. At the same time, compared to existing bit-and-crossbar slicing evaluation methods, the proposed non-slicing absolute accuracy evaluation method achieves up to 105.8\u00d7 speedup with average evaluation errors as low as 0.29%."
    },
    {
      "paperId": "1a7f89fced43f52382c5e9c8ef89534be2e64888",
      "externalIds": {
        "DOI": "10.1109/ICCAD66269.2025.11240811",
        "CorpusId": 283145356
      },
      "corpusId": 283145356,
      "title": "ToMamba: Towards Token-Efficient Mamba Architecture on FPGA",
      "venue": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCAD66269.2025.11240811?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCAD66269.2025.11240811, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2268968695",
          "name": "Kejia Shi"
        },
        {
          "authorId": "2348930690",
          "name": "Yue Cao"
        },
        {
          "authorId": "2394057776",
          "name": "Yuhang Du"
        },
        {
          "authorId": "2241764658",
          "name": "Jianli Chen"
        },
        {
          "authorId": "2117884324",
          "name": "Jun Yu"
        },
        {
          "authorId": "2355220727",
          "name": "Kun Wang"
        }
      ],
      "abstract": "The State Space Model (SSM), particularly the Mamba implementation, has demonstrated impressive capabilities across various domains. It offers a significant reduction in computational complexity compared to Transformers while achieving higher algorithm accuracy. However, the ineffectiveness of spatially unfolding the SSM layer leads to increased latency as sentence length grows, especially when being deployed on FPGA. Previous token reduction methods introduced in Transformers fail to maintain high performance in Mamba. Moreover, the dispersed outliers, complex model structure and variety of non-linear operators obstruct its efficient implementation on FPGA. To address these challenges, we propose ToMamba, the first algorithm-architecture co-design to optimize Mamba implementation. At the algorithmic level, ToMamba incorporates a novel progressive token merging algorithm with minimal hardware consumption and a hardware-aware fine-grained quantization strategy. On the hardware side, a dualflow systolic array is designed to unify convolution and matrix multiplication, supporting both weight stationary and output stationary dataflow. A fine-grained pipeline design is adopted for SSM computation to maximize hardware efficiency and enhance throughput. Furthermore, efficient hardware architecture and approximation method for nonlinear function units are proposed. To enable merging after the Mamba layer, ToMamba also adopts a dedicated data mapping scheme. Comprehensive evaluations across multiple benchmarks demonstrate that the token reduction method of ToMamba achieves 10% sparsity with only 0.25% accuracy loss, improving up to 16.89% in accuracy compared to previous methods. ToMamba hardware implementation on U280 FPGA achieves up to 636.00\u00d7/11.01\u00d7/1.39\u00d7 speedup compared to Intel Xeon Platinum 8369B CPU, NVIDIA Tesla A100 GPU and ASIC platforms and 1280\u00d7/44.32\u00d7 energy efficiency improvement compared to CPU and GPU platforms."
    },
    {
      "paperId": "55f2b3a4c74f842615281ae96bc9582fc5b3b4ee",
      "externalIds": {
        "DOI": "10.1109/ICCAD66269.2025.11240790",
        "CorpusId": 283145799
      },
      "corpusId": 283145799,
      "title": "M3: Mamba-assisted Multi-Circuit Optimization via Model-based RL with Effective Scheduling",
      "venue": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCAD66269.2025.11240790?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCAD66269.2025.11240790, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2268981585",
          "name": "Youngmin Oh"
        },
        {
          "authorId": "2332178380",
          "name": "Jinje Park"
        },
        {
          "authorId": "2188780228",
          "name": "Taejin Paik"
        },
        {
          "authorId": "2305668937",
          "name": "Seunggeun Kim"
        },
        {
          "authorId": "2383357560",
          "name": "Suwan Kim"
        },
        {
          "authorId": "2269009175",
          "name": "Yoon Hyeok Lee"
        },
        {
          "authorId": "2393457252",
          "name": "David Z. Pan"
        }
      ],
      "abstract": "Recent advances in neural network architectures, such as the Transformer, have enabled a shift from task-specific models to unified foundation models, significantly enhancing generalization and scalability. In contrast, analog circuit design has traditionally relied on bespoke optimization models tailored to individual circuits. To address this gap, we propose M3, a novel unified reinforcement learning (RL) that concurrently optimizes multiple circuits with different topologies to meet target specifications, without requiring task-specific adjustments. The M3 framework employs the Mamba architecture, a recently emerging model regarded as a potential alternative to the Transformer. It combines model-based RL with a dynamic scheduling mechanism that adapts RL parameters to balance exploration (seeking novel designs) and exploitation (refining existing ones). Experimental results demonstrate that M3 successfully trains RL agents capable of simultaneously optimizing multiple circuits, having different topologies, to achieve target performance levels while prior RL-based methods are unable to do. This approach highlights the potential of developing a unified model for optimizing circuits across varying topologies and target specifications1."
    },
    {
      "paperId": "04e747db528a0769c58c14bbf12d519874bed56e",
      "externalIds": {
        "DOI": "10.1109/ICCAD66269.2025.11240845",
        "CorpusId": 283145376
      },
      "corpusId": 283145376,
      "title": "Robin: RWKV Accelerator using Block Circulant Matrices based on FPGA",
      "venue": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCAD66269.2025.11240845?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCAD66269.2025.11240845, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2394130000",
          "name": "Zeyu Li"
        },
        {
          "authorId": "2393884916",
          "name": "Shangkun Li"
        },
        {
          "authorId": "2394076167",
          "name": "Chuyi Dai"
        },
        {
          "authorId": "2385423374",
          "name": "Chaofang Ma"
        },
        {
          "authorId": "2360679400",
          "name": "Jiawei Liang"
        },
        {
          "authorId": "2395476423",
          "name": "Xin Li"
        },
        {
          "authorId": "2393432278",
          "name": "Wei Zhanh"
        }
      ],
      "abstract": "Recent advancements in linear-attention models, such as RWKV, have opened up new possibilities for efficient sequence processing by reducing the computational overhead of traditional Transformer architectures. Field-programmable gate arrays (FPGAs) offer a compelling solution for deep learning applications by providing customizable hardware architectures that enhance computational efficiency and flexibility. However, deploying these models on FPGAs introduces several challenges. Previous FPGA deployment workflows tend to focus on general machine learning tasks, lacking sufficient integration between software and hardware optimizations. Besides, FPGAs are constrained by limited on-chip and off-chip memory, posing significant challenges for weight storage. Moreover, the predominance of linear operations on the GPU runtime leads to significant computational bottlenecks. These obstacles necessitate innovative solutions to bridge the performance gap between FPGAs and GPUs while preserving model accuracyTo overcome these challenges, we introduce Robin, a fine-grained FPGA accelerator workflow that integrates both algorithm-level and hardware-level optimization. Robin leverages a weight compression technique based on Partial Block Circulant Matrices (PBCM), which effectively reduces storage demands while maintaining accuracy. Based on PBCM, our design employs a configurable circulant computing core that fully exploits the bit-width efficiency of DSP48E resources through two DSP packaging strategies to support both circulant and standard matrix operations. The combined end-to-end software-hardware co-design enables Robin to achieve up to a 3.09\u00d7 increase in throughput and a 7.31\u00d7 boost in energy efficiency compared to high-end Tesla A100 GPU implementations, making it a compelling solution for deploying RWKV models on FPGAs."
    },
    {
      "paperId": "cf287c34c3dc720612049bde4397269a956150eb",
      "externalIds": {
        "DOI": "10.1080/15481603.2025.2565866",
        "CorpusId": 282462541
      },
      "corpusId": 282462541,
      "title": "Texture and structure interaction guided generative adversarial network for multimodal remote sensing image change detection",
      "venue": "GIScience &amp; Remote Sensing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/15481603.2025.2565866?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/15481603.2025.2565866, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2268733758",
          "name": "Zhifu Zhu"
        },
        {
          "authorId": "2350413639",
          "name": "Xiping Yuan"
        },
        {
          "authorId": "2325161166",
          "name": "Shu Gan"
        },
        {
          "authorId": "2143441832",
          "name": "Raobo Li"
        },
        {
          "authorId": "2153435515",
          "name": "Weidong Luo"
        },
        {
          "authorId": "2059524026",
          "name": "Rui Bi"
        },
        {
          "authorId": "2216220833",
          "name": "Cheng Chen"
        }
      ],
      "abstract": "Abstract Generative adversarial networks (GANs) possess powerful image translation capabilities. They can transform images acquired from different sensors into a unified domain, effectively mitigating the incomparability problem caused by imaging discrepancies in multimodal remote sensing change detection (CD). However, existing approaches predominantly emphasize domain unification while neglecting the loss of fine-grained features inherent in the translation process, consequently compromising both image translation quality and CD accuracy. To overcome these limitations, we propose a novel texture and structure interaction guided GAN (TSIG-GAN). This network establishes interactive guidance between image texture and structural features through a carefully designed dual-stream cross encoder-decoder architecture, enabling in-depth mining of fine-grained features and significantly improving the fidelity of translated images. Furthermore, to address the spatial scale diversity and complexity of remote sensing images, we develop a multi-scale adaptive feature pyramid (MAFP) module and a contextual semantic interaction guidance (CSIG) mechanism, aiming to further strengthen the model's robust representation of fine-grained features across multiple scales and complex scenes. Specifically, the MAFP module effectively captures spatial details of targets at different resolutions by dynamically integrating multi-scale features, thereby preventing detail loss in small objects due to scale discrepancies. The CSIG mechanism achieves deep interaction between texture and structural features at the contextual semantic level, further promoting their mutual cooperation, thereby enhancing the consistency of fine-grained features representation and semantic integrity in complex scenes. Finally, the translated fine-grained images are fed into a custom CD network to extract changes. To evaluate the effectiveness of the proposed method, we conducted systematic experiments on five representative real-world datasets and performed comparative analysis with sixteen state-of-the-art multimodal CD methods. The experimental results demonstrate that TSIG-GAN achieves significant improvements in both image translation and CD performance, exhibiting superior fine-grained restoration capability and change identification capability."
    },
    {
      "paperId": "915705c96be81e72e9d02d8ff065b1f7e4b900b1",
      "externalIds": {
        "DOI": "10.1007/s10044-025-01568-w",
        "CorpusId": 282385811
      },
      "corpusId": 282385811,
      "title": "Image-text relationship-based feature interaction networks for multimodal aspect-based sentiment analysis",
      "venue": "Pattern Analysis and Applications",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10044-025-01568-w?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10044-025-01568-w, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2387834792",
          "name": "Hao Ma"
        },
        {
          "authorId": "2387804694",
          "name": "Hai Huan"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "6c3b629d5e326c8ed7a3f9d355a6b2b0ad20f7f5",
      "externalIds": {
        "DOI": "10.1109/CISP-BMEI68103.2025.11259379",
        "CorpusId": 283472591
      },
      "corpusId": 283472591,
      "title": "A Global-Local Feature Decomposition and Ternary Feature Weight Network for Image Fusion",
      "venue": "2025 18th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CISP-BMEI68103.2025.11259379?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CISP-BMEI68103.2025.11259379, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2397374872",
          "name": "Xin Hu"
        },
        {
          "authorId": "2278379635",
          "name": "Jinzheng Lu"
        },
        {
          "authorId": "2395928107",
          "name": "Ningzhi Zeng"
        }
      ],
      "abstract": "The technology, which fuses infrared and visible images, combines thermal radiation data from infrared images with texture details from visible images to create more comprehensible images. However, existing methods have some shortcomings in dealing with modal differences, such as insufficient feature extraction, a single fusion strategy, and weak detail retention. In this research, we propose a global-local feature decomposition and ternary feature weight network for image fusion (GL-TFNet). Firstly, the design of the Mamba-CNN dual-branch feature extraction strategy aims to enhance both global and local feature extraction processes. Secondly, the global branch utilizes the 2D selective scan (SS2D) state space model for isolating features dependent on long-range conditions, while the local branch employs dynamic convolutional adaptive modeling to address local differences. Finally, this research proposes the ternary feature weight (TFW) to improve the combined representation of information from multiple sources. It merges the differences, similarities, and complementary aspects between modalities. Experimental findings indicate GL-TFNet's effectiveness on publicly available TNO, MSRS, and RoadScene fusion datasets. The metrics for visual information fidelity (VIF) and quality assessment based on fusion (Qabf) are exceptional, signifying superior edge clarity and structural integrity. Compared with recent networks, GL-TFNet has a relatively small parameter count. Ablation experiments further confirm the effectiveness of the modules."
    },
    {
      "paperId": "14acc1bb3ef63910e7805f42f07a0fdf7667c9fb",
      "externalIds": {
        "DOI": "10.1109/CISP-BMEI68103.2025.11259179",
        "CorpusId": 283475316
      },
      "corpusId": 283475316,
      "title": "MS-DS3net: Research on a Multi-Scale Medical Image Segmentation Model Based on State Space Models",
      "venue": "2025 18th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CISP-BMEI68103.2025.11259179?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CISP-BMEI68103.2025.11259179, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2395908753",
          "name": "Hongbing Zhu"
        },
        {
          "authorId": "2396294464",
          "name": "Xiaorong Zhang"
        },
        {
          "authorId": "2395940602",
          "name": "Na Wang"
        },
        {
          "authorId": "2396725244",
          "name": "Xiqianlong Yuan"
        },
        {
          "authorId": "2395975578",
          "name": "Yunxiu Zhang"
        },
        {
          "authorId": "2397439905",
          "name": "Jun Wen"
        }
      ],
      "abstract": "Accurate segmentation of medical images is vital for disease diagnosis and treatment planning, particularly in identifying skin lesions. Conventional segmentation networks often struggle with indistinct lesion edges, complex spatial structures, and ineffective integration of features across different scales. To address these issues, this work presents MS-DS3Net, a multi-scale medical image segmentation model constructed on the principle of State Space Models (SSM). The proposed framework introduces two major components: a StructureGuided Fusion Module that strengthens semantic interaction between hierarchical feature layers, and a Boundary-Enhanced Decoding Module that explicitly emphasizes edge cues to refine lesion contours. Evaluated on the ISIC18 skin lesion dataset, MS-DS3Net demonstrates substantial improvements in accuracy and boundary preservation compared with mainstream networks such as UNet and UNet++, achieving metrics exceeding 80 %. These results confirm that incorporating state-space modeling with multi-scale feature learning can effectively enhance segmentation precision and generalization in medical imaging tasks."
    },
    {
      "paperId": "7eb1f580300e2ac64595ab0a13797645c5c1ea07",
      "externalIds": {
        "DOI": "10.1109/CISP-BMEI68103.2025.11259325",
        "CorpusId": 283474980
      },
      "corpusId": 283474980,
      "title": "A Self-Adaptive Vision Mamba Network for Image Quality Assessment",
      "venue": "2025 18th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CISP-BMEI68103.2025.11259325?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CISP-BMEI68103.2025.11259325, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2299184822",
          "name": "Rui Liu"
        },
        {
          "authorId": "2397336827",
          "name": "Xin Cheng"
        },
        {
          "authorId": "2395943904",
          "name": "Lu Che"
        },
        {
          "authorId": "2395946936",
          "name": "Chang Liu"
        },
        {
          "authorId": "2396121823",
          "name": "Yu Zhang"
        }
      ],
      "abstract": "With the rapid growth of visual media content and the continuous advancement of digital technologies, people are increasingly relying on images and videos as the primary means of information transmission and entertainment. However, due to device limitations and various factors during the transmission process, images and videos often suffer from distortion, which affects the viewing experience of the audience. Although subjective evaluation is accurate, it is time consuming, cumbersome, and impractical. Therefore, it is of great significance to study objective image quality assessment (IQA) methods. By developing efficient and accurate image quality assessment algorithms, the quality of images can be automatically evaluated, providing strong support for various aspects such as image processing, transmission, and display. This, in turn, can improve the viewing experience of the audience and promote a broader application and highquality development of visual media content. We propose a self-adaptive vision mamba network for blind image quality assessment (SAVM-IQA)."
    },
    {
      "paperId": "20f3c3940e8a4146da3bb25a40c2d3fc7f84d516",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-21258",
        "ArXiv": "2510.21258",
        "DOI": "10.48550/arXiv.2510.21258",
        "CorpusId": 282384608
      },
      "corpusId": 282384608,
      "title": "Correlation Dimension of Auto-Regressive Large Language Models",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.21258, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2261391289",
          "name": "Xin Du"
        },
        {
          "authorId": "2257297569",
          "name": "Kumiko Tanaka-Ishii"
        }
      ],
      "abstract": "Large language models (LLMs) have achieved remarkable progress in natural language generation, yet they continue to display puzzling behaviors -- such as repetition and incoherence -- even when exhibiting low perplexity. This highlights a key limitation of conventional evaluation metrics, which emphasize local prediction accuracy while overlooking long-range structural complexity. We introduce correlation dimension, a fractal-geometric measure of self-similarity, to quantify the epistemological complexity of text as perceived by a language model. This measure captures the hierarchical recurrence structure of language, bridging local and global properties in a unified framework. Through extensive experiments, we show that correlation dimension (1) reveals three distinct phases during pretraining, (2) reflects context-dependent complexity, (3) indicates a model's tendency toward hallucination, and (4) reliably detects multiple forms of degeneration in generated text. The method is computationally efficient, robust to model quantization (down to 4-bit precision), broadly applicable across autoregressive architectures (e.g., Transformer and Mamba), and provides fresh insight into the generative dynamics of LLMs."
    },
    {
      "paperId": "55d2e5705b9f9ae31c71a7a049fe76823e0f43f4",
      "externalIds": {
        "ArXiv": "2510.21450",
        "DBLP": "journals/corr/abs-2510-21450",
        "DOI": "10.48550/arXiv.2510.21450",
        "CorpusId": 282384191
      },
      "corpusId": 282384191,
      "title": "ParaRNN: Unlocking Parallel Training of Nonlinear RNNs for Large Language Models",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.21450, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2247596522",
          "name": "Federico Danieli"
        },
        {
          "authorId": "2350551427",
          "name": "Pau Rodriguez"
        },
        {
          "authorId": "2247600145",
          "name": "Miguel Sarabia"
        },
        {
          "authorId": "2270464",
          "name": "Xavier Suau"
        },
        {
          "authorId": "1753336",
          "name": "Luca Zappella"
        }
      ],
      "abstract": "Recurrent Neural Networks (RNNs) laid the foundation for sequence modeling, but their intrinsic sequential nature restricts parallel computation, creating a fundamental barrier to scaling. This has led to the dominance of parallelizable architectures like Transformers and, more recently, State Space Models (SSMs). While SSMs achieve efficient parallelization through structured linear recurrences, this linearity constraint limits their expressive power and precludes modeling complex, nonlinear sequence-wise dependencies. To address this, we present ParaRNN, a framework that breaks the sequence-parallelization barrier for nonlinear RNNs. Building on prior work, we cast the sequence of nonlinear recurrence relationships as a single system of equations, which we solve in parallel using Newton's iterations combined with custom parallel reductions. Our implementation achieves speedups of up to 665x over naive sequential application, allowing training nonlinear RNNs at unprecedented scales. To showcase this, we apply ParaRNN to adaptations of LSTM and GRU architectures, successfully training models of 7B parameters that attain perplexity comparable to similarly-sized Transformers and Mamba2 architectures. To accelerate research in efficient sequence modeling, we release the ParaRNN codebase as an open-source framework for automatic training-parallelization of nonlinear RNNs, enabling researchers and practitioners to explore new nonlinear RNN models at scale."
    },
    {
      "paperId": "d532596cc04fe0399e5ea69ca70be850cdba2f80",
      "externalIds": {
        "ArXiv": "2510.21079",
        "DBLP": "journals/corr/abs-2510-21079",
        "DOI": "10.48550/arXiv.2510.21079",
        "CorpusId": 282384494
      },
      "corpusId": 282384494,
      "title": "WaveSeg: Enhancing Segmentation Precision via High-Frequency Prior and Mamba-Driven Spectrum Decomposition",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.21079, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2115724819",
          "name": "Guoan Xu"
        },
        {
          "authorId": "2365313514",
          "name": "Yang Xiao"
        },
        {
          "authorId": "2238952299",
          "name": "Wenjing Jia"
        },
        {
          "authorId": "2323440968",
          "name": "Guangwei Gao"
        },
        {
          "authorId": "2350511779",
          "name": "Guojun Qi"
        },
        {
          "authorId": "2248600712",
          "name": "Chia-Wen Lin"
        }
      ],
      "abstract": "While recent semantic segmentation networks heavily rely on powerful pretrained encoders, most employ simplistic decoders, leading to suboptimal trade-offs between semantic context and fine-grained detail preservation. To address this, we propose a novel decoder architecture, WaveSeg, which jointly optimizes feature refinement in spatial and wavelet domains. Specifically, high-frequency components are first learned from input images as explicit priors to reinforce boundary details at early stages. A multi-scale fusion mechanism, Dual Domain Operation (DDO), is then applied, and the novel Spectrum Decomposition Attention (SDA) block is proposed, which is developed to leverage Mamba's linear-complexity long-range modeling to enhance high-frequency structural details. Meanwhile, reparameterized convolutions are applied to preserve low-frequency semantic integrity in the wavelet domain. Finally, a residual-guided fusion integrates multi-scale features with boundary-aware representations at native resolution, producing semantically and structurally rich feature maps. Extensive experiments on standard benchmarks demonstrate that WaveSeg, leveraging wavelet-domain frequency prior with Mamba-based attention, consistently outperforms state-of-the-art approaches both quantitatively and qualitatively, achieving efficient and precise segmentation."
    },
    {
      "paperId": "6634fdc96b0cc94a5504c2986ab3d71b998cf229",
      "externalIds": {
        "ArXiv": "2510.22049",
        "DBLP": "journals/corr/abs-2510-22049",
        "DOI": "10.48550/arXiv.2510.22049",
        "CorpusId": 282389164
      },
      "corpusId": 282389164,
      "title": "Massive Memorization with Hundreds of Trillions of Parameters for Sequential Transducer Generative Recommenders",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.22049, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2387879825",
          "name": "Zhimin Chen"
        },
        {
          "authorId": "2388080356",
          "name": "Chenyu Zhao"
        },
        {
          "authorId": "2387864571",
          "name": "Ka Chun Mo"
        },
        {
          "authorId": "2387878754",
          "name": "Yunjiang Jiang"
        },
        {
          "authorId": "2387875718",
          "name": "Jane H. Lee"
        },
        {
          "authorId": "2387875326",
          "name": "Shouwei Chen"
        },
        {
          "authorId": "151460112",
          "name": "Khushhall Chandra Mahajan"
        },
        {
          "authorId": "2388841089",
          "name": "Ning Jiang"
        },
        {
          "authorId": "2387865793",
          "name": "Kai Ren"
        },
        {
          "authorId": "2387878072",
          "name": "Jinhui Li"
        },
        {
          "authorId": "2389171388",
          "name": "Wen-Yun Yang"
        }
      ],
      "abstract": "Modern large-scale recommendation systems rely heavily on user interaction history sequences to enhance the model performance. The advent of large language models and sequential modeling techniques, particularly transformer-like architectures, has led to significant advancements recently (e.g., HSTU, SIM, and TWIN models). While scaling to ultra-long user histories (10k to 100k items) generally improves model performance, it also creates significant challenges on latency, queries per second (QPS) and GPU cost in industry-scale recommendation systems. Existing models do not adequately address these industrial scalability issues. In this paper, we propose a novel two-stage modeling framework, namely VIrtual Sequential Target Attention (VISTA), which decomposes traditional target attention from a candidate item to user history items into two distinct stages: (1) user history summarization into a few hundred tokens; followed by (2) candidate item attention to those tokens. These summarization token embeddings are then cached in storage system and then utilized as sequence features for downstream model training and inference. This novel design for scalability enables VISTA to scale to lifelong user histories (up to one million items) while keeping downstream training and inference costs fixed, which is essential in industry. Our approach achieves significant improvements in offline and online metrics and has been successfully deployed on an industry leading recommendation platform serving billions of users."
    },
    {
      "paperId": "53cf0b696caccde506c2d7bf4c46d0901e47e72f",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-22052",
        "ArXiv": "2510.22052",
        "DOI": "10.48550/arXiv.2510.22052",
        "CorpusId": 282388957
      },
      "corpusId": 282388957,
      "title": "Energy-Efficient Domain-Specific Artificial Intelligence Models and Agents: Pathways and Paradigms",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.22052, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2387867532",
          "name": "Abhijit Chatterjee"
        },
        {
          "authorId": "152393000",
          "name": "N. Jha"
        },
        {
          "authorId": "2262090057",
          "name": "Jonathan D. Cohen"
        },
        {
          "authorId": "2253463157",
          "name": "Thomas L. Griffiths"
        },
        {
          "authorId": "2388054507",
          "name": "Hongjing Lu"
        },
        {
          "authorId": "2387862689",
          "name": "Diana Marculescu"
        },
        {
          "authorId": "2205208428",
          "name": "Ashiqur Rasul"
        },
        {
          "authorId": "2314820335",
          "name": "Keshab K. Parhi"
        }
      ],
      "abstract": "The field of artificial intelligence (AI) has taken a tight hold on broad aspects of society, industry, business, and governance in ways that dictate the prosperity and might of the world's economies. The AI market size is projected to grow from 189 billion USD in 2023 to 4.8 trillion USD by 2033. Currently, AI is dominated by large language models that exhibit linguistic and visual intelligence. However, training these models requires a massive amount of data scraped from the web as well as large amounts of energy (50--60 GWh to train GPT-4). Despite these costs, these models often hallucinate, a characteristic that prevents them from being deployed in critical application domains. In contrast, the human brain consumes only 20~W of power. What is needed is the next level of AI evolution in which lightweight domain-specific multimodal models with higher levels of intelligence can reason, plan, and make decisions in dynamic environments with real-time data and prior knowledge, while learning continuously and evolving in ways that enhance future decision-making capability. This will define the next wave of AI, progressing from today's large models, trained with vast amounts of data, to nimble energy-efficient domain-specific agents that can reason and think in a world full of uncertainty. To support such agents, hardware will need to be reimagined to allow energy efficiencies greater than 1000x over the state of the art. Such a vision of future AI systems is developed in this work."
    },
    {
      "paperId": "ae9c60679affdfe22737799527c81a47e2d70d00",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-21956",
        "ArXiv": "2510.21956",
        "DOI": "10.48550/arXiv.2510.21956",
        "CorpusId": 282390002
      },
      "corpusId": 282390002,
      "title": "Transformer Based Linear Attention with Optimized GPU Kernel Implementation",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.21956, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2325908944",
          "name": "Armin Gerami"
        },
        {
          "authorId": "1719541",
          "name": "R. Duraiswami"
        }
      ],
      "abstract": "The original softmax-based attention mechanism (regular attention) in the extremely successful Transformer architecture computes attention between $N$ tokens, each embedded in a $D$-dimensional head, with a time complexity of $O(N^2D)$. Given the success of Transformers, improving their runtime during both training and inference is a popular research area. One such approach is the introduction of the linear attention (LA) mechanisms, which offers a linear time complexity of $O(ND^2)$ and have demonstrated comparable accuracy to regular attention. However, LA in practice lags behind its theoretical efficiency. We propose a novel method for LA's forward and backward passes, along with a highly-optimized CUDA implementation. Our approach outperforms the state-of-the-art by 3.3 times in speed and reduces memory consumption by 3.6 times. We validate these improvements in both single-layer and end-to-end settings by training a 1.4 billion parameter language model, which demonstrates similar expressivity to regular attention on major reasoning benchmarks."
    },
    {
      "paperId": "95d04c501e1f71f8be3430a861e91789d474cbfd",
      "externalIds": {
        "ArXiv": "2510.22037",
        "DBLP": "journals/corr/abs-2510-22037",
        "DOI": "10.48550/arXiv.2510.22037",
        "CorpusId": 282389622
      },
      "corpusId": 282389622,
      "title": "ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining, Finetuning, and Decoding the Curse of Multilinguality",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.22037, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2283848744",
          "name": "Shayne Longpre"
        },
        {
          "authorId": "2131614068",
          "name": "Sneha Kudugunta"
        },
        {
          "authorId": "2037383772",
          "name": "Niklas Muennighoff"
        },
        {
          "authorId": "2349535505",
          "name": "I-Hung Hsu"
        },
        {
          "authorId": "2387868274",
          "name": "Isaac Caswell"
        },
        {
          "authorId": "2326078389",
          "name": "Alex Pentland"
        },
        {
          "authorId": "2676352",
          "name": "Sercan \u00d6. Arik"
        },
        {
          "authorId": "2388962059",
          "name": "Chen-Yu Lee"
        },
        {
          "authorId": "27556211",
          "name": "Sayna Ebrahimi"
        }
      ],
      "abstract": "Scaling laws research has focused overwhelmingly on English -- yet the most prominent AI models explicitly serve billions of international users. In this work, we undertake the largest multilingual scaling laws study to date, totaling 774 multilingual training experiments, spanning 10M-8B model parameters, 400+ training languages and 48 evaluation languages. We introduce the Adaptive Transfer Scaling Law (ATLAS) for both monolingual and multilingual pretraining, which outperforms existing scaling laws'out-of-sample generalization often by more than 0.3 R^2. Our analyses of the experiments shed light on multilingual learning dynamics, transfer properties between languages, and the curse of multilinguality. First, we derive a cross-lingual transfer matrix, empirically measuring mutual benefit scores between 38 x 38=1444 language pairs. Second, we derive a language-agnostic scaling law that reveals how to optimally scale model size and data when adding languages without sacrificing performance. Third, we identify the computational crossover points for when to pretrain from scratch versus finetune from multilingual checkpoints. We hope these findings provide the scientific foundation for democratizing scaling laws across languages, and enable practitioners to efficiently scale models -- beyond English-first AI."
    },
    {
      "paperId": "72e51bc6ffdac85a918a99a66b89849b04112299",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-23640",
        "ArXiv": "2510.23640",
        "DOI": "10.48550/arXiv.2510.23640",
        "CorpusId": 282400756
      },
      "corpusId": 282400756,
      "title": "Structure-Aware Fusion with Progressive Injection for Multimodal Molecular Representation Learning",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.23640, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2342913448",
          "name": "Zihao Jing"
        },
        {
          "authorId": "2196446380",
          "name": "Y. Sun"
        },
        {
          "authorId": "2292974681",
          "name": "Yan Yi Li"
        },
        {
          "authorId": "2265760347",
          "name": "Sugitha Janarthanan"
        },
        {
          "authorId": "2387972015",
          "name": "Alana Deng"
        },
        {
          "authorId": "2344046267",
          "name": "Pingzhao Hu"
        }
      ],
      "abstract": "Multimodal molecular models often suffer from 3D conformer unreliability and modality collapse, limiting their robustness and generalization. We propose MuMo, a structured multimodal fusion framework that addresses these challenges in molecular representation through two key strategies. To reduce the instability of conformer-dependent fusion, we design a Structured Fusion Pipeline (SFP) that combines 2D topology and 3D geometry into a unified and stable structural prior. To mitigate modality collapse caused by naive fusion, we introduce a Progressive Injection (PI) mechanism that asymmetrically integrates this prior into the sequence stream, preserving modality-specific modeling while enabling cross-modal enrichment. Built on a state space backbone, MuMo supports long-range dependency modeling and robust information propagation. Across 29 benchmark tasks from Therapeutics Data Commons (TDC) and MoleculeNet, MuMo achieves an average improvement of 2.7% over the best-performing baseline on each task, ranking first on 22 of them, including a 27% improvement on the LD50 task. These results validate its robustness to 3D conformer noise and the effectiveness of multimodal fusion in molecular representation. The code is available at: github.com/selmiss/MuMo."
    },
    {
      "paperId": "b22c46eba94058889298d200c400243a8f1b7853",
      "externalIds": {
        "ArXiv": "2511.17526",
        "CorpusId": 283244773
      },
      "corpusId": 283244773,
      "title": "RadioMapMotion: A Dataset and Baseline for Proactive Spatio-Temporal Radio Environment Prediction",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.17526, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2367669110",
          "name": "Honggang Jia"
        },
        {
          "authorId": "2305823966",
          "name": "Nan Cheng"
        },
        {
          "authorId": "2118419158",
          "name": "Xiucheng Wang"
        }
      ],
      "abstract": "Radio maps (RMs), which provide location-based pathloss estimations, are fundamental to enabling proactive, environment-aware communication in 6G networks. However, existing deep learning-based methods for RM construction often model dynamic environments as a series of independent static snapshots, thereby omitting the temporal continuity inherent in signal propagation changes caused by the motion of dynamic entities. To address this limitation, we propose the task of spatio-temporal RM prediction, which involves forecasting a sequence of future maps from historical observations. A key barrier to this predictive approach has been the lack of datasets capturing continuous environmental evolution. To fill this gap, we introduce RadioMapMotion, the first large-scale public dataset of continuous RM sequences generated from physically consistent vehicle trajectories. As a baseline for this task, we propose RadioLSTM, a UNet architecture based on Convolutional Long Short-Term Memory (ConvLSTM) and designed for multi-step sequence forecasting. Experimental evaluations show that RadioLSTM achieves higher prediction accuracy and structural fidelity compared to representative baseline methods. Furthermore, the model exhibits a low inference latency, indicating its potential suitability for real-time network operations. Our project will be publicly released at: https://github.com/UNIC-Lab/RadioMapMotion upon paper acceptance."
    },
    {
      "paperId": "c43c714fb65f982264bdbc417f0e9da6954f704a",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-20787",
        "ArXiv": "2510.20787",
        "DOI": "10.48550/arXiv.2510.20787",
        "CorpusId": 282304430
      },
      "corpusId": 282304430,
      "title": "Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention and Contextualized Learnable Token Eviction",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.20787, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2365454544",
          "name": "Mutian He"
        },
        {
          "authorId": "2261831397",
          "name": "Philip N. Garner"
        }
      ],
      "abstract": "Linear-attention models that compress the entire input sequence into a fixed-size recurrent state offer an efficient alternative to Transformers, but their finite memory induces forgetfulness that harms retrieval-intensive tasks. To mitigate the issue, we explore a series of hybrid models that restore direct access to past tokens. We interleave token mixers with intermediate time and space complexity between linear and full attention, including sparse attention with token eviction, and the query-aware native sparse attention. Particularly, we propose a novel learnable token eviction approach. Combined with sliding-window attention, an end-to-end trainable lightweight CNN aggregates information from both past and future adjacent tokens to adaptively retain a limited set of critical KV-pairs per head, maintaining linear attention's constant time and space complexity. Efficient Triton kernels for the sparse attention mechanisms are provided. Empirical evaluations on retrieval-intensive benchmarks support the effectiveness of our approaches."
    },
    {
      "paperId": "c050144ad4edfffc54e3edb2d9d4de5fbd535d0b",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-20668",
        "ArXiv": "2510.20668",
        "DOI": "10.48550/arXiv.2510.20668",
        "CorpusId": 282304134
      },
      "corpusId": 282304134,
      "title": "From Masks to Worlds: A Hitchhiker's Guide to World Models",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.20668, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2386054393",
          "name": "Jinbin Bai"
        },
        {
          "authorId": "2387887667",
          "name": "Yu Lei"
        },
        {
          "authorId": "2387274322",
          "name": "Hecong Wu"
        },
        {
          "authorId": "2295790135",
          "name": "Yuchen Zhu"
        },
        {
          "authorId": "2273911687",
          "name": "Shufan Li"
        },
        {
          "authorId": "2387389782",
          "name": "Yi Xin"
        },
        {
          "authorId": "2319846730",
          "name": "Xiangtai Li"
        },
        {
          "authorId": "2312729683",
          "name": "Molei Tao"
        },
        {
          "authorId": "2273651589",
          "name": "Aditya Grover"
        },
        {
          "authorId": "2265615338",
          "name": "Ming-Hsuan Yang"
        }
      ],
      "abstract": "This is not a typical survey of world models; it is a guide for those who want to build worlds. We do not aim to catalog every paper that has ever mentioned a ``world model\". Instead, we follow one clear road: from early masked models that unified representation learning across modalities, to unified architectures that share a single paradigm, then to interactive generative models that close the action-perception loop, and finally to memory-augmented systems that sustain consistent worlds over time. We bypass loosely related branches to focus on the core: the generative heart, the interactive loop, and the memory system. We show that this is the most promising path towards true world models."
    },
    {
      "paperId": "38ee869417c8f1c4e72cab9081963468e9435abb",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-20261",
        "ArXiv": "2510.20261",
        "DOI": "10.48550/arXiv.2510.20261",
        "CorpusId": 282304308
      },
      "corpusId": 282304308,
      "title": "Kinaema: a recurrent sequence model for memory and pose in motion",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.20261, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "1413822807",
          "name": "Mert Bulent Sariyildiz"
        },
        {
          "authorId": "2492127",
          "name": "Philippe Weinzaepfel"
        },
        {
          "authorId": "51217184",
          "name": "G. Bono"
        },
        {
          "authorId": "2069988",
          "name": "G. Monaci"
        },
        {
          "authorId": "2281035256",
          "name": "Christian Wolf"
        }
      ],
      "abstract": "One key aspect of spatially aware robots is the ability to\"find their bearings\", ie. to correctly situate themselves in previously seen spaces. In this work, we focus on this particular scenario of continuous robotics operations, where information observed before an actual episode start is exploited to optimize efficiency. We introduce a new model, Kinaema, and agent, capable of integrating a stream of visual observations while moving in a potentially large scene, and upon request, processing a query image and predicting the relative position of the shown space with respect to its current position. Our model does not explicitly store an observation history, therefore does not have hard constraints on context length. It maintains an implicit latent memory, which is updated by a transformer in a recurrent way, compressing the history of sensor readings into a compact representation. We evaluate the impact of this model in a new downstream task we call\"Mem-Nav\". We show that our large-capacity recurrent model maintains a useful representation of the scene, navigates to goals observed before the actual episode start, and is computationally efficient, in particular compared to classical transformers with attention over an observation history."
    },
    {
      "paperId": "f0bf93c421215ad078960d834ac111837b6dfcb5",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-20406",
        "ArXiv": "2510.20406",
        "DOI": "10.48550/arXiv.2510.20406",
        "CorpusId": 282304088
      },
      "corpusId": 282304088,
      "title": "PointMapPolicy: Structured Point Cloud Processing for Multi-Modal Imitation Learning",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.20406, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2263213428",
          "name": "Xiaogang Jia"
        },
        {
          "authorId": "2305814350",
          "name": "Qian Wang"
        },
        {
          "authorId": "2387513973",
          "name": "Anrui Wang"
        },
        {
          "authorId": "2345961413",
          "name": "Han A. Wang"
        },
        {
          "authorId": "92549984",
          "name": "Bal\u00e1zs Gyenes"
        },
        {
          "authorId": "2329099374",
          "name": "Emiliyan Gospodinov"
        },
        {
          "authorId": "2243176115",
          "name": "Xinkai Jiang"
        },
        {
          "authorId": "2186951194",
          "name": "Ge Li"
        },
        {
          "authorId": "2280293831",
          "name": "Hongyi Zhou"
        },
        {
          "authorId": "2331177608",
          "name": "Weiran Liao"
        },
        {
          "authorId": "2346047390",
          "name": "Xi Huang"
        },
        {
          "authorId": "2387212857",
          "name": "Maximilian Beck"
        },
        {
          "authorId": "2165874245",
          "name": "Moritz Reuss"
        },
        {
          "authorId": "2931067",
          "name": "Rudolf Lioutikov"
        },
        {
          "authorId": "2263395917",
          "name": "Gerhard Neumann"
        }
      ],
      "abstract": "Robotic manipulation systems benefit from complementary sensing modalities, where each provides unique environmental information. Point clouds capture detailed geometric structure, while RGB images provide rich semantic context. Current point cloud methods struggle to capture fine-grained detail, especially for complex tasks, which RGB methods lack geometric awareness, which hinders their precision and generalization. We introduce PointMapPolicy, a novel approach that conditions diffusion policies on structured grids of points without downsampling. The resulting data type makes it easier to extract shape and spatial relationships from observations, and can be transformed between reference frames. Yet due to their structure in a regular grid, we enable the use of established computer vision techniques directly to 3D data. Using xLSTM as a backbone, our model efficiently fuses the point maps with RGB data for enhanced multi-modal perception. Through extensive experiments on the RoboCasa and CALVIN benchmarks and real robot evaluations, we demonstrate that our method achieves state-of-the-art performance across diverse manipulation tasks. The overview and demos are available on our project page: https://point-map.github.io/Point-Map/"
    },
    {
      "paperId": "8208e398c39115207ec67a309c888ee1f78b37b6",
      "externalIds": {
        "DOI": "10.1101/2025.10.22.683913",
        "CorpusId": 282375480
      },
      "corpusId": 282375480,
      "title": "FveOCRs - a database for open chromatin prediction in wild strawberry based on a large language model",
      "venue": "bioRxiv",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1101/2025.10.22.683913?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1101/2025.10.22.683913, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2266648697",
          "name": "Muzi Li"
        },
        {
          "authorId": "2387764838",
          "name": "Yuzhu Wang"
        },
        {
          "authorId": "2388817278",
          "name": "Zhennan Zhao"
        },
        {
          "authorId": "2255552541",
          "name": "Stephen M. Mount"
        },
        {
          "authorId": "2387753519",
          "name": "Qing Ma"
        },
        {
          "authorId": "2265159008",
          "name": "Zhongchi Liu"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "4ff30d2815369c7ec1ac9d20b8f108c6b9e6cb6f",
      "externalIds": {
        "PubMedCentral": "12550090",
        "DOI": "10.1038/s41598-025-21837-2",
        "CorpusId": 282320035,
        "PubMed": "41131342"
      },
      "corpusId": 282320035,
      "title": "Super Mamba feature enhancement framework for small object detection",
      "venue": "Scientific Reports",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 1,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBYNCND",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12550090, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2382795629",
          "name": "Na Shi"
        },
        {
          "authorId": "2387783278",
          "name": "Zheng Yang"
        },
        {
          "authorId": "2359998927",
          "name": "Guang Yang"
        },
        {
          "authorId": "2387781028",
          "name": "Kai Li"
        },
        {
          "authorId": "2387812361",
          "name": "Zhiliang Yang"
        },
        {
          "authorId": "2151864031",
          "name": "Jianping An"
        },
        {
          "authorId": "2357109187",
          "name": "Sicheng Li"
        },
        {
          "authorId": "2387788183",
          "name": "Liangliang Zhang"
        },
        {
          "authorId": "2387349373",
          "name": "Senyang Jing"
        }
      ],
      "abstract": "It is very challenging to accurately and timely detect small object containing dozens of pixels from infrared images. Compared with the complex background in infrared images taken by low-altitude drones, a framework is designed to learn a strong feature representation separating the object from the background, which usually leads to a large computational amount. In this paper, we proposed a Super Mamba (SMamba) framework for UAV infrared small object detection, which performs deep learning of nonlinear complex data. Our SMamba framework performs high resolution object detection on multi-scale objects, considering both detection accuracy and computational cost. First, the Receptive Field Attention Convolution (RFAConv) is used into the backbone network and replaced the commonly convolution, and the multi-scale features is adjusted through the dynamic receptive field to optimize the computing efficiency. Furthermore, the Spatial Attention Mechanism (SAM) and Squeeze-Excitation (SE) are added to the State Space Model (SSM) to achieve multi-scale and multi-feature extraction for small object. Moreover, in the neck, the Feature Enhancement Module (FEM) is introduced to Bidirectional Feature Pyramid Network (BiFPN) can enhance the local context information of small objects and improve the detection efficiency. The experimental results show that Super Mamba achieved more than 92% accuracy on VEDAI dataset (in terms of mAP@ 0.5), which is more than 20% higher than the existing large models such as Yolov5, Yolov8, and Yolov11. The pytorch code is available at: https://github.com/wolfololo/Super-Mamba-A-Framework-for-Small-Object-Detection-with-Enhanced-Detection."
    },
    {
      "paperId": "0ffd011cf4eb87ceb1a4edb4b7a1b51dbdb157ab",
      "externalIds": {
        "ArXiv": "2510.20952",
        "DBLP": "journals/corr/abs-2510-20952",
        "DOI": "10.48550/arXiv.2510.20952",
        "CorpusId": 282384155
      },
      "corpusId": 282384155,
      "title": "LLM-Integrated Bayesian State Space Models for Multimodal Time-Series Forecasting",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.20952, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2352653126",
          "name": "Sungjun Cho"
        },
        {
          "authorId": "2054260304",
          "name": "Changho Shin"
        },
        {
          "authorId": "2351809477",
          "name": "Suenggwan Jo"
        },
        {
          "authorId": "2351818937",
          "name": "Xinya Yan"
        },
        {
          "authorId": "2351804877",
          "name": "Shourjo Aditya Chaudhuri"
        },
        {
          "authorId": "2385786825",
          "name": "Frederic Sala"
        }
      ],
      "abstract": "Forecasting in the real world requires integrating structured time-series data with unstructured textual information, but existing methods are architecturally limited by fixed input/output horizons and are unable to model or quantify uncertainty. We address this challenge by introducing LLM-integrated Bayesian State space models (LBS), a novel probabilistic framework for multimodal temporal forecasting. At a high level, LBS consists of two components: (1) a state space model (SSM) backbone that captures the temporal dynamics of latent states from which both numerical and textual observations are generated and (2) a pretrained large language model (LLM) that is adapted to encode textual inputs for posterior state estimation and decode textual forecasts consistent with the latent trajectory. This design enables flexible lookback and forecast windows, principled uncertainty quantification, and improved temporal generalization thanks to the well-suited inductive bias of SSMs toward modeling dynamical systems. Experiments on the TextTimeCorpus benchmark demonstrate that LBS improves the previous state-of-the-art by 13.20% while providing human-readable summaries of each forecast. Our work is the first to unify LLMs and SSMs for joint numerical and textual prediction, offering a novel foundation for multimodal temporal reasoning."
    },
    {
      "paperId": "431b8625018b61cc0d329494575888683bcc8303",
      "externalIds": {
        "DOI": "10.1080/10589759.2025.2577840",
        "CorpusId": 282427500
      },
      "corpusId": 282427500,
      "title": "SD-INPFormer: an unsupervised anomaly detection model for steel surface defect",
      "venue": "Nondestructive Testing and Evaluation",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/10589759.2025.2577840?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/10589759.2025.2577840, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2233749457",
          "name": "Guanghu Liu"
        },
        {
          "authorId": "47174955",
          "name": "Mao-xiang Chu"
        },
        {
          "authorId": "48096178",
          "name": "Rongfen Gong"
        },
        {
          "authorId": "2385647769",
          "name": "Peinan Zong"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "9784d8ffd89c661eabd21fd248d3850245aaae47",
      "externalIds": {
        "PubMedCentral": "12550077",
        "DOI": "10.1038/s41467-025-64385-z",
        "CorpusId": 282321896,
        "PubMed": "41130949"
      },
      "corpusId": 282321896,
      "title": "Categorical and phenotypic image synthetic learning as an alternative to federated learning",
      "venue": "Nature Communications",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12550077, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2238106304",
          "name": "Nghi C D Truong"
        },
        {
          "authorId": "1455583026",
          "name": "C. G. Bangalore Yogananda"
        },
        {
          "authorId": "2290267919",
          "name": "Ben C. Wagner"
        },
        {
          "authorId": "2056535148",
          "name": "J. Holcomb"
        },
        {
          "authorId": "2238103233",
          "name": "Divya D. Reddy"
        },
        {
          "authorId": "2238106189",
          "name": "Niloufar Saadat"
        },
        {
          "authorId": "2293871884",
          "name": "Jason Bowerman"
        },
        {
          "authorId": "2534159",
          "name": "K. Hatanpaa"
        },
        {
          "authorId": "2287953909",
          "name": "T. Patel"
        },
        {
          "authorId": "2279117143",
          "name": "B. Fei"
        },
        {
          "authorId": "2115791916",
          "name": "Matthew D. Lee"
        },
        {
          "authorId": "2295051437",
          "name": "Rajan Jain"
        },
        {
          "authorId": "2289716125",
          "name": "Richard J Bruce"
        },
        {
          "authorId": "2353472376",
          "name": "A. Madhuranthakam"
        },
        {
          "authorId": "2238078841",
          "name": "M. Pinho"
        },
        {
          "authorId": "2384976790",
          "name": "Joseph A. Maldjian"
        }
      ],
      "abstract": "Multi-center collaborations are crucial in developing robust and generalizable machine learning models in medical imaging. Traditional methods, such as centralized data sharing or federated learning (FL), face challenges, including privacy issues, communication burdens, and synchronization complexities. We present CATegorical and PHenotypic Image SyntHetic learnING (CATphishing), an alternative to FL using Latent Diffusion Models (LDM) to generate synthetic multi-contrast three-dimensional magnetic resonance imaging data for downstream tasks, eliminating the need for raw data sharing or iterative inter-site communication. Each institution trains an LDM to capture site-specific data distributions, producing synthetic samples aggregated at a central server. We evaluate CATphishing using data from 2491 patients across seven institutions for isocitrate dehydrogenase mutation classification and three-class tumor-type classification. CATphishing achieves accuracy comparable to centralized training and FL, with synthetic data exhibiting high fidelity. This method addresses privacy, scalability, and communication challenges, offering a promising alternative for collaborative artificial intelligence development in medical imaging. Methods for developing machine learning models in medical imaging across multi-centre collaborations face important challenges, including technical burdens and privacy issues. Here, the authors introduce CATegorical and PHenotypic Image SyntHetic learnING - CATphishing - as an alternative to Federated Learning to generate synthetic multi-contrast 3D MRI data for downstream tasks."
    },
    {
      "paperId": "e4d271bf923ff3955994ec8e47a08016cf29d692",
      "externalIds": {
        "PubMedCentral": "12610830",
        "DOI": "10.3390/s25216536",
        "CorpusId": 282455374,
        "PubMed": "41228758"
      },
      "corpusId": 282455374,
      "title": "Traffic Scene Semantic Segmentation Enhancement Based on Cylinder3D with Multi-Scale 3D Attention",
      "venue": "Italian National Conference on Sensors",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12610830, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2344414498",
          "name": "Yun Bai"
        },
        {
          "authorId": "2390112841",
          "name": "Xu Zhou"
        },
        {
          "authorId": "2305464583",
          "name": "Yuxuan Gong"
        },
        {
          "authorId": "2312303471",
          "name": "Yuanhao Huang"
        }
      ],
      "abstract": "With the rapid development of 3D sensor technology, point cloud semantic segmentation has found widespread applications in autonomous driving, remote sensing, mapping, and industrial manufacturing. However, outdoor traffic scenes present significant challenges: point clouds are inherently disordered, unevenly distributed, and unstructured. As a result, traditional point cloud semantic segmentation methods often suffer from low accuracy and unstable performance in complex tasks such as semantic segmentation and object detection. To address these limitations, this paper proposes an improved point cloud semantic segmentation method based on Cylinder3D. The proposed approach integrates the PointMamba and MS3DAM modules, which enhance the model\u2019s ability to capture global features while preserving local details, thereby improving adaptability and recognition across multiple feature scales. Furthermore, leveraging the linear computational complexity of Mamba enables the method to maintain high efficiency when processing large-scale point cloud data. In addition, incorporating the KAT module into the encoder improves the model\u2019s perceptual capacity and robustness in handling point clouds. Experimental results on the SemanticKITTI dataset demonstrate that the proposed method achieves a mean Intersection over Union (mIoU) of 64.98%, representing a 2.81% improvement over Cylinder3D, thereby confirming its superior segmentation accuracy compared with existing models."
    },
    {
      "paperId": "8af0097aa070860d0a18390dec8ae82c8ef5dc5d",
      "externalIds": {
        "DOI": "10.1109/APSIPAASC65261.2025.11249077",
        "CorpusId": 283356651
      },
      "corpusId": 283356651,
      "title": "Data Augmentation-Driven Segmentation of Ovarian Tumor Ultrasound Images Using Vision Mamba",
      "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/APSIPAASC65261.2025.11249077?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/APSIPAASC65261.2025.11249077, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2394990580",
          "name": "Thanh-Phuc Dao"
        },
        {
          "authorId": "2395001002",
          "name": "Huyen-Trang To"
        },
        {
          "authorId": "2261958639",
          "name": "Hoang-Son Bui"
        },
        {
          "authorId": "2308631261",
          "name": "Thi-Lan Le"
        }
      ],
      "abstract": "In this study, we investigate the impact of ultrasound-specific augmentation techniques on the segmentation of ovarian tumors in ultrasound images. These augmentations are designed to simulate common imaging artifacts such as shadowing, speckle noise, and contrast degradation, thereby enhancing model robustness and generalization. We integrate these augmentations into a training pipeline based on Vision Mamba, a state-space model known for its ability to model long-range dependencies with high computational efficiency. Experiments on the OTU2D dataset show notable improvements in segmentation performance. Compared to CNN-based baselines such as U-Net, our approach achieves higher Dice and Jaccard scores while maintaining a lower computational cost, making it suitable for real-time clinical deployment."
    },
    {
      "paperId": "71cdd62abcb85f8a6a0aaa0f04b85f13790d49db",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-19475",
        "ArXiv": "2510.19475",
        "DOI": "10.48550/arXiv.2510.19475",
        "CorpusId": 282272005
      },
      "corpusId": 282272005,
      "title": "PRGCN: A Graph Memory Network for Cross-Sequence Pattern Reuse in 3D Human Pose Estimation",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.19475, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2387131856",
          "name": "Zhuoyang Xie"
        },
        {
          "authorId": "2222603520",
          "name": "Yibo Zhao"
        },
        {
          "authorId": "2387848745",
          "name": "Hui Huang"
        },
        {
          "authorId": "2305122812",
          "name": "Riwei Wang"
        },
        {
          "authorId": "2237080707",
          "name": "Zan Gao"
        }
      ],
      "abstract": "Monocular 3D human pose estimation remains a fundamentally ill-posed inverse problem due to the inherent depth ambiguity in 2D-to-3D lifting. While contemporary video-based methods leverage temporal context to enhance spatial reasoning, they operate under a critical paradigm limitation: processing each sequence in isolation, thereby failing to exploit the strong structural regularities and repetitive motion patterns that pervade human movement across sequences. This work introduces the Pattern Reuse Graph Convolutional Network (PRGCN), a novel framework that formalizes pose estimation as a problem of pattern retrieval and adaptation. At its core, PRGCN features a graph memory bank that learns and stores a compact set of pose prototypes, encoded as relational graphs, which are dynamically retrieved via an attention mechanism to provide structured priors. These priors are adaptively fused with hard-coded anatomical constraints through a memory-driven graph convolution, ensuring geometrical plausibility. To underpin this retrieval process with robust spatiotemporal features, we design a dual-stream hybrid architecture that synergistically combines the linear-complexity, local temporal modeling of Mamba-based state-space models with the global relational capacity of self-attention. Extensive evaluations on Human3.6M and MPI-INF-3DHP benchmarks demonstrate that PRGCN establishes a new state-of-the-art, achieving an MPJPE of 37.1mm and 13.4mm, respectively, while exhibiting enhanced cross-domain generalization capability. Our work posits that the long-overlooked mechanism of cross-sequence pattern reuse is pivotal to advancing the field, shifting the paradigm from per-sequence optimization towards cumulative knowledge learning."
    },
    {
      "paperId": "5edfa6f6fb4c76f2c6f1ebb6f415f2160a991c01",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-19493",
        "ArXiv": "2510.19493",
        "DOI": "10.18653/v1/2025.babylm-main.10",
        "CorpusId": 282272667
      },
      "corpusId": 282272667,
      "title": "What is the Best Sequence Length for BABYLM?",
      "venue": "Proceedings of the First BabyLM Workshop",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.19493, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2370929556",
          "name": "Suchir Salhan"
        },
        {
          "authorId": "2266941716",
          "name": "Richard Diehl Martinez"
        },
        {
          "authorId": "2239085253",
          "name": "Z\u00e9bulon Goriely"
        },
        {
          "authorId": "2353953643",
          "name": "Paula Buttery"
        }
      ],
      "abstract": "Transformer language models typically operate with a fixed-length context window, which has grown in step with large-scale pretraining datasets. In the BabyLM Challenge, however, many past submissions have defaulted to using much shorter sequence lengths. We examine the impact of sequence length on BabyLM pretraining, to answer the simple question: what sequence length should we be using when training Baby LMs? Using 100M-word training data and fixed compute budgets, we compare 125M-parameter Mamba and OPT models, finding that although longer is often better, the optimal length depends on both task and architecture. Shorter sequences are sufficient for grammatical generalization tasks whereas longer contexts benefit morphological analogical reasoning tasks."
    },
    {
      "paperId": "39b51797fabc661dedf7c753f2e20474927315bf",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-19338",
        "ArXiv": "2510.19338",
        "DOI": "10.48550/arXiv.2510.19338",
        "CorpusId": 282272452
      },
      "corpusId": 282272452,
      "title": "Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.19338, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2349238786",
          "name": "Ling Team"
        },
        {
          "authorId": "2392378902",
          "name": "Bin Han"
        },
        {
          "authorId": "2110459169",
          "name": "Caizhi Tang"
        },
        {
          "authorId": "2279878742",
          "name": "Chen Liang"
        },
        {
          "authorId": "2389578537",
          "name": "Donghao Zhang"
        },
        {
          "authorId": "2387767408",
          "name": "Fan Yuan"
        },
        {
          "authorId": "2367751198",
          "name": "Feng Zhu"
        },
        {
          "authorId": "2391756181",
          "name": "Jie Gao"
        },
        {
          "authorId": "2355032279",
          "name": "Jingyu Hu"
        },
        {
          "authorId": "2373462520",
          "name": "Longfei Li"
        },
        {
          "authorId": "2387174606",
          "name": "Meng Li"
        },
        {
          "authorId": "2387032486",
          "name": "Mingyang Zhang"
        },
        {
          "authorId": "2354616112",
          "name": "Peijie Jiang"
        },
        {
          "authorId": "2386993934",
          "name": "Peng Jiao"
        },
        {
          "authorId": "2364852293",
          "name": "Qian Zhao"
        },
        {
          "authorId": "2387851985",
          "name": "Qingyuan Yang"
        },
        {
          "authorId": "2387633447",
          "name": "Wenbo Shen"
        },
        {
          "authorId": "2198884407",
          "name": "Xin-Yao Yang"
        },
        {
          "authorId": "2267314319",
          "name": "Yalin Zhang"
        },
        {
          "authorId": "2109899183",
          "name": "Yankun Ren"
        },
        {
          "authorId": "2278398317",
          "name": "Yao Zhao"
        },
        {
          "authorId": "2387741539",
          "name": "Yibo Cao"
        },
        {
          "authorId": "2387119814",
          "name": "Yixuan Sun"
        },
        {
          "authorId": "2365333585",
          "name": "Yue Zhang"
        },
        {
          "authorId": "2296250274",
          "name": "Yu Fang"
        },
        {
          "authorId": "2359960984",
          "name": "Zi-Ang Lin"
        },
        {
          "authorId": "2395854276",
          "name": "Zixuan Cheng"
        },
        {
          "authorId": "2238908467",
          "name": "Jun Zhou"
        }
      ],
      "abstract": "In this technical report, we present the Ring-linear model series, specifically including Ring-mini-linear-2.0 and Ring-flash-linear-2.0. Ring-mini-linear-2.0 comprises 16B parameters and 957M activations, while Ring-flash-linear-2.0 contains 104B parameters and 6.1B activations. Both models adopt a hybrid architecture that effectively integrates linear attention and softmax attention, significantly reducing I/O and computational overhead in long-context inference scenarios. Compared to a 32 billion parameter dense model, this series reduces inference cost to 1/10, and compared to the original Ring series, the cost is also reduced by over 50%. Furthermore, through systematic exploration of the ratio between different attention mechanisms in the hybrid architecture, we have identified the currently optimal model structure. Additionally, by leveraging our self-developed high-performance FP8 operator library-linghe, overall training efficiency has been improved by 50%. Benefiting from the high alignment between the training and inference engine operators, the models can undergo long-term, stable, and highly efficient optimization during the reinforcement learning phase, consistently maintaining SOTA performance across multiple challenging complex reasoning benchmarks."
    },
    {
      "paperId": "dbc43a73ff5ae4e803796fe3e842a8b18eb09f8d",
      "externalIds": {
        "ArXiv": "2510.19174",
        "CorpusId": 282272457
      },
      "corpusId": 282272457,
      "title": "Auditory Attention Decoding from Ear-EEG Signals: A Dataset with Dynamic Attention Switching and Rigorous Cross-Validation",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.19174, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2275173382",
          "name": "Yuanming Zhang"
        },
        {
          "authorId": "2337084764",
          "name": "Zeyan Song"
        },
        {
          "authorId": "2275773475",
          "name": "Jing Lu"
        },
        {
          "authorId": "2275608375",
          "name": "Fei Chen"
        },
        {
          "authorId": "2275898358",
          "name": "Zhibin Lin"
        }
      ],
      "abstract": "Recent promising results in auditory attention decoding (AAD) using scalp electroencephalography (EEG) have motivated the exploration of cEEGrid, a flexible and portable ear-EEG system. While prior cEEGrid-based studies have confirmed the feasibility of AAD, they often neglect the dynamic nature of attentional states in real-world contexts. To address this gap, a novel cEEGrid dataset featuring three concurrent speakers distributed across three of five distinct spatial locations is introduced. The novel dataset is designed to probe attentional tracking and switching in realistic scenarios. Nested leave-one-out validation-an approach more rigorous than conventional single-loop leave-one-out validation-is employed to reduce biases stemming from EEG's intricate temporal dynamics. Four rule-based models are evaluated: Wiener filter (WF), canonical component analysis (CCA), common spatial pattern (CSP) and Riemannian Geometry-based classifier (RGC). With a 30-second decision window, WF and CCA models achieve decoding accuracies of 41.5% and 41.4%, respectively, while CSP and RGC models yield 37.8% and 37.6% accuracies using a 10-second window. Notably, both WF and CCA successfully track attentional state switches across all experimental tasks. Additionally, higher decoding accuracies are observed for electrodes positioned at the upper cEEGrid layout and near the listener's right ear. These findings underscore the utility of dynamic, ecologically valid paradigms and rigorous validation in advancing AAD research with cEEGrid."
    },
    {
      "paperId": "1a4d5736168080c0ca8d4a4f559fa9030faf1246",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-19315",
        "ArXiv": "2510.19315",
        "DOI": "10.48550/arXiv.2510.19315",
        "CorpusId": 282272142
      },
      "corpusId": 282272142,
      "title": "Transformers are Inherently Succinct",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.19315, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2052391267",
          "name": "Pascal Bergstr\u00e4\u00dfer"
        },
        {
          "authorId": "2326993387",
          "name": "Ryan Cotterell"
        },
        {
          "authorId": "2387672587",
          "name": "Anthony W. Lin"
        }
      ],
      "abstract": "We propose succinctness as a measure of the expressive power of a transformer in describing a concept. To this end, we prove that transformers are highly expressive in that they can represent formal languages substantially more succinctly than standard representations of formal languages like finite automata and Linear Temporal Logic (LTL) formulas. As a by-product of this expressivity, we show that verifying properties of transformers is provably intractable (i.e. EXPSPACE-complete)."
    },
    {
      "paperId": "4e951d68a0164fc8b9242a7f04dc5a89b47539bf",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-19144",
        "ArXiv": "2510.19144",
        "DOI": "10.48550/arXiv.2510.19144",
        "CorpusId": 282271936
      },
      "corpusId": 282271936,
      "title": "Tibetan Language and AI: A Comprehensive Survey of Resources, Methods and Challenges",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.19144, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2275607695",
          "name": "Cheng Huang"
        },
        {
          "authorId": "4577369",
          "name": "N. Tashi"
        },
        {
          "authorId": "2340191210",
          "name": "Fan Gao"
        },
        {
          "authorId": "2351808095",
          "name": "Yutong Liu"
        },
        {
          "authorId": "2276668271",
          "name": "Jiahao Li"
        },
        {
          "authorId": "2387881482",
          "name": "Hao Tian"
        },
        {
          "authorId": "2387117356",
          "name": "Siyang Jiang"
        },
        {
          "authorId": "1395602706",
          "name": "Thupten Tsering"
        },
        {
          "authorId": "2350513105",
          "name": "Ma-bao Ban"
        },
        {
          "authorId": "2350514280",
          "name": "Renzeg Duojie"
        },
        {
          "authorId": "2338177409",
          "name": "Gadeng Luosang"
        },
        {
          "authorId": "2195041814",
          "name": "Rinchen Dongrub"
        },
        {
          "authorId": "2338177725",
          "name": "Dorje Tashi"
        },
        {
          "authorId": "2155115538",
          "name": "Jin Zhang"
        },
        {
          "authorId": "2350493590",
          "name": "Xiao Feng"
        },
        {
          "authorId": "2256769995",
          "name": "Hao Wang"
        },
        {
          "authorId": "2310480698",
          "name": "Jie Tang"
        },
        {
          "authorId": "2391006094",
          "name": "Guojie Tang"
        },
        {
          "authorId": "50141063",
          "name": "Xiangxiang Wang"
        },
        {
          "authorId": "2247850320",
          "name": "Jia Zhang"
        },
        {
          "authorId": "2294376453",
          "name": "Tsengdar J. Lee"
        },
        {
          "authorId": "2241970724",
          "name": "Yongbin Yu"
        }
      ],
      "abstract": "Tibetan, one of the major low-resource languages in Asia, presents unique linguistic and sociocultural characteristics that pose both challenges and opportunities for AI research. Despite increasing interest in developing AI systems for underrepresented languages, Tibetan has received limited attention due to a lack of accessible data resources, standardized benchmarks, and dedicated tools. This paper provides a comprehensive survey of the current state of Tibetan AI in the AI domain, covering textual and speech data resources, NLP tasks, machine translation, speech recognition, and recent developments in LLMs. We systematically categorize existing datasets and tools, evaluate methods used across different tasks, and compare performance where possible. We also identify persistent bottlenecks such as data sparsity, orthographic variation, and the lack of unified evaluation metrics. Additionally, we discuss the potential of cross-lingual transfer, multi-modal learning, and community-driven resource creation. This survey aims to serve as a foundational reference for future work on Tibetan AI research and encourages collaborative efforts to build an inclusive and sustainable AI ecosystem for low-resource languages."
    },
    {
      "paperId": "57089b16658dd42829dc732b489be0bb5d624dfa",
      "externalIds": {
        "DOI": "10.3389/frwa.2025.1638839",
        "CorpusId": 282351532
      },
      "corpusId": 282351532,
      "title": "SMamba-KAN: an advanced temporal-nonlinear model for precise water level prediction",
      "venue": "Frontiers in Water",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3389/frwa.2025.1638839?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3389/frwa.2025.1638839, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2387872161",
          "name": "Xiangrui Yan"
        },
        {
          "authorId": "2388755299",
          "name": "Huijuan Zhao"
        },
        {
          "authorId": "2390446562",
          "name": "Ruyan Zhou"
        }
      ],
      "abstract": "\n \n River water levels are influenced by a combination of meteorological and environmental factors. In recent years, with the widespread adoption of Transformer architectures in time series modeling, numerous structural variants have emerged, including Mamba based on structured state space models (SSM) and iTransformer, which employs a Variate Token Embedding strategy. Meanwhile, traditional multilayer perceptron (MLP) structures are increasingly being replaced by Kolmogorov\u2013Arnold Networks (KAN) to enhance non-linear modeling capabilities.\n \n \n \n Building upon the SMamba variant of Mamba, this study introduces a KAN module to construct a hybrid model named SMamba-KAN. The model is applied to multivariate hydrological and meteorological data from several stations in the Yangtze River Basin to forecast water levels at the Datong hydrological station over the next 15 days.\n \n \n \n Experimental results demonstrate that the proposed model achieves excellent performance across multiple evaluation metrics, with MSE, RMSE, MAE, and MAPE reaching 0.013, 0.117, 0.099, and 2.095%, respectively. Quantitative performance analysis demonstrates that SMamba-KAN exhibits substantial error reduction compared with the original Mamba, decreasing prediction errors by over 90% in MSE.\n \n \n \n Relative to its direct baseline SMamba, the incorporation of the KAN module facilitates a significant enhancement in predictive accuracy, further lowering MSE by 54% while maintaining consistent performance stability in MAPE metrics. These results verify the model's high accuracy and strong generalization ability in multivariate water level prediction tasks.\n"
    },
    {
      "paperId": "bfa0a23d9677c3474131faa2e05ad2d0789abc32",
      "externalIds": {
        "ArXiv": "2510.20853",
        "DBLP": "journals/corr/abs-2510-20853",
        "DOI": "10.48550/arXiv.2510.20853",
        "CorpusId": 282384572
      },
      "corpusId": 282384572,
      "title": "Beyond Hearing: Learning Task-agnostic ExG Representations from Earphones via Physiology-informed Tokenization",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.20853, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2184033278",
          "name": "Hyungjun Yoon"
        },
        {
          "authorId": "2261676297",
          "name": "Seungjoo Lee"
        },
        {
          "authorId": "2387821897",
          "name": "Yu Yvonne Wu"
        },
        {
          "authorId": "2375082891",
          "name": "Xiaomeng Chen"
        },
        {
          "authorId": "2387915388",
          "name": "Taiting Lu"
        },
        {
          "authorId": "2387814774",
          "name": "Freddy Yifei Liu"
        },
        {
          "authorId": "2258877903",
          "name": "Taeckyung Lee"
        },
        {
          "authorId": "104531945",
          "name": "Hyeong-Tae Cha"
        },
        {
          "authorId": "2387986111",
          "name": "Haochen Zhao"
        },
        {
          "authorId": "2387812318",
          "name": "Gaoteng Zhao"
        },
        {
          "authorId": "2294829533",
          "name": "Sung-Ju Lee"
        },
        {
          "authorId": "2387789101",
          "name": "Cecilia Mascolo"
        },
        {
          "authorId": "2305577915",
          "name": "Dongyao Chen"
        },
        {
          "authorId": "2374301659",
          "name": "Lili Qiu"
        }
      ],
      "abstract": "Electrophysiological (ExG) signals offer valuable insights into human physiology, yet building foundation models that generalize across everyday tasks remains challenging due to two key limitations: (i) insufficient data diversity, as most ExG recordings are collected in controlled labs with bulky, expensive devices; and (ii) task-specific model designs that require tailored processing (i.e., targeted frequency filters) and architectures, which limit generalization across tasks. To address these challenges, we introduce an approach for scalable, task-agnostic ExG monitoring in the wild. We collected 50 hours of unobtrusive free-living ExG data with an earphone-based hardware prototype to narrow the data diversity gap. At the core of our approach is Physiology-informed Multi-band Tokenization (PiMT), which decomposes ExG signals into 12 physiology-informed tokens, followed by a reconstruction task to learn robust representations. This enables adaptive feature recognition across the full frequency spectrum while capturing task-relevant information. Experiments on our new DailySense dataset-the first to enable ExG-based analysis across five human senses-together with four public ExG benchmarks, demonstrate that PiMT consistently outperforms state-of-the-art methods across diverse tasks."
    },
    {
      "paperId": "5465413a781d49d754aa3d0d3b5e7d2fc5a4d264",
      "externalIds": {
        "DOI": "10.1109/APSIPAASC65261.2025.11249139",
        "CorpusId": 283355542
      },
      "corpusId": 283355542,
      "title": "Speaker Localization in Classroom Environments Using GCC-PHAT Features and Mamba State Space Models with Ad-Hoc Microphone Arrays",
      "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/APSIPAASC65261.2025.11249139?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/APSIPAASC65261.2025.11249139, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2313970707",
          "name": "Rashed Iqbal"
        },
        {
          "authorId": "2313974173",
          "name": "Christian Ritz"
        },
        {
          "authorId": "2395307087",
          "name": "Jack Yang"
        },
        {
          "authorId": "2313976842",
          "name": "Sarah K. Howard"
        }
      ],
      "abstract": "Accurately understanding classroom interactions is essential for assessing teaching quality and student engagement. This study presents a novel audio-only source localization (SSL) framework designed for locating speakers within classroom environments using two microphone arrays located at unknown locations in the room. The proposed method integrates Generalized Cross-Correlation with Phase Transform (GCCPHAT) for time difference of arrival (TDOA) feature extraction and the Mamba State Space Model (SSM) for sequence modelling to estimate speaker locations. A dataset of reverberant speech recordings corresponding to three simulated classroom environments of different size and reverberation levels containing a teacher and multiple students and recorded with two different microphone setups, including First Order Ambisonic (FOA) Bformat arrays and 6-channel circular arrays. Experimental results demonstrate that our approach significantly outperforms conventional baselines in both Mean Angular Error (MAE) and Mean Distance Error (MDE)."
    },
    {
      "paperId": "885c5d4cc1abff24ff782d823adb1b95f5b6c0c6",
      "externalIds": {
        "DOI": "10.1109/APSIPAASC65261.2025.11249214",
        "CorpusId": 283358367
      },
      "corpusId": 283358367,
      "title": "And Regional Selective Mixup",
      "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/APSIPAASC65261.2025.11249214?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/APSIPAASC65261.2025.11249214, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2394972679",
          "name": "Yu-Chen Lin"
        },
        {
          "authorId": "2395017321",
          "name": "Yi-Jing Chen"
        },
        {
          "authorId": "2400483",
          "name": "Chih-Chang Yu"
        },
        {
          "authorId": "38239715",
          "name": "Hsu-Yung Cheng"
        }
      ],
      "abstract": "Current studies on micro-expression recognition still achieve limited performance, typically yielding less than 50% accuracy. Although utilizing recent powerful models, such as Mamba, improves performance, they usually require enormous data for training. Therefore, this study proposes a regional selective mixup (RSMix) strategy that relies on facial Action Unit (AU) information to augment the dataset. The proposed approach systematically fuses facial images from micro-expressiongenerating regions with corresponding expressionless regions to synthesize new micro-expression samples. RSMix ensures that salient features from micro-expression-generating regions are preserved within these synthetic samples, facilitating the model's capacity to focus on essential visual cues for distinguishing different micro-expressions. We applied the VideoMamba model as the backbone to test the efficiency of RSMix and other augmentation methods. Experimental results on the $\\operatorname{CAS}(\\text{ME})^{3}$ dataset demonstrate that with RSMix, the VideoMamba-Ti model achieves an accuracy improvement of Unweighted F1-score (UF1) of 0.0138 and Unweighted average recall (UAR) of 0.0048 compared to baseline configurations without augmentation. When flip augmentation is combined with the RSMix, the VideoMambaS model achieves UF1 of 0.5040 and UAR of 0.4941, superior to the Mamba-based approach. These results prove that the RSMix approach effectively enhances data diversity, which also helps VideoMamba for the micro-expression recognition task."
    },
    {
      "paperId": "1bae0cca47f35460aaf8c24163c38ab801d441a5",
      "externalIds": {
        "DBLP": "journals/sivp/GuoTMHS25",
        "DOI": "10.1007/s11760-025-04870-6",
        "CorpusId": 282293213
      },
      "corpusId": 282293213,
      "title": "Dual-branch network with adaptive rational nonlinear function for image deblurring",
      "venue": "Signal, Image and Video Processing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11760-025-04870-6?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11760-025-04870-6, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2387210391",
          "name": "Wenjing Guo"
        },
        {
          "authorId": "2260607661",
          "name": "Jieqing Tan"
        },
        {
          "authorId": "2387066254",
          "name": "Linsong Mao"
        },
        {
          "authorId": "2372279603",
          "name": "Naimang Hu"
        },
        {
          "authorId": "2387020321",
          "name": "Shijie Sun"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "abaaed9b0069053fd81dd0a3f89282f4758211b1",
      "externalIds": {
        "ArXiv": "2510.19003",
        "CorpusId": 282272205
      },
      "corpusId": 282272205,
      "title": "$\\Delta$t-Mamba3D: A Time-Aware Spatio-Temporal State-Space Model for Breast Cancer Risk Prediction",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.19003, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2295091010",
          "name": "Zhengbo Zhou"
        },
        {
          "authorId": "6118685",
          "name": "D. Arefan"
        },
        {
          "authorId": "2302983772",
          "name": "Margarita L. Zuley"
        },
        {
          "authorId": "2284184673",
          "name": "Shandong Wu"
        }
      ],
      "abstract": "Longitudinal analysis of sequential radiological images is hampered by a fundamental data challenge: how to effectively model a sequence of high-resolution images captured at irregular time intervals. This data structure contains indispensable spatial and temporal cues that current methods fail to fully exploit. Models often compromise by either collapsing spatial information into vectors or applying spatio-temporal models that are computationally inefficient and incompatible with non-uniform time steps. We address this challenge with Time-Aware $\\Delta$t-Mamba3D, a novel state-space architecture adapted for longitudinal medical imaging. Our model simultaneously encodes irregular inter-visit intervals and rich spatio-temporal context while remaining computationally efficient. Its core innovation is a continuous-time selective scanning mechanism that explicitly integrates the true time difference between exams into its state transitions. This is complemented by a multi-scale 3D neighborhood fusion module that robustly captures spatio-temporal relationships. In a comprehensive breast cancer risk prediction benchmark using sequential screening mammogram exams, our model shows superior performance, improving the validation c-index by 2-5 percentage points and achieving higher 1-5 year AUC scores compared to established variants of recurrent, transformer, and state-space models. Thanks to its linear complexity, the model can efficiently process long and complex patient screening histories of mammograms, forming a new framework for longitudinal image analysis."
    },
    {
      "paperId": "4341a2f054f4e812404943d8ff79a76140ea55a5",
      "externalIds": {
        "DOI": "10.3390/fi17100480",
        "CorpusId": 282291443
      },
      "corpusId": 282291443,
      "title": "MambaNet0: Mamba-Based Sustainable Cloud Resource Prediction Framework Towards Net Zero Goals",
      "venue": "Future Internet",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/fi17100480?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/fi17100480, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2387087068",
          "name": "Thananont Chevaphatrakul"
        },
        {
          "authorId": "2346299943",
          "name": "Han Wang"
        },
        {
          "authorId": "31043248",
          "name": "S. Gill"
        }
      ],
      "abstract": "With the ever-growing reliance on cloud computing, efficient resource allocation is crucial for maximising the effective use of provisioned resources from cloud service providers. Proactive resource management is therefore critical for minimising costs and striving for net zero emission goals. One of the most promising methods involves the use of Artificial Intelligence (AI) techniques to analyse and predict resource demand, such as cloud CPU utilisation. This paper presents MambaNet0, a Mamba-based cloud resource prediction framework. The model is implemented on Google\u2019s Vertex AI workbench and uses the real-world Bitbrains Grid Workload Archive-T-12 dataset, which contains the resource usage metrics of 1750 virtual machines. The Mamba model\u2019s performance is then evaluated against established baseline models, including Autoregressive Integrated Moving Average (ARIMA), Long Short-Term Memory (LSTM), and Amazon Chronos, to demonstrate its potential for accurate prediction of CPU utilisation. The MambaNet0 model achieved a 29% improvement in Symmetric Mean Absolute Percentage Error (SMAPE) compared to the best-performing baseline Amazon Chronos. These findings reinforce the Mamba model\u2019s ability to forecast accurate CPU utilisation, highlighting its potential for optimising cloud resource allocation in contribution to net zero goals."
    },
    {
      "paperId": "2ee77d6eb081a507fd05482ec5de7fddc1f17688",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-18239",
        "ArXiv": "2510.18239",
        "DOI": "10.48550/arXiv.2510.18239",
        "CorpusId": 282245943
      },
      "corpusId": 282245943,
      "title": "LIME: Link-based user-item Interaction Modeling with decoupled xor attention for Efficient test time scaling",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.18239, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2386942665",
          "name": "Yunjiang Jiang"
        },
        {
          "authorId": "2387770132",
          "name": "Ayush Agarwal"
        },
        {
          "authorId": "2387121574",
          "name": "Yang Liu"
        },
        {
          "authorId": "2386817770",
          "name": "Bi Xue"
        }
      ],
      "abstract": "Scaling large recommendation systems requires advancing three major frontiers: processing longer user histories, expanding candidate sets, and increasing model capacity. While promising, transformers'computational cost scales quadratically with the user sequence length and linearly with the number of candidates. This trade-off makes it prohibitively expensive to expand candidate sets or increase sequence length at inference, despite the significant performance improvements. We introduce \\textbf{LIME}, a novel architecture that resolves this trade-off. Through two key innovations, LIME fundamentally reduces computational complexity. First, low-rank ``link embeddings\"enable pre-computation of attention weights by decoupling user and candidate interactions, making the inference cost nearly independent of candidate set size. Second, a linear attention mechanism, \\textbf{LIME-XOR}, reduces the complexity with respect to user sequence length from quadratic ($O(N^2)$) to linear ($O(N)$). Experiments on public and industrial datasets show LIME achieves near-parity with state-of-the-art transformers but with a 10$\\times$ inference speedup on large candidate sets or long sequence lengths. When tested on a major recommendation platform, LIME improved user engagement while maintaining minimal inference costs with respect to candidate set size and user history length, establishing a new paradigm for efficient and expressive recommendation systems."
    },
    {
      "paperId": "2b66295b8c11f2894e48cc7e820de6b51304e1b8",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-18245",
        "ArXiv": "2510.18245",
        "DOI": "10.48550/arXiv.2510.18245",
        "CorpusId": 282246281
      },
      "corpusId": 282246281,
      "title": "Scaling Laws Meet Model Architecture: Toward Inference-Efficient LLMs",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.18245, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2342276462",
          "name": "Song Bian"
        },
        {
          "authorId": "2300996598",
          "name": "Tao Yu"
        },
        {
          "authorId": "2257307243",
          "name": "Shivaram Venkataraman"
        },
        {
          "authorId": "2348402852",
          "name": "Youngsuk Park"
        }
      ],
      "abstract": "Scaling the number of parameters and the size of training data has proven to be an effective strategy for improving large language model (LLM) performance. Yet, as these models grow increasingly powerful and widely deployed, the cost of inference has become a pressing concern. Despite its importance, the trade-off between model accuracy and inference efficiency remains underexplored. In this work, we examine how key architectural factors, hidden size, the allocation of parameters between MLP and attention (mlp-to-attention ratio), and grouped-query attention (GQA), influence both inference cost and accuracy. We introduce a conditional scaling law that augments the Chinchilla framework with architectural information, along with a search framework for identifying architectures that are simultaneously inference-efficient and accurate. To validate our approach, we train more than 200 models spanning 80M to 3B parameters and 8B to 100B training tokens, and fit the proposed conditional scaling law. Our results show that the conditional scaling law reliably predicts optimal architectural choices and that the resulting models outperform existing open-source baselines. Under the same training budget, optimized architectures achieve up to 2.1% higher accuracy and 42% greater inference throughput compared to LLaMA-3.2."
    },
    {
      "paperId": "ffcdf49da95feb4c6f7f8693b6ee922347b02ef0",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-18684",
        "ArXiv": "2510.18684",
        "DOI": "10.48550/arXiv.2510.18684",
        "CorpusId": 282246833
      },
      "corpusId": 282246833,
      "title": "MLMA: Towards Multilingual ASR With Mamba-based Architectures",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.18684, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "1986698073",
          "name": "Mohamed Nabih Ali"
        },
        {
          "authorId": "2127333806",
          "name": "Daniele Falavigna"
        },
        {
          "authorId": "2020562",
          "name": "A. Brutti"
        }
      ],
      "abstract": "Multilingual automatic speech recognition (ASR) remains a challenging task, especially when balancing performance across high- and low-resource languages. Recent advances in sequence modeling suggest that architectures beyond Transformers may offer better scalability and efficiency. In this work, we introduce MLMA (Multilingual Language Modeling with Mamba for ASR), a new approach that leverages the Mamba architecture -- an efficient state-space model optimized for long-context sequence processing -- for multilingual ASR. Using Mamba, MLMA implicitly incorporates language-aware conditioning and shared representations to support robust recognition across diverse languages. Experiments on standard multilingual benchmarks show that MLMA achieves competitive performance compared to Transformer-based architectures. These results highlight Mamba's potential as a strong backbone for scalable, efficient, and accurate multilingual speech recognition."
    },
    {
      "paperId": "fe902ace4c9c5eb21ca5096f709a126c25df7eb4",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-18467",
        "ArXiv": "2510.18467",
        "DOI": "10.48550/arXiv.2510.18467",
        "CorpusId": 282246492
      },
      "corpusId": 282246492,
      "title": "Simple and Efficient Heterogeneous Temporal Graph Neural Network",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.18467, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2373446172",
          "name": "Yili Wang"
        },
        {
          "authorId": "2352197754",
          "name": "Tairan Huang"
        },
        {
          "authorId": "2296957615",
          "name": "Changlong He"
        },
        {
          "authorId": "2167223624",
          "name": "Qiutong Li"
        },
        {
          "authorId": "2352209284",
          "name": "Jianliang Gao"
        }
      ],
      "abstract": "Heterogeneous temporal graphs (HTGs) are ubiquitous data structures in the real world. Recently, to enhance representation learning on HTGs, numerous attention-based neural networks have been proposed. Despite these successes, existing methods rely on a decoupled temporal and spatial learning paradigm, which weakens interactions of spatio-temporal information and leads to a high model complexity. To bridge this gap, we propose a novel learning paradigm for HTGs called Simple and Efficient Heterogeneous Temporal Graph N}eural Network (SE-HTGNN). Specifically, we innovatively integrate temporal modeling into spatial learning via a novel dynamic attention mechanism, which retains attention information from historical graph snapshots to guide subsequent attention computation, thereby improving the overall discriminative representations learning of HTGs. Additionally, to comprehensively and adaptively understand HTGs, we leverage large language models to prompt SE-HTGNN, enabling the model to capture the implicit properties of node types as prior knowledge. Extensive experiments demonstrate that SE-HTGNN achieves up to 10x speed-up over the state-of-the-art and latest baseline while maintaining the best forecasting accuracy."
    },
    {
      "paperId": "81cbff58a84444b501cb5c028af985543529a197",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-18775",
        "ArXiv": "2510.18775",
        "DOI": "10.48550/arXiv.2510.18775",
        "CorpusId": 282246540
      },
      "corpusId": 282246540,
      "title": "UltraGen: High-Resolution Video Generation with Hierarchical Attention",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 5,
      "influentialCitationCount": 1,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.18775, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2238209063",
          "name": "Teng Hu"
        },
        {
          "authorId": "2306831155",
          "name": "Jiangning Zhang"
        },
        {
          "authorId": "2385316527",
          "name": "Zihan Su"
        },
        {
          "authorId": "2273556840",
          "name": "Ran Yi"
        }
      ],
      "abstract": "Recent advances in video generation have made it possible to produce visually compelling videos, with wide-ranging applications in content creation, entertainment, and virtual reality. However, most existing diffusion transformer based video generation models are limited to low-resolution outputs (<=720P) due to the quadratic computational complexity of the attention mechanism with respect to the output width and height. This computational bottleneck makes native high-resolution video generation (1080P/2K/4K) impractical for both training and inference. To address this challenge, we present UltraGen, a novel video generation framework that enables i) efficient and ii) end-to-end native high-resolution video synthesis. Specifically, UltraGen features a hierarchical dual-branch attention architecture based on global-local attention decomposition, which decouples full attention into a local attention branch for high-fidelity regional content and a global attention branch for overall semantic consistency. We further propose a spatially compressed global modeling strategy to efficiently learn global dependencies, and a hierarchical cross-window local attention mechanism to reduce computational costs while enhancing information flow across different local windows. Extensive experiments demonstrate that UltraGen can effectively scale pre-trained low-resolution video models to 1080P and even 4K resolution for the first time, outperforming existing state-of-the-art methods and super-resolution based two-stage pipelines in both qualitative and quantitative evaluations."
    },
    {
      "paperId": "5c08528942de10e269b8aad53e1ee2e66ba62173",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-18313",
        "ArXiv": "2510.18313",
        "DOI": "10.48550/arXiv.2510.18313",
        "CorpusId": 282245900
      },
      "corpusId": 282245900,
      "title": "OmniNWM: Omniscient Driving Navigation World Models",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 1,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.18313, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2381415958",
          "name": "Bohan Li"
        },
        {
          "authorId": "2288375599",
          "name": "Zhuang Ma"
        },
        {
          "authorId": "2268680501",
          "name": "Dalong Du"
        },
        {
          "authorId": "2318349113",
          "name": "Bao Peng"
        },
        {
          "authorId": "2266466707",
          "name": "Zhujin Liang"
        },
        {
          "authorId": "2288057759",
          "name": "Zhenqiang Liu"
        },
        {
          "authorId": "2387122163",
          "name": "Chao Ma"
        },
        {
          "authorId": "2387079028",
          "name": "Yueming Jin"
        },
        {
          "authorId": "2334565430",
          "name": "Hao Zhao"
        },
        {
          "authorId": "2247938835",
          "name": "Wenjun Zeng"
        },
        {
          "authorId": "2365147330",
          "name": "Xin Jin"
        }
      ],
      "abstract": "Autonomous driving world models are expected to work effectively across three core dimensions: state, action, and reward. Existing models, however, are typically restricted to limited state modalities, short video sequences, imprecise action control, and a lack of reward awareness. In this paper, we introduce OmniNWM, an omniscient panoramic navigation world model that addresses all three dimensions within a unified framework. For state, OmniNWM jointly generates panoramic videos of RGB, semantics, metric depth, and 3D occupancy. A flexible forcing strategy enables high-quality long-horizon auto-regressive generation. For action, we introduce a normalized panoramic Plucker ray-map representation that encodes input trajectories into pixel-level signals, enabling highly precise and generalizable control over panoramic video generation. Regarding reward, we move beyond learning reward functions with external image-based models: instead, we leverage the generated 3D occupancy to directly define rule-based dense rewards for driving compliance and safety. Extensive experiments demonstrate that OmniNWM achieves state-of-the-art performance in video generation, control accuracy, and long-horizon stability, while providing a reliable closed-loop evaluation framework through occupancy-grounded rewards. Project page is available at https://arlo0o.github.io/OmniNWM/."
    },
    {
      "paperId": "c5ee8fb705077e28131eb3379bfc8a0c6017300d",
      "externalIds": {
        "PubMedCentral": "12539697",
        "DOI": "10.1371/journal.pone.0334412",
        "CorpusId": 282268149,
        "PubMed": "41118441"
      },
      "corpusId": 282268149,
      "title": "K-M LLM-pro: Physics-guided cross-modal adaptation for fine-grained spatiotemporal trajectory classification",
      "venue": "PLoS ONE",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12539697, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2266507054",
          "name": "Chenglong Ge"
        },
        {
          "authorId": "2303429346",
          "name": "Jing Zhang"
        },
        {
          "authorId": "2381233894",
          "name": "Jianping Du"
        },
        {
          "authorId": "2115389883",
          "name": "Jiacheng Yao"
        },
        {
          "authorId": "2366152726",
          "name": "Tianhe Yang"
        },
        {
          "authorId": "2303409887",
          "name": "Xuebin Wang"
        },
        {
          "authorId": "2219670521",
          "name": "Linyu Wang"
        }
      ],
      "abstract": "Spatiotemporal trajectory classification is essential for intelligent perception systems but faces challenges including weak separability of dynamic features, representation collapse under limited samples, and heterogeneous conflicts in multimodal data. To address these issues, we propose K-M LLM-pro, a physics-guided cross-modal adaptation framework that integrates statistical mechanics with large language models (LLMs) to improve trajectory understanding. Our approach incorporates: (1) physics-informed prompt engineering based on Kramers-Moyal coefficients, embedding physical constraints via reproducing kernel Hilbert space projection; (2) a dynamic patching optimization mechanism combining variance maximization and Lyapunov stability criteria for unified modeling of heterogeneous trajectories; and (3) dual spatiotemporal adapters with a parameter-efficient expansion strategy, injecting domain knowledge while optimizing only 3.8% of new parameters. Experimental results on public datasets such as Geolife and AIS show that K-M LLM-pro outperforms state-of-the-art models in classification accuracy, demonstrating strong performance even in few-shot scenarios with only 1% of training data. To our knowledge, this is the first work to integrate K-M coefficients as interpretable statistical priors into LLMs, offering a lightweight and effective solution for modeling complex spatiotemporal dynamics."
    },
    {
      "paperId": "71866aa5a5d42d681bbaeadfaeca1c69fab19287",
      "externalIds": {
        "PubMedCentral": "12540823",
        "DOI": "10.1038/s41598-025-18062-2",
        "CorpusId": 282270020,
        "PubMed": "41120350"
      },
      "corpusId": 282270020,
      "title": "A lightweight network for brain MRI segmentation",
      "venue": "Scientific Reports",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12540823, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2989440",
          "name": "Pubali Chatterjee"
        },
        {
          "authorId": "2273045826",
          "name": "Amlan Chakrabarti"
        },
        {
          "authorId": "134244122",
          "name": "Kaushik Das Sharma"
        }
      ],
      "abstract": "Brain MRI segmentation plays a crucial role in medical imaging, aiding in the identification and monitoring of brain diseases. This research presents a novel deep learning-based framework designed to achieve high segmentation accuracy while maintaining a lightweight architecture suitable for real-world deployment. The proposed method utilizes EfficientNet B0 as an encoder, which ensures rich multi-scale feature extraction with significantly reduced model complexity. To enhance global context modeling without increasing the computational burden, the framework incorporates Visual State-Space blocks. These blocks leverage patch merging and state-space modeling to capture long-range spatial dependencies efficiently. Additionally, a multi-scale attention mechanism inspired by the Mamba architecture is introduced to refine feature representations across different scales, improving the network\u2019s ability to segment complex anatomical structures and lesions. The decoder follows a U-Net-inspired design, integrating skip connections to preserve spatial details and enable high-resolution segmentation map reconstruction. The training process is optimized using a hybrid loss function, combining Active Contour Loss for precise boundary delineation and Focal Loss mitigates class imbalance, ensuring robust segmentation performance. By effectively balancing segmentation accuracy with a lightweight model design, the proposed approach provides visually superior segmentation results compared to other state-of-the-art."
    },
    {
      "paperId": "c18abad3a511cfc1eaa7814adc74e714ecdcd886",
      "externalIds": {
        "DOI": "10.3390/electronics14204122",
        "CorpusId": 282290469
      },
      "corpusId": 282290469,
      "title": "Catch Me If You Can: Rogue AI Detection and Correction at Scale",
      "venue": "Electronics",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/electronics14204122?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/electronics14204122, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "105047524",
          "name": "Fatemeh Stodt"
        },
        {
          "authorId": "1452349665",
          "name": "Jan Stodt"
        },
        {
          "authorId": "2350456841",
          "name": "Mohammed B. Alshawki"
        },
        {
          "authorId": "2387086860",
          "name": "Javad Salimi Sratakhti"
        },
        {
          "authorId": "2284301674",
          "name": "Christoph Reich"
        }
      ],
      "abstract": "Modern AI systems can strategically misreport information when incentives diverge from truthfulness, posing risks for oversight and deployment. Prior studies often examine this behavior within a single paradigm; systematic, cross-architecture evidence under a unified protocol has been limited. We introduce the Strategy Elicitation Battery (SEB), a standardized probe suite for measuring deceptive reporting across large language models (LLMs), reinforcement-learning agents, vision-only classifiers, multimodal encoders, state-space models, and diffusion models. SEB uses Bayesian inference tasks with persona-controlled instructions, schema-constrained outputs, deterministic decoding where supported, and a probe mix (near-threshold, repeats, neutralized, cross-checks). Estimates use clustered bootstrap intervals, and significance is assessed with a logistic regression by architecture; a mixed-effects analysis is planned once the per-round agent/episode traces are exported. On the latest pre-correction runs, SEB shows a consistent cross-architecture pattern in deception rates: ViT 80.0%, CLIP 15.0%, Mamba 10.0%, RL agents 10.0%, Stable Diffusion 10.0%, and LLMs 5.0% (20 scenarios/architecture). A logistic regression on per-scenario flags finds a significant overall architecture effect (likelihood-ratio test vs. intercept-only: \u03c72(5)=41.56, p=7.22\u00d710\u22128). Holm-adjusted contrasts indicate ViT is significantly higher than all other architectures in this snapshot; the remaining pairs are not significant. Post-correction acceptance decisions are evaluated separately using residual deception and override rates under SEB-Correct. Latency varies by architecture (sub-second to minutes), enabling pre-deployment screening broadly and real-time auditing for low-latency classes. Results indicate that SEB-Detect deception flags are not confined to any one paradigm, that distinct architectures can converge to similar levels under a common interface, and that reporting interfaces and incentive framing are central levers for mitigation. We operationalize \u201cdeception\u201d as reward-sensitive misreport flags, and we separate detection from intervention via a correction wrapper (SEB-Correct), supporting principled acceptance decisions for deployment."
    },
    {
      "paperId": "8912ca5858f801f6b389186cc29aae60c9401d10",
      "externalIds": {
        "ArXiv": "2511.01868",
        "CorpusId": 282748948
      },
      "corpusId": 282748948,
      "title": "Condition-Invariant fMRI Decoding of Speech Intelligibility with Deep State Space Model",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.01868, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2376395855",
          "name": "Ching-Chih Sung"
        },
        {
          "authorId": "2386720563",
          "name": "Shuntaro Suzuki"
        },
        {
          "authorId": "2390492317",
          "name": "Francis Pingfan Chien"
        },
        {
          "authorId": "2390492803",
          "name": "Komei Sugiura"
        },
        {
          "authorId": "2390492624",
          "name": "Yu Tsao"
        }
      ],
      "abstract": "Clarifying the neural basis of speech intelligibility is critical for computational neuroscience and digital speech processing. Recent neuroimaging studies have shown that intelligibility modulates cortical activity beyond simple acoustics, primarily in the superior temporal and inferior frontal gyri. However, previous studies have been largely confined to clean speech, leaving it unclear whether the brain employs condition-invariant neural codes across diverse listening environments. To address this gap, we propose a novel architecture built upon a deep state space model for decoding intelligibility from fMRI signals, specifically tailored to their high-dimensional temporal structure. We present the first attempt to decode intelligibility across acoustically distinct conditions, showing our method significantly outperforms classical approaches. Furthermore, region-wise analysis highlights contributions from auditory, frontal, and parietal regions, and cross-condition transfer indicates the presence of condition-invariant neural codes, thereby advancing understanding of abstract linguistic representations in the brain."
    },
    {
      "paperId": "142afc66d6fc6326fdec82f0349bb3dc1bd5f01b",
      "externalIds": {
        "DOI": "10.1109/ISSRE66568.2025.00041",
        "CorpusId": 283014036
      },
      "corpusId": 283014036,
      "title": "Integrating GraphSAGE and Mamba for Self-Supervised Spatio-Temporal Fault Detection in Microservice Systems",
      "venue": "IEEE International Symposium on Software Reliability Engineering",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ISSRE66568.2025.00041?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ISSRE66568.2025.00041, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2841424",
          "name": "Shenglin Zhang"
        },
        {
          "authorId": "2300514212",
          "name": "Yingke Li"
        },
        {
          "authorId": "2392994920",
          "name": "Jianjin Tang"
        },
        {
          "authorId": "2329256860",
          "name": "Chenyu Zhao"
        },
        {
          "authorId": "2396384112",
          "name": "Wenwei Gu"
        },
        {
          "authorId": "2116606497",
          "name": "Yongqian Sun"
        },
        {
          "authorId": "2287263274",
          "name": "Dan Pei"
        }
      ],
      "abstract": "Monitoring and fault detection in microservice systems is crucial for ensuring service stability. However, most existing methods either rely heavily on labeled data or fail to model complex spatial-temporal dependencies across services. To address these limitations, we propose ChronoSage, a spatiotemporal fault detection framework that integrates GraphSAGE and Mamba for unified graph-stream-based modeling. GraphSAGE captures the evolving topological structures by aggregating neighborhood features, while Mamba efficiently models long-range temporal dependencies through a selective state-space mechanism. We adopt a self-supervised training strategy to reduce label dependence and enhance generalization. Experiments on two real-world datasets demonstrate that ChronoSage achieves superior accuracy and efficiency compared to state-of-art baselines, such as ART and Eadro. The results validate ChronoSage\u2019s ability to support system-level fault detection in dynamic microservice environments, achieving an F1-score of 0.872 on D1 and 0.972 on D2, surpassing all compared methods."
    },
    {
      "paperId": "c2275b8620c2df1afcd2795be1aedce68874d3cd",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-17802",
        "ArXiv": "2510.17802",
        "DOI": "10.48550/arXiv.2510.17802",
        "CorpusId": 282209275
      },
      "corpusId": 282209275,
      "title": "Unbiased Gradient Low-Rank Projection",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.17802, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2192845956",
          "name": "Rui Pan"
        },
        {
          "authorId": "2218050858",
          "name": "Yang Luo"
        },
        {
          "authorId": "2276262237",
          "name": "Yuxing Liu"
        },
        {
          "authorId": "2286900390",
          "name": "Yang You"
        },
        {
          "authorId": "2276321620",
          "name": "Tong Zhang"
        }
      ],
      "abstract": "Memory-efficient optimization is critical for training increasingly large language models (LLMs). A popular strategy involves gradient low-rank projection, storing only the projected optimizer states, with GaLore being a representative example. However, a significant drawback of many such methods is their lack of convergence guarantees, as various low-rank projection approaches introduce inherent biases relative to the original optimization algorithms, which contribute to performance gaps compared to full-parameter training. Aiming to tackle this problem, this paper investigates the layerwise sampling technique for debiasing low-rank projection mechanisms. In particular, an instantiation of the paradigm gives rise to a novel and unbiased low-rank optimization method built upon GaLore's mechanism and the Muon algorithm, named GaLore Unbiased with Muon (GUM). We theoretically prove our method matches the convergence guarantees of the base Muon algorithm while preserving the memory efficiency of low-rank techniques. Empirical experiments on LLM fine-tuning and pretraining also demonstrate non-trivial improvements over GaLore and even better performance than full-parameter training. Further investigation shows that the improvement of this technique comes from a more uniform distribution of knowledge inside layers, leading to more efficient utilization of the model parameter space and better memorization."
    },
    {
      "paperId": "94e4a71492171f80a773dd44c036a805c7461441",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-17529",
        "ArXiv": "2510.17529",
        "DOI": "10.48550/arXiv.2510.17529",
        "CorpusId": 282209585
      },
      "corpusId": 282209585,
      "title": "MambaX-Net: Dual-Input Mamba-Enhanced Cross-Attention Network for Longitudinal MRI Segmentation",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.17529, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2345707971",
          "name": "Y. Yahathugoda"
        },
        {
          "authorId": "2314895931",
          "name": "Davide Prezzi"
        },
        {
          "authorId": "2100592783",
          "name": "Piyalitt Ittichaiwong"
        },
        {
          "authorId": "2249533430",
          "name": "Vicky Goh"
        },
        {
          "authorId": "2360080552",
          "name": "Sebastien Ourselin"
        },
        {
          "authorId": "51504413",
          "name": "M. Antonelli"
        }
      ],
      "abstract": "Active Surveillance (AS) is a treatment option for managing low and intermediate-risk prostate cancer (PCa), aiming to avoid overtreatment while monitoring disease progression through serial MRI and clinical follow-up. Accurate prostate segmentation is an important preliminary step for automating this process, enabling automated detection and diagnosis of PCa. However, existing deep-learning segmentation models are often trained on single-time-point and expertly annotated datasets, making them unsuitable for longitudinal AS analysis, where multiple time points and a scarcity of expert labels hinder their effective fine-tuning. To address these challenges, we propose MambaX-Net, a novel semi-supervised, dual-scan 3D segmentation architecture that computes the segmentation for time point t by leveraging the MRI and the corresponding segmentation mask from the previous time point. We introduce two new components: (i) a Mamba-enhanced Cross-Attention Module, which integrates the Mamba block into cross attention to efficiently capture temporal evolution and long-range spatial dependencies, and (ii) a Shape Extractor Module that encodes the previous segmentation mask into a latent anatomical representation for refined zone delination. Moreover, we introduce a semi-supervised self-training strategy that leverages pseudo-labels generated from a pre-trained nnU-Net, enabling effective learning without expert annotations. MambaX-Net was evaluated on a longitudinal AS dataset, and results showed that it significantly outperforms state-of-the-art U-Net and Transformer-based models, achieving superior prostate zone segmentation even when trained on limited and noisy data."
    },
    {
      "paperId": "0ae16b5165d9bf0f4a4763fc0b6e27ffdec7b941",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-17545",
        "ArXiv": "2510.17545",
        "DOI": "10.48550/arXiv.2510.17545",
        "CorpusId": 282210277
      },
      "corpusId": 282210277,
      "title": "TrajMamba: An Efficient and Semantic-rich Vehicle Trajectory Pre-training Model",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.17545, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2311716113",
          "name": "Yicheng Liu"
        },
        {
          "authorId": "2149202986",
          "name": "Yan Lin"
        },
        {
          "authorId": "1910814",
          "name": "S. Guo"
        },
        {
          "authorId": "2225207164",
          "name": "Zeyu Zhou"
        },
        {
          "authorId": "2237808031",
          "name": "Youfang Lin"
        },
        {
          "authorId": "39699556",
          "name": "Huaiyu Wan"
        }
      ],
      "abstract": "Vehicle GPS trajectories record how vehicles move over time, storing valuable travel semantics, including movement patterns and travel purposes. Learning travel semantics effectively and efficiently is crucial for real-world applications of trajectory data, which is hindered by two major challenges. First, travel purposes are tied to the functions of the roads and points-of-interest (POIs) involved in a trip. Such information is encoded in textual addresses and descriptions and introduces heavy computational burden to modeling. Second, real-world trajectories often contain redundant points, which harm both computational efficiency and trajectory embedding quality. To address these challenges, we propose TrajMamba, a novel approach for efficient and semantically rich vehicle trajectory learning. TrajMamba introduces a Traj-Mamba Encoder that captures movement patterns by jointly modeling both GPS and road perspectives of trajectories, enabling robust representations of continuous travel behaviors. It also incorporates a Travel Purpose-aware Pre-training procedure to integrate travel purposes into the learned embeddings without introducing extra overhead to embedding calculation. To reduce redundancy in trajectories, TrajMamba features a Knowledge Distillation Pre-training scheme to identify key trajectory points through a learnable mask generator and obtain effective compressed trajectory embeddings. Extensive experiments on two real-world datasets and three downstream tasks show that TrajMamba outperforms state-of-the-art baselines in both efficiency and accuracy."
    },
    {
      "paperId": "6f86ecf657f076019c125d74eda72c99c26190f9",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-17318",
        "ArXiv": "2510.17318",
        "DOI": "10.48550/arXiv.2510.17318",
        "CorpusId": 282210409
      },
      "corpusId": 282210409,
      "title": "CausalMamba: Scalable Conditional State Space Models for Neural Causal Inference",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.17318, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2104225206",
          "name": "Sang-Peel Bae"
        },
        {
          "authorId": "2386624154",
          "name": "Jiook Cha"
        }
      ],
      "abstract": "We introduce CausalMamba, a scalable framework that addresses fundamental limitations in fMRI-based causal inference: the ill-posed nature of inferring neural causality from hemodynamically distorted BOLD signals and the computational intractability of existing methods like Dynamic Causal Modeling (DCM). Our approach decomposes this complex inverse problem into two tractable stages: BOLD deconvolution to recover latent neural activity, followed by causal graph inference using a novel Conditional Mamba architecture. On simulated data, CausalMamba achieves 37% higher accuracy than DCM. Critically, when applied to real task fMRI data, our method recovers well-established neural pathways with 88% fidelity, whereas conventional approaches fail to identify these canonical circuits in over 99% of subjects. Furthermore, our network analysis of working memory data reveals that the brain strategically shifts its primary causal hub-recruiting executive or salience networks depending on the stimulus-a sophisticated reconfiguration that remains undetected by traditional methods. This work provides neuroscientists with a practical tool for large-scale causal inference that captures both fundamental circuit motifs and flexible network dynamics underlying cognitive function."
    },
    {
      "paperId": "a89c19eb02c11ee319f76b8a163c6bbd6b5bc89f",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-17111",
        "ArXiv": "2510.17111",
        "DOI": "10.48550/arXiv.2510.17111",
        "CorpusId": 282210569
      },
      "corpusId": 282210569,
      "title": "Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 6,
      "influentialCitationCount": 1,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.17111, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2300552604",
          "name": "Weifan Guan"
        },
        {
          "authorId": "2571792",
          "name": "Qinghao Hu"
        },
        {
          "authorId": "2386778584",
          "name": "Aosheng Li"
        },
        {
          "authorId": "2386920151",
          "name": "Jian Cheng"
        }
      ],
      "abstract": "Vision-Language-Action (VLA) models extend vision-language models to embodied control by mapping natural-language instructions and visual observations to robot actions. Despite their capabilities, VLA systems face significant challenges due to their massive computational and memory demands, which conflict with the constraints of edge platforms such as on-board mobile manipulators that require real-time performance. Addressing this tension has become a central focus of recent research. In light of the growing efforts toward more efficient and scalable VLA systems, this survey provides a systematic review of approaches for improving VLA efficiency, with an emphasis on reducing latency, memory footprint, and training and inference costs. We categorize existing solutions into four dimensions: model architecture, perception feature, action generation, and training/inference strategies, summarizing representative techniques within each category. Finally, we discuss future trends and open challenges, highlighting directions for advancing efficient embodied intelligence."
    },
    {
      "paperId": "4c95b1cef66cda6693fa6ab192698703dd3c070f",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-18165",
        "ArXiv": "2510.18165",
        "DOI": "10.48550/arXiv.2510.18165",
        "CorpusId": 282246725
      },
      "corpusId": 282246725,
      "title": "Saber: An Efficient Sampling with Adaptive Acceleration and Backtracking Enhanced Remasking for Diffusion Language Model",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.18165, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "26845858",
          "name": "Yihong Dong"
        },
        {
          "authorId": "2386931789",
          "name": "Zhaoyu Ma"
        },
        {
          "authorId": "2199808863",
          "name": "Xue Jiang"
        },
        {
          "authorId": "2390886042",
          "name": "Zhiyuan Fan"
        },
        {
          "authorId": "2375328320",
          "name": "Jiaru Qian"
        },
        {
          "authorId": "2135349350",
          "name": "Yongming Li"
        },
        {
          "authorId": "2387977757",
          "name": "Jianha Xiao"
        },
        {
          "authorId": "2152843753",
          "name": "Zhi Jin"
        },
        {
          "authorId": "2304468929",
          "name": "Rongyu Cao"
        },
        {
          "authorId": "66200440",
          "name": "Binhua Li"
        },
        {
          "authorId": "2304136336",
          "name": "Fei Huang"
        },
        {
          "authorId": "2323761746",
          "name": "Yongbin Li"
        },
        {
          "authorId": "2325210561",
          "name": "Ge Li"
        }
      ],
      "abstract": "Diffusion language models (DLMs) are emerging as a powerful and promising alternative to the dominant autoregressive paradigm, offering inherent advantages in parallel generation and bidirectional context modeling. However, the performance of DLMs on code generation tasks, which have stronger structural constraints, is significantly hampered by the critical trade-off between inference speed and output quality. We observed that accelerating the code generation process by reducing the number of sampling steps usually leads to a catastrophic collapse in performance. In this paper, we introduce efficient Sampling with Adaptive acceleration and Backtracking Enhanced Remasking (i.e., Saber), a novel training-free sampling algorithm for DLMs to achieve better inference speed and output quality in code generation. Specifically, Saber is motivated by two key insights in the DLM generation process: 1) it can be adaptively accelerated as more of the code context is established; 2) it requires a backtracking mechanism to reverse the generated tokens. Extensive experiments on multiple mainstream code generation benchmarks show that Saber boosts Pass@1 accuracy by an average improvement of 1.9% over mainstream DLM sampling methods, meanwhile achieving an average 251.4% inference speedup. By leveraging the inherent advantages of DLMs, our work significantly narrows the performance gap with autoregressive models in code generation."
    },
    {
      "paperId": "b7c06083b1f91e9c1e8b4c3b56390884c8f70f3d",
      "externalIds": {
        "DOI": "10.21105/joss.08869",
        "CorpusId": 282266330
      },
      "corpusId": 282266330,
      "title": "BONSAI: A framework for processing and analysing Electronic Health Records (EHR) data using transformer-based models",
      "venue": "Journal of Open Source Software",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.21105/joss.08869?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.21105/joss.08869, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2274650571",
          "name": "Maria Elkj\u00e6r Montgomery"
        },
        {
          "authorId": "2204538503",
          "name": "K. Klein"
        },
        {
          "authorId": "2386924053",
          "name": "Mikkel Odgaard"
        },
        {
          "authorId": "2276434019",
          "name": "Stephan Sloth Lorenzen"
        },
        {
          "authorId": "2386924748",
          "name": "Zahra Sobhaninia"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "a2abdafc4f03e6e872677268a966a6d08e15796c",
      "externalIds": {
        "DOI": "10.1515/bmt-2024-0624",
        "CorpusId": 282196061,
        "PubMed": "41105530"
      },
      "corpusId": 282196061,
      "title": "CVM-fusion: parallel cross-axes mamba fusion for medical image segmentation",
      "venue": "Biomedizinische Technik. Biomedical engineering",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1515/bmt-2024-0624?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1515/bmt-2024-0624, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2362996579",
          "name": "Rui Peng"
        },
        {
          "authorId": "2237960923",
          "name": "Longfeng Shen"
        },
        {
          "authorId": "2363135164",
          "name": "Zhengtian Lu"
        },
        {
          "authorId": "2329103371",
          "name": "Liangjin Diao"
        },
        {
          "authorId": "2196929002",
          "name": "Fangzheng Ge"
        }
      ],
      "abstract": "Abstract Objectives In the domain of medical image segmentation, models utilizing convolutional neural network (CNN) and Transformer have been t extensively studied and widely implemented. However, the self-attention mechanism in Transformer is incapable of adapting its focus to target structures at varying scales, resulting in discontinuities in segmentation. The objective of this study is to propose a multi-directional dynamic modeling network for medical image segmentation. Methods We propose a Cross-axis Mamba attention (CMA) to capture global info and establish long-range dependencies. It integrates both global context and local details, enhancing segmentation performance. We also introduce an Edge Feature Enhancement Model (EFCN) to improve edge feature detection. We evaluated the method on the ISIC2018 dataset, as well as the CVC-300 and Kvasir-SEG datasets. Results The dice similarity coefficient and intersection-over-union (IoU) metrics achieved values of 91.12 and 85.07, 90.35 and 83.43, and 94.14 and 89.62, respectively. These results outperform those of advanced models such as VM-Unet and Swin-UMamba. Conclusions The experimental results indicate that the proposed method has good generalization ability and robustness. It also provides important support for clinical diagnosis and treatment."
    },
    {
      "paperId": "86bfe37853503de3c85bb265120f65f6ee1238c4",
      "externalIds": {
        "ArXiv": "2510.16674",
        "DBLP": "journals/corr/abs-2510-16674",
        "DOI": "10.48550/arXiv.2510.16674",
        "CorpusId": 282209328
      },
      "corpusId": 282209328,
      "title": "Evaluating protein binding interfaces with PUMBA",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.16674, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "51054599",
          "name": "Azam Shirali"
        },
        {
          "authorId": "2257176086",
          "name": "Giri Narasimhan"
        }
      ],
      "abstract": "Protein-protein docking tools help in studying interactions between proteins, and are essential for drug, vaccine, and therapeutic development. However, the accuracy of a docking tool depends on a robust scoring function that can reliably differentiate between native and non-native complexes. PIsToN is a state-of-the-art deep learning-based scoring function that uses Vision Transformers in its architecture. Recently, the Mamba architecture has demonstrated exceptional performance in both natural language processing and computer vision, often outperforming Transformer-based models in their domains. In this study, we introduce PUMBA (Protein-protein interface evaluation with Vision Mamba), which improves PIsToN by replacing its Vision Transformer backbone with Vision Mamba. This change allows us to leverage Mamba's efficient long-range sequence modeling for sequences of image patches. As a result, the model's ability to capture both global and local patterns in protein-protein interface features is significantly improved. Evaluation on several widely-used, large-scale public datasets demonstrates that PUMBA consistently outperforms its original Transformer-based predecessor, PIsToN."
    },
    {
      "paperId": "de03d86a9898e98c5dbc7c8de3520b9752537f08",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-16834",
        "ArXiv": "2510.16834",
        "DOI": "10.48550/arXiv.2510.16834",
        "CorpusId": 282209835
      },
      "corpusId": 282209835,
      "title": "Schr\u00f6dinger Bridge Mamba for One-Step Speech Enhancement",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.16834, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2279862714",
          "name": "Jing Yang"
        },
        {
          "authorId": "2387015376",
          "name": "Sirui Wang"
        },
        {
          "authorId": "2370791549",
          "name": "Chao Wu"
        },
        {
          "authorId": "2279541044",
          "name": "Fan Fan"
        }
      ],
      "abstract": "We propose Schr\\\"odinger Bridge Mamba (SBM), a new concept of training-inference framework motivated by the inherent compatibility between Schr\\\"odinger Bridge (SB) training paradigm and selective state-space model Mamba. We exemplify the concept of SBM with an implementation for generative speech enhancement. Experiments on a joint denoising and dereverberation task using four benchmark datasets demonstrate that SBM, with only 1-step inference, outperforms strong baselines with 1-step or iterative inference and achieves the best real-time factor (RTF). Beyond speech enhancement, we discuss the integration of SB paradigm and selective state-space model architecture based on their underlying alignment, which indicates a promising direction for exploring new deep generative models potentially applicable to a broad range of generative tasks. Demo page: https://sbmse.github.io"
    },
    {
      "paperId": "3af611e47c30a439f8acad312f16cf7837ddbce7",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-16756",
        "ArXiv": "2510.16756",
        "DOI": "10.48550/arXiv.2510.16756",
        "CorpusId": 282209096
      },
      "corpusId": 282209096,
      "title": "End-to-end Listen, Look, Speak and Act",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.16756, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2239283908",
          "name": "Siyin Wang"
        },
        {
          "authorId": "2257283478",
          "name": "Wenyi Yu"
        },
        {
          "authorId": "2135092817",
          "name": "Xianzhao Chen"
        },
        {
          "authorId": "2323002776",
          "name": "Xiaohai Tian"
        },
        {
          "authorId": "2305803685",
          "name": "Jun Zhang"
        },
        {
          "authorId": "2257383962",
          "name": "Lu Lu"
        },
        {
          "authorId": "2305964831",
          "name": "Chao Zhang"
        }
      ],
      "abstract": "Human interaction is inherently multimodal and full-duplex: we listen while watching, speak while acting, and fluidly adapt to turn-taking and interruptions. Realizing these capabilities is essential for building models simulating humans. We present ELLSA (End-to-end Listen, Look, Speak and Act), which, to our knowledge, is the first full-duplex, end-to-end model that simultaneously perceives and generates across vision, text, speech, and action within a single architecture, enabling interaction patterns previously out of reach, yielding more natural, human-like behaviors. At its core is a novel SA-MoE architecture (Self-Attention Mixture-of-Experts) that routes each modality to specialized experts and fuses them through a unified attention backbone. This provides a generalizable solution for joint multimodal perception and concurrent generation, leveraging strong pre-trained components while enabling efficient modality integration and mitigating modality interference. On speech-interaction and robot-manipulation benchmarks, ELLSA matches modality-specific baselines, while uniquely supporting advanced multimodal and full-duplex behaviors such as dialogue and action turn-taking, defective instruction rejection, speaking-while-acting, context-grounded visual question answering, and action barge-ins. We contend that ELLSA represents a step toward more natural and general interactive intelligence, contributing to the broader pursuit of artificial general intelligence. All data, code and model checkpoints will be released upon acceptance."
    },
    {
      "paperId": "600fd1174aa5acee93a9ef3257c2f3ba86566efc",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-16776",
        "ArXiv": "2510.16776",
        "DOI": "10.48550/arXiv.2510.16776",
        "CorpusId": 282209105
      },
      "corpusId": 282209105,
      "title": "EMRRG: Efficient Fine-Tuning Pre-trained X-ray Mamba Networks for Radiology Report Generation",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.16776, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2386643091",
          "name": "Mingzheng Zhang"
        },
        {
          "authorId": "2371082210",
          "name": "Jinfeng Gao"
        },
        {
          "authorId": "2386815360",
          "name": "Dan Xu"
        },
        {
          "authorId": "2386827730",
          "name": "Jiangrui Yu"
        },
        {
          "authorId": "2375305360",
          "name": "Yuhan Qiao"
        },
        {
          "authorId": "2223146782",
          "name": "Langlang Chen"
        },
        {
          "authorId": "2375279982",
          "name": "Jin Tang"
        },
        {
          "authorId": "2374810324",
          "name": "Xiao Wang"
        }
      ],
      "abstract": "X-ray image-based medical report generation (MRG) is a pivotal area in artificial intelligence that can significantly reduce diagnostic burdens for clinicians and patient wait times. Existing MRG models predominantly rely on Large Language Models (LLMs) to improve report generation, with limited exploration of pre-trained vision foundation models or advanced fine-tuning techniques. Mainstream frameworks either avoid fine-tuning or utilize simplistic methods like LoRA, often neglecting the potential of enhancing cross-attention mechanisms. Additionally, while Transformer-based models dominate vision-language tasks, non-Transformer architectures, such as the Mamba network, remain underexplored for medical report generation, presenting a promising avenue for future research. In this paper, we propose EMRRG, a novel X-ray report generation framework that fine-tunes pre-trained Mamba networks using parameter-efficient methods. Specifically, X-ray images are divided into patches, tokenized, and processed by an SSM-based vision backbone for feature extraction, with Partial LoRA yielding optimal performance. An LLM with a hybrid decoder generates the medical report, enabling end-to-end training and achieving strong results on benchmark datasets. Extensive experiments on three widely used benchmark datasets fully validated the effectiveness of our proposed strategies for the X-ray MRG. The source code of this paper will be released on https://github.com/Event-AHU/Medical_Image_Analysis."
    },
    {
      "paperId": "36510f2f6141a6e710a2a234940d4408745ae37f",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-16765",
        "ArXiv": "2510.16765",
        "DOI": "10.48550/arXiv.2510.16765",
        "CorpusId": 282210417
      },
      "corpusId": 282210417,
      "title": "WaMaIR: Image Restoration via Multiscale Wavelet Convolutions and Mamba-based Channel Modeling with Texture Enhancement",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.16765, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2260854640",
          "name": "Shengyu Zhu"
        },
        {
          "authorId": "2391009061",
          "name": "Congyi Fan"
        },
        {
          "authorId": "2386857212",
          "name": "Fuxuan Zhang"
        }
      ],
      "abstract": "Image restoration is a fundamental and challenging task in computer vision, where CNN-based frameworks demonstrate significant computational efficiency. However, previous CNN-based methods often face challenges in adequately restoring fine texture details, which are limited by the small receptive field of CNN structures and the lack of channel feature modeling. In this paper, we propose WaMaIR, which is a novel framework with a large receptive field for image perception and improves the reconstruction of texture details in restored images. Specifically, we introduce the Global Multiscale Wavelet Transform Convolutions (GMWTConvs) for expandding the receptive field to extract image features, preserving and enriching texture features in model inputs. Meanwhile, we propose the Mamba-Based Channel-Aware Module (MCAM), explicitly designed to capture long-range dependencies within feature channels, which enhancing the model sensitivity to color, edges, and texture information. Additionally, we propose Multiscale Texture Enhancement Loss (MTELoss) for image restoration to guide the model in preserving detailed texture structures effectively. Extensive experiments confirm that WaMaIR outperforms state-of-the-art methods, achieving better image restoration and efficient computational performance of the model."
    },
    {
      "paperId": "ab1c83deae2bd44e40ac56c3ae1e48f07994fb2c",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-16851",
        "ArXiv": "2510.16851",
        "DOI": "10.48550/arXiv.2510.16851",
        "CorpusId": 282210024
      },
      "corpusId": 282210024,
      "title": "Neuronal Group Communication for Efficient Neural representation",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.16851, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "116406922",
          "name": "Zhengqi Pei"
        },
        {
          "authorId": "2256640224",
          "name": "Qingming Huang"
        },
        {
          "authorId": "2319173985",
          "name": "Shuhui Wang"
        }
      ],
      "abstract": "The ever-increasing scale of modern neural networks has brought unprecedented performance alongside daunting challenges in efficiency and interpretability. This paper addresses the core question of how to build large neural systems that learn efficient, modular, and interpretable representations. We propose Neuronal Group Communication (NGC), a theory-driven framework that reimagines a neural network as a dynamical system of interacting neuronal groups rather than a monolithic collection of neural weights. Instead of treating each weight as an independent trainable parameter, NGC treats weights as transient interactions between embedding-like neuronal states, with neural computation unfolding through iterative communication among groups of neurons. This low-rank, modular representation yields compact models: groups of neurons exchange low-dimensional signals, enabling intra-group specialization and inter-group information sharing while dramatically reducing redundant parameters. By drawing on dynamical systems theory, we introduce a neuronal stability metric (analogous to Lyapunov stability) that quantifies the contraction of neuron activations toward stable patterns during sequence processing. Using this metric, we reveal that emergent reasoning capabilities correspond to an external driving force or ``potential'', which nudges the neural dynamics away from trivial trajectories while preserving stability. Empirically, we instantiate NGC in large language models (LLMs) and demonstrate improved performance on complex reasoning benchmarks under moderate compression. NGC consistently outperforms standard low-rank approximations and cross-layer basis-sharing methods at comparable compression rates. We conclude by discussing the broader implications of NGC, including how structured neuronal group dynamics might relate to generalization in high-dimensional learning systems."
    },
    {
      "paperId": "455e0107fbf66cf7829ae979f58e14a011109a14",
      "externalIds": {
        "DOI": "10.1109/IROS60139.2025.11246122",
        "CorpusId": 283355706
      },
      "corpusId": 283355706,
      "title": "MambaSFLNet: A Mamba-based Model for Low-Light Image Enhancement with Spatial and Frequency Features",
      "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/IROS60139.2025.11246122?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/IROS60139.2025.11246122, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2258947403",
          "name": "Mingyu Liu"
        },
        {
          "authorId": "2277664093",
          "name": "Yuning Cui"
        },
        {
          "authorId": "2162393893",
          "name": "Leah Strand"
        },
        {
          "authorId": "2261365902",
          "name": "Xingcheng Zhou"
        },
        {
          "authorId": "2343774588",
          "name": "Jiajie Zhang"
        },
        {
          "authorId": "2265256650",
          "name": "Alois Knoll"
        }
      ],
      "abstract": "Low-light image enhancement (LLIE) aims to enhance the illumination of images that are captured under dark conditions, which is critical for various applications in dim environments, such as robotics and autonomous driving. Existing convolutional neural network (CNN)-based methods usually struggle to capture long-range dependencies, while transformer-based methods, despite their effectiveness, are resource-consuming. Besides, the frequency domain includes important lightness degradation information. To this end, we propose a Mamba-based framework called MambaSFLNet to effectively address LLIE by integrating spatial and frequency features. Our approach utilizes the Visual State Space Module to establish relationships across different regions of the input image while maintaining low model complexity. Furthermore, The spatial module not only balances illumination distribution but also suppresses noise and artifacts during enhancement. In addition, the frequency module enhances image contrast and sharpness by leveraging frequency-domain information. Extensive experiments on nine widely used benchmarks demonstrate that our approach achieves superior performance and exhibits strong generalization capabilities compared to existing methods. The codes are available at https://github.com/MingyuLiu1/MambaSFLNet.git"
    },
    {
      "paperId": "cbf0dd02853bd9ea2c5329a79a5ec1b62265c561",
      "externalIds": {
        "DOI": "10.1109/IROS60139.2025.11247410",
        "CorpusId": 283355784
      },
      "corpusId": 283355784,
      "title": "EFCWM-Mamba-YOLO: Real-Time Underwater Object Detection with Adaptive Feature Representation and Domain Adaptation",
      "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/IROS60139.2025.11247410?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/IROS60139.2025.11247410, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2075416144",
          "name": "Pan Sun"
        },
        {
          "authorId": "2196135962",
          "name": "Yufei Lu"
        },
        {
          "authorId": "2112913991",
          "name": "Shijie Shi"
        },
        {
          "authorId": "2315946648",
          "name": "Meng Li"
        },
        {
          "authorId": "2265120309",
          "name": "Qiang Li"
        },
        {
          "authorId": "2315310867",
          "name": "Huilin Ge"
        }
      ],
      "abstract": "Underwater object detection (UOD) is crucial for monitoring marine ecosystems, underwater robotics, environmental protection, and autonomous underwater vehicles (AUVs). Despite progress, many models struggle under real-world conditions due to poor visibility, dynamic lighting, and domain shifts. Traditional methods like Faster R-CNN are computationally expensive, while YOLO-based models suffer in challenging underwater scenarios. The scarcity of large-scale annotated datasets further limits model generalization. To address these challenges, we introduce UOD-SZTU-2025, a new dataset of 3,133 high-quality underwater images, sourced primarily from video platforms. The dataset is used in EFCWM (Enhanced Feature Correction and Weighting Module) to extract and refine a feature material library for detection targets. We propose EFCWM-Mamba-YOLO, a lightweight, real-time detection model designed to enhance feature representation and adapt to diverse underwater environments. The EFCWM module incorporates domain adaptation for improved robustness. Additionally, a two-stage training strategy first trains on a source domain and fine-tunes with limited target domain samples to enhance generalization. Experiments show our approach surpasses existing lightweight UOD models in accuracy, real-time performance, and robustness. Our dataset, model, and benchmark establish a strong foundation for future UOD research. The dataset for EFCWM-Mamba-YOLO is available at https://github.com/wojiaosun/UOD-SZTU-2025."
    },
    {
      "paperId": "90072ea1b8e0e3753ab1c5e03ba9a9ad43faf06c",
      "externalIds": {
        "DOI": "10.1109/IROS60139.2025.11247758",
        "CorpusId": 283356357
      },
      "corpusId": 283356357,
      "title": "MRMT-PR: A Multi-Scale Reverse-View Mamba-Transformer for LiDAR Place Recognition",
      "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/IROS60139.2025.11247758?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/IROS60139.2025.11247758, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2290751913",
          "name": "Kan Luo"
        },
        {
          "authorId": "2290766353",
          "name": "Jingwen Wang"
        },
        {
          "authorId": "2395623276",
          "name": "Hongshan Yu"
        },
        {
          "authorId": "2395580058",
          "name": "Yaonan Wang"
        },
        {
          "authorId": "2394986802",
          "name": "Javier Civera"
        },
        {
          "authorId": "2281902072",
          "name": "Xieyuanli Chen"
        }
      ],
      "abstract": "Place recognition is a fundamental technology of high relevance for autonomous robot navigation. Existing methods encounter significant challenges arising from scene variations (e.g., illumination changes, dynamic objects), view-point shifts, and difficulties in data fusion and alignment. These factors often lead to a substantial drop in recognition recall, which is typically addressed in the literature by training deep neural networks to learn invariant feature representations. In this paper, we propose MRMT-PR, a novel multi-scale reverse-view Mamba-Transformer architecture for LiDAR-based place recognition that uses a single-frame point cloud as its input. Our MRMT-PR framework consists of a multi-scale reverse-view preprocessing module for LiDAR point clouds, a Mamba-Transformer feature encoder, and a global feature fusion module. This architecture effectively mitigates the impact of perspective and illumination variations, enhances the global representational capacity of LiDAR features, and significantly improves recognition robustness under challenging conditions such as viewpoint changes and long-term localization. Experiments conducted on NCLT dataset with challenging scenarios demonstrate that MRMT-PR outperforms existing LiDAR-based place recognition baselines in terms of overall performance."
    },
    {
      "paperId": "0aaaaa816a846df6ea9e0ccfc6700b6662620b34",
      "externalIds": {
        "DOI": "10.1109/IROS60139.2025.11247336",
        "CorpusId": 283357550
      },
      "corpusId": 283357550,
      "title": "MambaGCN: Synergistic Integration of Graph Convolutional Networks and State Space Models for Point Cloud Processing",
      "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/IROS60139.2025.11247336?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/IROS60139.2025.11247336, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2395004065",
          "name": "Zhifeng Rao"
        },
        {
          "authorId": "2112412594",
          "name": "Zhiyun Lin"
        }
      ],
      "abstract": "Graph Neural Networks have emerged as a formidable tool for analyzing point clouds, leveraging their capacity to aggregate local features across multiple spatial scales via layered structures. However, a significant challenge lies in effectively and selectively integrating these multi-scale features to maximize overall performance. To tackle this integration challenge, we design a novel model, MambaGCN, which employs a state space model to dynamically adjust the feature weights across spatial scales during aggregation, enabling more refined feature integration while ensuring computational efficiency. Unlike transformers with their quadratic complexity, MambaGCN achieves linear complexity, substantially reducing GPU memory usage and computational cost. Moreover, we have enhanced the architectural depth by designing a density-based farthest point sampling algorithm, which allows us to selectively downsample the input data to achieve varying levels of point density. This innovation facilitates the seamless concatenation of multiple MambaGCN layers, significantly deepening the structure of the network and enhancing its ability to tackle complex point cloud tasks effectively. Through these strategic developments, MambaGCN has demonstrated outstanding performance in tasks such as point cloud classification and part segmentation, affirming its robustness and efficiency in processing point cloud data."
    },
    {
      "paperId": "f652b6dc216e4a86aefa8573ee3d7622741c71a6",
      "externalIds": {
        "DOI": "10.3390/fi17100476",
        "CorpusId": 281400029
      },
      "corpusId": 281400029,
      "title": "Integrating Large Language Models into Automated Software Testing",
      "venue": "Future Internet",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/fi17100476?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/fi17100476, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2381113754",
          "name": "Yanet S\u00e1ez Iznaga"
        },
        {
          "authorId": "2381113277",
          "name": "Lu\u00eds Rato"
        },
        {
          "authorId": "2273359410",
          "name": "Pedro Salgueiro"
        },
        {
          "authorId": "2386829804",
          "name": "Javier Lamar Le\u00f3n"
        }
      ],
      "abstract": "This work investigates the use of LLMs to enhance automation in software testing, with a particular focus on generating high-quality, context-aware test scripts from natural language descriptions, while addressing both text-to-code and text+code-to-code generation tasks. The Codestral Mamba model was fine-tuned by proposing a way to integrate LoRA matrices into its architecture, enabling efficient domain-specific adaptation and positioning Mamba as a viable alternative to Transformer-based models. The model was trained and evaluated on two benchmark datasets: CONCODE/CodeXGLUE and the proprietary TestCase2Code dataset. Through structured prompt engineering, the system was optimized to generate syntactically valid and semantically meaningful code for test cases. Experimental results demonstrate that the proposed methodology successfully enables the automatic generation of code-based test cases using large language models. In addition, this work reports secondary benefits, including improvements in test coverage, automation efficiency, and defect detection when compared to traditional manual approaches. The integration of LLMs into the software testing pipeline also showed potential for reducing time and cost while enhancing developer productivity and software quality. The findings suggest that LLM-driven approaches can be effectively aligned with continuous integration and deployment workflows. This work contributes to the growing body of research on AI-assisted software engineering and offers practical insights into the capabilities and limitations of current LLM technologies for testing automation."
    },
    {
      "paperId": "4000709afaaa44fc6ca69444217ed0d3added214",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-16439",
        "ArXiv": "2510.16439",
        "DOI": "10.48550/arXiv.2510.16439",
        "CorpusId": 282210012
      },
      "corpusId": 282210012,
      "title": "FrugalPrompt: Reducing Contextual Overhead in Large Language Models via Token Attribution",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.16439, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2216812032",
          "name": "Syed Rifat Raiyan"
        },
        {
          "authorId": "2316056000",
          "name": "Md Farhan Ishmam"
        },
        {
          "authorId": "2372635414",
          "name": "Abdullah Al Imran"
        },
        {
          "authorId": "145781245",
          "name": "M. A. Moni"
        }
      ],
      "abstract": "Large language models (LLMs) owe much of their stellar performance to expansive input contexts, yet such verbosity inflates monetary costs, carbon footprint, and inference-time latency. Much of this overhead manifests from the redundant low-utility tokens present in typical prompts, as only a fraction of tokens typically carries the majority of the semantic weight. We address this inefficiency by introducing FrugalPrompt, a novel prompt compression framework for LLMs, which retains only the most semantically significant tokens. Leveraging two state-of-the-art token attribution methods, GlobEnc and DecompX, we assign salience scores to every token in an input sequence, rank them to preserve the top-k% tokens in their original order, and obtain a sparse frugalized prompt. We evaluate the approach across four NLP tasks: Sentiment Analysis, Commonsense QA, Summarization, and Mathematical Reasoning, using a suite of frontier LLMs. For the first three tasks, a 20% prompt reduction incurs only a marginal loss in task performance, demonstrating that contemporary LLMs can reconstruct elided context from high-salience cues. In contrast, performance on mathematical reasoning deteriorates sharply, reflecting a stronger dependence on complete token continuity. Further analysis with bottom-k% and random-k% tokens reveals asymmetric performance patterns that may suggest potential task contamination effects, wherein models may resort to shallow memorized patterns from pretraining exposure for conventional NLP tasks. We posit that our work contributes to a more nuanced understanding of LLM behavior in performance-efficiency trade-offs, and delineate the boundary between tasks tolerant to contextual sparsity and those requiring exhaustive context. Our source code and models are available at: https://github.com/Starscream-11813/Frugal-ICL."
    },
    {
      "paperId": "75885d9986bb3a7c8a8398dd4c87f8905aa88f09",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-16540",
        "ArXiv": "2510.16540",
        "DOI": "10.48550/arXiv.2510.16540",
        "CorpusId": 282210397
      },
      "corpusId": 282210397,
      "title": "Enhancing Compositional Reasoning in CLIP via Reconstruction and Alignment of Text Descriptions",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.16540, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2333843725",
          "name": "Jihoon Kwon"
        },
        {
          "authorId": "2386621653",
          "name": "Kyle Min"
        },
        {
          "authorId": "8414722",
          "name": "Jy-yong Sohn"
        }
      ],
      "abstract": "Despite recent advances, vision-language models trained with standard contrastive objectives still struggle with compositional reasoning -- the ability to understand structured relationships between visual and linguistic elements. This shortcoming is largely due to the tendency of the text encoder to focus on individual words rather than their relations, a limitation reinforced by contrastive training that primarily aligns words with visual objects. In this paper, we introduce REconstruction and Alignment of text Descriptions (READ), a fine-tuning method designed to enhance compositional reasoning by adding two auxiliary objectives to the contrastive learning: (1) a token-level reconstruction objective, where a frozen pre-trained decoder reconstructs alternative captions based on the embedding of the original caption; and (2) a sentence-level alignment objective, which explicitly aligns paraphrased sentences in the embedding space. We show that READ-CLIP, a model derived by applying the READ method to the pre-trained CLIP model, achieves the state-of-the-art performance across five major compositional reasoning benchmarks, outperforming the strongest conventional fine-tuning baseline by up to 4.1%. Furthermore, applying the READ to existing CLIP variants (including NegCLIP and FSC-CLIP) also improves performance on these benchmarks. Quantitative and qualitative analyses reveal that our proposed objectives -- reconstruction and alignment -- offer complementary benefits: the former encourages the encoder to capture relationships between words within a caption, while the latter ensures consistent representations for paraphrases expressed with different wording."
    },
    {
      "paperId": "76659a7ab2566d65a45b440f2c1ce4811127fd28",
      "externalIds": {
        "DOI": "10.1109/CyberC66434.2025.00026",
        "CorpusId": 282762018
      },
      "corpusId": 282762018,
      "title": "Clustering-Driven Mamba with Balanced Contrastive Learning for Next POI Recommendation",
      "venue": "International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CyberC66434.2025.00026?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CyberC66434.2025.00026, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2255430222",
          "name": "Jiahao Li"
        },
        {
          "authorId": "2255753809",
          "name": "Fuqiang Liu"
        },
        {
          "authorId": "2256977379",
          "name": "Lei Li"
        },
        {
          "authorId": "2253883917",
          "name": "Junyuan Wang"
        }
      ],
      "abstract": "Next Point-of-Interest (POI) recommendation predicts users\u2019 future POIs based on their historical check-in trajectories, which is important for providing personalized travel services. Current methods typically model users\u2019 long- and short-term check-in sequences to capture their preferences, achieving significant performance improvements. However, they adopt uniform network parameters for all users, ignoring the difference in their check-in patterns such as time and frequency across user groups, which limits the performance. To address this issue, we propose a novel framework, CDMBCRec, which incorporates balanced contrastive learning to effectively alleviates the problems of data sparsity and user group heterogeneity, thereby significantly improving the POI recommendation performance. Specifically, to improve adaptability to different user group behaviors, we assign differentiated sequence modeling branches based on the clustering of check-in features such as time, frequency, and category. Unlike existing methods that rely on additional complex structures to handle sequences at different temporal scales, we leverage Mamba, a selective structured state space model (SSSM), to adaptively model users\u2019 long and short-term check-in sequences, simplifying the model structure. Additionally, we design a balanced contrastive learning strategy to better capture intra-user preference similarities and distinguish inter-user travel preferences, thereby enhancing POI recommendation performance. Experimental results on three real-world datasets demonstrate that CDMBCRec outperforms the state-of-the-art methods, validating its performance advantages."
    },
    {
      "paperId": "f5c6f5ac052dcbc148f9b30c06672f23010d25a1",
      "externalIds": {
        "DOI": "10.3390/fractalfract9100673",
        "CorpusId": 282254344
      },
      "corpusId": 282254344,
      "title": "MDF-YOLO: A H\u00f6lder-Based Regularity-Guided Multi-Domain Fusion Detection Model for Indoor Objects",
      "venue": "Fractal and Fractional",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/fractalfract9100673?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/fractalfract9100673, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "1486434150",
          "name": "Fengkai Luan"
        },
        {
          "authorId": "2297635413",
          "name": "Jiaxing Yang"
        },
        {
          "authorId": "2232251553",
          "name": "Hu Zhang"
        }
      ],
      "abstract": "With the rise of embodied agents and indoor service robots, object detection has become a critical component supporting semantic mapping, path planning, and human\u2013robot interaction. However, indoor scenes often face challenges such as severe occlusion, large-scale variations, small and densely packed objects, and complex textures, making existing methods struggle in terms of both robustness and accuracy. This paper proposes MDF-YOLO, a multi-domain fusion detection framework based on H\u00f6lder regularity guidance. In the backbone, neck, and feature recovery stages, the framework introduces the CrossGrid Memory Block, H\u00f6lder-Based Regularity Guidance\u2013Hierarchical Context Aggregation module, and Frequency-Guided Residual Block, achieving complementary feature modeling across the state space, spatial domain, and frequency domain. In particular, the HG-HCA module uses the H\u00f6lder regularity map as a guiding signal to balance the dynamic equilibrium between the macro and micro paths, thus achieving adaptive coordination between global consistency and local discriminability. Experimental results show that MDF-YOLO significantly outperforms mainstream detectors in metrics such as mAP@0.5, mAP@0.75, and mAP@0.5:0.95, achieving values of 0.7158, 0.6117, and 0.5814, respectively, while maintaining near real-time inference efficiency in terms of FPS and latency. Ablation studies further validate the independent and synergistic contributions of CGMB, HG-HCA, and FGRB in improving small-object detection, occlusion handling, and cross-scale robustness. This study demonstrates the potential of H\u00f6lder regularity and multi-domain fusion modeling in object detection, offering new insights for efficient visual modeling in complex indoor environments."
    },
    {
      "paperId": "5efe092a2095077073dc4b6c618185d1a68e2b25",
      "externalIds": {
        "DOI": "10.1109/ICCSNT67644.2025.11291307",
        "CorpusId": 284021716
      },
      "corpusId": 284021716,
      "title": "Research on a VxL-based Multimodal Big Five Personality Prediction Method",
      "venue": "International Conference on Computer Science and Network Technology",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCSNT67644.2025.11291307?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCSNT67644.2025.11291307, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2319243137",
          "name": "Jin Zhang"
        },
        {
          "authorId": "2317502740",
          "name": "Tao Ning"
        },
        {
          "authorId": "2399610929",
          "name": "Panjie Wang"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "4b1a3160375ac19dac432f468b041cf2ba521aa5",
      "externalIds": {
        "ArXiv": "2510.15596",
        "DBLP": "journals/corr/abs-2510-15596",
        "DOI": "10.48550/arXiv.2510.15596",
        "CorpusId": 282203702
      },
      "corpusId": 282203702,
      "title": "PRISM: Probabilistic Runtime Insights and Scalable Performance Modeling for Large-Scale Distributed Training",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.15596, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2253623020",
          "name": "Alicia Golden"
        },
        {
          "authorId": "2328091773",
          "name": "Michael Kuchnik"
        },
        {
          "authorId": "1481699378",
          "name": "Samuel Hsia"
        },
        {
          "authorId": "2257218988",
          "name": "Zach DeVito"
        },
        {
          "authorId": "2266733171",
          "name": "Gu-Yeon Wei"
        },
        {
          "authorId": "2300100031",
          "name": "David Brooks"
        },
        {
          "authorId": "2354511253",
          "name": "Carole-Jean Wu"
        }
      ],
      "abstract": "Large model training beyond tens of thousands of GPUs is an uncharted territory. At such scales, disruptions to the training process are not a matter of if, but a matter of when -- a stochastic process degrading training productivity. Dynamic runtime variation will become increasingly more frequent as training scales up and GPUs are operated in increasingly power-limited and thermally-stressed environments. At the 64k GPU scale, we already observed 9% GPU time variability for frontier foundation model training. To understand potential causes of variability, we analyze GPU microbenchmarks at scale across a variety of platforms, showing up to 14% variation in GPU performance on GEMM workloads depending on training hardware and deployed environment. Motivated by our analysis and the large design space around performance variability, we present PRISM -- a performance modeling framework that considers the stochastic nature of the large-scale distributed training. The core of PRISM is the statistical method that provides a quantifiable measure for probabilistic guarantees on training time. Using PRISM, we explore the design and optimization space of distributed training, from parallelization methods to next-generation training systems. PRISM is validated with real-system measurement, showing training time prediction accuracy with 20.8% Kolmogorov-Smirnov distance. Using PRISM, we demonstrate that, depending on computation node placement, up to 1.26x performance improvement potential is available if we factor in sensitivities of parallelization strategies to variation. In addition, we use PRISM to identify kernels to optimize for reducing performance variability and predict probability of slow-down for large-scale jobs where variation is magnified. We find optimizing communication kernels, such as AllGather and ReduceScatter, contribute most to minimizing variability in training step time."
    },
    {
      "paperId": "e87ca8fda79fa912e272ab3e393fe8e64c6c1e66",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-15439",
        "ArXiv": "2510.15439",
        "DOI": "10.48550/arXiv.2510.15439",
        "CorpusId": 282203521
      },
      "corpusId": 282203521,
      "title": "Rethinking Convergence in Deep Learning: The Predictive-Corrective Paradigm for Anatomy-Informed Brain MRI Segmentation",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.15439, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2335315167",
          "name": "Feifei Zhang"
        },
        {
          "authorId": "2257131437",
          "name": "Zhenhong Jia"
        },
        {
          "authorId": "2003816254",
          "name": "Sensen Song"
        },
        {
          "authorId": "2349843998",
          "name": "Fei Shi"
        },
        {
          "authorId": "2292141757",
          "name": "Dayong Ren"
        }
      ],
      "abstract": "Despite the remarkable success of the end-to-end paradigm in deep learning, it often suffers from slow convergence and heavy reliance on large-scale datasets, which fundamentally limits its efficiency and applicability in data-scarce domains such as medical imaging. In this work, we introduce the Predictive-Corrective (PC) paradigm, a framework that decouples the modeling task to fundamentally accelerate learning. Building upon this paradigm, we propose a novel network, termed PCMambaNet. PCMambaNet is composed of two synergistic modules. First, the Predictive Prior Module (PPM) generates a coarse approximation at low computational cost, thereby anchoring the search space. Specifically, the PPM leverages anatomical knowledge-bilateral symmetry-to predict a'focus map'of diagnostically relevant asymmetric regions. Next, the Corrective Residual Network (CRN) learns to model the residual error, focusing the network's full capacity on refining these challenging regions and delineating precise pathological boundaries. Extensive experiments on high-resolution brain MRI segmentation demonstrate that PCMambaNet achieves state-of-the-art accuracy while converging within only 1-5 epochs-a performance unattainable by conventional end-to-end models. This dramatic acceleration highlights that by explicitly incorporating domain knowledge to simplify the learning objective, PCMambaNet effectively mitigates data inefficiency and overfitting."
    },
    {
      "paperId": "736992e231d01a3b5c08412b604bd77526747f4d",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-15366",
        "ArXiv": "2510.15366",
        "DOI": "10.48550/arXiv.2510.15366",
        "CorpusId": 282203491
      },
      "corpusId": 282203491,
      "title": "Sequence Modeling with Spectral Mean Flows",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.15366, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2144171168",
          "name": "Jinwoo Kim"
        },
        {
          "authorId": "3355645",
          "name": "Maximilian Beier"
        },
        {
          "authorId": "1693503091",
          "name": "Petar Bevanda"
        },
        {
          "authorId": "2386669680",
          "name": "Nayun Kim"
        },
        {
          "authorId": "2266585447",
          "name": "Seunghoon Hong"
        }
      ],
      "abstract": "A key question in sequence modeling with neural networks is how to represent and learn highly nonlinear and probabilistic state dynamics. Operator theory views such dynamics as linear maps on Hilbert spaces containing mean embedding vectors of distributions, offering an appealing but currently overlooked perspective. We propose a new approach to sequence modeling based on an operator-theoretic view of a hidden Markov model (HMM). Instead of materializing stochastic recurrence, we embed the full sequence distribution as a tensor in the product Hilbert space. A generative process is then defined as maximum mean discrepancy (MMD) gradient flow in the space of sequences. To overcome challenges with large tensors and slow sampling convergence, we introduce spectral mean flows, a novel tractable algorithm integrating two core concepts. First, we propose a new neural architecture by leveraging spectral decomposition of linear operators to derive a scalable tensor network decomposition of sequence mean embeddings. Second, we extend MMD gradient flows to time-dependent Hilbert spaces and connect them to flow matching via the continuity equation, enabling simulation-free learning and faster sampling. We demonstrate competitive results on a range of time-series modeling datasets. Code is available at https://github.com/jw9730/spectral-mean-flow."
    },
    {
      "paperId": "a1c9a5f4c45e1259e79e05858e056ecdc8e0d047",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-15371",
        "ArXiv": "2510.15371",
        "DOI": "10.48550/arXiv.2510.15371",
        "CorpusId": 282203487
      },
      "corpusId": 282203487,
      "title": "Cortical-SSM: A Deep State Space Model for EEG and ECoG Motor Imagery Decoding",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.15371, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2386720563",
          "name": "Shuntaro Suzuki"
        },
        {
          "authorId": "2221064375",
          "name": "Shunya Nagashima"
        },
        {
          "authorId": "2365623257",
          "name": "Masayuki Hirata"
        },
        {
          "authorId": "2265582244",
          "name": "Komei Sugiura"
        }
      ],
      "abstract": "Classification of electroencephalogram (EEG) and electrocorticogram (ECoG) signals obtained during motor imagery (MI) has substantial application potential, including for communication assistance and rehabilitation support for patients with motor impairments. These signals remain inherently susceptible to physiological artifacts (e.g., eye blinking, swallowing), which pose persistent challenges. Although Transformer-based approaches for classifying EEG and ECoG signals have been widely adopted, they often struggle to capture fine-grained dependencies within them. To overcome these limitations, we propose Cortical-SSM, a novel architecture that extends deep state space models to capture integrated dependencies of EEG and ECoG signals across temporal, spatial, and frequency domains. We validated our method across three benchmarks: 1) two large-scale public MI EEG datasets containing more than 50 subjects, and 2) a clinical MI ECoG dataset recorded from a patient with amyotrophic lateral sclerosis. Our method outperformed baseline methods on the three benchmarks. Furthermore, visual explanations derived from our model indicate that it effectively captures neurophysiologically relevant regions of both EEG and ECoG signals."
    },
    {
      "paperId": "4e0d7b757c9d38f2155099d6e46828b81c923e92",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-15615",
        "ArXiv": "2510.15615",
        "DOI": "10.48550/arXiv.2510.15615",
        "CorpusId": 282203068
      },
      "corpusId": 282203068,
      "title": "Deep Learning Based Domain Adaptation Methods in Remote Sensing: A Comprehensive Survey",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.15615, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "51129190",
          "name": "Shuchang Lyu"
        },
        {
          "authorId": "2261365994",
          "name": "Qi Zhao"
        },
        {
          "authorId": "2277402930",
          "name": "Zheng Zhou"
        },
        {
          "authorId": "2316710990",
          "name": "Meng Li"
        },
        {
          "authorId": "2373572771",
          "name": "You Zhou"
        },
        {
          "authorId": "2386524727",
          "name": "Dingding Yao"
        },
        {
          "authorId": "2266405312",
          "name": "Guangliang Cheng"
        },
        {
          "authorId": "2359138963",
          "name": "Huiyu Zhou"
        },
        {
          "authorId": "2242082758",
          "name": "Z. Shi"
        }
      ],
      "abstract": "Domain adaptation is a crucial and increasingly important task in remote sensing, aiming to transfer knowledge from a source domain a differently distributed target domain. It has broad applications across various real-world applications, including remote sensing element interpretation, ecological environment monitoring, and urban/rural planning. However, domain adaptation in remote sensing poses significant challenges due to differences in data, such as variations in ground sampling distance, imaging modes from various sensors, geographical landscapes, and environmental conditions. In recent years, deep learning has emerged as a powerful tool for feature representation and cross-domain knowledge transfer, leading to widespread adoption in remote sensing tasks. In this paper, we present a comprehensive survey of significant advancements in deep learning based domain adaptation for remote sensing. We first introduce the preliminary knowledge to clarify key concepts, mathematical notations, and the taxonomy of methodologies. We then organize existing algorithms from multiple perspectives, including task categorization, input mode, supervision paradigm, and algorithmic granularity, providing readers with a structured understanding of the field. Next, we review widely used datasets and summarize the performance of state-of-the-art methods to provide an overview of current progress. We also identify open challenges and potential directions to guide future research in domain adaptation for remote sensing. Compared to previous surveys, this work addresses a broader range of domain adaptation tasks in remote sensing, rather than concentrating on a few subfields. It also presents a systematic taxonomy, providing a more comprehensive and organized understanding of the field. As a whole, this survey can inspire the research community, foster understanding, and guide future work in the field."
    },
    {
      "paperId": "5303193fa9d7805773d6c31b5aa127af4eddc13a",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-16220",
        "ArXiv": "2510.16220",
        "DOI": "10.48550/arXiv.2510.16220",
        "CorpusId": 282209250
      },
      "corpusId": 282209250,
      "title": "VM-BeautyNet: A Synergistic Ensemble of Vision Transformer and Mamba for Facial Beauty Prediction",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.16220, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2267648129",
          "name": "D. E. Boukhari"
        }
      ],
      "abstract": "Facial Beauty Prediction (FBP) is a complex and challenging computer vision task, aiming to model the subjective and intricate nature of human aesthetic perception. While deep learning models, particularly Convolutional Neural Networks (CNNs), have made significant strides, they often struggle to capture the global, holistic facial features that are critical to human judgment. Vision Transformers (ViT) address this by effectively modeling long-range spatial relationships, but their quadratic complexity can be a bottleneck. This paper introduces a novel, heterogeneous ensemble architecture, \\textbf{VM-BeautyNet}, that synergistically fuses the complementary strengths of a Vision Transformer and a Mamba-based Vision model, a recent advancement in State-Space Models (SSMs). The ViT backbone excels at capturing global facial structure and symmetry, while the Mamba backbone efficiently models long-range dependencies with linear complexity, focusing on sequential features and textures. We evaluate our approach on the benchmark SCUT-FBP5500 dataset. Our proposed VM-BeautyNet achieves state-of-the-art performance, with a \\textbf{Pearson Correlation (PC) of 0.9212}, a \\textbf{Mean Absolute Error (MAE) of 0.2085}, and a \\textbf{Root Mean Square Error (RMSE) of 0.2698}. Furthermore, through Grad-CAM visualizations, we provide interpretability analysis that confirms the complementary feature extraction of the two backbones, offering new insights into the model's decision-making process and presenting a powerful new architectural paradigm for computational aesthetics."
    },
    {
      "paperId": "788c08e886a42f3fa6e5dbf80fcf0d0a2ba85c66",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-16209",
        "ArXiv": "2510.16209",
        "DOI": "10.48550/arXiv.2510.16209",
        "CorpusId": 282209335
      },
      "corpusId": 282209335,
      "title": "StretchySnake: Flexible SSM Training Unlocks Action Recognition Across Spatio-Temporal Scales",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.16209, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2273489236",
          "name": "Nyle Siddiqui"
        },
        {
          "authorId": "2110003398",
          "name": "Rohit Gupta"
        },
        {
          "authorId": "143951905",
          "name": "Sirnam Swetha"
        },
        {
          "authorId": "2287971163",
          "name": "Mubarak Shah"
        }
      ],
      "abstract": "State space models (SSMs) have emerged as a competitive alternative to transformers in various tasks. Their linear complexity and hidden-state recurrence make them particularly attractive for modeling long sequences, whereas attention becomes quadratically expensive. However, current training methods for video understanding are tailored towards transformers and fail to fully leverage the unique attributes of SSMs. For example, video models are often trained at a fixed resolution and video length to balance the quadratic scaling of attention cost against performance. Consequently, these models suffer from degraded performance when evaluated on videos with spatial and temporal resolutions unseen during training; a property we call spatio-temporal inflexibility. In the context of action recognition, this severely limits a model's ability to retain performance across both short- and long-form videos. Therefore, we propose a flexible training method that leverages and improves the inherent adaptability of SSMs. Our method samples videos at varying temporal and spatial resolutions during training and dynamically interpolates model weights to accommodate any spatio-temporal scale. This instills our SSM, which we call StretchySnake, with spatio-temporal flexibility and enables it to seamlessly handle videos ranging from short, fine-grained clips to long, complex activities. We introduce and compare five different variants of flexible training, and identify the most effective strategy for video SSMs. On short-action (UCF-101, HMDB-51) and long-action (COIN, Breakfast) benchmarks, StretchySnake outperforms transformer and SSM baselines alike by up to 28%, with strong adaptability to fine-grained actions (SSV2, Diving-48). Therefore, our method provides a simple drop-in training recipe that makes video SSMs more robust, resolution-agnostic, and efficient across diverse action recognition scenarios."
    },
    {
      "paperId": "7fc86d3ad0e37725872c1d22dd964249ba5df51a",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-16161",
        "ArXiv": "2510.16161",
        "DOI": "10.48550/arXiv.2510.16161",
        "CorpusId": 282209042
      },
      "corpusId": 282209042,
      "title": "Still Competitive: Revisiting Recurrent Models for Irregular Time Series Prediction",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.16161, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2294244709",
          "name": "A. Joshi"
        },
        {
          "authorId": "2294272299",
          "name": "M. Hauskrecht"
        }
      ],
      "abstract": "Modeling irregularly sampled multivariate time series is a persistent challenge in domains like healthcare and sensor networks. While recent works have explored a variety of complex learning architectures to solve the prediction problems for irregularly sampled time series, it remains unclear what are the true benefits of some of these architectures, and whether clever modifications of simpler and more efficient RNN-based algorithms are still competitive, i.e. they are on par with or even superior to these methods. In this work, we propose and study GRUwE: Gated Recurrent Unit with Exponential basis functions, that builds upon RNN-based architectures for observations made at irregular times. GRUwE supports both regression-based and event-based predictions in continuous time. GRUwE works by maintaining a Markov state representation of the time series that updates with the arrival of irregular observations. The Markov state update relies on two reset mechanisms: (i) observation-triggered reset, and (ii) time-triggered reset of the GRU state using learnable exponential decays, to support the predictions in continuous time. Our empirical evaluations across several real-world benchmarks on next-observation and next-event prediction tasks demonstrate that GRUwE can indeed achieve competitive to superior performance compared to the recent state-of-the-art (SOTA) methods. Thanks to its simplicity, GRUwE offers compelling advantages: it is easy to implement, requires minimal hyper-parameter tuning efforts, and significantly reduces the computational overhead in the online deployment."
    },
    {
      "paperId": "7026f38dadeb7f28bb58f944d1f08bed9df0190b",
      "externalIds": {
        "DOI": "10.1145/3725843.3756115",
        "CorpusId": 282219620
      },
      "corpusId": 282219620,
      "title": "HLX: A Unified Pipelined Architecture for Optimized Performance of Hybrid Transformer-Mamba Language Models",
      "venue": "Proceedings of the 2025 58th IEEE/ACM International Symposium on Microarchitecture",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3725843.3756115?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3725843.3756115, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2121371502",
          "name": "I. Jung"
        },
        {
          "authorId": "2380326276",
          "name": "Gyeongrok Yang"
        },
        {
          "authorId": "2380448887",
          "name": "Jaeha Min"
        },
        {
          "authorId": "2269140307",
          "name": "Joo-Young Kim"
        }
      ],
      "abstract": "The rapid increase in demand for long-context language models has revealed fundamental performance limitations in conventional Transformer architectures, particularly their quadratic computational complexity. Hybrid Transformer-Mamba models, which interleave attention layers with efficient state-space model layers such as Mamba-2, have emerged as promising solutions combining the strengths of both Transformer and Mamba. However, maintaining a high compute utilization and performance across workloads (e.g., varying sequence length and batch size) in the Hybrid models is challenging due to their heterogeneous compute patterns and shifting performance bottlenecks between the two key computational kernels: FlashAttention-2 (FA-2) and State-Space Duality (SSD). In this paper, we introduce HLX, a unified pipelined architecture designed to ensure optimized performance across workloads for Hybrid models. Through detailed kernel-level analysis, we identify two key blockers that limit compute utilization: inter-operation dependencies in FA-2 and excessive memory traffic in SSD. To overcome these hurdles, we propose two novel fine-grained pipelined dataflows named PipeFlash and PipeSSD. PipeFlash effectively hides operational dependencies in attention computations, while PipeSSD firstly introduces the fused pipelined execution for SSD computations, substantially enhancing data reuse and reducing memory traffic. In addition, we propose a unified hardware architecture that can process both PipeFlash and PipeSSD in an efficient pipelining scheme to maximize the compute utilization. Finally, across sequence lengths from 1K to 128K, the proposed HLX architecture achieves up to 97.5% and 78.4% compute utilization for FA-2 and SSD, respectively, resulting in an average speedup of 1.75 \u00d7 and 2.91 \u00d7 over A100, and an average 2.78 \u00d7 (FA-2), 1.84 \u00d7 (FA-3), and 4.95 \u00d7 speedups over H100. For end-to-end latency and batching, HLX achieves a 1.56 \u00d7 and 1.38 \u00d7 speedup over A100 and a 2.08 \u00d7 and 1.76 \u00d7 (1.84 \u00d7 and 1.72 \u00d7) speedup when running FA-2 (FA-3) on H100. It also significantly reduces area and power consumption by up to 89.8% and 63.8% compared to GPU baselines."
    },
    {
      "paperId": "b20573d6624fb929a0ca81801885f62e874f5b89",
      "externalIds": {
        "DBLP": "journals/ijmir/BoukhariC25",
        "DOI": "10.1007/s13735-025-00387-3",
        "CorpusId": 282199830
      },
      "corpusId": 282199830,
      "title": "Enhancing Facial Beauty Prediction via a Dual-Pathway Hybrid Architecture Integrating Vmamba and ViT",
      "venue": "International Journal of Multimedia Information Retrieval",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s13735-025-00387-3?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s13735-025-00387-3, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2267648129",
          "name": "D. E. Boukhari"
        },
        {
          "authorId": "2673286",
          "name": "A. Chemsa"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "82e3057b44d7b474a1fb0f400833bcc6074bd707",
      "externalIds": {
        "DOI": "10.1007/s42235-025-00790-w",
        "CorpusId": 282234746
      },
      "corpusId": 282234746,
      "title": "MSAMamba-UNet: A Lightweight Multi-Scale Adaptive Mamba Network for Skin Lesion Segmentation",
      "venue": "Journal of Bionic Engineering",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s42235-025-00790-w?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s42235-025-00790-w, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2354189015",
          "name": "Shouming Hou"
        },
        {
          "authorId": "2299118085",
          "name": "Jianchao Hou"
        },
        {
          "authorId": "2299042487",
          "name": "Yuteng Pang"
        },
        {
          "authorId": "2353152912",
          "name": "Aoyu Xia"
        },
        {
          "authorId": "2386647207",
          "name": "Beibei Hou"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "030a3249d0afcac02400c3cd393048c49532f0ee",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-14573",
        "ArXiv": "2510.14573",
        "DOI": "10.48550/arXiv.2510.14573",
        "CorpusId": 282139284
      },
      "corpusId": 282139284,
      "title": "State-Space Models for Tabular Prior-Data Fitted Networks",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.14573, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2378132027",
          "name": "Felix Koch"
        },
        {
          "authorId": "2386032397",
          "name": "Marcel Wever"
        },
        {
          "authorId": "2341716712",
          "name": "Fabian Raisch"
        },
        {
          "authorId": "2341713300",
          "name": "Benjamin Tischler"
        }
      ],
      "abstract": "Recent advancements in foundation models for tabular data, such as TabPFN, demonstrated that pretrained Transformer architectures can approximate Bayesian inference with high predictive performance. However, Transformers suffer from quadratic complexity with respect to sequence length, motivating the exploration of more efficient sequence models. In this work, we investigate the potential of using Hydra, a bidirectional linear-time structured state space model (SSM), as an alternative to Transformers in TabPFN. A key challenge lies in SSM's inherent sensitivity to the order of input tokens - an undesirable property for tabular datasets where the row order is semantically meaningless. We investigate to what extent a bidirectional approach can preserve efficiency and enable symmetric context aggregation. Our experiments show that this approach reduces the order-dependence, achieving predictive performance competitive to the original TabPFN model."
    },
    {
      "paperId": "33d0efb0f7beed2d21179568e0461ba9657e010b",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-14386",
        "ArXiv": "2510.14386",
        "DOI": "10.48550/arXiv.2510.14386",
        "CorpusId": 282138854
      },
      "corpusId": 282138854,
      "title": "SHaRe-SSM: An Oscillatory Spiking Neural Network for Target Variable Modeling in Long Sequences",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.14386, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2337031997",
          "name": "Kartikay Agrawal"
        },
        {
          "authorId": "2386014019",
          "name": "Abhijeet Vikram"
        },
        {
          "authorId": "2387894102",
          "name": "Vedant Sharma"
        },
        {
          "authorId": "2386024572",
          "name": "N. Vaishnavi"
        },
        {
          "authorId": "22697091",
          "name": "Ayon Borthakur"
        }
      ],
      "abstract": "In recent years, with the emergence of large models, there has been a significant interest in spiking neural networks (SNNs) primarily due to their energy efficiency, multiplication-free, and sparse event-based deep learning. Similarly, state space models (SSMs) in varying designs have evolved as a powerful alternative to transformers for target modeling in long sequences, thereby overcoming the quadratic dependence on sequence length of a transformer. Inspired by this progress, we here design SHaRe-SSM (Spiking Harmonic Resonate and Fire State Space Model), for target variable modeling (including both classification and regression) for very-long-range sequences. Our second-order spiking SSM, on average, performs better than transformers or first-order SSMs while circumventing multiplication operations, making it ideal for resource-constrained applications. The proposed block consumes $73 \\times$ less energy than second-order ANN-based SSMs for an 18k sequence, while retaining performance. To ensure learnability over the long-range sequences, we propose exploiting the stable and efficient implementation of the dynamical system using parallel scans. Moreover, for the first time, we propose a kernel-based spiking regressor using resonate and fire neurons for very long-range sequences. Our network shows superior performance on even a 50k sequence while being significantly energy-efficient. In addition, we conducted a systematic analysis of the impact of heterogeneity, dissipation, and conservation in resonate-and-fire SSMs."
    },
    {
      "paperId": "6423ec3669db5f497bbd94e93822b152cf8c15bd",
      "externalIds": {
        "ArXiv": "2510.14826",
        "DBLP": "journals/corr/abs-2510-14826",
        "DOI": "10.48550/arXiv.2510.14826",
        "CorpusId": 282139630
      },
      "corpusId": 282139630,
      "title": "To Infinity and Beyond: Tool-Use Unlocks Length Generalization in State Space Models",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.14826, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "19201820",
          "name": "Eran Malach"
        },
        {
          "authorId": "2438203",
          "name": "O. Saremi"
        },
        {
          "authorId": "2385480488",
          "name": "Sinead Williamson"
        },
        {
          "authorId": "2261389630",
          "name": "Arwen Bradley"
        },
        {
          "authorId": "2047543498",
          "name": "Aryo Lotfi"
        },
        {
          "authorId": "2258723954",
          "name": "Emmanuel Abbe"
        },
        {
          "authorId": "2243336902",
          "name": "Josh Susskind"
        },
        {
          "authorId": "1762320",
          "name": "Etai Littwin"
        }
      ],
      "abstract": "State Space Models (SSMs) have become the leading alternative to Transformers for sequence modeling. Their primary advantage is efficiency in long-context and long-form generation, enabled by fixed-size memory and linear scaling of computational complexity. We begin this work by showing a simple theoretical result stating that SSMs cannot accurately solve any ``truly long-form''generation problem (in a sense we formally define), undermining their main competitive advantage. However, we show that this limitation can be mitigated by allowing SSMs interactive access to external tools. In fact, we show that given the right choice of tool access and problem-dependent training data, SSMs can learn to solve any tractable problem and generalize to arbitrary problem length/complexity (i.e., achieve length generalization). Following our theoretical finding, we demonstrate that tool-augmented SSMs achieve remarkable length generalization on a variety of arithmetic, reasoning, and coding tasks. These findings highlight SSMs as a potential efficient alternative to Transformers in interactive tool-based and agentic settings."
    },
    {
      "paperId": "2b07e95205a3495851b1269999e3fb8f488b9c4e",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-14516",
        "ArXiv": "2510.14516",
        "DOI": "10.48550/arXiv.2510.14516",
        "CorpusId": 282139317
      },
      "corpusId": 282139317,
      "title": "Vision Mamba for Permeability Prediction of Porous Media",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.14516, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2267437739",
          "name": "Ali Kashefi"
        },
        {
          "authorId": "2288648348",
          "name": "Tapan Mukerji"
        }
      ],
      "abstract": "Vision Mamba has recently received attention as an alternative to Vision Transformers (ViTs) for image classification. The network size of Vision Mamba scales linearly with input image resolution, whereas ViTs scale quadratically, a feature that improves computational and memory efficiency. Moreover, Vision Mamba requires a significantly smaller number of trainable parameters than traditional convolutional neural networks (CNNs), and thus, they can be more memory efficient. Because of these features, we introduce, for the first time, a neural network that uses Vision Mamba as its backbone for predicting the permeability of three-dimensional porous media. We compare the performance of Vision Mamba with ViT and CNN models across multiple aspects of permeability prediction and perform an ablation study to assess the effects of its components on accuracy. We demonstrate in practice the aforementioned advantages of Vision Mamba over ViTs and CNNs in the permeability prediction of three-dimensional porous media. We make the source code publicly available to facilitate reproducibility and to enable other researchers to build on and extend this work. We believe the proposed framework has the potential to be integrated into large vision models in which Vision Mamba is used instead of ViTs."
    },
    {
      "paperId": "455ab42502b5cfd2e0ea55c66f8faf72cccbd2d7",
      "externalIds": {
        "ArXiv": "2510.14542",
        "DBLP": "journals/corr/abs-2510-14542",
        "DOI": "10.48550/arXiv.2510.14542",
        "CorpusId": 282139265
      },
      "corpusId": 282139265,
      "title": "A Deep State-Space Model Compression Method using Upper Bound on Output Error",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.14542, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2211681158",
          "name": "Hiroki Sakamoto"
        },
        {
          "authorId": "2287119044",
          "name": "Kazuhiro Sato"
        }
      ],
      "abstract": "We study deep state-space models (Deep SSMs) that contain linear-quadratic-output (LQO) systems as internal blocks and present a compression method with a provable output error guarantee. We first derive an upper bound on the output error between two Deep SSMs and show that the bound can be expressed via the $h^2$-error norms between the layerwise LQO systems, thereby providing a theoretical justification for existing model order reduction (MOR)-based compression. Building on this bound, we formulate an optimization problem in terms of the $h^2$-error norm and develop a gradient-based MOR method. On the IMDb task from the Long Range Arena benchmark, we demonstrate that our compression method achieves strong performance. Moreover, unlike prior approaches, we reduce roughly 80% of trainable parameters without retraining, with only a 4-5% performance drop."
    },
    {
      "paperId": "19b4c29a12ea0357672cbf10853983deaa922cdb",
      "externalIds": {
        "ArXiv": "2510.14946",
        "DBLP": "journals/corr/abs-2510-14946",
        "DOI": "10.48550/arXiv.2510.14946",
        "CorpusId": 282138880
      },
      "corpusId": 282138880,
      "title": "EdgeNavMamba: Mamba Optimized Object Detection for Energy Efficient Edge Devices",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.14946, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2346327200",
          "name": "Romina Aalishah"
        },
        {
          "authorId": "2180067662",
          "name": "Mozhgan Navardi"
        },
        {
          "authorId": "2393902",
          "name": "T. Mohsenin"
        }
      ],
      "abstract": "Deployment of efficient and accurate Deep Learning models has long been a challenge in autonomous navigation, particularly for real-time applications on resource-constrained edge devices. Edge devices are limited in computing power and memory, making model efficiency and compression essential. In this work, we propose EdgeNavMamba, a reinforcement learning-based framework for goal-directed navigation using an efficient Mamba object detection model. To train and evaluate the detector, we introduce a custom shape detection dataset collected in diverse indoor settings, reflecting visual cues common in real-world navigation. The object detector serves as a pre-processing module, extracting bounding boxes (BBOX) from visual input, which are then passed to an RL policy to control goal-oriented navigation. Experimental results show that the student model achieved a reduction of 67% in size, and up to 73% in energy per inference on edge devices of NVIDIA Jetson Orin Nano and Raspberry Pi 5, while keeping the same performance as the teacher model. EdgeNavMamba also maintains high detection accuracy in MiniWorld and IsaacLab simulators while reducing parameters by 31% compared to the baseline. In the MiniWorld simulator, the navigation policy achieves over 90% success across environments of varying complexity."
    },
    {
      "paperId": "4d09482cb13d53f79b607f8b7163fa120d27d13b",
      "externalIds": {
        "PubMedCentral": "12533092",
        "DOI": "10.1038/s41598-025-20179-3",
        "CorpusId": 282146536,
        "PubMed": "41102387"
      },
      "corpusId": 282146536,
      "title": "A music source separation method integrating time\u2013frequency decoupling and mamba-based state space modeling",
      "venue": "Scientific Reports",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBYNCND",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12533092, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2345324227",
          "name": "Chongbin Zhang"
        },
        {
          "authorId": "2325916863",
          "name": "Jiaxiang Zheng"
        },
        {
          "authorId": "2325708678",
          "name": "Moxi Cao"
        }
      ],
      "abstract": "Music source separation, as a fundamental task in intelligent audio processing, plays a critical role in enhancing the performance of music generation, editing, and understanding systems. However, existing separation models often suffer from structural limitations such as reliance on a single modeling path, entangled time-frequency representations, and difficulty in adapting to heterogeneous sound sources. Furthermore, they struggle to maintain an effective balance between long-range dependency modeling and inference efficiency. To address these challenges, this paper proposes a novel dual-path state space modeling architecture, MSNet. By introducing decoupled modeling mechanisms for temporal and frequency pathways, and incorporating Mamba-based state space units for multidimensional structural parsing of audio signals, MSNet enhances selective control and structural representation in time-frequency modeling. Experimental results demonstrate that MSNet achieves state-of-the-art performance on the MUSDB18 dataset across multiple evaluation metrics. In particular, it shows superior robustness and stability when dealing with dynamically complex sources such as vocals and drums. Additionally, the model achieves a real-time factor (RTF) below 0.1 while maintaining superior separation quality, making it suitable for deployment in practical applications. This study not only demonstrates the feasibility of state space models for complex audio modeling but also introduces a new architectural paradigm for music source separation that balances accuracy and efficiency. The implementation is publicly available at: https://github.com/NMLAB8/Mamba-S-Net."
    },
    {
      "paperId": "f07eece83a1e3c7c218d3b55e57b2e16a9365b00",
      "externalIds": {
        "ArXiv": "2510.13253",
        "DBLP": "journals/corr/abs-2510-13253",
        "DOI": "10.48550/arXiv.2510.13253",
        "CorpusId": 282102707
      },
      "corpusId": 282102707,
      "title": "End-to-End Multi-Modal Diffusion Mamba",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 3,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.13253, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2332301276",
          "name": "Chunhao Lu"
        },
        {
          "authorId": "2321058972",
          "name": "Qiang Lu"
        },
        {
          "authorId": "2386793334",
          "name": "Meichen Dong"
        },
        {
          "authorId": "2111695985",
          "name": "Jake Luo"
        }
      ],
      "abstract": "Current end-to-end multi-modal models utilize different encoders and decoders to process input and output information. This separation hinders the joint representation learning of various modalities. To unify multi-modal processing, we propose a novel architecture called MDM (Multi-modal Diffusion Mamba). MDM utilizes a Mamba-based multi-step selection diffusion model to progressively generate and refine modality-specific information through a unified variational autoencoder for both encoding and decoding. This innovative approach allows MDM to achieve superior performance when processing high-dimensional data, particularly in generating high-resolution images and extended text sequences simultaneously. Our evaluations in areas such as image generation, image captioning, visual question answering, text comprehension, and reasoning tasks demonstrate that MDM significantly outperforms existing end-to-end models (MonoFormer, LlamaGen, and Chameleon etc.) and competes effectively with SOTA models like GPT-4V, Gemini Pro, and Mistral. Our results validate MDM's effectiveness in unifying multi-modal processes while maintaining computational efficiency, establishing a new direction for end-to-end multi-modal architectures."
    },
    {
      "paperId": "6b9423004774239af099670afe48d8c4062a9930",
      "externalIds": {
        "ArXiv": "2510.14027",
        "DBLP": "journals/corr/abs-2510-14027",
        "DOI": "10.48550/arXiv.2510.14027",
        "CorpusId": 282138493
      },
      "corpusId": 282138493,
      "title": "Context-Selective State Space Models: Feedback is All You Need",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.14027, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2386013618",
          "name": "Riccardo Zattra"
        },
        {
          "authorId": "2308294878",
          "name": "Giacomo Baggio"
        },
        {
          "authorId": "2247459898",
          "name": "Umberto Casti"
        },
        {
          "authorId": "2285340595",
          "name": "Augusto Ferrante"
        },
        {
          "authorId": "2302469268",
          "name": "Francesco Ticozzi"
        }
      ],
      "abstract": "Transformers, powered by the attention mechanism, are the backbone of most foundation models, yet they suffer from quadratic complexity and difficulties in dealing with long-range dependencies in the input sequence. Recent work has shown that state space models (SSMs) provide an efficient alternative, with the S6 module at the core of the Mamba architecture achieving state-of-the-art results on long-sequence benchmarks. In this paper, we introduce the COFFEE (COntext From FEEdback) model, a novel time-varying SSM that incorporates state feedback to enable context-dependent selectivity, while still allowing for parallel implementation. Whereas the selectivity mechanism of S6 only depends on the current input, COFFEE computes it from the internal state, which serves as a compact representation of the sequence history. This shift allows the model to regulate its dynamics based on accumulated context, improving its ability to capture long-range dependencies. In addition to state feedback, we employ an efficient model parametrization that removes redundancies present in S6 and leads to a more compact and trainable formulation. On the induction head task, COFFEE achieves near-perfect accuracy with two orders of magnitude fewer parameters and training sequences compared to S6. On MNIST, COFFEE largely outperforms S6 within the same architecture, reaching 97% accuracy with only 3585 parameters. These results showcase the role of state feedback as a key mechanism for building scalable and efficient sequence models."
    },
    {
      "paperId": "ffc532c27e24b495da207e45ad974aac788b0a6a",
      "externalIds": {
        "DOI": "10.1109/JSEN.2025.3605401",
        "CorpusId": 282105109
      },
      "corpusId": 282105109,
      "title": "Self-Supervised Monocular Depth Estimation Using Hybrid CNN-VMamba Architecture",
      "venue": "IEEE Sensors Journal",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JSEN.2025.3605401?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JSEN.2025.3605401, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2108862221",
          "name": "Baigan Zhao"
        },
        {
          "authorId": "2314601985",
          "name": "Jiejia Wang"
        },
        {
          "authorId": "2313876388",
          "name": "Zhiyang Guo"
        },
        {
          "authorId": "2386492646",
          "name": "Zhihua Zhang"
        },
        {
          "authorId": "2385962594",
          "name": "Huihui Hu"
        },
        {
          "authorId": "2385972382",
          "name": "Chunhui Ma"
        },
        {
          "authorId": "2386204751",
          "name": "Ruifeng Li"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "b682afaa5e7063350669b95d11bab434115393e7",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-13245",
        "ArXiv": "2510.13245",
        "DOI": "10.48550/arXiv.2510.13245",
        "CorpusId": 282102202
      },
      "corpusId": 282102202,
      "title": "CymbaDiff: Structured Spatial Diffusion for Sketch-based 3D Semantic Urban Scene Generation",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.13245, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2307892971",
          "name": "Li Liang"
        },
        {
          "authorId": "2372611620",
          "name": "Bo Miao"
        },
        {
          "authorId": "2344939625",
          "name": "Xinyu Wang"
        },
        {
          "authorId": "47398812",
          "name": "Naveed Akhtar"
        },
        {
          "authorId": "1500528066",
          "name": "J. Vice"
        },
        {
          "authorId": "2247160161",
          "name": "Ajmal Mian"
        }
      ],
      "abstract": "Outdoor 3D semantic scene generation produces realistic and semantically rich environments for applications such as urban simulation and autonomous driving. However, advances in this direction are constrained by the absence of publicly available, well-annotated datasets. We introduce SketchSem3D, the first large-scale benchmark for generating 3D outdoor semantic scenes from abstract freehand sketches and pseudo-labeled annotations of satellite images. SketchSem3D includes two subsets, Sketch-based SemanticKITTI and Sketch-based KITTI-360 (containing LiDAR voxels along with their corresponding sketches and annotated satellite images), to enable standardized, rigorous, and diverse evaluations. We also propose Cylinder Mamba Diffusion (CymbaDiff) that significantly enhances spatial coherence in outdoor 3D scene generation. CymbaDiff imposes structured spatial ordering, explicitly captures cylindrical continuity and vertical hierarchy, and preserves both physical neighborhood relationships and global context within the generated scenes. Extensive experiments on SketchSem3D demonstrate that CymbaDiff achieves superior semantic consistency, spatial realism, and cross-dataset generalization. The code and dataset will be available at https://github.com/Lillian-research-hub/CymbaDiff"
    },
    {
      "paperId": "7c6006f9c8ce651ef2ec717e9fa42acd9dde4f5a",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-13219",
        "ArXiv": "2510.13219",
        "DOI": "10.48550/arXiv.2510.13219",
        "CorpusId": 282103008
      },
      "corpusId": 282103008,
      "title": "Prompt-based Adaptation in Large-scale Vision Models: A Survey",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 4,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.13219, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2349232014",
          "name": "Xi Xiao"
        },
        {
          "authorId": "2221081346",
          "name": "Yunbei Zhang"
        },
        {
          "authorId": "2376345448",
          "name": "Lin Zhao"
        },
        {
          "authorId": "2322605728",
          "name": "Yiyang Liu"
        },
        {
          "authorId": "2385463236",
          "name": "Xiaoying Liao"
        },
        {
          "authorId": "2349870546",
          "name": "Zheda Mai"
        },
        {
          "authorId": "2307292971",
          "name": "Xingjian Li"
        },
        {
          "authorId": "2352220179",
          "name": "Xiao Wang"
        },
        {
          "authorId": "2370720726",
          "name": "Hao Xu"
        },
        {
          "authorId": "2282741360",
          "name": "Jihun Hamm"
        },
        {
          "authorId": "2393659078",
          "name": "Xueyan Lin"
        },
        {
          "authorId": "2308674053",
          "name": "Min Xu"
        },
        {
          "authorId": "2288041686",
          "name": "Qifan Wang"
        },
        {
          "authorId": "2350009636",
          "name": "Tianyang Wang"
        },
        {
          "authorId": "2289791261",
          "name": "Cheng Han"
        }
      ],
      "abstract": "In computer vision, Visual Prompting (VP) and Visual Prompt Tuning (VPT) have recently emerged as lightweight and effective alternatives to full fine-tuning for adapting large-scale vision models within the ``pretrain-then-finetune''paradigm. However, despite rapid progress, their conceptual boundaries remain blurred, as VP and VPT are frequently used interchangeably in current research, reflecting a lack of systematic distinction between these techniques and their respective applications. In this survey, we revisit the designs of VP and VPT from first principles, and conceptualize them within a unified framework termed Prompt-based Adaptation (PA). We provide a taxonomy that categorizes existing methods into learnable, generative, and non-learnable prompts, and further organizes them by injection granularity -- pixel-level and token-level. Beyond the core methodologies, we examine PA's integrations across diverse domains, including medical imaging, 3D point clouds, and vision-language tasks, as well as its role in test-time adaptation and trustworthy AI. We also summarize current benchmarks and identify key challenges and future directions. To the best of our knowledge, we are the first comprehensive survey dedicated to PA's methodologies and applications in light of their distinct characteristics. Our survey aims to provide a clear roadmap for researchers and practitioners in all area to understand and explore the evolving landscape of PA-related research."
    },
    {
      "paperId": "de318eaf21e14f16ea79b4fec30a982e4a7bc03c",
      "externalIds": {
        "ArXiv": "2510.13366",
        "DBLP": "journals/corr/abs-2510-13366",
        "DOI": "10.48550/arXiv.2510.13366",
        "CorpusId": 282102917
      },
      "corpusId": 282102917,
      "title": "Document Intelligence in the Era of Large Language Models: A Survey",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.13366, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2384785420",
          "name": "Weishi Wang"
        },
        {
          "authorId": "2385974524",
          "name": "Hengchang Hu"
        },
        {
          "authorId": "2385967352",
          "name": "Zhijie Zhang"
        },
        {
          "authorId": "2385834805",
          "name": "Zhaochen Li"
        },
        {
          "authorId": "2385788765",
          "name": "Hongxin Shao"
        },
        {
          "authorId": "2384774982",
          "name": "Daniel Dahlmeier"
        }
      ],
      "abstract": "Document AI (DAI) has emerged as a vital application area, and is significantly transformed by the advent of large language models (LLMs). While earlier approaches relied on encoder-decoder architectures, decoder-only LLMs have revolutionized DAI, bringing remarkable advancements in understanding and generation. This survey provides a comprehensive overview of DAI's evolution, highlighting current research attempts and future prospects of LLMs in this field. We explore key advancements and challenges in multimodal, multilingual, and retrieval-augmented DAI, while also suggesting future research directions, including agent-based approaches and document-specific foundation models. This paper aims to provide a structured analysis of the state-of-the-art in DAI and its implications for both academic and practical applications."
    },
    {
      "paperId": "4b9b3c8460158eef1261b58d88c0c2d12a50b289",
      "externalIds": {
        "DOI": "10.1109/JSEN.2025.3606150",
        "CorpusId": 281274086
      },
      "corpusId": 281274086,
      "title": "ARFF-VO: A Self-Supervised Monocular Visual Odometry With Adaptive Region-Based Feature Filtering in Dynamic Scenes",
      "venue": "IEEE Sensors Journal",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JSEN.2025.3606150?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JSEN.2025.3606150, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2370309470",
          "name": "Guofeng Tong"
        },
        {
          "authorId": "2380555872",
          "name": "Yuanyang Zhang"
        },
        {
          "authorId": "2155012181",
          "name": "Zheng Li"
        },
        {
          "authorId": "2380186021",
          "name": "Jiaru Sun"
        }
      ],
      "abstract": "Self-supervised monocular visual odometry (MVO) provides a solution for robot localization and mapping without the need for labeled data by minimizing image reconstruction loss to train the network. However, existing methods explicitly remove dynamic objects by introducing semantic masks, which limits their adaptability to dynamic pixels. In this article, we propose ARFF-VO, which integrates a dynamic removal strategy into the network to enable the model to self-adaptively suppress redundant information. To fully exploit nonredundant features, we introduce region-structure perception (R-SP) module that utilizes high-level semantic information to construct perception features and confidence scores. Additionally, we employ the Vim block, with selective state-space models (SSMs) as its core operator, to build the pose decoder. The model effectively compresses contextual information to enhance long-sequence modeling capability. Furthermore, since monocular depth estimation and pose prediction are simultaneously trained, the performance improvement of visual odometry also positively impacts depth estimation. Evaluations on the KITTI dataset demonstrate that our method achieves superior performance compared to state-of-the-art self-supervised methods."
    },
    {
      "paperId": "4dbff2eddc51ce462fef583b8773bc7228b1af63",
      "externalIds": {
        "PubMedCentral": "12522341",
        "DOI": "10.1186/s12915-025-02407-4",
        "CorpusId": 282134824,
        "PubMed": "41094478"
      },
      "corpusId": 282134824,
      "title": "BiMA-DTI: a bidirectional Mamba-Attention hybrid framework for enhanced drug-target interaction prediction",
      "venue": "BMC Biology",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBYNCND",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12522341, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2296754240",
          "name": "Youyuan Shui"
        },
        {
          "authorId": "2279976452",
          "name": "Xuewen Ge"
        },
        {
          "authorId": "2327842681",
          "name": "Chen Cao"
        },
        {
          "authorId": "2109764376",
          "name": "Junjie Wang"
        },
        {
          "authorId": "2116198229",
          "name": "Jie Hu"
        },
        {
          "authorId": "2296768801",
          "name": "Yun Liu"
        }
      ],
      "abstract": "Predicting drug-target interactions (DTIs) is essential for accelerating drug discovery, yet traditional experimental methods are time-consuming and costly. Computational approaches, especially those using machine learning and deep learning, offer a more efficient alternative. This paper presents the BiMA-DTI framework, which integrates Mamba\u2019s State Space Model (SSM) with multi-head attention mechanisms. This combination maximizes Mamba\u2019s ability to process long sequences while taking advantage of the attention mechanism\u2019s strength in handling short sequences. We designed a hybrid Mamba-Attention Network (MAN) and a Graph Mamba Network (GMN) for processing multimodal inputs, including protein amino acid sequences, Simplified Molecular Input Line Entry System (SMILES) strings, and molecular graphs of drugs, enabling comprehensive feature extraction and fusion. To enhance the complementarity between features extracted from sequences and graphs, BiMA-DTI performs a two-step weighted fusion of sequence features of drugs and proteins with molecular graph features of drugs. Finally, these fused features are concatenated and passed through a fully connected network to predict DTIs. BiMA-DTI demonstrates its potential to discover new drugs, offering a powerful tool for drug discovery. Experimental results show that BiMA-DTI outperforms state-of-the-art competing methods on benchmark datasets. Additionally, ablation experiments validate the rationality of BiMA-DTI\u2019s architecture and its generalization ability. Visualization studies provide interpretability of biological mechanisms. Finally, case studies further confirm that BiMA-DTI is a reliable drug-target interaction prediction tool."
    },
    {
      "paperId": "2df153f9cd66401fd884852313ab803bb555348e",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-17860",
        "ArXiv": "2510.17860",
        "DOI": "10.48550/arXiv.2510.17860",
        "CorpusId": 282246360
      },
      "corpusId": 282246360,
      "title": "DMTrack: Deformable State-Space Modeling for UAV Multi-Object Tracking with Kalman Fusion and Uncertainty-Aware Association",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.17860, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2354260566",
          "name": "Zenghuang Fu"
        },
        {
          "authorId": "2353440123",
          "name": "Xiaofeng Han"
        },
        {
          "authorId": "2308386842",
          "name": "Mingda Jia"
        },
        {
          "authorId": "2386939873",
          "name": "Jin ming Yang"
        },
        {
          "authorId": "2387893685",
          "name": "Qi Zeng"
        },
        {
          "authorId": "2386819067",
          "name": "Muyang Zahng"
        },
        {
          "authorId": "2387288489",
          "name": "Changwei Wang"
        },
        {
          "authorId": "35965884",
          "name": "Weiliang Meng"
        },
        {
          "authorId": "2386930994",
          "name": "Xiaopeng Zhang"
        }
      ],
      "abstract": "Multi-object tracking (MOT) from unmanned aerial vehicles (UAVs) presents unique challenges due to unpredictable object motion, frequent occlusions, and limited appearance cues inherent to aerial viewpoints. These issues are further exacerbated by abrupt UAV movements, leading to unreliable trajectory estimation and identity switches. Conventional motion models, such as Kalman filters or static sequence encoders, often fall short in capturing both linear and non-linear dynamics under such conditions. To tackle these limitations, we propose DMTrack, a deformable motion tracking framework tailored for UAV-based MOT. Our DMTrack introduces three key components: DeformMamba, a deformable state-space predictor that dynamically aggregates historical motion states for adaptive trajectory modeling; MotionGate, a lightweight gating module that fuses Kalman and Mamba predictions based on motion context and uncertainty; and an uncertainty-aware association strategy that enhances identity preservation by aligning motion trends with prediction confidence. Extensive experiments on the VisDrone-MOT and UAVDT benchmarks demonstrate that our DMTrack achieves state-of-the-art performance in identity consistency and tracking accuracy, particularly under high-speed and non-linear motion. Importantly, our method operates without appearance models and maintains competitive efficiency, highlighting its practicality for robust UAV-based tracking."
    },
    {
      "paperId": "f4b09a3e36247f44ef05e072c456ef040405d355",
      "externalIds": {
        "DOI": "10.1101/2025.10.14.682350",
        "CorpusId": 282306661
      },
      "corpusId": 282306661,
      "title": "FlashRNA: An Efficient Model for Regulatory Genomics",
      "venue": "bioRxiv",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1101/2025.10.14.682350?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1101/2025.10.14.682350, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "120153134",
          "name": "A. Jung"
        },
        {
          "authorId": "2249072080",
          "name": "Helen Zhu"
        },
        {
          "authorId": "2248086238",
          "name": "A. J. Gao"
        },
        {
          "authorId": "2387846052",
          "name": "Roujia Li"
        },
        {
          "authorId": "2212383504",
          "name": "Mykhaylo Slobodyanyuk"
        },
        {
          "authorId": "2387253426",
          "name": "Vivian Chu"
        },
        {
          "authorId": "2290279992",
          "name": "Declan Lim"
        },
        {
          "authorId": "2258684419",
          "name": "Leo J. Lee"
        },
        {
          "authorId": "6460452",
          "name": "Albi Celaj"
        },
        {
          "authorId": "2258552144",
          "name": "Brendan J. Frey"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "9f2393ddc9c57fee26189cf5747e1222cff43777",
      "externalIds": {
        "ArXiv": "2510.12573",
        "DBLP": "journals/corr/abs-2510-12573",
        "DOI": "10.48550/arXiv.2510.12573",
        "CorpusId": 282064756
      },
      "corpusId": 282064756,
      "title": "Learning Human Motion with Temporally Conditional Mamba",
      "venue": "Proceedings of the SIGGRAPH Asia 2025 Conference Papers",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.12573, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2349424638",
          "name": "Quang Nguyen"
        },
        {
          "authorId": "2349435780",
          "name": "Tri Le"
        },
        {
          "authorId": "2243927673",
          "name": "Baoru Huang"
        },
        {
          "authorId": "2261390984",
          "name": "M. Vu"
        },
        {
          "authorId": "144556922",
          "name": "Ngan Le"
        },
        {
          "authorId": "2196372534",
          "name": "T. Vo"
        },
        {
          "authorId": "2374188616",
          "name": "Anh Nguyen"
        }
      ],
      "abstract": "Learning human motion based on a time-dependent input signal presents a challenging yet impactful task with various applications. The goal of this task is to generate or estimate human movement that consistently reflects the temporal patterns of conditioning inputs. Existing methods typically rely on cross-attention mechanisms to fuse the condition with motion. However, this approach primarily captures global interactions and struggles to maintain step-by-step temporal alignment. To address this limitation, we introduce Temporally Conditional Mamba, a new mamba-based model for human motion generation. Our approach integrates conditional information into the recurrent dynamics of the Mamba block, enabling better temporally aligned motion. To validate the effectiveness of our method, we evaluate it on a variety of human motion tasks. Extensive experiments demonstrate that our model significantly improves temporal alignment, motion realism, and condition consistency over state-of-the-art approaches. Our project page is available at https://zquang2202.github.io/TCM."
    },
    {
      "paperId": "8ab56b0a687a4adc43539cd6b63c8ad3ae775fda",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-12111",
        "ArXiv": "2510.12111",
        "DOI": "10.48550/arXiv.2510.12111",
        "CorpusId": 282064587
      },
      "corpusId": 282064587,
      "title": "Chimera: State Space Models Beyond Sequences",
      "venue": "Trans. Mach. Learn. Res.",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.12111, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2218599184",
          "name": "Aakash Lahoti"
        },
        {
          "authorId": "2385559702",
          "name": "Tanya Marwah"
        },
        {
          "authorId": "2990638",
          "name": "Ratish Puduppully"
        },
        {
          "authorId": "2319601320",
          "name": "Albert Gu"
        }
      ],
      "abstract": "Transformer-based deep learning methods have become the standard approach for modeling diverse data such as sequences, images, and graphs. These methods rely on self-attention, which treats data as an unordered set of elements. This ignores the neighborhood structure or graph topology of the data and requires inductive biases--such as position embeddings in sequences and images, or random walks in graphs--to incorporate topology. However, designing such task-specific biases requires significant effort and can introduce side effects that hinder generalization. We introduce Chimera, a unified model that directly incorporates data topology in a principled way, removing the need for domain-specific biases. The key idea is that state space models--which naturally do not require position embeddings--can be generalized to capture any graph topology. Our experiments show that Chimera achieves strong performance across language, vision, and graph domains, outperforming BERT on GLUE by 0.7 points, ViT on ImageNet-1k by 2.6%, and all baselines on the Long Range Graph Benchmark. We further propose algorithmic optimizations to improve Chimera's efficiency: (1) for Directed Acyclic Graphs, Chimera can be implemented as a linear-time recurrence; (2) for general graphs, a simple mathematical relaxation achieves Transformer's quadratic complexity without domain-specific heuristics. These results validate Chimera's core contribution and support the idea that data topology is a powerful inductive bias across modalities."
    },
    {
      "paperId": "1f4618406ea62eb75cb15c90d9a5393d276d29eb",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-12160",
        "ArXiv": "2510.12160",
        "DOI": "10.48550/arXiv.2510.12160",
        "CorpusId": 282064301
      },
      "corpusId": 282064301,
      "title": "State Space Prompting via Gathering and Spreading Spatio-Temporal Information for Video Understanding",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.12160, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2287324055",
          "name": "Jiahuan Zhou"
        },
        {
          "authorId": "2386403993",
          "name": "Kai Zhu"
        },
        {
          "authorId": "2279335994",
          "name": "Zhenyu Cui"
        },
        {
          "authorId": "2293575785",
          "name": "Zichen Liu"
        },
        {
          "authorId": "2293562310",
          "name": "Xu Zou"
        },
        {
          "authorId": "2359150752",
          "name": "Gang Hua"
        }
      ],
      "abstract": "Recently, pre-trained state space models have shown great potential for video classification, which sequentially compresses visual tokens in videos with linear complexity, thereby improving the processing efficiency of video data while maintaining high performance. To apply powerful pre-trained models to downstream tasks, prompt learning is proposed to achieve efficient downstream task adaptation with only a small number of fine-tuned parameters. However, the sequentially compressed visual prompt tokens fail to capture the spatial and temporal contextual information in the video, thus limiting the effective propagation of spatial information within a video frame and temporal information between frames in the state compression model and the extraction of discriminative information. To tackle the above issue, we proposed a State Space Prompting (SSP) method for video understanding, which combines intra-frame and inter-frame prompts to aggregate and propagate key spatiotemporal information in the video. Specifically, an Intra-Frame Gathering (IFG) module is designed to aggregate spatial key information within each frame. Besides, an Inter-Frame Spreading (IFS) module is designed to spread discriminative spatio-temporal information across different frames. By adaptively balancing and compressing key spatio-temporal information within and between frames, our SSP effectively propagates discriminative information in videos in a complementary manner. Extensive experiments on four video benchmark datasets verify that our SSP significantly outperforms existing SOTA methods by 2.76% on average while reducing the overhead of fine-tuning parameters."
    },
    {
      "paperId": "ee7db4936ab89c5d63404731c121f531a76557ea",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-12355",
        "ArXiv": "2510.12355",
        "DOI": "10.48550/arXiv.2510.12355",
        "CorpusId": 282064575
      },
      "corpusId": 282064575,
      "title": "Fine-grained Analysis of Brain-LLM Alignment through Input Attribution",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.12355, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2261556679",
          "name": "Michela Proietti"
        },
        {
          "authorId": "2261556582",
          "name": "Roberto Capobianco"
        },
        {
          "authorId": "2822168",
          "name": "Mariya Toneva"
        }
      ],
      "abstract": "Understanding the alignment between large language models (LLMs) and human brain activity can reveal computational principles underlying language processing. We introduce a fine-grained input attribution method to identify the specific words most important for brain-LLM alignment, and leverage it to study a contentious research question about brain-LLM alignment: the relationship between brain alignment (BA) and next-word prediction (NWP). Our findings reveal that BA and NWP rely on largely distinct word subsets: NWP exhibits recency and primacy biases with a focus on syntax, while BA prioritizes semantic and discourse-level information with a more targeted recency effect. This work advances our understanding of how LLMs relate to human language processing and highlights differences in feature reliance between BA and NWP. Beyond this study, our attribution method can be broadly applied to explore the cognitive relevance of model predictions in diverse language processing tasks."
    },
    {
      "paperId": "c671208414426c425399bc398c1307bc7c401cb5",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-12026",
        "ArXiv": "2510.12026",
        "DOI": "10.48550/arXiv.2510.12026",
        "CorpusId": 282064245
      },
      "corpusId": 282064245,
      "title": "Mamba Can Learn Low-Dimensional Targets In-Context via Test-Time Feature Learning",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.12026, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2391509444",
          "name": "Junsoo Oh"
        },
        {
          "authorId": "2322384542",
          "name": "Wei Huang"
        },
        {
          "authorId": "2305335446",
          "name": "Taiji Suzuki"
        }
      ],
      "abstract": "Mamba, a recently proposed linear-time sequence model, has attracted significant attention for its computational efficiency and strong empirical performance. However, a rigorous theoretical understanding of its underlying mechanisms remains limited. In this work, we provide a theoretical analysis of Mamba's in-context learning (ICL) capability by focusing on tasks defined by low-dimensional nonlinear target functions. Specifically, we study in-context learning of a single-index model $y \\approx g_*(\\langle \\boldsymbol{\\beta}, \\boldsymbol{x} \\rangle)$, which depends on only a single relevant direction $\\boldsymbol{\\beta}$, referred to as feature. We prove that Mamba, pretrained by gradient-based methods, can achieve efficient ICL via test-time feature learning, extracting the relevant direction directly from context examples. Consequently, we establish a test-time sample complexity that improves upon linear Transformers -- analyzed to behave like kernel methods -- and is comparable to nonlinear Transformers, which have been shown to surpass the Correlational Statistical Query (CSQ) lower bound and achieve near information-theoretically optimal rate in previous works. Our analysis reveals the crucial role of the nonlinear gating mechanism in Mamba for feature extraction, highlighting it as the fundamental driver behind Mamba's ability to achieve both computational efficiency and high performance."
    },
    {
      "paperId": "cab147da0f1369415a3337a84e0fe826503a2d64",
      "externalIds": {
        "ArXiv": "2510.13046",
        "DBLP": "journals/corr/abs-2510-13046",
        "DOI": "10.48550/arXiv.2510.13046",
        "CorpusId": 282102684
      },
      "corpusId": 282102684,
      "title": "One Dimensional CNN ECG Mamba for Multilabel Abnormality Classification in 12 Lead ECG",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.13046, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2289064278",
          "name": "Huawei Jiang"
        },
        {
          "authorId": "3196251",
          "name": "Husna Mutahira"
        },
        {
          "authorId": "2385976828",
          "name": "Gan Huang"
        },
        {
          "authorId": "145615859",
          "name": "S. M. Mannan"
        }
      ],
      "abstract": "Accurate detection of cardiac abnormalities from electrocardiogram recordings is regarded as essential for clinical diagnostics and decision support. Traditional deep learning models such as residual networks and transformer architectures have been applied successfully to this task, but their performance has been limited when long sequential signals are processed. Recently, state space models have been introduced as an efficient alternative. In this study, a hybrid framework named One Dimensional Convolutional Neural Network Electrocardiogram Mamba is introduced, in which convolutional feature extraction is combined with Mamba, a selective state space model designed for effective sequence modeling. The model is built upon Vision Mamba, a bidirectional variant through which the representation of temporal dependencies in electrocardiogram data is enhanced. Comprehensive experiments on the PhysioNet Computing in Cardiology Challenges of 2020 and 2021 were conducted, and superior performance compared with existing methods was achieved. Specifically, the proposed model achieved substantially higher AUPRC and AUROC scores than those reported by the best previously published algorithms on twelve lead electrocardiograms. These results demonstrate the potential of Mamba-based architectures to advance reliable ECG classification. This capability supports early diagnosis and personalized treatment, while enhancing accessibility in telemedicine and resource-constrained healthcare systems."
    },
    {
      "paperId": "05bbf80fafdb7cde70d9b239f6d52fcaccea4c19",
      "externalIds": {
        "DOI": "10.1186/s44398-025-00012-7",
        "CorpusId": 282114693
      },
      "corpusId": 282114693,
      "title": "Advancing non-coding RNA annotation with RNA sequence foundation models: structure and function perspectives",
      "venue": "BMC Artificial Intelligence",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1186/s44398-025-00012-7?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1186/s44398-025-00012-7, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "11130814",
          "name": "Naima Vahab"
        },
        {
          "authorId": "2385869553",
          "name": "Sonika Tyagi"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "c4302f31fbbf97e8335b9e247f5527917f34cd33",
      "externalIds": {
        "DOI": "10.1109/IECON58223.2025.11221982",
        "CorpusId": 282824216
      },
      "corpusId": 282824216,
      "title": "KUformer: KAN-UKF-Based Transformer for Battery SoH Prediction",
      "venue": "Annual Conference of the IEEE Industrial Electronics Society",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/IECON58223.2025.11221982?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/IECON58223.2025.11221982, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2336041313",
          "name": "Jinhao Zhang"
        },
        {
          "authorId": "2312183172",
          "name": "Xilin Dai"
        },
        {
          "authorId": "2336095447",
          "name": "Ruidi Zhou"
        },
        {
          "authorId": "2382061098",
          "name": "Weifeng Zhang"
        },
        {
          "authorId": "2312167881",
          "name": "Hao Ma"
        }
      ],
      "abstract": "Accurately predicting the State of Health (SoH) of batteries has become increasingly important with the widespread adoption of lithium-ion batteries in electric vehicles (EVs). However, existing SoH prediction methods, such as equivalent circuit models and machine learning models, often neglect the integration of machine learning with signal processing. Therefore, the prediction accuracy of SoH on certain battery datasets can be further improved. In this paper, KUformer is proposed, integrating Kolmogorov\u2013Arnold Networks (KAN) and Unscented Kalman Filter (UKF) within a Transformer framework. This model is designed for battery SoH data input characterized by long-term complex dependencies and smooth outputs, achieving high prediction accuracy. Experimental results on two battery datasets with different charging protocols demonstrate that KU-former achieves an average Root Mean Square Percentage Error (RMSPE) of 0.183%, representing at least 38% improvement over the current models, such as iTransformer and PINN."
    },
    {
      "paperId": "0a51d8910eaa559ae48b3c60096c4c02e6ca1731",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-11058",
        "ArXiv": "2510.11058",
        "DOI": "10.48550/arXiv.2510.11058",
        "CorpusId": 282058326
      },
      "corpusId": 282058326,
      "title": "Robust Photoplethysmography Signal Denoising via Mamba Networks",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.11058, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2334358254",
          "name": "I. Chiu"
        },
        {
          "authorId": "2282985241",
          "name": "Yu-Tung Liu"
        },
        {
          "authorId": "2283296637",
          "name": "Kuan-Chen Wang"
        },
        {
          "authorId": "2373601628",
          "name": "Hung-Yu Wei"
        },
        {
          "authorId": "2273989996",
          "name": "Yu Tsao"
        }
      ],
      "abstract": "Photoplethysmography (PPG) is widely used in wearable health monitoring, but its reliability is often degraded by noise and motion artifacts, limiting downstream applications such as heart rate (HR) estimation. This paper presents a deep learning framework for PPG denoising with an emphasis on preserving physiological information. In this framework, we propose DPNet, a Mamba-based denoising backbone designed for effective temporal modeling. To further enhance denoising performance, the framework also incorporates a scale-invariant signal-to-distortion ratio (SI-SDR) loss to promote waveform fidelity and an auxiliary HR predictor (HRP) that provides physiological consistency through HR-based supervision. Experiments on the BIDMC dataset show that our method achieves strong robustness against both synthetic noise and real-world motion artifacts, outperforming conventional filtering and existing neural models. Our method can effectively restore PPG signals while maintaining HR accuracy, highlighting the complementary roles of SI-SDR loss and HR-guided supervision. These results demonstrate the potential of our approach for practical deployment in wearable healthcare systems."
    },
    {
      "paperId": "d79e4ce863edee19e9de25ae209e92e2a3d7b0cc",
      "externalIds": {
        "ArXiv": "2510.11017",
        "DBLP": "journals/corr/abs-2510-11017",
        "DOI": "10.48550/arXiv.2510.11017",
        "CorpusId": 282058720
      },
      "corpusId": 282058720,
      "title": "High-Resolution Spatiotemporal Modeling with Global-Local State Space Models for Video-Based Human Pose Estimation",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.11017, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2053521634",
          "name": "Runyang Feng"
        },
        {
          "authorId": "2331764742",
          "name": "Hyung Jin Chang"
        },
        {
          "authorId": "1424135418",
          "name": "Tze Ho Elden Tse"
        },
        {
          "authorId": "2344128847",
          "name": "B. Kim"
        },
        {
          "authorId": "2385504557",
          "name": "Yi Chang"
        },
        {
          "authorId": "1563165771",
          "name": "Yixing Gao"
        }
      ],
      "abstract": "Modeling high-resolution spatiotemporal representations, including both global dynamic contexts (e.g., holistic human motion tendencies) and local motion details (e.g., high-frequency changes of keypoints), is essential for video-based human pose estimation (VHPE). Current state-of-the-art methods typically unify spatiotemporal learning within a single type of modeling structure (convolution or attention-based blocks), which inherently have difficulties in balancing global and local dynamic modeling and may bias the network to one of them, leading to suboptimal performance. Moreover, existing VHPE models suffer from quadratic complexity when capturing global dependencies, limiting their applicability especially for high-resolution sequences. Recently, the state space models (known as Mamba) have demonstrated significant potential in modeling long-range contexts with linear complexity; however, they are restricted to 1D sequential data. In this paper, we present a novel framework that extends Mamba from two aspects to separately learn global and local high-resolution spatiotemporal representations for VHPE. Specifically, we first propose a Global Spatiotemporal Mamba, which performs 6D selective space-time scan and spatial- and temporal-modulated scan merging to efficiently extract global representations from high-resolution sequences. We further introduce a windowed space-time scan-based Local Refinement Mamba to enhance the high-frequency details of localized keypoint motions. Extensive experiments on four benchmark datasets demonstrate that the proposed model outperforms state-of-the-art VHPE approaches while achieving better computational trade-offs."
    },
    {
      "paperId": "cdfbaf0d8ea75eaef4f7c17a3e82032f80db0c2d",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-11602",
        "ArXiv": "2510.11602",
        "DOI": "10.48550/arXiv.2510.11602",
        "CorpusId": 282058126
      },
      "corpusId": 282058126,
      "title": "Deconstructing Attention: Investigating Design Principles for Effective Language Modeling",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.11602, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2152263068",
          "name": "Huiyin Xue"
        },
        {
          "authorId": "2182290",
          "name": "N. Moosavi"
        },
        {
          "authorId": "3238627",
          "name": "Nikolaos Aletras"
        }
      ],
      "abstract": "The success of Transformer language models is widely credited to their dot-product attention mechanism, which interweaves a set of key design principles: mixing information across positions (enabling multi-token interactions), sequence-dependent activations (where attention weights adapt to each input), a specific mathematical form (dot-product similarities plus softmax weighting), and coupling of queries and keys to evolving hidden states (grounding attention in the current layer). However, the necessity of each of these principles remains largely untested. In this work, we systematically deconstruct attention by designing controlled variants that selectively relax these principles, applied both uniformly across all layers and in hybrid architectures where only some layers retain standard attention. Our empirical analysis reveals that mechanisms for mixing tokens are indispensable, as their absence collapses models to near-random behavior, while the exact mathematical form and sequence dependency can be substantially relaxed, especially when preserved in just a subset of layers. Surprisingly, even variants that fail in isolation can achieve robust performance when interleaved with standard attention, highlighting a cooperative effect. These findings deepen our understanding of what truly underpins attention's effectiveness and open new avenues for simplifying language models without sacrificing performance."
    },
    {
      "paperId": "1a46409023bf82cd90b0e771538c01c9c4611496",
      "externalIds": {
        "ArXiv": "2510.11509",
        "DBLP": "journals/corr/abs-2510-11509",
        "DOI": "10.48550/arXiv.2510.11509",
        "CorpusId": 282058561
      },
      "corpusId": 282058561,
      "title": "Situat3DChange: Situated 3D Change Understanding Dataset for Multimodal Large Language Model",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.11509, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2273522374",
          "name": "Ruiping Liu"
        },
        {
          "authorId": "2210176414",
          "name": "Junwei Zheng"
        },
        {
          "authorId": "2243360598",
          "name": "Yufan Chen"
        },
        {
          "authorId": "2385498850",
          "name": "Zirui Wang"
        },
        {
          "authorId": "91549683",
          "name": "Kunyu Peng"
        },
        {
          "authorId": "8689702",
          "name": "Kailun Yang"
        },
        {
          "authorId": "2313700347",
          "name": "Jiaming Zhang"
        },
        {
          "authorId": "2385471198",
          "name": "Marc Pollefeys"
        },
        {
          "authorId": "2320597200",
          "name": "Rainer Stiefelhagen"
        }
      ],
      "abstract": "Physical environments and circumstances are fundamentally dynamic, yet current 3D datasets and evaluation benchmarks tend to concentrate on either dynamic scenarios or dynamic situations in isolation, resulting in incomplete comprehension. To overcome these constraints, we introduce Situat3DChange, an extensive dataset supporting three situation-aware change understanding tasks following the perception-action model: 121K question-answer pairs, 36K change descriptions for perception tasks, and 17K rearrangement instructions for the action task. To construct this large-scale dataset, Situat3DChange leverages 11K human observations of environmental changes to establish shared mental models and shared situational awareness for human-AI collaboration. These observations, enriched with egocentric and allocentric perspectives as well as categorical and coordinate spatial relations, are integrated using an LLM to support understanding of situated changes. To address the challenge of comparing pairs of point clouds from the same scene with minor changes, we propose SCReasoner, an efficient 3D MLLM approach that enables effective point cloud comparison with minimal parameter overhead and no additional tokens required for the language decoder. Comprehensive evaluation on Situat3DChange tasks highlights both the progress and limitations of MLLMs in dynamic scene and situation understanding. Additional experiments on data scaling and cross-domain transfer demonstrate the task-agnostic effectiveness of using Situat3DChange as a training dataset for MLLMs."
    },
    {
      "paperId": "3df6d6ab73915f82aecb0cbb3a5b429311c06056",
      "externalIds": {
        "ArXiv": "2510.11129",
        "DBLP": "journals/corr/abs-2510-11129",
        "DOI": "10.48550/arXiv.2510.11129",
        "CorpusId": 282058120
      },
      "corpusId": 282058120,
      "title": "video-SALMONN S: Streaming Audio-Visual LLMs Beyond Length Limits via Memory",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.11129, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2107310187",
          "name": "Guangzhi Sun"
        },
        {
          "authorId": "2322710992",
          "name": "Yixuan Li"
        },
        {
          "authorId": "2305623684",
          "name": "Xiaodong Wu"
        },
        {
          "authorId": "2321417870",
          "name": "Yudong Yang"
        },
        {
          "authorId": "2256598801",
          "name": "Wei Li"
        },
        {
          "authorId": "2257135061",
          "name": "Zejun Ma"
        },
        {
          "authorId": "2293024936",
          "name": "Chao Zhang"
        }
      ],
      "abstract": "Continuous, high-frame-rate, high-resolution processing of long video streams is critical for future AI agents, yet current video-understanding LLMs struggle to scale. Offline, fixed-frame-number methods require the stream length to adapt frame rates; streaming methods constrain memory by merging or discarding tokens, losing information. We propose video-SALMONN S, a streaming audio-visual LLM that, to our knowledge, is the first to process 3-hour videos at 1 FPS and 360p resolution under a fixed memory budget. Our model introduces (i) a test-time-training (TTT) memory module that continually updates token representations to capture long-range dependencies by replacing token merging, and (ii) a prompt-dependent memory reader that selectively retrieves context-relevant content from fixed-size memory. The TTT module is optimised with a Hessian-free conjugate-gradient procedure (TTT_HF) for efficient adaptation. On long-video benchmarks (Video-MME, LVBench, VideoEvalPro), video-SALMONN S sustains high-quality understanding on multi-hour videos with 10k frames and 1M tokens. Our 8B-parameter model achieves 74.2% overall and 67.8% on the Video-MME long split, outperforming both offline and streaming baselines."
    },
    {
      "paperId": "38c858ac9c78a2493177da3735ef4dae67457ef8",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-12842",
        "ArXiv": "2510.12842",
        "DOI": "10.48550/arXiv.2510.12842",
        "CorpusId": 282102424
      },
      "corpusId": 282102424,
      "title": "Protenix-Mini+: efficient structure prediction model with scalable pairformer",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.12842, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2365127111",
          "name": "Bo Qiang"
        },
        {
          "authorId": "2339671951",
          "name": "Chengyue Gong"
        },
        {
          "authorId": "2339717774",
          "name": "Xinshi Chen"
        },
        {
          "authorId": "2339774528",
          "name": "Yuxuan Zhang"
        },
        {
          "authorId": "2342307598",
          "name": "Wenzhi Xiao"
        }
      ],
      "abstract": "Lightweight inference is critical for biomolecular structure prediction and downstream tasks, enabling efficient real-world deployment and inference-time scaling for large-scale applications. While AF3 and its variants (e.g., Protenix, Chai-1) have advanced structure prediction results, they suffer from critical limitations: high inference latency and cubic time complexity with respect to token count, both of which restrict scalability for large biomolecular complexes. To address the core challenge of balancing model efficiency and prediction accuracy, we introduce three key innovations: (1) compressing non-scalable operations to mitigate cubic time complexity, (2) removing redundant blocks across modules to reduce unnecessary overhead, and (3) adopting a few-step sampler for the atom diffusion module to accelerate inference. Building on these design principles, we develop Protenix-Mini+, a highly lightweight and scalable variant of the Protenix model. Within an acceptable range of performance degradation, it substantially improves computational efficiency. For example, in the case of low-homology single-chain proteins, Protenix-Mini+ experiences an intra-protein LDDT drop of approximately 3% relative to the full Protenix model -- an acceptable performance trade-off given its substantially 90%+ improved computational efficiency."
    },
    {
      "paperId": "bb7817c8cf3faa438f2858965d6135a6aab2c705",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-13860",
        "ArXiv": "2510.13860",
        "DOI": "10.48550/arXiv.2510.13860",
        "CorpusId": 282138561
      },
      "corpusId": 282138561,
      "title": "ShishuLM: Lightweight Language Model with Hybrid Decoder-MLP Architecture and Paired Weight Sharing",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.13860, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2386082077",
          "name": "Shivanshu Kumar"
        },
        {
          "authorId": "2386030500",
          "name": "Gopalakrishnan Srinivasan"
        }
      ],
      "abstract": "While the transformer architecture has achieved state-of-the-art performance on natural language processing tasks, these models impose substantial memory and computational overhead. Recent research has identified significant architectural redundancies within these models, presenting opportunities for optimization without compromising performance. Taking insights from research in AI interpretability and inference-time layer pruning, we introduce an efficient language model architecture, referred to as ShishuLM, which reduces both the parameter count and Key-Value (KV) cache requirements. Given the increasing importance of Small Language Models (SLMs) in agentic AI systems, we evaluate our approach on two SLMs of different scales. Our analysis reveals that for moderate-context scenarios, normalization coupled with attention computation is roughly linear with the input, enabling entire transformer blocks to be approximated through Multi-Layer Perceptrons (MLPs). Our results show that ShishuLM provides up to 25% reduction in memory requirements and up to 40% improvement in latency during both training and inference, compared to parent models. Our experimental and analytical findings provide insights towards building more efficient SLM architectures from a pre-training standpoint."
    },
    {
      "paperId": "fd2d431006e39b1f78e0b33300ec367222593c87",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-10478",
        "ArXiv": "2510.10478",
        "DOI": "10.48550/arXiv.2510.10478",
        "CorpusId": 282058513
      },
      "corpusId": 282058513,
      "title": "MSF-Mamba: Motion-aware State Fusion Mamba for Efficient Micro-Gesture Recognition",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.10478, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2299323092",
          "name": "Deng Li"
        },
        {
          "authorId": "2387882481",
          "name": "Jun Shao"
        },
        {
          "authorId": "2299158157",
          "name": "Bohao Xing"
        },
        {
          "authorId": "2201696188",
          "name": "Rong Gao"
        },
        {
          "authorId": "2299158380",
          "name": "Bihan Wen"
        },
        {
          "authorId": "2646507",
          "name": "H. K\u00e4lvi\u00e4inen"
        },
        {
          "authorId": "2299537202",
          "name": "Xin Liu"
        }
      ],
      "abstract": "Micro-gesture recognition (MGR) targets the identification of subtle and fine-grained human motions and requires accurate modeling of both long-range and local spatiotemporal dependencies. While CNNs are effective at capturing local patterns, they struggle with long-range dependencies due to their limited receptive fields. Transformer-based models address this limitation through self-attention mechanisms but suffer from high computational costs. Recently, Mamba has shown promise as an efficient model, leveraging state space models (SSMs) to enable linear-time processing However, directly applying the vanilla Mamba to MGR may not be optimal. This is because Mamba processes inputs as 1D sequences, with state updates relying solely on the previous state, and thus lacks the ability to model local spatiotemporal dependencies. In addition, previous methods lack a design of motion-awareness, which is crucial in MGR. To overcome these limitations, we propose motion-aware state fusion mamba (MSF-Mamba), which enhances Mamba with local spatiotemporal modeling by fusing local contextual neighboring states. Our design introduces a motion-aware state fusion module based on central frame difference (CFD). Furthermore, a multiscale version named MSF-Mamba+ has been proposed. Specifically, MSF-Mamba supports multiscale motion-aware state fusion, as well as an adaptive scale weighting module that dynamically weighs the fused states across different scales. These enhancements explicitly address the limitations of vanilla Mamba by enabling motion-aware local spatiotemporal modeling, allowing MSF-Mamba and MSF-Mamba to effectively capture subtle motion cues for MGR. Experiments on two public MGR datasets demonstrate that even the lightweight version, namely, MSF-Mamba, achieves SoTA performance, outperforming existing CNN-, Transformer-, and SSM-based models while maintaining high efficiency."
    },
    {
      "paperId": "457df83064ec46733f6e4a1abe0857cd4256509f",
      "externalIds": {
        "DOI": "10.1145/3765612.3767198",
        "CorpusId": 283723309
      },
      "corpusId": 283723309,
      "title": "Addressing COVID-19 Data Imbalance Issue with Multimodal Image Generation",
      "venue": "Proceedings of the 16th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3765612.3767198?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3765612.3767198, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2312132027",
          "name": "Maxwell Sam"
        },
        {
          "authorId": "2331510959",
          "name": "Hailemicael Lulseged Yimer"
        },
        {
          "authorId": "2067694915",
          "name": "Salil S. Desai"
        },
        {
          "authorId": "2281411917",
          "name": "Xiaohong Yuan"
        },
        {
          "authorId": "2295946763",
          "name": "Jinsheng Xu"
        },
        {
          "authorId": "46493422",
          "name": "Huiming Yu"
        },
        {
          "authorId": "31158976",
          "name": "L. Qingge"
        }
      ],
      "abstract": "Data imbalance represents a critical challenge in medical AI systems, particularly for COVID-19 diagnosis where limited and skewed datasets leads to poor model generalization and reduced diagnostic accuracy for underrepresented classes. While generative AI has shown promise for addressing data imbalance, existing approaches in medical imaging often produce synthetic images lacking clinical fidelity and fail to leverage interpretable features that drive diagnostic decisions. This paper introduces a novel multimodal image generation framework that combines explainable AI insights with advanced generative architectures to synthesize high-quality COVID-19 CT images. Our approach leverages SHAP (SHapley Additive exPlanations) to identify diagnostically important regions in the COVID-19 images, which serve as one input modality alongside contrast-enhanced and original images in our multimodal pipeline. We proposed three hybrid architectures namely, GAN-Diffusion, MAMBA-Diffusion, and MAMBA-XLSTM. Each of these architectures were designed to capture complementary aspects of medical image synthesis through weighted fusion mechanisms (40% original images, 40% SHAP-identified regions, 20% enhanced images). Our experimental validation demonstrates substantial improvements in both image quality and classification performance. The MAMBA-Diffusion model achieved superior generation quality with FID score of 5.9, SSIM score of 0.997, LPIPS score of 0.002, and Inception Score of 2.01. This significantly outperforms baseline methods including DCGAN, LAPGAN, CycleGAN, and standard diffusion models. Most importantly, augmenting imbalanced datasets with our synthetic images improved the adopted COVID-CNN model classification performance across multiple scenarios: (1) Accuracy on the Iran dataset increased from 65.33% to 83.28% after augmenting the sample distribution from 349 (positive)/397 (negative) to 698 (positive)/794 (negative). (2) For the China dataset, accuracy improved from 66.67% to 91.43% by augmenting 85/200 samples to 1695/1629. (3) In a severely imbalanced scenario, accuracy increased from 18.18% to 97.81% by addressing both class bias and data scarcity (30/300 training samples expanded to 640/640). (4) Additionally, for the Russia dataset, which is not part of our multimodal training dataset, the model prediction accuracy increased from 80.0% to 93.75% by addressing data imbalance (40/10 samples to 40/40 samples). This demonstrates robust performance across different imbalance scenarios. The proposed framework addresses both data scarcity and class imbalance simultaneously while maintaining clinical relevance through interpretability-guided generation. This technique can be used not only for COVID-19 image generation and data balancing, but also for other image generation tasks."
    },
    {
      "paperId": "18c9fff45ac6842bfa0241ba0350d072c6b7a0f6",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-10160",
        "ArXiv": "2510.10160",
        "DOI": "10.48550/arXiv.2510.10160",
        "CorpusId": 282058061
      },
      "corpusId": 282058061,
      "title": "SaFiRe: Saccade-Fixation Reiteration with Mamba for Referring Image Segmentation",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.10160, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2371140161",
          "name": "Zhenjie Mao"
        },
        {
          "authorId": "2108773876",
          "name": "Yu-Hao Yang"
        },
        {
          "authorId": "2124034538",
          "name": "Ma"
        },
        {
          "authorId": "2386577994",
          "name": "Dongsheng Jiang"
        },
        {
          "authorId": "3213796",
          "name": "Jiangchao Yao"
        },
        {
          "authorId": "2373411242",
          "name": "Ya Zhang"
        },
        {
          "authorId": "2332863357",
          "name": "Yanfeng Wang"
        }
      ],
      "abstract": "Referring Image Segmentation (RIS) aims to segment the target object in an image given a natural language expression. While recent methods leverage pre-trained vision backbones and more training corpus to achieve impressive results, they predominantly focus on simple expressions--short, clear noun phrases like\"red car\"or\"left girl\". This simplification often reduces RIS to a key word/concept matching problem, limiting the model's ability to handle referential ambiguity in expressions. In this work, we identify two challenging real-world scenarios: object-distracting expressions, which involve multiple entities with contextual cues, and category-implicit expressions, where the object class is not explicitly stated. To address the challenges, we propose a novel framework, SaFiRe, which mimics the human two-phase cognitive process--first forming a global understanding, then refining it through detail-oriented inspection. This is naturally supported by Mamba's scan-then-update property, which aligns with our phased design and enables efficient multi-cycle refinement with linear complexity. We further introduce aRefCOCO, a new benchmark designed to evaluate RIS models under ambiguous referring expressions. Extensive experiments on both standard and proposed datasets demonstrate the superiority of SaFiRe over state-of-the-art baselines."
    },
    {
      "paperId": "295d1e57a4ca202e37cf0d82ff4998a2ea801fac",
      "externalIds": {
        "DOI": "10.1109/ICIRT66379.2025.11216717",
        "CorpusId": 282824709
      },
      "corpusId": 282824709,
      "title": "Context-Aware LiDAR Point Cloud Segmentation with Expertised Decision Strategy",
      "venue": "2025 IEEE International Conference on Intelligent Rail Transportation (ICIRT)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICIRT66379.2025.11216717?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICIRT66379.2025.11216717, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2391092698",
          "name": "ChuQiao Zhang"
        },
        {
          "authorId": "2391697531",
          "name": "Rui Wang"
        },
        {
          "authorId": "2335521154",
          "name": "Shuai Su"
        }
      ],
      "abstract": "With the growing demand for accurate 3D perception in autonomous driving, point cloud segmentation has become a key task in scene understanding. However, existing methods struggle to improve spatial adaptability and feature specialization. In order to improve the performance of point cloud segmentation, we design a novel point cloud segmentation method based on the Transformer framework, named SEPT, which integrates a Serialized Shortcut Linear Attention (SSLA) mechanism and a sparse Mixture-of-Experts (MoE) module. The SSLA module enables efficient global context modeling while preserving spatial structure, and the MoE component employs a conditional sparse gating strategy to dynamically activate top-k expert networks based on input characteristics. The experimental results show that SEPT achieved an average intersection-over-union of 81.1%, which is better than the 79.0% achieved by eight representative baseline models since 2020, fully proving its effectiveness in improving segmentation accuracy."
    },
    {
      "paperId": "aaaac78c6523f94d46ff7e596b42651016a29649",
      "externalIds": {
        "ArXiv": "2510.09088",
        "DBLP": "journals/corr/abs-2510-09088",
        "DOI": "10.48550/arXiv.2510.09088",
        "CorpusId": 282055532
      },
      "corpusId": 282055532,
      "title": "MambaH-Fit: Rethinking Hyper-surface Fitting-based Point Cloud Normal Estimation via State Space Modelling",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.09088, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2244287836",
          "name": "Weijia Wang"
        },
        {
          "authorId": "2228240959",
          "name": "Yuanzhi Su"
        },
        {
          "authorId": "1492145910",
          "name": "P. Ye"
        },
        {
          "authorId": "2314788158",
          "name": "Yuan-Gen Wang"
        },
        {
          "authorId": "2385442063",
          "name": "Xuequan Lu"
        }
      ],
      "abstract": "We present MambaH-Fit, a state space modelling framework tailored for hyper-surface fitting-based point cloud normal estimation. Existing normal estimation methods often fall short in modelling fine-grained geometric structures, thereby limiting the accuracy of the predicted normals. Recently, state space models (SSMs), particularly Mamba, have demonstrated strong modelling capability by capturing long-range dependencies with linear complexity and inspired adaptations to point cloud processing. However, existing Mamba-based approaches primarily focus on understanding global shape structures, leaving the modelling of local, fine-grained geometric details largely under-explored. To address the issues above, we first introduce an Attention-driven Hierarchical Feature Fusion (AHFF) scheme to adaptively fuse multi-scale point cloud patch features, significantly enhancing geometric context learning in local point cloud neighbourhoods. Building upon this, we further propose Patch-wise State Space Model (PSSM) that models point cloud patches as implicit hyper-surfaces via state dynamics, enabling effective fine-grained geometric understanding for normal prediction. Extensive experiments on benchmark datasets show that our method outperforms existing ones in terms of accuracy, robustness, and flexibility. Ablation studies further validate the contribution of the proposed components."
    },
    {
      "paperId": "b76e908e14424d3a01854e8d08394a61b6c24cfe",
      "externalIds": {
        "DBLP": "journals/mms/JingZZR25",
        "DOI": "10.1007/s00530-025-02010-1",
        "CorpusId": 282033813
      },
      "corpusId": 282033813,
      "title": "GL-MambaNet: Mamba-based global and local feature fusion for image dehazing",
      "venue": "Multimedia Systems",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s00530-025-02010-1?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s00530-025-02010-1, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2238627286",
          "name": "Hongyuan Jing"
        },
        {
          "authorId": "2314674440",
          "name": "Hui Zhang"
        },
        {
          "authorId": "2330862727",
          "name": "Mengmeng Zhang"
        },
        {
          "authorId": "2330680776",
          "name": "Qiyu Rong"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "2f846e349638d5da1b80b017208a3b6e23f8979b",
      "externalIds": {
        "ArXiv": "2510.09367",
        "DBLP": "journals/corr/abs-2510-09367",
        "DOI": "10.48550/arXiv.2510.09367",
        "CorpusId": 282055530
      },
      "corpusId": 282055530,
      "title": "Minkowski-MambaNet: A Point Cloud Framework with Selective State Space Models for Forest Biomass Quantification",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.09367, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2385422850",
          "name": "Jinxiang Tu"
        },
        {
          "authorId": "2292141757",
          "name": "Dayong Ren"
        },
        {
          "authorId": "2320572382",
          "name": "Fei Shi"
        },
        {
          "authorId": "2257131437",
          "name": "Zhenhong Jia"
        },
        {
          "authorId": "2385480514",
          "name": "Yahong Ren"
        },
        {
          "authorId": "2386497796",
          "name": "Jiwei Qin"
        },
        {
          "authorId": "2321105760",
          "name": "Fang He"
        }
      ],
      "abstract": "Accurate forest biomass quantification is vital for carbon cycle monitoring. While airborne LiDAR excels at capturing 3D forest structure, directly estimating woody volume and Aboveground Biomass (AGB) from point clouds is challenging due to difficulties in modeling long-range dependencies needed to distinguish trees.We propose Minkowski-MambaNet, a novel deep learning framework that directly estimates volume and AGB from raw LiDAR. Its key innovation is integrating the Mamba model's Selective State Space Model (SSM) into a Minkowski network, enabling effective encoding of global context and long-range dependencies for improved tree differentiation. Skip connections are incorporated to enhance features and accelerate convergence.Evaluated on Danish National Forest Inventory LiDAR data, Minkowski-MambaNet significantly outperforms state-of-the-art methods, providing more accurate and robust estimates. Crucially, it requires no Digital Terrain Model (DTM) and is robust to boundary artifacts. This work offers a powerful tool for large-scale forest biomass analysis, advancing LiDAR-based forest inventories."
    },
    {
      "paperId": "50948d721ea514a17c83b46f7867b24ef1381e7f",
      "externalIds": {
        "ArXiv": "2510.09182",
        "DBLP": "journals/corr/abs-2510-09182",
        "DOI": "10.48550/arXiv.2510.09182",
        "CorpusId": 282055942
      },
      "corpusId": 282055942,
      "title": "Online Video Depth Anything: Temporally-Consistent Depth Prediction with Low Memory Consumption",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.09182, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2273532704",
          "name": "Johann-Friedrich Feiden"
        },
        {
          "authorId": "2391819780",
          "name": "Tim K\u00fcchler"
        },
        {
          "authorId": "2273532639",
          "name": "Denis Zavadski"
        },
        {
          "authorId": "1792728",
          "name": "Bogdan Savchynskyy"
        },
        {
          "authorId": "2273548940",
          "name": "Carsten Rother"
        }
      ],
      "abstract": "Depth estimation from monocular video has become a key component of many real-world computer vision systems. Recently, Video Depth Anything (VDA) has demonstrated strong performance on long video sequences. However, it relies on batch-processing which prohibits its use in an online setting. In this work, we overcome this limitation and introduce online VDA (oVDA). The key innovation is to employ techniques from Large Language Models (LLMs), namely, caching latent features during inference and masking frames at training. Our oVDA method outperforms all competing online video depth estimation methods in both accuracy and VRAM usage. Low VRAM usage is particularly important for deployment on edge devices. We demonstrate that oVDA runs at 42 FPS on an NVIDIA A100 and at 20 FPS on an NVIDIA Jetson edge device. We will release both, code and compilation scripts, making oVDA easy to deploy on low-power hardware."
    },
    {
      "paperId": "6db41a44427e69188d8cbab1b7661a2280ccc171",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-09551",
        "ArXiv": "2510.09551",
        "DOI": "10.48550/arXiv.2510.09551",
        "CorpusId": 282056148
      },
      "corpusId": 282056148,
      "title": "Titans Revisited: A Lightweight Reimplementation and Critical Analysis of a Test-Time Memory Model",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.09551, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2385424092",
          "name": "Gavriel Di Nepi"
        },
        {
          "authorId": "1752951302",
          "name": "F. Siciliano"
        },
        {
          "authorId": "2276426254",
          "name": "Fabrizio Silvestri"
        }
      ],
      "abstract": "By the end of 2024, Google researchers introduced Titans: Learning at Test Time, a neural memory model achieving strong empirical results across multiple tasks. However, the lack of publicly available code and ambiguities in the original description hinder reproducibility. In this work, we present a lightweight reimplementation of Titans and conduct a comprehensive evaluation on Masked Language Modeling, Time Series Forecasting, and Recommendation tasks. Our results reveal that Titans does not always outperform established baselines due to chunking. However, its Neural Memory component consistently improves performance compared to attention-only models. These findings confirm the model's innovative potential while highlighting its practical limitations and raising questions for future research."
    },
    {
      "paperId": "c411e89ff7bedcbcc5a2abf959349ff0e5a7d344",
      "externalIds": {
        "ArXiv": "2510.09883",
        "DBLP": "journals/corr/abs-2510-09883",
        "DOI": "10.48550/arXiv.2510.09883",
        "CorpusId": 282058410
      },
      "corpusId": 282058410,
      "title": "DELTA: Dynamic Layer-Aware Token Attention for Efficient Long-Context Reasoning",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.09883, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "97814531",
          "name": "Hossein Entezari Zarch"
        },
        {
          "authorId": "2291209211",
          "name": "Lei Gao"
        },
        {
          "authorId": "2310901316",
          "name": "Chaoyi Jiang"
        },
        {
          "authorId": "2385476198",
          "name": "Murali Annavarm"
        }
      ],
      "abstract": "Large reasoning models (LRMs) achieve state-of-the-art performance on challenging benchmarks by generating long chains of intermediate steps, but their inference cost is dominated by decoding, where each new token must attend to the entire growing sequence. Existing sparse attention methods reduce computation by pruning the key-value (KV) cache, yet they suffer from severe accuracy degradation on reasoning tasks due to cumulative selection errors and the dynamic importance of tokens over long derivations. We present \\textbf{DELTA}, a training-free sparse attention mechanism that achieves computational efficiency without sacrificing model accuracy. DELTA partitions transformer layers into three groups: initial layers that use full attention, a small set of \\emph{selection layers} that identify salient tokens via aggregated head-level attention scores, and subsequent \\emph{sparse-attention layers} that attend only to the selected subset. This design preserves the full KV cache in GPU memory for accuracy, while avoiding expensive full-attention computation over many layers. On reasoning benchmarks such as AIME and GPQA-Diamond, DELTA matches or surpasses full attention in accuracy, while reducing the number of attended tokens by up to $5\\times$ and delivering $1.5\\times$ end-to-end speedup. Our results show that selective reuse of intermediate attention maps offers a robust path toward efficient long-context reasoning."
    },
    {
      "paperId": "7b8335b8ba2ba570a2c7fa6106ead974ad4354e9",
      "externalIds": {
        "ArXiv": "2510.08318",
        "DBLP": "journals/corr/abs-2510-08318",
        "DOI": "10.48550/arXiv.2510.08318",
        "CorpusId": 281950663
      },
      "corpusId": 281950663,
      "title": "LinVideo: A Post-Training Framework towards O(n) Attention in Efficient Video Generation",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.08318, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2268626103",
          "name": "Yushi Huang"
        },
        {
          "authorId": "2287929890",
          "name": "Xingtong Ge"
        },
        {
          "authorId": "2275155785",
          "name": "Ruihao Gong"
        },
        {
          "authorId": "2261398158",
          "name": "Chengtao Lv"
        },
        {
          "authorId": "2363726076",
          "name": "Jun Zhang"
        }
      ],
      "abstract": "Video diffusion models (DMs) have enabled high-quality video synthesis. However, their computation costs scale quadratically with sequence length because self-attention has quadratic complexity. While linear attention lowers the cost, fully replacing quadratic attention requires expensive pretraining due to the limited expressiveness of linear attention and the complexity of spatiotemporal modeling in video generation. In this paper, we present LinVideo, an efficient data-free post-training framework that replaces a target number of self-attention modules with linear attention while preserving the original model's performance. First, we observe a significant disparity in the replaceability of different layers. Instead of manual or heuristic choices, we frame layer selection as a binary classification problem and propose selective transfer, which automatically and progressively converts layers to linear attention with minimal performance impact. Additionally, to overcome the ineffectiveness and inefficiency of existing objectives for this transfer process, we introduce an anytime distribution matching (ADM) objective that aligns the distributions of samples across any timestep along the sampling trajectory. This objective is efficient and recovers model performance. Extensive experiments show that our method achieves a 1.25-2.00x speedup while preserving generation quality, and our 4-step distilled model further delivers a 15.92x latency reduction with minimal visual quality drop."
    },
    {
      "paperId": "bb15d15655a41d12c714b3cd88c810ee5bb2886e",
      "externalIds": {
        "ArXiv": "2510.08525",
        "DBLP": "journals/corr/abs-2510-08525",
        "DOI": "10.48550/arXiv.2510.08525",
        "CorpusId": 281950459
      },
      "corpusId": 281950459,
      "title": "Which Heads Matter for Reasoning? RL-Guided KV Cache Compression",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.08525, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2385496428",
          "name": "Wenjie Du"
        },
        {
          "authorId": "2384809078",
          "name": "Li Jiang"
        },
        {
          "authorId": "2331856915",
          "name": "Keda Tao"
        },
        {
          "authorId": "2385784192",
          "name": "Xue Liu"
        },
        {
          "authorId": "2331889370",
          "name": "Huan Wang"
        }
      ],
      "abstract": "Reasoning large language models exhibit complex reasoning behaviors through the extended chain-of-thought generation, creating unprecedented Key-Value (KV) cache overhead during the decoding phase. Existing KV cache compression methods underperform on reasoning models: token-dropping methods break reasoning integrity by discarding critical information, while head-reallocating methods mistakenly compress reasoning-critical heads since they are designed for retrieval tasks, resulting in significant performance degradation as compression rates increase. We hypothesize that KV heads exhibit functional heterogeneity in reasoning models-some heads are critical for chain-of-thought consistency while others are compressible. To validate and exploit this insight, we propose RLKV, a novel reasoning-critical head identification framework, which uses reinforcement learning to directly optimize the relationship between each head's cache usage and reasoning quality. As RLKV produces rewards from actual generated samples during training, it naturally identifies heads relevant to reasoning behaviors. We then allocate full KV cache to these heads while applying compressed constant KV cache to others for efficient inference. Our experiments reveal that only a small fraction of attention heads is essential for reasoning, enabling our KV compression approach to outperform baseline methods while achieving 20-50% cache reduction with near lossless performance compared to uncompressed results."
    },
    {
      "paperId": "7e5428aec542022f8a988801d7f3ca2428c75c08",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-08450",
        "ArXiv": "2510.08450",
        "DOI": "10.48550/arXiv.2510.08450",
        "CorpusId": 281950629
      },
      "corpusId": 281950629,
      "title": "gLSTM: Mitigating Over-Squashing by Increasing Storage Capacity",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.08450, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2384765555",
          "name": "Hugh Blayney"
        },
        {
          "authorId": "2238953003",
          "name": "Alvaro Arroyo"
        },
        {
          "authorId": "2239060705",
          "name": "Xiaowen Dong"
        },
        {
          "authorId": "2261389035",
          "name": "Michael M. Bronstein"
        }
      ],
      "abstract": "Graph Neural Networks (GNNs) leverage the graph structure to transmit information between nodes, typically through the message-passing mechanism. While these models have found a wide variety of applications, they are known to suffer from over-squashing, where information from a large receptive field of node representations is collapsed into a single fixed sized vector, resulting in an information bottleneck. In this paper, we re-examine the over-squashing phenomenon through the lens of model storage and retrieval capacity, which we define as the amount of information that can be stored in a node's representation for later use. We study some of the limitations of existing tasks used to measure over-squashing and introduce a new synthetic task to demonstrate that an information bottleneck can saturate this capacity. Furthermore, we adapt ideas from the sequence modeling literature on associative memories, fast weight programmers, and the xLSTM model to develop a novel GNN architecture with improved capacity. We demonstrate strong performance of this architecture both on our capacity synthetic task, as well as a range of real-world graph benchmarks."
    },
    {
      "paperId": "79f89316fcda36bf866682aaf9a12e0e41388cf3",
      "externalIds": {
        "ArXiv": "2510.08009",
        "DBLP": "journals/corr/abs-2510-08009",
        "DOI": "10.48550/arXiv.2510.08009",
        "CorpusId": 281950628
      },
      "corpusId": 281950628,
      "title": "Language Models Do Not Embed Numbers Continuously",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.08009, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2220290677",
          "name": "Alex O. Davies"
        },
        {
          "authorId": "2323792998",
          "name": "R. Nzoyem"
        },
        {
          "authorId": "2889245",
          "name": "Nirav Ajmeri"
        },
        {
          "authorId": "2220290122",
          "name": "T. M. S. Filho"
        }
      ],
      "abstract": "Recent research has extensively studied how large language models manipulate integers in specific arithmetic tasks, and on a more fundamental level, how they represent numeric values. These previous works have found that language model embeddings can be used to reconstruct the original values, however, they do not evaluate whether language models actually model continuous values as continuous. Using expected properties of the embedding space, including linear reconstruction and principal component analysis, we show that language models not only represent numeric spaces as non-continuous but also introduce significant noise. Using models from three major providers (OpenAI, Google Gemini and Voyage AI), we show that while reconstruction is possible with high fidelity ($R^2 \\geq 0.95$), principal components only explain a minor share of variation within the embedding space. This indicates that many components within the embedding space are orthogonal to the simple numeric input space. Further, both linear reconstruction and explained variance suffer with increasing decimal precision, despite the ordinal nature of the input space being fundamentally unchanged. The findings of this work therefore have implications for the many areas where embedding models are used, in-particular where high numerical precision, large magnitudes or mixed-sign values are common."
    },
    {
      "paperId": "b733e507e4847e7714e5f10f834b07badfb1358c",
      "externalIds": {
        "ArXiv": "2510.07835",
        "DBLP": "journals/corr/abs-2510-07835",
        "DOI": "10.48550/arXiv.2510.07835",
        "CorpusId": 281951380
      },
      "corpusId": 281951380,
      "title": "MetaDefense: Defending Finetuning-based Jailbreak Attack Before and During Generation",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 1,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.07835, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2152123946",
          "name": "Weisen Jiang"
        },
        {
          "authorId": "2249722081",
          "name": "Sinno Jialin Pan"
        }
      ],
      "abstract": "This paper introduces MetaDefense, a novel framework for defending against finetuning-based jailbreak attacks in large language models (LLMs). We observe that existing defense mechanisms fail to generalize to harmful queries disguised by unseen attack templates, despite LLMs being capable of distinguishing disguised harmful queries in the embedding space. Based on these insights, we propose a two-stage defense approach: (i) pre-generation defense that detects harmful queries before response generation begins, and (ii) mid-generation defense that monitors partial responses during generation to prevent outputting more harmful content. Our MetaDefense trains the LLM to predict the harmfulness of both queries and partial responses using specialized prompts, enabling early termination of potentially harmful interactions. Extensive experiments across multiple LLM architectures (LLaMA-2-7B, Qwen-2.5-3B-Instruct, and LLaMA-3.2-3B-Instruct) demonstrate that MetaDefense significantly outperforms existing defense mechanisms, achieving robust defense against harmful queries with seen and unseen attack templates while maintaining competitive performance on benign tasks. Code is available at https://github.com/ws-jiang/MetaDefense."
    },
    {
      "paperId": "c6a0ef1b7fc58ce1c0522187fdfd9ce0159259c2",
      "externalIds": {
        "ArXiv": "2510.08836",
        "DBLP": "journals/corr/abs-2510-08836",
        "DOI": "10.48550/arXiv.2510.08836",
        "CorpusId": 282056281
      },
      "corpusId": 282056281,
      "title": "Long-Tailed Recognition via Information-Preservable Two-Stage Learning",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.08836, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "9310428",
          "name": "Fudong Lin"
        },
        {
          "authorId": "2366144539",
          "name": "Xu Yuan"
        }
      ],
      "abstract": "The imbalance (or long-tail) is the nature of many real-world data distributions, which often induces the undesirable bias of deep classification models toward frequent classes, resulting in poor performance for tail classes. In this paper, we propose a novel two-stage learning approach to mitigate such a majority-biased tendency while preserving valuable information within datasets. Specifically, the first stage proposes a new representation learning technique from the information theory perspective. This approach is theoretically equivalent to minimizing intra-class distance, yielding an effective and well-separated feature space. The second stage develops a novel sampling strategy that selects mathematically informative instances, able to rectify majority-biased decision boundaries without compromising a model's overall performance. As a result, our approach achieves the state-of-the-art performance across various long-tailed benchmark datasets, validated via extensive experiments. Our code is available at https://github.com/fudong03/BNS_IPDPP."
    },
    {
      "paperId": "38e779de742a8ef6dd0e0d188ea13b19c92281f8",
      "externalIds": {
        "DBLP": "journals/mms/WuYM25",
        "DOI": "10.1007/s00530-025-02003-0",
        "CorpusId": 281971361
      },
      "corpusId": 281971361,
      "title": "Research and application of sign language recognition and target tracking model based on YOLO-Mamba",
      "venue": "Multimedia Systems",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s00530-025-02003-0?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s00530-025-02003-0, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2269006791",
          "name": "Jialin Wu"
        },
        {
          "authorId": "2269118456",
          "name": "Tao Yang"
        },
        {
          "authorId": "2065105522",
          "name": "Jintao Meng"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "972768785c213c433e853d6ca68a02cb5b3af745",
      "externalIds": {
        "DOI": "10.1109/DSAA65442.2025.11247985",
        "CorpusId": 283269853
      },
      "corpusId": 283269853,
      "title": "Small and Fast LLMs on Commodity Hardware: Post-Training Quantization in llama. cpp",
      "venue": "International Conference on Data Science and Advanced Analytics",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/DSAA65442.2025.11247985?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/DSAA65442.2025.11247985, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2340521965",
          "name": "Lorenz Sparrenberg"
        },
        {
          "authorId": "2292278147",
          "name": "Tobias Deu\u03b2er"
        },
        {
          "authorId": "2280492948",
          "name": "Armin Berger"
        },
        {
          "authorId": "2018549",
          "name": "R. Sifa"
        }
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities but their significant computational and memory demands hinder widespread deployment, especially on resource-constrained devices. Quantization, the process of reducing the numerical precision of model parameters, has emerged as a critical technique for compressing LLMs and accelerating inference. This paper provides an overview of LLM quantization, with a particular focus on the Post-Training Quantization (PTQ) methods implemented within the popular llama. cpp framework and its GGUF file format. We begin by covering quantization fundamentals, including the distinction between PTQ and Quantization-Aware Training (QAT). We then describe the specific PTQ schemes employed by llama. cpp, including legacy methods, advanced K-quants, and recent IQ-quants, along with their underlying mathematical principles. The paper also discusses the impact of these techniques on model fidelity, hardware requirements, inference speed, and traces the adoption of GGUF as a de facto standard in the open-source community. This work serves as a practical guide and comprehensive reference for researchers aiming to deploy LLMs on resource-constrained hardware. By systematically documenting and comparing the PTQ methods within llama. cpp, we provide the necessary insights to navigate the trade-offs between model fidelity, inference speed, and memory footprint. This enables informed decision-making for real-world applications, from local CPU-based inference to efficient edge deployment."
    },
    {
      "paperId": "34d3228a2fdfd74ea85017f3324b018ce80c1405",
      "externalIds": {
        "ArXiv": "2510.06640",
        "DBLP": "journals/corr/abs-2510-06640",
        "DOI": "10.48550/arXiv.2510.06640",
        "CorpusId": 281892147
      },
      "corpusId": 281892147,
      "title": "A Comparative Analysis of Contextual Representation Flow in State-Space and Transformer Architectures",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.06640, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2240194981",
          "name": "Nhat M. Hoang"
        },
        {
          "authorId": "2060491855",
          "name": "Do Xuan Long"
        },
        {
          "authorId": "2386434418",
          "name": "Cong-Duy Nguyen"
        },
        {
          "authorId": "2257033898",
          "name": "Min-Yen Kan"
        },
        {
          "authorId": "26336902",
          "name": "Anh Tuan Luu"
        }
      ],
      "abstract": "State Space Models (SSMs) have recently emerged as efficient alternatives to Transformer-Based Models (TBMs) for long-sequence processing, offering linear scaling and lower memory use. Yet, how contextual information flows across layers and tokens in these architectures remains understudied. We present the first unified, token- and layer-level analysis of representation propagation in SSMs and TBMs. Using centered kernel alignment, stability metrics, and probing, we characterize how representations evolve within and across layers. We find a key divergence: TBMs rapidly homogenize token representations, with diversity reemerging only in later layers, while SSMs preserve token uniqueness early but converge to homogenization deeper. Theoretical analysis and parameter randomization further reveal that oversmoothing in TBMs stems from architectural design, whereas in SSMs it arises mainly from training dynamics. These insights clarify the inductive biases of both architectures and inform future model and training designs for long-context reasoning."
    },
    {
      "paperId": "e36235f1550a142c199a49725dc1f5200081e9f9",
      "externalIds": {
        "ArXiv": "2510.07151",
        "DBLP": "journals/corr/abs-2510-07151",
        "DOI": "10.48550/arXiv.2510.07151",
        "CorpusId": 281892007
      },
      "corpusId": 281892007,
      "title": "ELMUR: External Layer Memory with Update/Rewrite for Long-Horizon RL",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.07151, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2334474457",
          "name": "Egor Cherepanov"
        },
        {
          "authorId": "153780959",
          "name": "A. Kovalev"
        },
        {
          "authorId": "2238010956",
          "name": "Aleksandr I. Panov"
        }
      ],
      "abstract": "Real-world robotic agents must act under partial observability and long horizons, where key cues may appear long before they affect decision making. However, most modern approaches rely solely on instantaneous information, without incorporating insights from the past. Standard recurrent or transformer models struggle with retaining and leveraging long-term dependencies: context windows truncate history, while naive memory extensions fail under scale and sparsity. We propose ELMUR (External Layer Memory with Update/Rewrite), a transformer architecture with structured external memory. Each layer maintains memory embeddings, interacts with them via bidirectional cross-attention, and updates them through an Least Recently Used (LRU) memory module using replacement or convex blending. ELMUR extends effective horizons up to 100,000 times beyond the attention window and achieves a 100% success rate on a synthetic T-Maze task with corridors up to one million steps. In POPGym, it outperforms baselines on more than half of the tasks. On MIKASA-Robo sparse-reward manipulation tasks with visual observations, it nearly doubles the performance of strong baselines. These results demonstrate that structured, layer-local external memory offers a simple and scalable approach to decision making under partial observability."
    },
    {
      "paperId": "82fad9ae850d342bf2f58a6489a406c37f9cce45",
      "externalIds": {
        "ArXiv": "2510.06940",
        "DBLP": "journals/corr/abs-2510-06940",
        "DOI": "10.48550/arXiv.2510.06940",
        "CorpusId": 281891857
      },
      "corpusId": 281891857,
      "title": "Revisiting Node Affinity Prediction in Temporal Graphs",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.06940, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2209207087",
          "name": "Krishna Sri Ipsit Mantri"
        },
        {
          "authorId": "2151789902",
          "name": "Or Feldman"
        },
        {
          "authorId": "104172497",
          "name": "Moshe Eliasof"
        },
        {
          "authorId": "46906102",
          "name": "Chaim Baskin"
        }
      ],
      "abstract": "Node affinity prediction is a common task that is widely used in temporal graph learning with applications in social and financial networks, recommender systems, and more. Recent works have addressed this task by adapting state-of-the-art dynamic link property prediction models to node affinity prediction. However, simple heuristics, such as Persistent Forecast or Moving Average, outperform these models. In this work, we analyze the challenges in training current Temporal Graph Neural Networks for node affinity prediction and suggest appropriate solutions. Combining the solutions, we develop NAViS - Node Affinity prediction model using Virtual State, by exploiting the equivalence between heuristics and state space models. While promising, training NAViS is non-trivial. Therefore, we further introduce a novel loss function for node affinity prediction. We evaluate NAViS on TGB and show that it outperforms the state-of-the-art, including heuristics. Our source code is available at https://github.com/orfeld415/NAVIS"
    },
    {
      "paperId": "88284df124f1a963ff33b0945e8c0b43eb04c75b",
      "externalIds": {
        "ArXiv": "2510.06746",
        "DBLP": "journals/corr/abs-2510-06746",
        "DOI": "10.1109/LSP.2025.3616020",
        "CorpusId": 281810010
      },
      "corpusId": 281810010,
      "title": "DeRainMamba: A Frequency-Aware State Space Model With Detail Enhancement for Image Deraining",
      "venue": "IEEE Signal Processing Letters",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.06746, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2308242546",
          "name": "Zhiliang Zhu"
        },
        {
          "authorId": "2320562365",
          "name": "Tao Zeng"
        },
        {
          "authorId": "2384059084",
          "name": "Tao Yang"
        },
        {
          "authorId": "2308186482",
          "name": "Guoliang Luo"
        },
        {
          "authorId": "2349557310",
          "name": "Jiyong Zeng"
        }
      ],
      "abstract": "Image deraining is crucial for improving visual quality and supporting reliable downstream vision tasks. Although Mamba-based models provide efficient sequence modeling, their limited ability to capture fine-grained details and lack of frequency-domain awareness restrict further improvements. To address these issues, we propose DeRainMamba, which integrates a Frequency-Aware State-Space Module (FASSM) and Multi-Directional Perception Convolution (MDPConv). FASSM leverages Fourier transform to distinguish rain streaks from high-frequency image details, balancing rain removal and detail preservation. MDPConv further restores local structures by capturing anisotropic gradient features and efficiently fusing multiple convolution branches. Extensive experiments on four public benchmarks demonstrate that DeRainMamba consistently outperforms state-of-the-art methods in PSNR and SSIM, while requiring fewer parameters and lower computational costs. These results validate the effectiveness of combining frequency-domain modeling and spatial detail enhancement within a state-space framework for single image deraining."
    },
    {
      "paperId": "280e04bffb99c0aeb32370f5ef5148aeab59b6f0",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-07019",
        "ArXiv": "2510.07019",
        "DOI": "10.48550/arXiv.2510.07019",
        "CorpusId": 281892137
      },
      "corpusId": 281892137,
      "title": "Native Hybrid Attention for Efficient Sequence Modeling",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.07019, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2346284954",
          "name": "Jusen Du"
        },
        {
          "authorId": "2346167233",
          "name": "Jiaxi Hu"
        },
        {
          "authorId": "2384707359",
          "name": "Tao Zhang"
        },
        {
          "authorId": "2346291295",
          "name": "Weigao Sun"
        },
        {
          "authorId": "2344895705",
          "name": "Yu Cheng"
        }
      ],
      "abstract": "Transformers excel at sequence modeling but face quadratic complexity, while linear attention offers improved efficiency but often compromises recall accuracy over long contexts. In this work, we introduce Native Hybrid Attention (NHA), a novel hybrid architecture of linear and full attention that integrates both intra \\&inter-layer hybridization into a unified layer design. NHA maintains long-term context in key-value slots updated by a linear RNN, and augments them with short-term tokens from a sliding window. A single \\texttt{softmax attention} operation is then applied over all keys and values, enabling per-token and per-head context-dependent weighting without requiring additional fusion parameters. The inter-layer behavior is controlled through a single hyperparameter, the sliding window size, which allows smooth adjustment between purely linear and full attention while keeping all layers structurally uniform. Experimental results show that NHA surpasses Transformers and other hybrid baselines on recall-intensive and commonsense reasoning tasks. Furthermore, pretrained LLMs can be structurally hybridized with NHA, achieving competitive accuracy while delivering significant efficiency gains. Code is available at https://github.com/JusenD/NHA."
    },
    {
      "paperId": "6bb4d2b7aa09f5b505ef7d6781e689be56b69627",
      "externalIds": {
        "ArXiv": "2510.06557",
        "DBLP": "journals/corr/abs-2510-06557",
        "DOI": "10.48550/arXiv.2510.06557",
        "CorpusId": 281891675
      },
      "corpusId": 281891675,
      "title": "The Markovian Thinker",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.06557, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "113383974",
          "name": "Milad Aghajohari"
        },
        {
          "authorId": "1598457720",
          "name": "Kamran Chitsaz"
        },
        {
          "authorId": "1754452702",
          "name": "Amirhossein Kazemnejad"
        },
        {
          "authorId": "123607932",
          "name": "Sarath Chandar"
        },
        {
          "authorId": "2041695",
          "name": "Alessandro Sordoni"
        },
        {
          "authorId": "2285877959",
          "name": "Aaron C. Courville"
        },
        {
          "authorId": "2323901201",
          "name": "Siva Reddy"
        }
      ],
      "abstract": "Reinforcement learning (RL) has recently become a strong recipe for training reasoning LLMs that produce long chains of thought (LongCoT). Yet the standard RL\"thinking environment\", where the state is the prompt plus all prior reasoning tokens, makes the state unbounded and forces attention-based policies to pay quadratic compute as thoughts lengthen. We revisit the environment itself. We propose Markovian Thinking, a paradigm in which the policy advances reasoning while conditioning on a constant-size state, decoupling thinking length from context size. As an immediate consequence this yields linear compute with constant memory. We instantiate this idea with Delethink, an RL environment that structures reasoning into fixed-size chunks. Within each chunk, the model thinks as usual; at the boundary, the environment resets the context and reinitializes the prompt with a short carryover. Through RL, the policy learns to write a textual state near the end of each chunk sufficient for seamless continuation of reasoning after reset. Trained in this environment, an R1-Distill 1.5B model reasons in 8K-token chunks yet thinks up to 24K tokens, matching or surpassing LongCoT-RL trained with a 24K budget. With test-time scaling, Delethink continues to improve where LongCoT plateaus. The effect of linear compute is substantial: we empirically estimate at 96K average thinking length LongCoT-RL costs 27 H100-months vs. 7 for Delethink. Analysis at RL initialization shows off-the-shelf reasoning models (1.5B-120B) often sample Markovian traces zero-shot across diverse benchmarks, providing positive samples that make RL effective at scale. Our results show that redesigning the thinking environment is a powerful lever: it enables very long reasoning without quadratic overhead and opens a path toward efficient, scalable reasoning LLMs."
    },
    {
      "paperId": "c7e97e22da90d07382a0588e6df74002ed195409",
      "externalIds": {
        "ArXiv": "2510.07318",
        "DBLP": "journals/corr/abs-2510-07318",
        "DOI": "10.48550/arXiv.2510.07318",
        "CorpusId": 281891425
      },
      "corpusId": 281891425,
      "title": "Artificial Hippocampus Networks for Efficient Long-Context Modeling",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.07318, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2384694981",
          "name": "Yunhao Fang"
        },
        {
          "authorId": "2360539226",
          "name": "Weihao Yu"
        },
        {
          "authorId": "2384810576",
          "name": "Shu Zhong"
        },
        {
          "authorId": "2385444330",
          "name": "Qinghao Ye"
        },
        {
          "authorId": "2385446747",
          "name": "Xuehan Xiong"
        },
        {
          "authorId": "2384716376",
          "name": "Lai Wei"
        }
      ],
      "abstract": "Long-sequence modeling faces a fundamental trade-off between the efficiency of compressive fixed-size memory in RNN-like models and the fidelity of lossless growing memory in attention-based Transformers. Inspired by the Multi-Store Model in cognitive science, we introduce a memory framework of artificial neural networks. Our method maintains a sliding window of the Transformer's KV cache as lossless short-term memory, while a learnable module termed Artificial Hippocampus Network (AHN) recurrently compresses out-of-window information into a fixed-size compact long-term memory. To validate this framework, we instantiate AHNs using modern RNN-like architectures, including Mamba2, DeltaNet, and GatedDeltaNet to augment open-weight LLMs. We also propose an efficient self-distillation training method where the base model's all parameters are frozen and only the parameters from AHNs are optimized. For inference, our method sets a default large sliding window size of 32k for attention, and AHNs activate only when the sequence length exceeds the 32k window, addressing the quadratic-complexity issue of attention that emerges at that scale. Extensive experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate that AHN-augmented models consistently outperform sliding window baselines and achieve performance comparable or even superior to full-attention models, while substantially reducing computational and memory requirements. For instance, augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5% and memory cache by 74.0%, while improving its average score on LV-Eval (128k sequence length) from 4.41 to 5.88. Code is available at: https://github.com/ByteDance-Seed/AHN."
    },
    {
      "paperId": "dee482ca82ab6db93edf2b0c4f9c8aba03eca63c",
      "externalIds": {
        "ArXiv": "2510.06904",
        "CorpusId": 281891763
      },
      "corpusId": 281891763,
      "title": "Microstructure sensitive recurrent neural network surrogate model of crystal plasticity",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.06904, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2351807106",
          "name": "Michael D. Atkinson"
        },
        {
          "authorId": "2281000526",
          "name": "Michael D. White"
        },
        {
          "authorId": "2312217345",
          "name": "Adam J. Plowman"
        },
        {
          "authorId": "30541882",
          "name": "P. Shanthraj"
        }
      ],
      "abstract": "The development of next-generation structural materials for harsh environments requires rapid assessment of mechanical performance and its dependence on microstructure. While full-field crystal plasticity (CP) models provide detailed insights, the high computational cost limits their use with uncertainty quantification workflows and in component-scale simulation. Surrogate models based on recurrent neural networks (RNNs) have shown promise in reproducing history-dependent mechanical behaviour but are applied to models with either fixed microstructure or representative volume elements. Here, we develop a microstructure sensitive RNN surrogate that predicts homogenised stress responses directly from three-dimensional grain structures and arbitrary deformation histories. The architecture employs a gated recurrent unit (GRU) with mappings from microstructure to both the initial hidden state and sequence inputs, allowing the model to capture path dependence and microstructure variability. Training data comprised of over 300,000 CP simulations generated using combinations of randomly generated microstructures and loading paths. The model was found to reproduce CP predictions for both in-distribution validation data and unseen deformation modes, with errors of 2 MPa to 3 MPa. Out-of-distribution microstructures were more difficult to predict, emphasising the need for representative training data with, for example, heavily textured microstructures. Embedding the model into a multiscale framework demonstrates its ability to replace conventional constitutive updates, reducing computational cost while preserving key features of the stress distribution. These results establish microstructure-informed RNN surrogates as a promising alternative to direct CP simulations, offering a pathway toward rapid multiscale modelling and uncertainty quantification."
    },
    {
      "paperId": "d631d79bd9e06e2461603330c7513d7e6d0d15c8",
      "externalIds": {
        "ArXiv": "2510.06532",
        "DBLP": "journals/corr/abs-2510-06532",
        "DOI": "10.48550/arXiv.2510.06532",
        "CorpusId": 281891827
      },
      "corpusId": 281891827,
      "title": "CLAQS: Compact Learnable All-Quantum Token Mixer with Shared-ansatz for Text Classification",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.06532, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2325897685",
          "name": "Junhao Chen"
        },
        {
          "authorId": "2325891087",
          "name": "Yifan Zhou"
        },
        {
          "authorId": "2273631049",
          "name": "Hanqi Jiang"
        },
        {
          "authorId": "2364948477",
          "name": "Yi Pan"
        },
        {
          "authorId": "2368555293",
          "name": "Yiwei Li"
        },
        {
          "authorId": "2276747984",
          "name": "Huaqin Zhao"
        },
        {
          "authorId": "2322347361",
          "name": "Wei Zhang"
        },
        {
          "authorId": "2339975678",
          "name": "Yingfeng Wang"
        },
        {
          "authorId": "2363359797",
          "name": "Tianming Liu"
        }
      ],
      "abstract": "Quantum compute is scaling fast, from cloud QPUs to high throughput GPU simulators, making it timely to prototype quantum NLP beyond toy tasks. However, devices remain qubit limited and depth limited, training can be unstable, and classical attention is compute and memory heavy. This motivates compact, phase aware quantum token mixers that stabilize amplitudes and scale to long sequences. We present CLAQS, a compact, fully quantum token mixer for text classification that jointly learns complex-valued mixing and nonlinear transformations within a unified quantum circuit. To enable stable end-to-end optimization, we apply l1 normalization to regulate amplitude scaling and introduce a two-stage parameterized quantum architecture that decouples shared token embeddings from a window-level quantum feed-forward module. Operating under a sliding-window regime with document-level aggregation, CLAQS requires only eight data qubits and shallow circuits, yet achieves 91.64% accuracy on SST-2 and 87.08% on IMDB, outperforming both classical Transformer baselines and strong hybrid quantum-classical counterparts."
    },
    {
      "paperId": "b0b2949afafdc3c7ca85f0e2ed4e2c84257864be",
      "externalIds": {
        "ArXiv": "2510.07499",
        "DBLP": "journals/corr/abs-2510-07499",
        "DOI": "10.48550/arXiv.2510.07499",
        "CorpusId": 281951312
      },
      "corpusId": 281951312,
      "title": "When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.07499, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "8599185",
          "name": "Soyeong Jeong"
        },
        {
          "authorId": "97638739",
          "name": "Taehee Jung"
        },
        {
          "authorId": "2260611009",
          "name": "Sung Ju Hwang"
        },
        {
          "authorId": "2284775339",
          "name": "Joo-Kyung Kim"
        },
        {
          "authorId": "2306271988",
          "name": "Dongyeop Kang"
        }
      ],
      "abstract": "Recent Long-Context Language Models (LCLMs) can process hundreds of thousands of tokens in a single prompt, enabling new opportunities for knowledge-intensive multi-hop reasoning by integrating large sets of retrieved documents or, in some cases, directly all necessary information. However, simply feeding more documents into the context window fails to capture how evidence should be connected. We address this gap with thought templates, which recast reasoning as reusable thought caches, derived from prior problem solving traces, structuring how evidence is combined and guiding multi-hop inference with factual documents. To keep these templates effective, we propose an update strategy that iteratively refines templates derived from training data through natural-language feedback. Across diverse benchmarks and LCLM families, our approach delivers consistent gains over strong baselines in both retrieval-based and retrieval-free settings. Furthermore, we show that optimized templates can be distilled into smaller open-source models, demonstrating its broad applicability and transparent reasoning reuse. We refer to our framework as Thought Template Augmented LCLMs (ToTAL)."
    },
    {
      "paperId": "298540324a3ec9d453998ffa5913c87026f53309",
      "externalIds": {
        "ArXiv": "2510.07578",
        "DBLP": "journals/corr/abs-2510-07578",
        "DOI": "10.48550/arXiv.2510.07578",
        "CorpusId": 281951565
      },
      "corpusId": 281951565,
      "title": "Accuracy, Memory Efficiency and Generalization: A Comparative Study on Liquid Neural Networks and Recurrent Neural Networks",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.07578, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2384770098",
          "name": "Shilong Zong"
        },
        {
          "authorId": "2384770079",
          "name": "Alex Bierly"
        },
        {
          "authorId": "2313870",
          "name": "Almuatazbellah M. Boker"
        },
        {
          "authorId": "3355952",
          "name": "Hoda Eldardiry"
        }
      ],
      "abstract": "This review aims to conduct a comparative analysis of liquid neural networks (LNNs) and traditional recurrent neural networks (RNNs) and their variants, such as long short-term memory networks (LSTMs) and gated recurrent units (GRUs). The core dimensions of the analysis include model accuracy, memory efficiency, and generalization ability. By systematically reviewing existing research, this paper explores the basic principles, mathematical models, key characteristics, and inherent challenges of these neural network architectures in processing sequential data. Research findings reveal that LNN, as an emerging, biologically inspired, continuous-time dynamic neural network, demonstrates significant potential in handling noisy, non-stationary data, and achieving out-of-distribution (OOD) generalization. Additionally, some LNN variants outperform traditional RNN in terms of parameter efficiency and computational speed. However, RNN remains a cornerstone in sequence modeling due to its mature ecosystem and successful applications across various tasks. This review identifies the commonalities and differences between LNNs and RNNs, summarizes their respective shortcomings and challenges, and points out valuable directions for future research, particularly emphasizing the importance of improving the scalability of LNNs to promote their application in broader and more complex scenarios."
    },
    {
      "paperId": "d29ce88c461fa0cb5275743757ca98d3cc43a0e2",
      "externalIds": {
        "ArXiv": "2510.09679",
        "DBLP": "journals/corr/abs-2510-09679",
        "DOI": "10.48550/arXiv.2510.09679",
        "CorpusId": 282057449
      },
      "corpusId": 282057449,
      "title": "Knowledge-Aware Mamba for Joint Change Detection and Classification from MODIS Times Series",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.09679, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2375375635",
          "name": "Zhengsen Xu"
        },
        {
          "authorId": "2347677379",
          "name": "Yimin Zhu"
        },
        {
          "authorId": "2313918078",
          "name": "Zack Dewis"
        },
        {
          "authorId": "2355348178",
          "name": "Mabel Heffring"
        },
        {
          "authorId": "2355350831",
          "name": "Motasem Alkayid"
        },
        {
          "authorId": "2163115939",
          "name": "Saeid Taleghanidoozdoozan"
        },
        {
          "authorId": "2355398953",
          "name": "Lincoln Linlin Xu"
        }
      ],
      "abstract": "Although change detection using MODIS time series is critical for environmental monitoring, it is a highly challenging task due to key MODIS difficulties, e.g., mixed pixels, spatial-spectral-temporal information coupling effect, and background class heterogeneity. This paper presents a novel knowledge-aware Mamba (KAMamba) for enhanced MODIS change detection, with the following contributions. First, to leverage knowledge regarding class transitions, we design a novel knowledge-driven transition-matrix-guided approach, leading to a knowledge-aware transition loss (KAT-loss) that can enhance detection accuracies. Second, to improve model constraints, a multi-task learning approach is designed, where three losses, i.e., pre-change classification loss (PreC-loss), post-change classification loss (PostC-loss), and change detection loss (Chg-loss) are used for improve model learning. Third, to disentangle information coupling in MODIS time series, novel spatial-spectral-temporal Mamba (SSTMamba) modules are designed. Last, to improve Mamba model efficiency and remove computational cost, a sparse and deformable Mamba (SDMamba) backbone is used in SSTMamba. On the MODIS time-series dataset for Saskatchewan, Canada, we evaluate the method on land-cover change detection and LULC classification; results show about 1.5-6% gains in average F1 for change detection over baselines, and about 2% improvements in OA, AA, and Kappa for LULC classification."
    },
    {
      "paperId": "8d2f1c327ea135bbc2c9ef66dc70f74bb1297783",
      "externalIds": {
        "ArXiv": "2510.05709",
        "DBLP": "journals/corr/abs-2510-05709",
        "DOI": "10.48550/arXiv.2510.05709",
        "CorpusId": 281886328
      },
      "corpusId": 281886328,
      "title": "Towards Reliable and Practical LLM Security Evaluations via Bayesian Modelling",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.05709, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2384361496",
          "name": "Mary Llewellyn"
        },
        {
          "authorId": "2384363461",
          "name": "Annie Gray"
        },
        {
          "authorId": "2384361470",
          "name": "Josh Collyer"
        },
        {
          "authorId": "2384361691",
          "name": "Michael Harries"
        }
      ],
      "abstract": "Before adopting a new large language model (LLM) architecture, it is critical to understand vulnerabilities accurately. Existing evaluations can be difficult to trust, often drawing conclusions from LLMs that are not meaningfully comparable, relying on heuristic inputs or employing metrics that fail to capture the inherent uncertainty. In this paper, we propose a principled and practical end-to-end framework for evaluating LLM vulnerabilities to prompt injection attacks. First, we propose practical approaches to experimental design, tackling unfair LLM comparisons by considering two practitioner scenarios: when training an LLM and when deploying a pre-trained LLM. Second, we address the analysis of experiments and propose a Bayesian hierarchical model with embedding-space clustering. This model is designed to improve uncertainty quantification in the common scenario that LLM outputs are not deterministic, test prompts are designed imperfectly, and practitioners only have a limited amount of compute to evaluate vulnerabilities. We show the improved inferential capabilities of the model in several prompt injection attack settings. Finally, we demonstrate the pipeline to evaluate the security of Transformer versus Mamba architectures. Our findings show that consideration of output variability can suggest less definitive findings. However, for some attacks, we find notably increased Transformer and Mamba-variant vulnerabilities across LLMs with the same training data or mathematical ability."
    },
    {
      "paperId": "3b5c97e3c70245714526be31413d092917b833c6",
      "externalIds": {
        "ArXiv": "2510.05901",
        "DBLP": "journals/corr/abs-2510-05901",
        "DOI": "10.48550/arXiv.2510.05901",
        "CorpusId": 281887134
      },
      "corpusId": 281887134,
      "title": "Untangling Component Imbalance in Hybrid Linear Attention Conversion Methods",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.05901, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2286339509",
          "name": "Martin A Benfeghoul"
        },
        {
          "authorId": "2384368992",
          "name": "Teresa Delgado"
        },
        {
          "authorId": "2311114789",
          "name": "Adnan Oomerjee"
        },
        {
          "authorId": "46257744",
          "name": "H. Ammar"
        },
        {
          "authorId": "2360470978",
          "name": "Jun Wang"
        },
        {
          "authorId": "2931718",
          "name": "Z. Fountas"
        }
      ],
      "abstract": "Transformers'quadratic computational complexity limits their scalability despite remarkable performance. While linear attention reduces this to linear complexity, pre-training such models from scratch remains, in most cases, prohibitively expensive. Recent post-training linearisation methods convert pre-trained Transformers to linear models efficiently, often using hybrid approaches that combine linear attention with sliding-window softmax. We identify a critical flaw: existing hybrid methods inadvertently bypass the linear component, relying almost entirely on SWA. Component-level diagnostics reveal this previously undetected behaviour stems from overlooked evaluation practices on common-sense benchmarks. We propose three solutions to ensure balanced component usage: (i) inference-time hybridisation of linear-only conversions with sliding-window softmax; (ii) HedgeCATs, combining attention-weight transfer with targeted LoRA fine-tuning; and (iii) Scheduled Sliding-window Dropout (SSD), which stochastically suppresses the softmax branch during training to prevent component collapse. Our methods maintain computational efficiency while recovering most base model performance and ensuring genuine linear attention adoption, restoring the validity of performance attributions in hybrid conversions."
    },
    {
      "paperId": "f2a5456a4708c39ab11159ff8d418b8547b4cef7",
      "externalIds": {
        "ArXiv": "2511.11581",
        "CorpusId": 283071802
      },
      "corpusId": 283071802,
      "title": "The Anatomy of a Triton Attention Kernel",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.11581, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "9298246",
          "name": "Burkhard Ringlein"
        },
        {
          "authorId": "2513955",
          "name": "J. V. Lunteren"
        },
        {
          "authorId": "2304933216",
          "name": "Radu Stoica"
        },
        {
          "authorId": "2359453365",
          "name": "Thomas Parnell"
        }
      ],
      "abstract": "A long-standing goal in both industry and academia is to develop an LLM inference platform that is portable across hardware architectures, eliminates the need for low-level hand-tuning, and still delivers best-in-class efficiency. In this work, we demonstrate that portable, efficient cross-platform LLM inference is indeed possible and share our experience. We develop a state-of-the-art paged attention kernel, the core performance-critical component of many LLM deployments, that builds exclusively on the domain-specific just-in-time compiled language Triton to achieve state-of-the-art performance on both NVIDIA and AMD GPUs. We describe our high-level approach, the key algorithmic and system-level improvements, the parameter auto-tuning required to unlock efficiency, and the integrations into a popular inference server that are necessary to bring the performance of a generic Triton attention kernel from 19.7% of the state-of-the-art to 105.9%. Our results highlight how open-source domain-specific languages can be leveraged to unlock model portability across different GPU vendors."
    },
    {
      "paperId": "7030f3ec865f30b5279256d7c71a1eb8487a1da2",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-04800",
        "ArXiv": "2510.04800",
        "DOI": "10.48550/arXiv.2510.04800",
        "CorpusId": 281843121
      },
      "corpusId": 281843121,
      "title": "Hybrid Architectures for Language Models: Systematic Analysis and Design Insights",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 4,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.04800, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2384097427",
          "name": "Sangmin Bae"
        },
        {
          "authorId": "2064056189",
          "name": "Bilge Acun"
        },
        {
          "authorId": "30279076",
          "name": "Haroun Habeeb"
        },
        {
          "authorId": "2328160131",
          "name": "Seungyeon Kim"
        },
        {
          "authorId": "2383262350",
          "name": "Chien-Yu Lin"
        },
        {
          "authorId": "2387141355",
          "name": "Liang Luo"
        },
        {
          "authorId": "2384317754",
          "name": "Junjie Wang"
        },
        {
          "authorId": "2253786890",
          "name": "Carole-Jean Wu"
        }
      ],
      "abstract": "Recent progress in large language models demonstrates that hybrid architectures--combining self-attention mechanisms with structured state space models like Mamba--can achieve a compelling balance between modeling quality and computational efficiency, particularly for long-context tasks. While these hybrid models show promising performance, systematic comparisons of hybridization strategies and analyses on the key factors behind their effectiveness have not been clearly shared to the community. In this work, we present a holistic evaluation of hybrid architectures based on inter-layer (sequential) or intra-layer (parallel) fusion. We evaluate these designs from a variety of perspectives: language modeling performance, long-context capabilities, scaling analysis, and training and inference efficiency. By investigating the core characteristics of their computational primitive, we identify the most critical elements for each hybridization strategy and further propose optimal design recipes for both hybrid models. Our comprehensive analysis provides practical guidance and valuable insights for developing hybrid language models, facilitating the optimization of architectural configurations."
    },
    {
      "paperId": "5429404183e63430481048aaec17f956f6906c1f",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-04738",
        "ArXiv": "2510.04738",
        "DOI": "10.48550/arXiv.2510.04738",
        "CorpusId": 281843807
      },
      "corpusId": 281843807,
      "title": "Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.04738, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2384131558",
          "name": "Baher Mohammad"
        },
        {
          "authorId": "50995338",
          "name": "Magauiya Zhussip"
        },
        {
          "authorId": "1766572",
          "name": "Stamatios Lefkimmiatis"
        }
      ],
      "abstract": "We introduce MAVE (Mamba with Cross-Attention for Voice Editing and Synthesis), a novel autoregressive architecture for text-conditioned voice editing and high-fidelity text-to-speech (TTS) synthesis, built on a cross-attentive Mamba backbone. MAVE achieves state-of-the-art performance in speech editing and very competitive results in zero-shot TTS, while not being explicitly trained on the latter task, outperforming leading autoregressive and diffusion models on diverse, real-world audio. By integrating Mamba for efficient audio sequence modeling with cross-attention for precise text-acoustic alignment, MAVE enables context-aware voice editing with exceptional naturalness and speaker consistency. In pairwise human evaluations on a random 40-sample subset of the RealEdit benchmark (400 judgments), 57.2% of listeners rated MAVE - edited speech as perceptually equal to the original, while 24.8% prefered the original and 18.0% MAVE - demonstrating that in the majority of cases edits are indistinguishable from the source. MAVE compares favorably with VoiceCraft and FluentSpeech both on pairwise comparisons and standalone mean opinion score (MOS) evaluations. For zero-shot TTS, MAVE exceeds VoiceCraft in both speaker similarity and naturalness, without requiring multiple inference runs or post-processing. Remarkably, these quality gains come with a significantly lower memory cost and approximately the same latency: MAVE requires ~6x less memory than VoiceCraft during inference on utterances from the RealEdit database (mean duration: 6.21s, A100, FP16, batch size 1). Our results demonstrate that MAVE establishes a new standard for flexible, high-fidelity voice editing and synthesis through the synergistic integration of structured state-space modeling and cross-modal attention."
    },
    {
      "paperId": "f0be13d09e0f8bd1f652b50cde0183a0324dec73",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-05364",
        "ArXiv": "2510.05364",
        "DOI": "10.48550/arXiv.2510.05364",
        "CorpusId": 281886741
      },
      "corpusId": 281886741,
      "title": "The End of Transformers? On Challenging Attention and the Rise of Sub-Quadratic Architectures",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.05364, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2275600165",
          "name": "Alexander Fichtl"
        },
        {
          "authorId": "2361704078",
          "name": "Jeremias Bohn"
        },
        {
          "authorId": "2384366117",
          "name": "Josefin Kelber"
        },
        {
          "authorId": "2238214714",
          "name": "Edoardo Mosca"
        },
        {
          "authorId": "2361727567",
          "name": "Georg Groh"
        }
      ],
      "abstract": "Transformers have dominated sequence processing tasks for the past seven years -- most notably language modeling. However, the inherent quadratic complexity of their attention mechanism remains a significant bottleneck as context length increases. This paper surveys recent efforts to overcome this bottleneck, including advances in (sub-quadratic) attention variants, recurrent neural networks, state space models, and hybrid architectures. We critically analyze these approaches in terms of compute and memory complexity, benchmark results, and fundamental limitations to assess whether the dominance of pure-attention transformers may soon be challenged."
    },
    {
      "paperId": "a9d7537ee2230bd5c416e43142f4dc9f3560aad0",
      "externalIds": {
        "ArXiv": "2510.04944",
        "DBLP": "journals/corr/abs-2510-04944",
        "DOI": "10.48550/arXiv.2510.04944",
        "CorpusId": 281843931
      },
      "corpusId": 281843931,
      "title": "On Structured State-Space Duality",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.04944, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2266720128",
          "name": "Jerry Yao-Chieh Hu"
        },
        {
          "authorId": "2363502088",
          "name": "Xiwen Zhang"
        },
        {
          "authorId": "2289966059",
          "name": "Weimin Wu"
        },
        {
          "authorId": "2193647521",
          "name": "Han Liu"
        }
      ],
      "abstract": "Structured State-Space Duality (SSD) [Dao&Gu, ICML 2024] is an equivalence between a simple Structured State-Space Model (SSM) and a masked attention mechanism. In particular, a state-space model with a scalar-times-identity state matrix is equivalent to a masked self-attention with a $1$-semiseparable causal mask. Consequently, the same sequence transformation (model) has two algorithmic realizations: as a linear-time $O(T)$ recurrence or as a quadratic-time $O(T^2)$ attention. In this note, we formalize and generalize this duality: (i) we extend SSD from the scalar-identity case to general diagonal SSMs (diagonal state matrices); (ii) we show that these diagonal SSMs match the scalar case's training complexity lower bounds while supporting richer dynamics; (iii) we establish a necessary and sufficient condition under which an SSM is equivalent to $1$-semiseparable masked attention; and (iv) we show that such duality fails to extend to standard softmax attention due to rank explosion. Together, these results tighten bridge between recurrent SSMs and Transformers, and widen the design space for expressive yet efficient sequence models."
    },
    {
      "paperId": "a916b6e0a90e135865958fe3d7be0a9156432dcd",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-04476",
        "ArXiv": "2510.04476",
        "DOI": "10.48550/arXiv.2510.04476",
        "CorpusId": 281843802
      },
      "corpusId": 281843802,
      "title": "Compressed Convolutional Attention: Efficient Attention in a Compressed Latent Space",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.04476, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2304451033",
          "name": "Tomas Figliolia"
        },
        {
          "authorId": "2304449197",
          "name": "Nick Alonso"
        },
        {
          "authorId": "28962903",
          "name": "Rishi Iyer"
        },
        {
          "authorId": "2282542187",
          "name": "Quentin Anthony"
        },
        {
          "authorId": "150045277",
          "name": "Beren Millidge"
        }
      ],
      "abstract": "Multi-headed Attention's (MHA) quadratic compute and linearly growing KV-cache make long-context transformers expensive to train and serve. Prior works such as Grouped Query Attention (GQA) and Multi-Latent Attention (MLA) shrink the cache, speeding decode, but leave compute, which determines prefill and training speed, largely unchanged. We introduce Compressed Convolutional Attention (CCA), a novel attention method which down-projects queries, keys, and values and performs the entire attention operation inside the shared latent space. This simple design dramatically cuts parameters, KV-cache, and FLOPs all at once by the desired compression factor. Because CCA is orthogonal to head-sharing, we combine the two to form Compressed Convolutional Grouped Query Attention (CCGQA), which further tightens the compute-bandwidth Pareto frontier so that users can tune compression toward either FLOP or memory limits without sacrificing quality. Experiments show that CCGQA consistently outperforms both GQA and MLA at equal KV-cache compression on dense and MoE models. Additionally, we show that CCGQA outperforms all other attention methods on MoE models with half the KV-cache of GQA and MLA, achieving an 8x KV-cache compression with no drop in performance compared to standard MHA. CCA and CCGQA also dramatically reduce the FLOP cost of attention which leads to substantially faster training and prefill than existing methods. On H100 GPUs, our fused CCA/CCGQA kernel reduces prefill latency by about 1.7x at a sequence length of 16k relative to MHA, and accelerates backward by about 1.3x."
    },
    {
      "paperId": "fb046b7e432fd2f7980128cda8bfbf921f5ddd28",
      "externalIds": {
        "ArXiv": "2510.04908",
        "DBLP": "journals/corr/abs-2510-04908",
        "DOI": "10.48550/arXiv.2510.04908",
        "CorpusId": 281843956
      },
      "corpusId": 281843956,
      "title": "How Different from the Past? Spatio-Temporal Time Series Forecasting with Self-Supervised Deviation Learning",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.04908, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2269371004",
          "name": "Haotian Gao"
        },
        {
          "authorId": "2203116078",
          "name": "Zheng Dong"
        },
        {
          "authorId": "2064967385",
          "name": "Jiawei Yong"
        },
        {
          "authorId": "2053705074",
          "name": "Shintaro Fukushima"
        },
        {
          "authorId": "1724468",
          "name": "K. Taura"
        },
        {
          "authorId": "2310800507",
          "name": "Renhe Jiang"
        }
      ],
      "abstract": "Spatio-temporal forecasting is essential for real-world applications such as traffic management and urban computing. Although recent methods have shown improved accuracy, they often fail to account for dynamic deviations between current inputs and historical patterns. These deviations contain critical signals that can significantly affect model performance. To fill this gap, we propose ST-SSDL, a Spatio-Temporal time series forecasting framework that incorporates a Self-Supervised Deviation Learning scheme to capture and utilize such deviations. ST-SSDL anchors each input to its historical average and discretizes the latent space using learnable prototypes that represent typical spatio-temporal patterns. Two auxiliary objectives are proposed to refine this structure: a contrastive loss that enhances inter-prototype discriminability and a deviation loss that regularizes the distance consistency between input representations and corresponding prototypes to quantify deviation. Optimized jointly with the forecasting objective, these components guide the model to organize its hidden space and improve generalization across diverse input conditions. Experiments on six benchmark datasets show that ST-SSDL consistently outperforms state-of-the-art baselines across multiple metrics. Visualizations further demonstrate its ability to adaptively respond to varying levels of deviation in complex spatio-temporal scenarios. Our code and datasets are available at https://github.com/Jimmy-7664/ST-SSDL."
    },
    {
      "paperId": "cced0b64829c12ffb423beca46d851572943ea07",
      "externalIds": {
        "ArXiv": "2510.04900",
        "DBLP": "journals/corr/abs-2510-04900",
        "DOI": "10.48550/arXiv.2510.04900",
        "CorpusId": 281842397
      },
      "corpusId": 281842397,
      "title": "Benchmarking M-LTSF: Frequency and Noise-Based Evaluation of Multivariate Long Time Series Forecasting Models",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.04900, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2348464326",
          "name": "N. Janssen"
        },
        {
          "authorId": "2347348873",
          "name": "Melanie Schaller"
        },
        {
          "authorId": "2261857813",
          "name": "Bodo Rosenhahn"
        }
      ],
      "abstract": "Understanding the robustness of deep learning models for multivariate long-term time series forecasting (M-LTSF) remains challenging, as evaluations typically rely on real-world datasets with unknown noise properties. We propose a simulation-based evaluation framework that generates parameterizable synthetic datasets, where each dataset instance corresponds to a different configuration of signal components, noise types, signal-to-noise ratios, and frequency characteristics. These configurable components aim to model real-world multivariate time series data without the ambiguity of unknown noise. This framework enables fine-grained, systematic evaluation of M-LTSF models under controlled and diverse scenarios. We benchmark four representative architectures S-Mamba (state-space), iTransformer (transformer-based), R-Linear (linear), and Autoformer (decomposition-based). Our analysis reveals that all models degrade severely when lookback windows cannot capture complete periods of seasonal patters in the data. S-Mamba and Autoformer perform best on sawtooth patterns, while R-Linear and iTransformer favor sinusoidal signals. White and Brownian noise universally degrade performance with lower signal-to-noise ratio while S-Mamba shows specific trend-noise and iTransformer shows seasonal-noise vulnerability. Further spectral analysis shows that S-Mamba and iTransformer achieve superior frequency reconstruction. This controlled approach, based on our synthetic and principle-driven testbed, offers deeper insights into model-specific strengths and limitations through the aggregation of MSE scores and provides concrete guidance for model selection based on signal characteristics and noise conditions."
    },
    {
      "paperId": "dedb94c4917711b5225d2781440500f1e4640d80",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-04667",
        "ArXiv": "2510.04667",
        "DOI": "10.48550/arXiv.2510.04667",
        "CorpusId": 281843042
      },
      "corpusId": 281843042,
      "title": "Noise or Signal? Deconstructing Contradictions and An Adaptive Remedy for Reversible Normalization in Time Series Forecasting",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.04667, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2310630445",
          "name": "Fanzhe Fu"
        },
        {
          "authorId": "2384446337",
          "name": "Yang Yang"
        }
      ],
      "abstract": "Reversible Instance Normalization (RevIN) is a key technique enabling simple linear models to achieve state-of-the-art performance in time series forecasting. While replacing its non-robust statistics with robust counterparts (termed R$^2$-IN) seems like a straightforward improvement, our findings reveal a far more complex reality. This paper deconstructs the perplexing performance of various normalization strategies by identifying four underlying theoretical contradictions. Our experiments provide two crucial findings: first, the standard RevIN catastrophically fails on datasets with extreme outliers, where its MSE surges by a staggering 683\\%. Second, while the simple R$^2$-IN prevents this failure and unexpectedly emerges as the best overall performer, our adaptive model (A-IN), designed to test a diagnostics-driven heuristic, unexpectedly suffers a complete and systemic failure. This surprising outcome uncovers a critical, overlooked pitfall in time series analysis: the instability introduced by a simple or counter-intuitive heuristic can be more damaging than the statistical issues it aims to solve. The core contribution of this work is thus a new, cautionary paradigm for time series normalization: a shift from a blind search for complexity to a diagnostics-driven analysis that reveals not only the surprising power of simple baselines but also the perilous nature of naive adaptation."
    },
    {
      "paperId": "ebad77d3a23fcdf5b0faeb8ed534524adbdc511c",
      "externalIds": {
        "ArXiv": "2510.04595",
        "DBLP": "journals/corr/abs-2510-04595",
        "DOI": "10.48550/arXiv.2510.04595",
        "CorpusId": 281842756
      },
      "corpusId": 281842756,
      "title": "SpikingMamba: Towards Energy-Efficient Large Language Models via Knowledge Distillation from Mamba",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.04595, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2232839913",
          "name": "Yulong Huang"
        },
        {
          "authorId": "2384560782",
          "name": "Jianxiong Tang"
        },
        {
          "authorId": "2307386262",
          "name": "Chao Wang"
        },
        {
          "authorId": "2342625584",
          "name": "Ziyi Wang"
        },
        {
          "authorId": "2307184694",
          "name": "Jianguo Zhang"
        },
        {
          "authorId": "2307128535",
          "name": "Zhichao Lu"
        },
        {
          "authorId": "2280523083",
          "name": "Bojun Cheng"
        },
        {
          "authorId": "48205902",
          "name": "Luziwei Leng"
        }
      ],
      "abstract": "Large Language Models (LLMs) have achieved remarkable performance across tasks but remain energy-intensive due to dense matrix operations. Spiking neural networks (SNNs) improve energy efficiency by replacing dense matrix multiplications with sparse accumulations. Their sparse spike activity enables efficient LLMs deployment on edge devices. However, prior SNN-based LLMs often sacrifice performance for efficiency, and recovering accuracy typically requires full pretraining, which is costly and impractical. To address this, we propose SpikingMamba, an energy-efficient SNN-based LLMs distilled from Mamba that improves energy efficiency with minimal accuracy sacrifice. SpikingMamba integrates two key components: (a) TI-LIF, a ternary-integer spiking neuron that preserves semantic polarity through signed multi-level spike representations. (b) A training-exclusive Smoothed Gradient Compensation (SGC) path mitigating quantization loss while preserving spike-driven efficiency. We employ a single-stage distillation strategy to transfer the zero-shot ability of pretrained Mamba and further enhance it via reinforcement learning (RL). Experiments show that SpikingMamba-1.3B achieves a 4.76$\\times$ energy benefit, with only a 4.78\\% zero-shot accuracy gap compared to the original Mamba, and achieves a further 2.55\\% accuracy improvement after RL."
    },
    {
      "paperId": "890150872cbeeb69d149ca3ff701cd80b75ed8b8",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-04923",
        "ArXiv": "2510.04923",
        "DOI": "10.48550/arXiv.2510.04923",
        "CorpusId": 281844182
      },
      "corpusId": 281844182,
      "title": "REN: Anatomically-Informed Mixture-of-Experts for Interstitial Lung Disease Diagnosis",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.04923, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2219625809",
          "name": "Alec Peltekian"
        },
        {
          "authorId": "2327049418",
          "name": "Halil Ertugrul Aktas"
        },
        {
          "authorId": "2275241127",
          "name": "Gorkem Durak"
        },
        {
          "authorId": "2302302782",
          "name": "Kevin M Grudzinski"
        },
        {
          "authorId": "2310302377",
          "name": "Bradford C Bemiss"
        },
        {
          "authorId": "2364973747",
          "name": "Carrie Richardson"
        },
        {
          "authorId": "2299245967",
          "name": "J.E. Dematte"
        },
        {
          "authorId": "2254705106",
          "name": "G. S. Budinger"
        },
        {
          "authorId": "2253512024",
          "name": "Anthony J. Esposito"
        },
        {
          "authorId": "2342252469",
          "name": "Alexander V. Misharin"
        },
        {
          "authorId": "2254309592",
          "name": "Alok N. Choudhary"
        },
        {
          "authorId": "2342254607",
          "name": "Ankit Agrawal"
        },
        {
          "authorId": "2373725797",
          "name": "Ulas Bagci"
        }
      ],
      "abstract": "Mixture-of-Experts (MoE) architectures have significantly contributed to scalable machine learning by enabling specialized subnetworks to tackle complex tasks efficiently. However, traditional MoE systems lack domain-specific constraints essential for medical imaging, where anatomical structure and regional disease heterogeneity strongly influence pathological patterns. Here, we introduce Regional Expert Networks (REN), the first anatomically-informed MoE framework tailored specifically for medical image classification. REN leverages anatomical priors to train seven specialized experts, each dedicated to distinct lung lobes and bilateral lung combinations, enabling precise modeling of region-specific pathological variations. Multi-modal gating mechanisms dynamically integrate radiomics biomarkers and deep learning (DL) features (CNN, ViT, Mamba) to weight expert contributions optimally. Applied to interstitial lung disease (ILD) classification, REN achieves consistently superior performance: the radiomics-guided ensemble reached an average AUC of 0.8646 +/- 0.0467, a +12.5 percent improvement over the SwinUNETR baseline (AUC 0.7685, p = 0.031). Region-specific experts further revealed that lower-lobe models achieved AUCs of 0.88-0.90, surpassing DL counterparts (CNN: 0.76-0.79) and aligning with known disease progression patterns. Through rigorous patient-level cross-validation, REN demonstrates strong generalizability and clinical interpretability, presenting a scalable, anatomically-guided approach readily extensible to other structured medical imaging applications."
    },
    {
      "paperId": "9163e6b0717d2a29e07889e4ace1d3816d57be6b",
      "externalIds": {
        "ArXiv": "2510.05305",
        "DBLP": "journals/corr/abs-2510-05305",
        "DOI": "10.48550/arXiv.2510.05305",
        "CorpusId": 281886528
      },
      "corpusId": 281886528,
      "title": "WaveSP-Net: Learnable Wavelet-Domain Sparse Prompt Tuning for Speech Deepfake Detection",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.05305, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2375044942",
          "name": "Xi Xuan"
        },
        {
          "authorId": "46521769",
          "name": "Xuechen Liu"
        },
        {
          "authorId": "2375863690",
          "name": "Wenxin Zhang"
        },
        {
          "authorId": "2375889566",
          "name": "Yi-Cheng Lin"
        },
        {
          "authorId": "2356705256",
          "name": "Xiaojian Lin"
        },
        {
          "authorId": "2375044994",
          "name": "Tomi Kinnunen"
        }
      ],
      "abstract": "Modern front-end design for speech deepfake detection relies on full fine-tuning of large pre-trained models like XLSR. However, this approach is not parameter-efficient and may lead to suboptimal generalization to realistic, in-the-wild data types. To address these limitations, we introduce a new family of parameter-efficient front-ends that fuse prompt-tuning with classical signal processing transforms. These include FourierPT-XLSR, which uses the Fourier Transform, and two variants based on the Wavelet Transform: WSPT-XLSR and Partial-WSPT-XLSR. We further propose WaveSP-Net, a novel architecture combining a Partial-WSPT-XLSR front-end and a bidirectional Mamba-based back-end. This design injects multi-resolution features into the prompt embeddings, which enhances the localization of subtle synthetic artifacts without altering the frozen XLSR parameters. Experimental results demonstrate that WaveSP-Net outperforms several state-of-the-art models on two new and challenging benchmarks, Deepfake-Eval-2024 and SpoofCeleb, with low trainable parameters and notable performance gains. The code and models are available at https://github.com/xxuan-acoustics/WaveSP-Net."
    },
    {
      "paperId": "cffd38740247f53b059688563d2bb45a905249e2",
      "externalIds": {
        "DOI": "10.1109/ICIDCA66325.2025.11280559",
        "CorpusId": 283924606
      },
      "corpusId": 283924606,
      "title": "A Comprehensive Study on the Architecture and Implementation of Large Language Models (LLMs)",
      "venue": "2025 7th International Conference on Innovative Data Communication Technologies and Application (ICIDCA)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICIDCA66325.2025.11280559?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICIDCA66325.2025.11280559, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2399066510",
          "name": "S.Jamalaiah"
        },
        {
          "authorId": "2399064702",
          "name": "CH.Uday"
        }
      ],
      "abstract": "Large Language Models (LLMs) have rapidly gone From research tools in labs to in dispensable components of smart systems in various sectors like healthcare, finance, education, and law. With the transformer architecture, LLMs make use of such as self-attention, positional encoding, kenization, and embedding for perception and generation of human-like speech. The models surround a very wide range of families such as the masked, autoregressive, and encoder families, best suiting certain natural language tasks. During the recent years, work of research spill over into skill adaptation, optimization strategies, large-scale training approach, and crossing challenges such as hallucination and bias. New technologies like multimodal integration, retrieval generation (RAG), and parameter-efficient fine-t tuning have greatly improved their efficiencies and productivity. The paper here presents the LLM Chartboard framework, a theoretical and partially coded system replicating the architecture and operations of LLMs in naturalistic settings. The framework is characterized by the general indicators, BLEU, accuracy, perplexity, and human evaluation, such as a high degree of factual accuracy, Fluency, and reference response similarity. Through the unification of theoretical underpinnings and real-world details, this work Describe in full detail how LLMs can be designed, trained, optimized, and deployed for various practical applications."
    },
    {
      "paperId": "4a2b31e7ae36932e025429fb4a58f7c10ec48eba",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-04386",
        "ArXiv": "2510.04386",
        "DOI": "10.48550/arXiv.2510.04386",
        "CorpusId": 281844212
      },
      "corpusId": 281844212,
      "title": "SSM-CGM: Interpretable State-Space Forecasting Model of Continuous Glucose Monitoring for Personalized Diabetes Management",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.04386, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2163402537",
          "name": "Shakson Isaac"
        },
        {
          "authorId": "2384134752",
          "name": "Yentl Collin"
        },
        {
          "authorId": "2384137926",
          "name": "Chirag Patel"
        }
      ],
      "abstract": "Continuous glucose monitoring (CGM) generates dense data streams critical for diabetes management, but most used forecasting models lack interpretability for clinical use. We present SSM-CGM, a Mamba-based neural state-space forecasting model that integrates CGM and wearable activity signals from the AI-READI cohort. SSM-CGM improves short-term accuracy over a Temporal Fusion Transformer baseline, adds interpretability through variable selection and temporal attribution, and enables counterfactual forecasts simulating how planned changes in physiological signals (e.g., heart rate, respiration) affect near-term glucose. Together, these features make SSM-CGM an interpretable, physiologically grounded framework for personalized diabetes management."
    },
    {
      "paperId": "cb388b349f900eeceeb9c3a5ea326b53db2966ef",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-04304",
        "ArXiv": "2510.04304",
        "DOI": "10.48550/arXiv.2510.04304",
        "CorpusId": 281842846
      },
      "corpusId": 281842846,
      "title": "Wave-PDE Nets: Trainable Wave-Equation Layers as an Alternative to Attention",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.04304, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2362303022",
          "name": "Harshil Vejendla"
        }
      ],
      "abstract": "We introduce Wave-PDE Nets, a neural architecture whose elementary operation is a differentiable simulation of the second-order wave equation. Each layer propagates its hidden state as a continuous field through a medium with trainable spatial velocity c(x) and damping {\\gamma}(x). A symplectic spectral solver based on FFTs realises this propagation in O(nlog n) time. This oscillatory, global mechanism provides a powerful alternative to attention and first-order state-space models. We prove that a single Wave-PDE layer is a universal approximator. On language and vision benchmarks, Wave-PDE Nets match or exceed Transformer performance while demonstrating superior practical efficiency, reducing wall-clock time by up to 30% and peak memory by 25%. Ablation studies confirm the critical role of symplectic integration and a spectral Laplacian for stability and performance. Visualizations of the learned physical parameters reveal that the model learns intuitive strategies for information propagation. These results position Wave-PDE Nets as a computationally efficient and robust architecture with a strong physical inductive bias."
    },
    {
      "paperId": "fe860304efe129ee389725e986bdeb9eab9fd90f",
      "externalIds": {
        "ArXiv": "2510.05184",
        "DBLP": "journals/corr/abs-2510-05184",
        "DOI": "10.48550/arXiv.2510.05184",
        "CorpusId": 281886753
      },
      "corpusId": 281886753,
      "title": "Representation Potentials of Foundation Models for Multimodal Alignment: A Survey",
      "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.05184, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2257319391",
          "name": "Jianglin Lu"
        },
        {
          "authorId": "2113220177",
          "name": "Hailing Wang"
        },
        {
          "authorId": "2257074262",
          "name": "Yi Xu"
        },
        {
          "authorId": "2269776388",
          "name": "Yizhou Wang"
        },
        {
          "authorId": "2384399962",
          "name": "Kuo Yang"
        },
        {
          "authorId": "2269755402",
          "name": "Yun Fu"
        }
      ],
      "abstract": "Foundation models learn highly transferable representations through large-scale pretraining on diverse data. An increasing body of research indicates that these representations exhibit a remarkable degree of similarity across architectures and modalities. In this survey, we investigate the representation potentials of foundation models, defined as the latent capacity of their learned representations to capture task-specific information within a single modality while also providing a transferable basis for alignment and unification across modalities. We begin by reviewing representative foundation models and the key metrics that make alignment measurable. We then synthesize empirical evidence of representation potentials from studies in vision, language, speech, multimodality, and neuroscience. The evidence suggests that foundation models often exhibit structural regularities and semantic consistencies in their representation spaces, positioning them as strong candidates for cross-modal transfer and alignment. We further analyze the key factors that foster representation potentials, discuss open questions, and highlight potential challenges."
    },
    {
      "paperId": "2948e5da8088ab43e2c09c4495889bd5d2a1b9b5",
      "externalIds": {
        "DOI": "10.3390/rs17193367",
        "CorpusId": 281906062
      },
      "corpusId": 281906062,
      "title": "LMVMamba: A Hybrid U-Shape Mamba for Remote Sensing Segmentation with Adaptation Fine-Tuning",
      "venue": "Remote Sensing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/rs17193367?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/rs17193367, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2386042292",
          "name": "Fan Li"
        },
        {
          "authorId": "2324025412",
          "name": "Xiao Wang"
        },
        {
          "authorId": "2280885922",
          "name": "Haochen Wang"
        },
        {
          "authorId": "2322222801",
          "name": "Hamed Karimian"
        },
        {
          "authorId": "2280913761",
          "name": "Juan Shi"
        },
        {
          "authorId": "2323230220",
          "name": "Guozhen Zha"
        }
      ],
      "abstract": "High-precision semantic segmentation of remote sensing imagery is crucial in geospatial analysis. It plays an immeasurable role in fields such as urban governance, environmental monitoring, and natural resource management. However, when confronted with complex objects (such as winding roads and dispersed buildings), existing semantic segmentation methods still suffer from inadequate target recognition capabilities and multi-scale representation issues. This paper proposes a neural network model, LMVMamba (LoRA Multi-scale Vision Mamba), for semantic segmentation of remote sensing images. This model integrates the advantages of convolutional neural networks (CNNs), Transformers, and state-space models (Mamba) with a multi-scale feature fusion strategy. It simultaneously captures global contextual information and fine-grained local features. Specifically, in the encoder stage, the ResT Transformer serves as the backbone network, employing a LoRA fine-tuning strategy to effectively enhance model accuracy by training only the introduced low-rank matrix pairs. The extracted features are then passed to the decoder, where a U-shaped Mamba decoder is designed. In this stage, a Multi-Scale Post-processing Block (MPB) is introduced, consisting of depthwise separable convolutions and residual concatenation. This block effectively extracts multi-scale features and enhances local detail extraction after the VSS block. Additionally, a Local Enhancement and Fusion Attention Module (LAS) is added at the end of each decoder block. LAS integrates the SimAM attention mechanism, further enhancing the model\u2019s multi-scale feature fusion capability and local detail segmentation capability. Through extensive comparative experiments, it was found that LMVMamba achieves superior performance on the OpenEarthMap dataset (mIoU 52.3%, OA 69.8%, mF1: 68.0%) and LoveDA (mIoU 67.9%, OA 80.3%, mF1: 80.5%) datasets. Ablation experiments validated the effectiveness of each module. The final results indicate that this model is highly suitable for high-precision land-cover classification tasks in remote sensing imagery. LMVMamba provides an effective solution for precise semantic segmentation of high-resolution remote sensing imagery."
    },
    {
      "paperId": "4112282243bdfdb7bbb2e032519d7696ac2bfa6b",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-03814",
        "ArXiv": "2510.03814",
        "DOI": "10.48550/arXiv.2510.03814",
        "CorpusId": 281843601
      },
      "corpusId": 281843601,
      "title": "Detecting Invariant Manifolds in ReLU-Based RNNs",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.03814, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2261734122",
          "name": "Lukas Eisenmann"
        },
        {
          "authorId": "2269169015",
          "name": "Alena Br\u00e4ndle"
        },
        {
          "authorId": "104047138",
          "name": "Z. Monfared"
        },
        {
          "authorId": "2259488649",
          "name": "Daniel Durstewitz"
        }
      ],
      "abstract": "Recurrent Neural Networks (RNNs) have found widespread applications in machine learning for time series prediction and dynamical systems reconstruction, and experienced a recent renaissance with improved training algorithms and architectural designs. Understanding why and how trained RNNs produce their behavior is important for scientific and medical applications, and explainable AI more generally. An RNN's dynamical repertoire depends on the topological and geometrical properties of its state space. Stable and unstable manifolds of periodic points play a particularly important role: They dissect a dynamical system's state space into different basins of attraction, and their intersections lead to chaotic dynamics with fractal geometry. Here we introduce a novel algorithm for detecting these manifolds, with a focus on piecewise-linear RNNs (PLRNNs) employing rectified linear units (ReLUs) as their activation function. We demonstrate how the algorithm can be used to trace the boundaries between different basins of attraction, and hence to characterize multistability, a computationally important property. We further show its utility in finding so-called homoclinic points, the intersections between stable and unstable manifolds, and thus establish the existence of chaos in PLRNNs. Finally we show for an empirical example, electrophysiological recordings from a cortical neuron, how insights into the underlying dynamics could be gained through our method."
    },
    {
      "paperId": "fd0e783fb20858cad93ae8467d28d0892ba977d0",
      "externalIds": {
        "ArXiv": "2510.03786",
        "DBLP": "journals/corr/abs-2510-03786",
        "DOI": "10.48550/arXiv.2510.03786",
        "CorpusId": 281843798
      },
      "corpusId": 281843798,
      "title": "MambaCAFU: Hybrid Multi-Scale and Multi-Attention Model with Mamba-Based Fusion for Medical Image Segmentation",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.03786, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2331323104",
          "name": "Thai Bui"
        },
        {
          "authorId": "73543637",
          "name": "F. Bougourzi"
        },
        {
          "authorId": "2254142473",
          "name": "Fadi Dornaika"
        },
        {
          "authorId": "9741305",
          "name": "Vinh Truong Hoang"
        }
      ],
      "abstract": "In recent years, deep learning has shown near-expert performance in segmenting complex medical tissues and tumors. However, existing models are often task-specific, with performance varying across modalities and anatomical regions. Balancing model complexity and performance remains challenging, particularly in clinical settings where both accuracy and efficiency are critical. To address these issues, we propose a hybrid segmentation architecture featuring a three-branch encoder that integrates CNNs, Transformers, and a Mamba-based Attention Fusion (MAF) mechanism to capture local, global, and long-range dependencies. A multi-scale attention-based CNN decoder reconstructs fine-grained segmentation maps while preserving contextual consistency. Additionally, a co-attention gate enhances feature selection by emphasizing relevant spatial and semantic information across scales during both encoding and decoding, improving feature interaction and cross-scale communication. Extensive experiments on multiple benchmark datasets show that our approach outperforms state-of-the-art methods in accuracy and generalization, while maintaining comparable computational complexity. By effectively balancing efficiency and effectiveness, our architecture offers a practical and scalable solution for diverse medical imaging tasks. Source code and trained models will be publicly released upon acceptance to support reproducibility and further research."
    },
    {
      "paperId": "ed64e5f1326522bf7ae3e41c4b7b623852df1a98",
      "externalIds": {
        "DBLP": "journals/sivp/YangJ25",
        "DOI": "10.1007/s11760-025-04840-y",
        "CorpusId": 281848380
      },
      "corpusId": 281848380,
      "title": "Multivariate charging curve prediction of electric vehicle battery pack via multi-scale features promoted Mamba-Transformer expert",
      "venue": "Signal, Image and Video Processing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11760-025-04840-y?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11760-025-04840-y, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2384148502",
          "name": "Yanli Yang"
        },
        {
          "authorId": "2384326890",
          "name": "Yizhe Jia"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "9e37c915992d0af95e4e74e1592de2e16914bd09",
      "externalIds": {
        "DBLP": "journals/tmlr/HeinsenK25",
        "ArXiv": "2510.03426",
        "DOI": "10.48550/arXiv.2510.03426",
        "CorpusId": 281726236
      },
      "corpusId": 281726236,
      "title": "Generalized Orders of Magnitude for Scalable, Parallel, High-Dynamic-Range Computation",
      "venue": "Trans. Mach. Learn. Res.",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.03426, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "1397503475",
          "name": "Franz A. Heinsen"
        },
        {
          "authorId": "2383327860",
          "name": "Leo Kozachkov"
        }
      ],
      "abstract": "Many domains, from deep learning to finance, require compounding real numbers over long sequences, often leading to catastrophic numerical underflow or overflow. We introduce generalized orders of magnitude (GOOMs), a principled extension of traditional orders of magnitude that incorporates floating-point numbers as a special case, and which in practice enables stable computation over significantly larger dynamic ranges of real numbers than previously possible. We implement GOOMs, along with an efficient custom parallel prefix scan, to support native execution on parallel hardware such as GPUs. We demonstrate that our implementation of GOOMs outperforms traditional approaches with three representative experiments, all of which were previously considered impractical or impossible, and now become possible and practical: (1) compounding real matrix products far beyond standard floating-point limits; (2) estimating spectra of Lyapunov exponents in parallel, orders of magnitude faster than with previous methods, applying a novel selective-resetting method to prevent state colinearity; and (3) capturing long-range dependencies in deep recurrent neural networks with non-diagonal recurrent states, computed in parallel via a prefix scan, without requiring any form of stabilization. Our results show that our implementation of GOOMs, combined with efficient parallel scanning, offers a scalable and numerically robust alternative to conventional floating-point numbers for high-dynamic-range applications."
    },
    {
      "paperId": "94a8473298c58918d68319e7d2c1efe0e368eab1",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-02936",
        "ArXiv": "2510.02936",
        "DOI": "10.48550/arXiv.2510.02936",
        "CorpusId": 281829385
      },
      "corpusId": 281829385,
      "title": "RAxSS: Retrieval-Augmented Sparse Sampling for Explainable Variable-Length Medical Time Series Classification",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.02936, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2100653341",
          "name": "Aydin Javadov"
        },
        {
          "authorId": "2100644754",
          "name": "Samir Garibov"
        },
        {
          "authorId": "2383973494",
          "name": "Tobias Hoesli"
        },
        {
          "authorId": "2325177407",
          "name": "Qiyang Sun"
        },
        {
          "authorId": "1958502",
          "name": "F. Wangenheim"
        },
        {
          "authorId": "121107252",
          "name": "J. Ollier"
        },
        {
          "authorId": "2333358958",
          "name": "Bjorn W. Schuller"
        }
      ],
      "abstract": "Medical time series analysis is challenging due to data sparsity, noise, and highly variable recording lengths. Prior work has shown that stochastic sparse sampling effectively handles variable-length signals, while retrieval-augmented approaches improve explainability and robustness to noise and weak temporal correlations. In this study, we generalize the stochastic sparse sampling framework for retrieval-informed classification. Specifically, we weight window predictions by within-channel similarity and aggregate them in probability space, yielding convex series-level scores and an explicit evidence trail for explainability. Our method achieves competitive iEEG classification performance and provides practitioners with greater transparency and explainability. We evaluate our method in iEEG recordings collected in four medical centers, demonstrating its potential for reliable and explainable clinical variable-length time series classification."
    },
    {
      "paperId": "d022cf010f9c9e6df5a9d6a77589c001ccf2d1c5",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-02848",
        "ArXiv": "2510.02848",
        "DOI": "10.48550/arXiv.2510.02848",
        "CorpusId": 281829730
      },
      "corpusId": 281829730,
      "title": "Flamed-TTS: Flow Matching Attention-Free Models for Efficient Generating and Dynamic Pacing Zero-shot Text-to-Speech",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.02848, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2362301768",
          "name": "Hieu-Nghia Huynh-Nguyen"
        },
        {
          "authorId": "2362301676",
          "name": "Huynh Nguyen Dang"
        },
        {
          "authorId": "2311748452",
          "name": "N. Nguyen"
        },
        {
          "authorId": "2313870457",
          "name": "Van Nguyen"
        }
      ],
      "abstract": "Zero-shot Text-to-Speech (TTS) has recently advanced significantly, enabling models to synthesize speech from text using short, limited-context prompts. These prompts serve as voice exemplars, allowing the model to mimic speaker identity, prosody, and other traits without extensive speaker-specific data. Although recent approaches incorporating language models, diffusion, and flow matching have proven their effectiveness in zero-shot TTS, they still encounter challenges such as unreliable synthesis caused by token repetition or unexpected content transfer, along with slow inference and substantial computational overhead. Moreover, temporal diversity-crucial for enhancing the naturalness of synthesized speech-remains largely underexplored. To address these challenges, we propose Flamed-TTS, a novel zero-shot TTS framework that emphasizes low computational cost, low latency, and high speech fidelity alongside rich temporal diversity. To achieve this, we reformulate the flow matching training paradigm and incorporate both discrete and continuous representations corresponding to different attributes of speech. Experimental results demonstrate that Flamed-TTS surpasses state-of-the-art models in terms of intelligibility, naturalness, speaker similarity, acoustic characteristics preservation, and dynamic pace. Notably, Flamed-TTS achieves the best WER of 4% compared to the leading zero-shot TTS baselines, while maintaining low latency in inference and high fidelity in generated speech. Code and audio samples are available at our demo page https://flamed-tts.github.io."
    },
    {
      "paperId": "25f984e9def88f94547525c376f50e080e9c7a0c",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-02970",
        "ArXiv": "2510.02970",
        "DOI": "10.1007/978-3-032-05185-1_21",
        "CorpusId": 281829642
      },
      "corpusId": 281829642,
      "title": "Flip Distribution Alignment VAE for Multi-phase MRI Synthesis",
      "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.02970, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2157632957",
          "name": "Xiaoyan Kui"
        },
        {
          "authorId": "2386412149",
          "name": "Qianmu Xiao"
        },
        {
          "authorId": "2384065805",
          "name": "Qqinsong Li"
        },
        {
          "authorId": "2150432036",
          "name": "Zexin Ji"
        },
        {
          "authorId": "2383980608",
          "name": "JIelin Zhang"
        },
        {
          "authorId": "2238657724",
          "name": "Beiji Zou"
        }
      ],
      "abstract": "Separating shared and independent features is crucial for multi-phase contrast-enhanced (CE) MRI synthesis. However, existing methods use deep autoencoder generators with low parameter efficiency and lack interpretable training strategies. In this paper, we propose Flip Distribution Alignment Variational Autoencoder (FDA-VAE), a lightweight feature-decoupled VAE model for multi-phase CE MRI synthesis. Our method encodes input and target images into two latent distributions that are symmetric concerning a standard normal distribution, effectively separating shared and independent features. The Y-shaped bidirectional training strategy further enhances the interpretability of feature separation. Experimental results show that compared to existing deep autoencoder-based end-to-end synthesis methods, FDA-VAE significantly reduces model parameters and inference time while effectively improving synthesis quality. The source code is publicly available at https://github.com/QianMuXiao/FDA-VAE."
    },
    {
      "paperId": "956469644b8f3353b8c0a05ef9f29d4077d6c2ef",
      "externalIds": {
        "ArXiv": "2510.03561",
        "DBLP": "journals/corr/abs-2510-03561",
        "DOI": "10.48550/arXiv.2510.03561",
        "CorpusId": 281843708
      },
      "corpusId": 281843708,
      "title": "Reactive Transformer (RxT) - Stateful Real-Time Processing for Event-Driven Reactive Language Models",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.03561, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2384366932",
          "name": "Adam Filipek"
        }
      ],
      "abstract": "The Transformer architecture has become the de facto standard for Large Language Models (LLMs), demonstrating remarkable capabilities in language understanding and generation. However, its application in conversational AI is fundamentally constrained by its stateless nature and the quadratic computational complexity ($O(L^2)$) with respect to sequence length $L$. Current models emulate memory by reprocessing an ever-expanding conversation history with each turn, leading to prohibitive costs and latency in long dialogues. This paper introduces the Reactive Transformer (RxT), a novel architecture designed to overcome these limitations by shifting from a data-driven to an event-driven paradigm. RxT processes each conversational turn as a discrete event in real-time, maintaining context in an integrated, fixed-size Short-Term Memory (STM) system. The architecture features a distinct operational cycle where a generator-decoder produces a response based on the current query and the previous memory state, after which a memory-encoder and a dedicated Memory Attention network asynchronously update the STM with a representation of the complete interaction. This design fundamentally alters the scaling dynamics, reducing the total user-facing cost of a conversation from quadratic ($O(N^2 \\cdot T)$) to linear ($O(N \\cdot T)$) with respect to the number of interactions $N$. By decoupling response generation from memory updates, RxT achieves low latency, enabling truly real-time, stateful, and economically viable long-form conversations. We validated our architecture with a series of proof-of-concept experiments on synthetic data, demonstrating superior performance and constant-time inference latency compared to a baseline stateless model of comparable size."
    },
    {
      "paperId": "d07b7c261243274711f5d3bfd2f02a722c204999",
      "externalIds": {
        "ArXiv": "2510.03568",
        "DBLP": "journals/corr/abs-2510-03568",
        "DOI": "10.48550/arXiv.2510.03568",
        "CorpusId": 281842966
      },
      "corpusId": 281842966,
      "title": "How We Won BraTS-SSA 2025: Brain Tumor Segmentation in the Sub-Saharan African Population Using Segmentation-Aware Data Augmentation and Model Ensembling",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.03568, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2384129642",
          "name": "Claudia Takyi Ankomah"
        },
        {
          "authorId": "2384123711",
          "name": "Livingstone Eli Ayivor"
        },
        {
          "authorId": "2392851882",
          "name": "Jones Yeboah Nyame"
        },
        {
          "authorId": "2384122871",
          "name": "Leslie Wambo"
        },
        {
          "authorId": "2384123654",
          "name": "Patrick Yeboah Bonsu"
        },
        {
          "authorId": "2384124172",
          "name": "Aondona Moses Iorumbur"
        },
        {
          "authorId": "2150564579",
          "name": "Raymond Confidence"
        },
        {
          "authorId": "2330587357",
          "name": "Toufiq Musah"
        }
      ],
      "abstract": "Brain tumors, particularly gliomas, pose significant chall-enges due to their complex growth patterns, infiltrative nature, and the variability in brain structure across individuals, which makes accurate diagnosis and monitoring difficult. Deep learning models have been developed to accurately delineate these tumors. However, most of these models were trained on relatively homogenous high-resource datasets, limiting their robustness when deployed in underserved regions. In this study, we performed segmentation-aware offline data augmentation on the BraTS-Africa dataset to increase the data sample size and diversity to enhance generalization. We further constructed an ensemble of three distinct architectures, MedNeXt, SegMamba, and Residual-Encoder U-Net, to leverage their complementary strengths. Our best-performing model, MedNeXt, was trained on 1000 epochs and achieved the highest average lesion-wise dice and normalized surface distance scores of 0.86 and 0.81 respectively. However, the ensemble model trained for 500 epochs produced the most balanced segmentation performance across the tumour subregions. This work demonstrates that a combination of advanced augmentation and model ensembling can improve segmentation accuracy and robustness on diverse and underrepresented datasets. Code available at: https://github.com/SPARK-Academy-2025/SPARK-2025/tree/main/SPARK2025_BraTs_MODELS/SPARK_NeuroAshanti"
    },
    {
      "paperId": "2e1860bd58710ee6efca9ebe985e58b84260336c",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-17833",
        "ArXiv": "2510.17833",
        "DOI": "10.48550/arXiv.2510.17833",
        "CorpusId": 282245959
      },
      "corpusId": 282245959,
      "title": "Brain-Language Model Alignment: Insights into the Platonic Hypothesis and Intermediate-Layer Advantage",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.17833, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2323786965",
          "name": "\u00c1ngela L\u00f3pez-Cardona"
        },
        {
          "authorId": "2350347641",
          "name": "Sebastian Idesis"
        },
        {
          "authorId": "2367191263",
          "name": "Mireia Masias Bruns"
        },
        {
          "authorId": "2330844467",
          "name": "Sergi Abadal"
        },
        {
          "authorId": "2309480030",
          "name": "Ioannis Arapakis"
        }
      ],
      "abstract": "Do brains and language models converge toward the same internal representations of the world? Recent years have seen a rise in studies of neural activations and model alignment. In this work, we review 25 fMRI-based studies published between 2023 and 2025 and explicitly confront their findings with two key hypotheses: (i) the Platonic Representation Hypothesis -- that as models scale and improve, they converge to a representation of the real world, and (ii) the Intermediate-Layer Advantage -- that intermediate (mid-depth) layers often encode richer, more generalizable features. Our findings provide converging evidence that models and brains may share abstract representational structures, supporting both hypotheses and motivating further research on brain-model alignment."
    },
    {
      "paperId": "3abc81092a80140dc66619fa613f6ccca9e56cae",
      "externalIds": {
        "DBLP": "journals/sivp/WuYZXMFXLYS25",
        "DOI": "10.1007/s11760-025-04789-y",
        "CorpusId": 281862152
      },
      "corpusId": 281862152,
      "title": "CMambaFuse: Circular mamba feature fusion for medical image segmentation",
      "venue": "Signal, Image and Video Processing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11760-025-04789-y?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11760-025-04789-y, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2339641107",
          "name": "Xudong Wu"
        },
        {
          "authorId": "2346533760",
          "name": "Baohua Yuan"
        },
        {
          "authorId": "2376806659",
          "name": "Juxiao Zhang"
        },
        {
          "authorId": "2385453733",
          "name": "Lin Xiang"
        },
        {
          "authorId": "2378120534",
          "name": "Jin Ma"
        },
        {
          "authorId": "2384168723",
          "name": "Teng Fang"
        },
        {
          "authorId": "2258404550",
          "name": "Dehao Xiao"
        },
        {
          "authorId": "2377255785",
          "name": "Ning Li"
        },
        {
          "authorId": "2269698303",
          "name": "Gaochao Yang"
        },
        {
          "authorId": "2258531462",
          "name": "Lin Shi"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "65f7ac60a32660705df1287bff6c2e21f9faac62",
      "externalIds": {
        "ArXiv": "2510.02206",
        "DBLP": "journals/corr/abs-2510-02206",
        "DOI": "10.48550/arXiv.2510.02206",
        "CorpusId": 281724391
      },
      "corpusId": 281724391,
      "title": "Poolformer: Recurrent Networks with Pooling for Long-Sequence Modeling",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.02206, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2383303513",
          "name": "Daniel Gallo Fern'andez"
        }
      ],
      "abstract": "Sequence-to-sequence models have become central in Artificial Intelligence, particularly following the introduction of the transformer architecture. While initially developed for Natural Language Processing, these models have demonstrated utility across domains, including Computer Vision. Such models require mechanisms to exchange information along the time dimension, typically using recurrent or self-attention layers. However, self-attention scales quadratically with sequence length, limiting its practicality for very long sequences. We introduce Poolformer, a sequence-to-sequence model that replaces self-attention with recurrent layers and incorporates pooling operations to reduce sequence length. Poolformer is defined recursively using SkipBlocks, which contain residual blocks, a down-pooling layer, a nested SkipBlock, an up-pooling layer, and additional residual blocks. We conduct extensive experiments to support our architectural choices. Our results show that pooling greatly accelerates training, improves perceptual metrics (FID and IS), and prevents overfitting. Our experiments also suggest that long-range dependencies are handled by deep layers, while shallow layers take care of short-term features. Evaluated on raw audio, which naturally features long sequence lengths, Poolformer outperforms state-of-the-art models such as SaShiMi and Mamba. Future directions include applications to text and vision, as well as multi-modal scenarios, where a Poolformer-based LLM could effectively process dense representations of images and videos."
    },
    {
      "paperId": "76e92949321e6c27c9e9168f6f62bc9125ac2dc1",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-01817",
        "ArXiv": "2510.01817",
        "DOI": "10.48550/arXiv.2510.01817",
        "CorpusId": 281724386
      },
      "corpusId": 281724386,
      "title": "Sparse Query Attention (SQA): A Computationally Efficient Attention Mechanism with Query Heads Reduction",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.01817, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2384366932",
          "name": "Adam Filipek"
        }
      ],
      "abstract": "The Transformer architecture, underpinned by the Multi-Head Attention (MHA) mechanism, has become the de facto standard for state-of-the-art models in artificial intelligence. However, the quadratic computational complexity of MHA with respect to sequence length presents a significant barrier to scaling, particularly for applications involving long contexts. Prevailing solutions, such as Multi-Query Attention (MQA) and Grouped-Query Attention (GQA), have effectively addressed the memory bandwidth bottleneck that dominates autoregressive inference latency by sharing Key and Value projections. While highly successful, these methods do not reduce the fundamental number of floating-point operations (FLOPs) required for the attention score computation, which remains a critical bottleneck for training and full-sequence processing. This paper introduces Sparse Query Attention (SQA), a novel attention architecture that pursues an alternative and complementary optimization path. Instead of reducing Key/Value heads, SQA reduces the number of Query heads. This architectural modification directly decreases the computational complexity of the attention mechanism by a factor proportional to the reduction in query heads, thereby lowering the overall FLOPs. This work presents the theoretical foundation of SQA, its mathematical formulation, and a family of architectural variants. Empirical benchmarks on long sequences (32k-200k tokens) demonstrate that SQA can achieve significant throughput improvements of up to 3x in computation-bound scenarios such as model pre-training, fine-tuning, and encoder-based tasks, with only a minimal impact on model quality in preliminary smallscale experiments. SQA was discovered serendipitously during the development of the upcoming Reactive Transformer architecture, suggesting its potential as a powerful tool for building more efficient and scalable models"
    },
    {
      "paperId": "f2a1ad4a2f8b5b576d06b8c6f336069cb407ff94",
      "externalIds": {
        "ArXiv": "2510.01958",
        "DBLP": "journals/corr/abs-2510-01958",
        "DOI": "10.48550/arXiv.2510.01958",
        "CorpusId": 281724315
      },
      "corpusId": 281724315,
      "title": "Exploring Resolution-Wise Shared Attention in Hybrid Mamba-U-Nets for Improved Cross-Corpus Speech Enhancement",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.01958, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2346121150",
          "name": "Nikolai Lund K\u00fchne"
        },
        {
          "authorId": "2188481370",
          "name": "Jesper Jensen"
        },
        {
          "authorId": "2253516770",
          "name": "Jan \u00d8stergaard"
        },
        {
          "authorId": "2256336386",
          "name": "Zheng-Hua Tan"
        }
      ],
      "abstract": "Recent advances in speech enhancement have shown that models combining Mamba and attention mechanisms yield superior cross-corpus generalization performance. At the same time, integrating Mamba in a U-Net structure has yielded state-of-the-art enhancement performance, while reducing both model size and computational complexity. Inspired by these insights, we propose RWSA-MambaUNet, a novel and efficient hybrid model combining Mamba and multi-head attention in a U-Net structure for improved cross-corpus performance. Resolution-wise shared attention (RWSA) refers to layerwise attention-sharing across corresponding time- and frequency resolutions. Our best-performing RWSA-MambaUNet model achieves state-of-the-art generalization performance on two out-of-domain test sets. Notably, our smallest model surpasses all baselines on the out-of-domain DNS 2020 test set in terms of PESQ, SSNR, and ESTOI, and on the out-of-domain EARS-WHAM_v2 test set in terms of SSNR, ESTOI, and SI-SDR, while using less than half the model parameters and a fraction of the FLOPs."
    },
    {
      "paperId": "d15b9ae68804e6c8e74262ed57e06a5d432722ad",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-01784",
        "ArXiv": "2510.01784",
        "DOI": "10.48550/arXiv.2510.01784",
        "CorpusId": 281725162
      },
      "corpusId": 281725162,
      "title": "Pack and Force Your Memory: Long-form and Consistent Video Generation",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.01784, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2284947827",
          "name": "Xiaofei Wu"
        },
        {
          "authorId": "2383421148",
          "name": "Guozhen Zhang"
        },
        {
          "authorId": "2344242774",
          "name": "Zhi-Ting Xu"
        },
        {
          "authorId": "2359525077",
          "name": "Yuan Zhou"
        },
        {
          "authorId": "2333353148",
          "name": "Qinglin Lu"
        },
        {
          "authorId": "2284935055",
          "name": "Xuming He"
        }
      ],
      "abstract": "Long-form video generation presents a dual challenge: models must capture long-range dependencies while preventing the error accumulation inherent in autoregressive decoding. To address these challenges, we make two contributions. First, for dynamic context modeling, we propose MemoryPack, a learnable context-retrieval mechanism that leverages both textual and image information as global guidance to jointly model short- and long-term dependencies, achieving minute-level temporal consistency. This design scales gracefully with video length, preserves computational efficiency, and maintains linear complexity. Second, to mitigate error accumulation, we introduce Direct Forcing, an efficient single-step approximating strategy that improves training-inference alignment and thereby curtails error propagation during inference. Together, MemoryPack and Direct Forcing substantially enhance the context consistency and reliability of long-form video generation, advancing the practical usability of autoregressive video models."
    },
    {
      "paperId": "98b6079999d934b231005c13fca574cd908dcfa6",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-02228",
        "ArXiv": "2510.02228",
        "DOI": "10.48550/arXiv.2510.02228",
        "CorpusId": 281724086
      },
      "corpusId": 281724086,
      "title": "xLSTM Scaling Laws: Competitive Performance with Linear Time-Complexity",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.02228, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2300175482",
          "name": "Maximilian Beck"
        },
        {
          "authorId": "2139329419",
          "name": "Kajetan Schweighofer"
        },
        {
          "authorId": "2364053917",
          "name": "Sebastian Bock"
        },
        {
          "authorId": "2268320216",
          "name": "Sebastian Lehner"
        },
        {
          "authorId": "3308557",
          "name": "Sepp Hochreiter"
        }
      ],
      "abstract": "Scaling laws play a central role in the success of Large Language Models (LLMs), enabling the prediction of model performance relative to compute budgets prior to training. While Transformers have been the dominant architecture, recent alternatives such as xLSTM offer linear complexity with respect to context length while remaining competitive in the billion-parameter regime. We conduct a comparative investigation on the scaling behavior of Transformers and xLSTM along the following lines, providing insights to guide future model design and deployment. First, we study the scaling behavior for xLSTM in compute-optimal and over-training regimes using both IsoFLOP and parametric fit approaches on a wide range of model sizes (80M-7B) and number of training tokens (2B-2T). Second, we examine the dependence of optimal model sizes on context length, a pivotal aspect that was largely ignored in previous work. Finally, we analyze inference-time scaling characteristics. Our findings reveal that in typical LLM training and inference scenarios, xLSTM scales favorably compared to Transformers. Importantly, xLSTM's advantage widens as training and inference contexts grow."
    },
    {
      "paperId": "8216b4c0d7209a3f2579b23e123392a04164cb79",
      "externalIds": {
        "PubMedCentral": "12529822",
        "DOI": "10.3389/fvets.2025.1674842",
        "CorpusId": 281758655,
        "PubMed": "41112158"
      },
      "corpusId": 281758655,
      "title": "S_TransNeXtM: a pig behavior recognition model based on the TransNeXtM and the sLSTM",
      "venue": "Frontiers in Veterinary Science",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12529822, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2720733",
          "name": "Wangli Hao"
        },
        {
          "authorId": "2353445522",
          "name": "Xinyuan Hu"
        },
        {
          "authorId": "2384423370",
          "name": "Yakui Xue"
        },
        {
          "authorId": "2326098690",
          "name": "Hao Shu"
        },
        {
          "authorId": "2108345659",
          "name": "Meng Han"
        }
      ],
      "abstract": "Pig behavior recognition serves as a crucial indicator for monitoring health and environmental conditions. However, conventional pig behavior recognition methods are limited in their ability to effectively extract image features and analyze long sequence dependencies, ultimately reducing pig behavior recognition performance. To address these challenges, we proposes a pig behavior recognition model S_TransNeXtM which leverages both spatial and temporal information underlying the video. Specifically, an innovative backbone, named TransNeXtM, has been developed for the spatial domain. It incorporates a bio-inspired Aggregated Attention Mechanism, a Convolutional GLU, and a Mamba unit, which allows the model to capture more discriminative global and local features. For the temporal domain, the sLSTM is proposed to process sequence data by utilizing an exponential gating mechanism and a stabilizer state. This design allows the model to establish longer temporal sequence dependencies, outperforming conventional GRU and LSTM. Based on the above insights, the S_TransNeXtM enhances the performance of pig behavior recognition. Experimental results demonstrate that the proposed S_TransNeXtM model achieves the state-of-the-art performance in pig behavior recognition task. Consequently, the S_TransNeXtM attains an accuracy of 94.53%, marking an improvement of up to 11.32% over previous benchmarks."
    },
    {
      "paperId": "ca0ec8444f98a954f99a34ab61198e25447bf9f2",
      "externalIds": {
        "PubMedCentral": "12529553",
        "DOI": "10.3389/fnbot.2025.1676787",
        "CorpusId": 281802842,
        "PubMed": "41113434"
      },
      "corpusId": 281802842,
      "title": "DWMamba: a structure-aware adaptive state space network for image quality improvement",
      "venue": "Frontiers in Neurorobotics",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12529553, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2349959673",
          "name": "Wenjun Fu"
        },
        {
          "authorId": "2383506935",
          "name": "Xiaobin Wang"
        },
        {
          "authorId": "2383660733",
          "name": "Chuncai Yang"
        },
        {
          "authorId": "2384128643",
          "name": "Liang Zhang"
        },
        {
          "authorId": "2298981677",
          "name": "Lin Feng"
        },
        {
          "authorId": "2358601485",
          "name": "Zhixiong Huang"
        }
      ],
      "abstract": "Overcoming visual degradation in challenging imaging scenarios is essential for accurate scene understanding. Although deep learning methods have integrated various perceptual capabilities and achieved remarkable progress, their high computational cost limits practical deployment under resource-constrained conditions. Moreover, when confronted with diverse degradation types, existing methods often fail to effectively model the inconsistent attenuation across color channels and spatial regions. To tackle these challenges, we propose DWMamba, a degradation-aware and weight-efficient Mamba network for image quality enhancement. Specifically, DWMamba introduces an Adaptive State Space Module (ASSM) that employs a dual-stream channel monitoring mechanism and a soft fusion strategy to capture global dependencies. With linear computational complexity, ASSM strengthens the models ability to address non-uniform degradations. In addition, by leveraging explicit edge priors and region partitioning as guidance, we design a Structure-guided Residual Fusion (SGRF) module to selectively fuse shallow and deep features, thereby restoring degraded details and enhancing low-light textures. Extensive experiments demonstrate that the proposed network delivers superior qualitative and quantitative performance, with strong generalization to diverse extreme lighting conditions. The code is available at https://github.com/WindySprint/DWMamba."
    },
    {
      "paperId": "5fa830a6c124b6c4fff4e7eb788f23cfd1588f08",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-03358",
        "ArXiv": "2510.03358",
        "DOI": "10.48550/arXiv.2510.03358",
        "CorpusId": 281843126
      },
      "corpusId": 281843126,
      "title": "Understanding Transformers for Time Series: Rank Structure, Flow-of-ranks, and Compressibility",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.03358, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2253472377",
          "name": "Annan Yu"
        },
        {
          "authorId": "40792304",
          "name": "Danielle C. Maddix"
        },
        {
          "authorId": "2313525582",
          "name": "Boran Han"
        },
        {
          "authorId": "2290975083",
          "name": "Xiyuan Zhang"
        },
        {
          "authorId": "2312210024",
          "name": "Abdul Fatir Ansari"
        },
        {
          "authorId": "2290914481",
          "name": "Oleksandr Shchur"
        },
        {
          "authorId": "2321723867",
          "name": "Christos Faloutsos"
        },
        {
          "authorId": "2290971024",
          "name": "Andrew Gordon Wilson"
        },
        {
          "authorId": "2290911111",
          "name": "Michael W. Mahoney"
        },
        {
          "authorId": "2305114246",
          "name": "Yuyang Wang"
        }
      ],
      "abstract": "Transformers are widely used across data modalities, and yet the principles distilled from text models often transfer imperfectly to models trained to other modalities. In this paper, we analyze Transformers through the lens of rank structure. Our focus is on the time series setting, where the structural properties of the data differ remarkably from those of text or vision. We show that time-series embeddings, unlike text or vision, exhibit sharply decaying singular value spectra: small patch sizes and smooth continuous mappings concentrate the data into low-rank subspaces. From this, we prove that the associated $Q/K/V$ projections admit accurate low-rank approximations, and that attention layers become compressible in proportion to the decay of the embedding spectrum. We introduce the concept of flow-of-ranks, a phenomenon by which nonlinear mixing across depth inflates the rank, explaining why early layers are most amenable to compression and why ranks grow with depth. Guided by these theoretical and empirical results, we use these insights to compress Chronos, a large time series foundation model, achieving a reduction of $65\\%$ in inference time and $81\\%$ in memory, without loss of accuracy. Our findings provide principled guidance for allocating width, depth, and heads in time series foundation models, and for exploiting their inherent compressibility."
    },
    {
      "paperId": "47408c178e067fa1373dd76ea3e2f2a78711b67b",
      "externalIds": {
        "DBLP": "journals/eswa/LianWHWZ26",
        "DOI": "10.1016/j.eswa.2025.129937",
        "CorpusId": 282036429
      },
      "corpusId": 282036429,
      "title": "AirMamba: A deep learning framework for long-term PM2.5 forecasting integrating multi-scale correlations and time-frequency dynamics",
      "venue": "Expert systems with applications",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.eswa.2025.129937?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.eswa.2025.129937, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2279064934",
          "name": "Jie Lian"
        },
        {
          "authorId": "2377899850",
          "name": "Xiao Wang"
        },
        {
          "authorId": "2300331849",
          "name": "Sirong Huang"
        },
        {
          "authorId": "2221234359",
          "name": "Dong Wang"
        },
        {
          "authorId": "2279147247",
          "name": "Qin Zhao"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "94c77d3ca28da60cfda91628b895fa8528f74c88",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-00862",
        "ArXiv": "2510.00862",
        "DOI": "10.48550/arXiv.2510.00862",
        "CorpusId": 281705891
      },
      "corpusId": 281705891,
      "title": "Gather-Scatter Mamba: Accelerating Propagation with Efficient State Space Model",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.00862, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2335566831",
          "name": "Hyun-kyu Ko"
        },
        {
          "authorId": "2334438357",
          "name": "Youbin Kim"
        },
        {
          "authorId": "2332470699",
          "name": "Jihyeon Park"
        },
        {
          "authorId": "2335627733",
          "name": "Dongheok Park"
        },
        {
          "authorId": "2295666507",
          "name": "Gyeongjin Kang"
        },
        {
          "authorId": "2383179495",
          "name": "Wonjun Cho"
        },
        {
          "authorId": "2383945777",
          "name": "Hyung Yi"
        },
        {
          "authorId": "2334533279",
          "name": "Eunbyung Park"
        }
      ],
      "abstract": "State Space Models (SSMs)-most notably RNNs-have historically played a central role in sequential modeling. Although attention mechanisms such as Transformers have since dominated due to their ability to model global context, their quadratic complexity and limited scalability make them less suited for long sequences. Video super-resolution (VSR) methods have traditionally relied on recurrent architectures to propagate features across frames. However, such approaches suffer from well-known issues including vanishing gradients, lack of parallelism, and slow inference speed. Recent advances in selective SSMs like Mamba offer a compelling alternative: by enabling input-dependent state transitions with linear-time complexity, Mamba mitigates these issues while maintaining strong long-range modeling capabilities. Despite this potential, Mamba alone struggles to capture fine-grained spatial dependencies due to its causal nature and lack of explicit context aggregation. To address this, we propose a hybrid architecture that combines shifted window self-attention for spatial context aggregation with Mamba-based selective scanning for efficient temporal propagation. Furthermore, we introduce Gather-Scatter Mamba (GSM), an alignment-aware mechanism that warps features toward a center anchor frame within the temporal window before Mamba propagation and scatters them back afterward, effectively reducing occlusion artifacts and ensuring effective redistribution of aggregated information across all frames. The official implementation is provided at: https://github.com/Ko-Lani/GSMamba."
    },
    {
      "paperId": "4b841ae7bd4db40437c27d765638685410f97c33",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-00399",
        "ArXiv": "2510.00399",
        "DOI": "10.48550/arXiv.2510.00399",
        "CorpusId": 281706292
      },
      "corpusId": 281706292,
      "title": "Can Mamba Learn In Context with Outliers? A Theoretical Generalization Analysis",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.00399, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2162633775",
          "name": "Hongkang Li"
        },
        {
          "authorId": "2261696143",
          "name": "Songtao Lu"
        },
        {
          "authorId": "2287879528",
          "name": "Xiaodong Cui"
        },
        {
          "authorId": "2257357506",
          "name": "Pin-Yu Chen"
        },
        {
          "authorId": "2261906420",
          "name": "Meng Wang"
        }
      ],
      "abstract": "The Mamba model has gained significant attention for its computational advantages over Transformer-based models, while achieving comparable performance across a wide range of language tasks. Like Transformers, Mamba exhibits in-context learning (ICL) capabilities, i.e., making predictions for new tasks based on a prompt containing input-label pairs and a query, without requiring fine-tuning. Despite its empirical success, the theoretical understanding of Mamba remains limited, largely due to the nonlinearity introduced by its gating mechanism. To the best of our knowledge, this paper presents the first theoretical analysis of the training dynamics of a one-layer Mamba model, which consists of a linear attention component followed by a nonlinear gating layer, and its ICL generalization on unseen binary classification tasks, even when the prompt includes additive outliers. Our analysis shows that Mamba leverages the linear attention layer to select informative context examples and uses the nonlinear gating layer to suppress the influence of outliers. By establishing and comparing to the analysis of linear Transformers under the same setting, we show that although Mamba may require more training iterations to converge, it maintains accurate predictions even when the proportion of outliers exceeds the threshold that a linear Transformer can tolerate. These theoretical findings are supported by empirical experiments."
    },
    {
      "paperId": "23431021b79be28bca1ce290b50b91b0dd9b7b37",
      "externalIds": {
        "DBLP": "journals/tcsv/YangXYFHZ25",
        "DOI": "10.1109/TCSVT.2025.3563411",
        "CorpusId": 278022695
      },
      "corpusId": 278022695,
      "title": "Vivim: A Video Vision Mamba for Ultrasound Video Segmentation",
      "venue": "IEEE transactions on circuits and systems for video technology (Print)",
      "year": 2025,
      "citationCount": 8,
      "influentialCitationCount": 1,
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2025.3563411?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2025.3563411, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2237943083",
          "name": "Yijun Yang"
        },
        {
          "authorId": "153107262",
          "name": "Zhaohu Xing"
        },
        {
          "authorId": "2274739977",
          "name": "Lequan Yu"
        },
        {
          "authorId": "2238561481",
          "name": "Huazhu Fu"
        },
        {
          "authorId": "1455822840",
          "name": "Chunwang Huang"
        },
        {
          "authorId": "2280201265",
          "name": "Lei Zhu"
        }
      ],
      "abstract": "Ultrasound video segmentation gains increasing attention in clinical practice due to the redundant dynamic references in video frames. However, traditional convolutional neural networks have a limited receptive field and transformer-based networks are unsatisfactory in constructing long-term dependency from the perspective of computational complexity. This bottleneck poses a significant challenge when processing longer sequences in medical video analysis tasks using available devices with limited memory. Recently, state space models (SSMs), famous by Mamba, have exhibited linear complexity and impressive achievements in efficient long sequence modeling, which have developed deep neural networks by expanding the receptive field on many vision tasks significantly. Unfortunately, vanilla SSMs failed to simultaneously capture causal temporal cues and preserve non-casual spatial information. To this end, this paper presents a Video Vision Mamba-based framework, dubbed as Vivim, for ultrasound video segmentation tasks. Our Vivim can effectively compress the long-term spatiotemporal representation into sequences at varying scales with our designed Temporal Mamba Block. We also introduce an improved boundary-aware affine constraint across frames to enhance the discriminative ability of Vivim on ambiguous lesions. Extensive experiments on thyroid segmentation in ultrasound videos, breast lesion segmentation in ultrasound videos, and polyp segmentation in colonoscopy videos demonstrate the effectiveness and efficiency of our Vivim, superior to existing methods. The code and dataset are available at: https://github.com/scott-yjyang/Vivim"
    },
    {
      "paperId": "f4786940841de5de374e30533092ad269b6f17ab",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-00563",
        "ArXiv": "2510.00563",
        "DOI": "10.48550/arXiv.2510.00563",
        "CorpusId": 281705588
      },
      "corpusId": 281705588,
      "title": "Memory Determines Learning Direction: A Theory of Gradient-Based Optimization in State Space Models",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.00563, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2319780257",
          "name": "JingChuan Guan"
        },
        {
          "authorId": "151400836",
          "name": "T. Kubota"
        },
        {
          "authorId": "2067238428",
          "name": "Yasuo Kuniyoshi"
        },
        {
          "authorId": "2256994524",
          "name": "Kohei Nakajima"
        }
      ],
      "abstract": "State space models (SSMs) have gained attention by showing potential to outperform Transformers. However, previous studies have not sufficiently addressed the mechanisms underlying their high performance owing to a lack of theoretical explanation of SSMs'learning dynamics. In this study, we provide such an explanation and propose an improved training strategy. The memory capacity of SSMs can be evaluated by examining how input time series are stored in their current state. Such an examination reveals a tradeoff between memory accuracy and length, as well as the theoretical equivalence between the structured state space sequence model (S4) and a simplified S4 with diagonal recurrent weights. This theoretical foundation allows us to elucidate the learning dynamics, proving the importance of initial parameters. Our analytical results suggest that successful learning requires the initial memory structure to be the longest possible even if memory accuracy may deteriorate or the gradient lose the teacher information. Experiments on tasks requiring long memory confirmed that extending memory is difficult, emphasizing the importance of initialization. Furthermore, we found that fixing recurrent weights can be more advantageous than adapting them because it achieves comparable or even higher performance with faster convergence. Our results provide a new theoretical foundation for SSMs and potentially offer a novel optimization strategy."
    },
    {
      "paperId": "a583dae408e1cf4688b9e933df4dfca0f04e4c10",
      "externalIds": {
        "ArXiv": "2510.01450",
        "DBLP": "journals/corr/abs-2510-01450",
        "DOI": "10.48550/arXiv.2510.01450",
        "CorpusId": 281724342
      },
      "corpusId": 281724342,
      "title": "Local Linear Attention: An Optimal Interpolation of Linear and Softmax Attention For Test-Time Regression",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.01450, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2343503850",
          "name": "Yifei Zuo"
        },
        {
          "authorId": "2342470727",
          "name": "Yutong Yin"
        },
        {
          "authorId": "2387215134",
          "name": "Zhichen Zeng"
        },
        {
          "authorId": "2383419405",
          "name": "Ang Li"
        },
        {
          "authorId": "2386950633",
          "name": "Banghua Zhu"
        },
        {
          "authorId": "2384270366",
          "name": "Zhaoran Wang"
        }
      ],
      "abstract": "Transformer architectures have achieved remarkable success in various domains. While efficient alternatives to Softmax Attention have been widely studied, the search for more expressive mechanisms grounded in theoretical insight-even at greater computational cost-has been relatively underexplored. In this work, we bridge this gap by proposing Local Linear Attention (LLA), a novel attention mechanism derived from nonparametric statistics through the lens of test-time regression. First, we show that LLA offers theoretical advantages over Linear and Softmax Attention for associative memory via a bias-variance trade-off analysis. Next, we address its computational challenges and propose two memory-efficient primitives to tackle the $\\Theta(n^2 d)$ and $\\Theta(n d^2)$ complexity. We then introduce FlashLLA, a hardware-efficient, blockwise algorithm that enables scalable and parallel computation on modern accelerators. In addition, we implement and profile a customized inference kernel that significantly reduces memory overheads. Finally, we empirically validate the advantages and limitations of LLA on test-time regression, in-context regression, associative recall and state tracking tasks. Experiment results demonstrate that LLA effectively adapts to non-stationarity, outperforming strong baselines in test-time training and in-context learning, and exhibiting promising evidence for its scalability and applicability in large-scale models. Code is available at https://github.com/Yifei-Zuo/Flash-LLA."
    },
    {
      "paperId": "6253d33912b098f22e7ba60c79e62fc0b4895bbc",
      "externalIds": {
        "DBLP": "journals/tcss/LiCLZZGMT25",
        "DOI": "10.1109/TCSS.2024.3509399",
        "CorpusId": 274749404
      },
      "corpusId": 274749404,
      "title": "TDG-Mamba: Advanced Spatiotemporal Embedding for Temporal Dynamic Graph Learning via Bidirectional Information Propagation",
      "venue": "IEEE Transactions on Computational Social Systems",
      "year": 2025,
      "citationCount": 4,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSS.2024.3509399?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSS.2024.3509399, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2282451016",
          "name": "Mengran Li"
        },
        {
          "authorId": "2282087105",
          "name": "Junzhou Chen"
        },
        {
          "authorId": "2283469847",
          "name": "Bo Li"
        },
        {
          "authorId": "2144290813",
          "name": "Yong Zhang"
        },
        {
          "authorId": "2152042536",
          "name": "Ronghui Zhang"
        },
        {
          "authorId": "2290532810",
          "name": "Siyuan Gong"
        },
        {
          "authorId": "2166690392",
          "name": "Xiaolei Ma"
        },
        {
          "authorId": "2069521369",
          "name": "Z. Tian"
        }
      ],
      "abstract": "Temporal dynamic graphs (TDGs), representing the dynamic evolution of entities and their relationships over time with intricate temporal features, are widely used in various real-world domains. Existing methods typically rely on mainstream techniques such as transformers and graph neural networks (GNNs) to capture the spatiotemporal information of TDGs. However, despite their advanced capabilities, these methods often struggle with significant computational complexity and limited ability to capture temporal dynamic contextual relationships. Recently, a new model architecture called mamba has emerged, noted for its capability to capture complex dependencies in sequences while significantly reducing computational complexity. Building on this, we propose a novel method, TDG-mamba, which integrates mamba for TDG learning. TDG-mamba introduces deep semantic spatiotemporal embeddings into the mamba architecture through a specially designed spatiotemporal prior tokenization module (SPTM). Furthermore, to better leverage temporal information differences and enhance the modeling of dynamic changes in graph structures, we separately design a bidirectional mamba and a directed GNN for improved spatiotemporal embedding learning. Link prediction experiments on multiple public datasets demonstrate that our method delivers superior performance, with an average improvement of 5.11% over baseline methods across various settings."
    },
    {
      "paperId": "e36994f6b83f161a2cb2073721f123f2f278eb62",
      "externalIds": {
        "DBLP": "journals/eaai/HuangZZH25",
        "DOI": "10.1016/j.engappai.2025.111477",
        "CorpusId": 279418684
      },
      "corpusId": 279418684,
      "title": "Spatial-temporal modeling for multi-scale wind speed predictions in rail transit networks: A graph neural network-based methodology",
      "venue": "Engineering applications of artificial intelligence",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.engappai.2025.111477?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.engappai.2025.111477, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2238888775",
          "name": "Yujie Huang"
        },
        {
          "authorId": "2238914102",
          "name": "Zhipeng Zhang"
        },
        {
          "authorId": "2239033828",
          "name": "Wenqiang Zhao"
        },
        {
          "authorId": "2118879162",
          "name": "Hao Hu"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "20b9f6afcebebce4ee95aa0b218ca21fb45f2052",
      "externalIds": {
        "DOI": "10.1016/j.engappai.2025.111503",
        "CorpusId": 279707840
      },
      "corpusId": 279707840,
      "title": "A lightweight defect detection network for titanium strip via efficient convolution and feature-driven mechanism",
      "venue": "Engineering applications of artificial intelligence",
      "year": 2025,
      "citationCount": 3,
      "influentialCitationCount": 1,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.engappai.2025.111503?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.engappai.2025.111503, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2369593332",
          "name": "Yang Zhu"
        },
        {
          "authorId": "2364337673",
          "name": "Yong-Cheng Lin"
        },
        {
          "authorId": "2112782719",
          "name": "Xiangyu Tan"
        },
        {
          "authorId": "2344209725",
          "name": "Shu-Xin Li"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "48a6b0b6517174adc7fb5aad48ee86db15c951e5",
      "externalIds": {
        "DBLP": "journals/cee/CaoBHS25",
        "DOI": "10.1016/j.compeleceng.2025.110558",
        "CorpusId": 280485431
      },
      "corpusId": 280485431,
      "title": "Traffic speed prediction network based on multi-view spatio-temporal graph convolution network",
      "venue": "Computers & electrical engineering",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.compeleceng.2025.110558?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.compeleceng.2025.110558, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2290442585",
          "name": "Chenyang Cao"
        },
        {
          "authorId": "1752765248",
          "name": "Yinxin Bao"
        },
        {
          "authorId": "2367787766",
          "name": "Yingyan Hou"
        },
        {
          "authorId": "2289612698",
          "name": "Quan Shi"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "50d4408f7f05a106afee87e4fa8564dd64c8b0d7",
      "externalIds": {
        "DBLP": "journals/iotj/TianZXHJ25",
        "DOI": "10.1109/JIOT.2025.3590470",
        "CorpusId": 280159239
      },
      "corpusId": 280159239,
      "title": "IoT Data Imputation Accuracy Enhancement: A Spatiotemporal Causal Mamba-Diffusion Imputation Model",
      "venue": "IEEE Internet of Things Journal",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JIOT.2025.3590470?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JIOT.2025.3590470, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2323002765",
          "name": "Xinying Tian"
        },
        {
          "authorId": "2185427378",
          "name": "Lei Zhao"
        },
        {
          "authorId": "144394896",
          "name": "Jun Xiong"
        },
        {
          "authorId": "2150463029",
          "name": "Xin Hao"
        },
        {
          "authorId": "2117807613",
          "name": "Yuan Jiang"
        }
      ],
      "abstract": "The accuracy of time-series data from Internet of Things (IoT) sensors plays a crucial role in contemporary intelligent transportation systems (ITSs). However, in the process of IoT data collection, the frequent occurrence of missing data poses significant challenges for data accuracy, thereby limiting the service-provisioning of ITS. To solve this issue, this article proposes a deep-learning-based data imputation model, namely, spatiotemporal causal Mamba-diffusion imputation (SCMDI) model, which effectively models complicated nonlinear dependencies and captures the implicit causal relationships inherent in traffic patterns. Technically, the innovative SCMDI model integrates the diffusion model with the Mamba module, enhancing the ability to capture complex dynamic interactions across space and time. In SCMDI, we first propose a Mamba-attention dual module (MADM) to further improve the joint modeling of spatiotemporal features. Additionally, SCMDI incorporates spatiotemporal causal prior knowledge, which not only improves imputation accuracy by capturing intrinsic relationships in IoT data but also effectively mitigates false correlations. Extensive experiments on three publicly available traffic datasets demonstrate that our SCMDI model outperforms state-of-the-art methods, achieving a 10% improvement in data accuracy, and proving its efficiency in handling complex spatiotemporal data in IoT data imputation. The ablation experimental results further validate the effectiveness of causal prior and MADM."
    },
    {
      "paperId": "26c9783b04577d4350ecb8f6eb141cd7b4651adf",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-00636",
        "ArXiv": "2510.00636",
        "DOI": "10.48550/arXiv.2510.00636",
        "CorpusId": 281705786
      },
      "corpusId": 281705786,
      "title": "Expected Attention: KV Cache Compression by Estimating Attention from Future Queries Distribution",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.00636, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2172309361",
          "name": "Alessio Devoto"
        },
        {
          "authorId": "2383162754",
          "name": "Maximilian Jeblick"
        },
        {
          "authorId": "2383162370",
          "name": "Simon J'egou"
        }
      ],
      "abstract": "Memory consumption of the Key-Value (KV) cache represents a major bottleneck for efficient large language model inference. While attention-score-based KV cache pruning shows promise, it faces critical practical limitations: attention scores from future tokens are unavailable during compression, and modern implementations like Flash Attention do not materialize the full attention matrix, making past scores inaccessible. To overcome these challenges, we introduce $\\textbf{Expected Attention, a training-free compression method}$ that estimates KV pairs importance by predicting how future queries will attend to them. Our approach leverages the distributional properties of LLM activations to compute expected attention scores in closed form for each KV pair. These scores enable principled ranking and pruning of KV pairs with minimal impact on the residual stream, achieving effective compression without performance degradation. Importantly, our method operates seamlessly across both prefilling and decoding phases, consistently outperforming state-of-the-art baselines in both scenarios. Finally, $\\textbf{we release KVPress, a comprehensive library to enable researchers to implement and benchmark KV cache compression methods, already including more than 20 techniques}$."
    },
    {
      "paperId": "daab8977d0d2a0083f62423d984ba5abef5bffdb",
      "externalIds": {
        "ArXiv": "2510.01030",
        "DBLP": "journals/corr/abs-2510-01030",
        "DOI": "10.48550/arXiv.2510.01030",
        "CorpusId": 281706423
      },
      "corpusId": 281706423,
      "title": "Uncovering the Computational Ingredients of Human-Like Representations in LLMs",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.01030, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2308100885",
          "name": "Zach Studdiford"
        },
        {
          "authorId": "2298886839",
          "name": "Timothy T. Rogers"
        },
        {
          "authorId": "1382262357",
          "name": "Kushin Mukherjee"
        },
        {
          "authorId": "1596903751",
          "name": "Siddharth Suresh"
        }
      ],
      "abstract": "The ability to translate diverse patterns of inputs into structured patterns of behavior has been thought to rest on both humans'and machines'ability to learn robust representations of relevant concepts. The rapid advancement of transformer-based large language models (LLMs) has led to a diversity of computational ingredients -- architectures, fine tuning methods, and training datasets among others -- but it remains unclear which of these ingredients are most crucial for building models that develop human-like representations. Further, most current LLM benchmarks are not suited to measuring representational alignment between humans and models, making benchmark scores unreliable for assessing if current LLMs are making progress towards becoming useful cognitive models. We address these limitations by first evaluating a set of over 70 models that widely vary in their computational ingredients on a triplet similarity task, a method well established in the cognitive sciences for measuring human conceptual representations, using concepts from the THINGS database. Comparing human and model representations, we find that models that undergo instruction-finetuning and which have larger dimensionality of attention heads are among the most human aligned, while multimodal pretraining and parameter size have limited bearing on alignment. Correlations between alignment scores and scores on existing benchmarks reveal that while some benchmarks (e.g., MMLU) are better suited than others (e.g., MUSR) for capturing representational alignment, no existing benchmark is capable of fully accounting for the variance of alignment scores, demonstrating their insufficiency in capturing human-AI alignment. Taken together, our findings help highlight the computational ingredients most essential for advancing LLMs towards models of human conceptual representation and address a key benchmarking gap in LLM evaluation."
    },
    {
      "paperId": "f2b9dfc71361fbd05f751b5b95651d9d149e2cd0",
      "externalIds": {
        "PubMedCentral": "12486570",
        "DOI": "10.1186/s12967-025-07021-0",
        "CorpusId": 281720791,
        "PubMed": "41034915"
      },
      "corpusId": 281720791,
      "title": "Joint prediction of glioma molecular marker status based on GDI-PMNet",
      "venue": "Journal of Translational Medicine",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBYNCND",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12486570, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2298563397",
          "name": "Hong Zhu"
        },
        {
          "authorId": "2298483933",
          "name": "Fengning Liang"
        },
        {
          "authorId": "2298947647",
          "name": "Teng Zhao"
        },
        {
          "authorId": "2298652240",
          "name": "Yaru Cao"
        },
        {
          "authorId": "2383407342",
          "name": "Ying Chen"
        },
        {
          "authorId": "2383398042",
          "name": "Houru Yan"
        },
        {
          "authorId": "2387890411",
          "name": "Xiang Xiao"
        }
      ],
      "abstract": "Determining the status of glioma molecular markers is a problem of clinical importance in medicine. Current medical-imaging-based approaches for this problem suffer from various limitations, such as incomplete fine-grained feature extraction of glioma imaging data and low prediction accuracy of molecular marker status. To address these issues, a deep learning method is presented for the simultaneous joint prediction of multi-label statuses of glioma molecular markers. Firstly, a Gradient-aware Spatially Partitioned Enhancement algorithm (GASPE) is proposed to optimize the glioma MR image preprocessing method and to enhance the local detail expression ability; secondly, a Dual Attention module with Depthwise Convolution (DADC) is constructed to improve the fine-grained feature extraction ability by combining channel attention and spatial attention; thirdly, a hybrid model PMNet is proposed, which combines the Pyramid-based Multi-Scale Feature Extraction module (PMSFEM) and the Mamba-based Projection Convolution module (MPCM) to achieve effective fusion of local and global information; finally, an Iterative Truth Calibration algorithm (ITC) is used to calibrate the joint state truth vector output by the model to optimize the accuracy of the prediction results. Based on GASPE, DADC, ITC and PMNet, the proposed method constructs the Gradient-Aware Dual Attention Iteration Truth Calibration-PMNet (GDI-PMNet) to simultaneously predict the status of glioma molecular markers (IDH1, Ki67, MGMT, P53), with accuracies of 98.31%, 99.24%, 97.96% and 98.54% respectively, achieving non-invasive preoperative prediction, thereby capable of assisting doctors in clinical diagnosis and treatment. The GDI-PMNet method demonstrates high accuracy in predicting glioma molecular markers, addressing the limitations of current approaches by enhancing fine-grained feature extraction and prediction accuracy. This non-invasive preoperative prediction tool holds significant potential to assist clinicians in glioma diagnosis and treatment, ultimately improving patient outcomes."
    },
    {
      "paperId": "5f7a3d8f0ffd9ac88dc00fa3ceb9bfbe953315ea",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-01363",
        "ArXiv": "2510.01363",
        "DOI": "10.48550/arXiv.2510.01363",
        "CorpusId": 281724716
      },
      "corpusId": 281724716,
      "title": "Retrieval-Augmented Framework for LLM-Based Clinical Decision Support",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.01363, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2298970481",
          "name": "Leon Garza"
        },
        {
          "authorId": "2040711822",
          "name": "Anantaa Kotal"
        },
        {
          "authorId": "2304383905",
          "name": "Michael A Grasso"
        },
        {
          "authorId": "2382343455",
          "name": "Emre Umucu"
        }
      ],
      "abstract": "The increasing complexity of clinical decision-making, alongside the rapid expansion of electronic health records (EHR), presents both opportunities and challenges for delivering data-informed care. This paper proposes a clinical decision support system powered by Large Language Models (LLMs) to assist prescribing clinicians. The system generates therapeutic suggestions by analyzing historical EHR data, including patient demographics, presenting complaints, clinical symptoms, diagnostic information, and treatment histories. The framework integrates natural language processing with structured clinical inputs to produce contextually relevant recommendations. Rather than replacing clinician judgment, it is designed to augment decision-making by retrieving and synthesizing precedent cases with comparable characteristics, drawing on local datasets or federated sources where applicable. At its core, the system employs a retrieval-augmented generation (RAG) pipeline that harmonizes unstructured narratives and codified data to support LLM-based inference. We outline the system's technical components, including representation representation alignment and generation strategies. Preliminary evaluations, conducted with de-identified and synthetic clinical datasets, examine the clinical plausibility and consistency of the model's outputs. Early findings suggest that LLM-based tools may provide valuable decision support in prescribing workflows when appropriately constrained and rigorously validated. This work represents an initial step toward integration of generative AI into real-world clinical decision-making with an emphasis on transparency, safety, and alignment with established practices."
    },
    {
      "paperId": "780b0955597a27d4390aac80618014382d26e0ba",
      "externalIds": {
        "DBLP": "journals/tcsv/LiLXMHF25",
        "DOI": "10.1109/TCSVT.2025.3560615",
        "CorpusId": 277847267
      },
      "corpusId": 277847267,
      "title": "Semi-Mamba: Mamba-Driven Semi-Supervised Multimodal Remote Sensing Feature Classification",
      "venue": "IEEE transactions on circuits and systems for video technology (Print)",
      "year": 2025,
      "citationCount": 10,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "https://doi.org/10.1109/tcsvt.2025.3560615",
        "status": "HYBRID",
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2025.3560615?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2025.3560615, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2258339560",
          "name": "Yunsong Li"
        },
        {
          "authorId": "2236746262",
          "name": "Daixun Li"
        },
        {
          "authorId": "2260453116",
          "name": "Weiying Xie"
        },
        {
          "authorId": "2277243228",
          "name": "Jitao Ma"
        },
        {
          "authorId": "2355777377",
          "name": "Sibo He"
        },
        {
          "authorId": "2237129019",
          "name": "Leyuan Fang"
        }
      ],
      "abstract": "Mamba architecture achieves the same performance as attention mechanisms with linear complexity, leading to significant progress in remote sensing land cover classification. However, existing Mamba methods rarely leverage the representational complementarity and consistency between different modalities, resulting in challenges such as incomplete fusion. To address these issues, we propose Semi-Mamba, a novel semi-supervised framework specifically designed for high-dimensional multi-modal data fusion. We introduce the Mamba Cross-Modality Fusion Module, which enables cross-modal learning of temporal features through state-space model interactions and smooth integration of input matrices, enhancing the fusion of richer feature representations. Additionally, to tackle the inherent difficulty of acquiring pixel-level annotations in remote sensing datasets, we introduce a multi-modal semi-supervised mechanism. This mechanism utilizes cross-modal supervision between different modalities to maximize data utilization and improve learning efficiency. It effectively enables joint training on both labeled and unlabeled data without relying on pseudo-labels. We integrate these innovations into a unified end-to-end framework. Compared to state-of-the-art CNN and Transformer-based architectures, our framework shows a significant improvement of over 3.12%, setting a new benchmark for semi-supervised multi-modal data fusion. The code has open sourced at https://github.com/LDXDU/Semi_Mamba_RS."
    },
    {
      "paperId": "f574e4d23fafae81350a2ea0e7aec46c0bad2ecb",
      "externalIds": {
        "DBLP": "journals/tcsv/MaoTQX25",
        "DOI": "10.1109/TCSVT.2025.3570725",
        "CorpusId": 278707518
      },
      "corpusId": 278707518,
      "title": "CamStegNet: A Robust Image Steganography Method Based on Camouflage Model",
      "venue": "IEEE transactions on circuits and systems for video technology (Print)",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2025.3570725?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2025.3570725, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2361972753",
          "name": "Le Mao"
        },
        {
          "authorId": "1575998714",
          "name": "Yun Tan"
        },
        {
          "authorId": "143901420",
          "name": "Jiaohua Qin"
        },
        {
          "authorId": "2869309",
          "name": "Xuyu Xiang"
        }
      ],
      "abstract": "Deep learning models are increasingly being employed in steganographic schemes for the embedding and extraction of secret information. However, steganographic models themselves are also at risk of detection and attacks. Although there are approaches proposed to hide deep learning models, making these models difficult to detect while achieving high-quality image steganography performance remains a challenging task. In this work, a robust image steganography method based on a camouflage model CamStegNet is proposed. The steganographic model is camouflaged as a routine deep learning model to significantly enhance its concealment. A sparse weight-filling paradigm is designed to enable the model to be flexibly switched among three modes by utilizing different keys: routine machine learning task, secret embedding task and secret recovery task. Furthermore, a residual state-space module and a neighborhood attention mechanism are constructed to improve the performance of image steganography. Experiments conducted on the DIV2K, ImageNet and COCO datasets demonstrate that the stego images generated by CamStegNet are superior to existing methods in terms of visual quality. They also exhibit enhanced resistance to steganalysis and maintain over 95% robustness against noise and scale attacks. Additionally, the model demonstrates high robustness which can achieve excellent performance in machine learning tasks and maintain stability across various weight initialization methods."
    },
    {
      "paperId": "74a98ceeb1f2d6da5d4e55d5bf4ba02547c9d5fa",
      "externalIds": {
        "PubMedCentral": "12521135",
        "DOI": "10.3389/fnins.2025.1637079",
        "CorpusId": 281763131,
        "PubMed": "41103723"
      },
      "corpusId": 281763131,
      "title": "PI-MMNet: a cross-modal neural network for predicting neurological deterioration in pontine infarction",
      "venue": "Frontiers in Neuroscience",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12521135, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2307914204",
          "name": "Hui Jin"
        },
        {
          "authorId": "2336103470",
          "name": "Xiaona Xu"
        },
        {
          "authorId": "2337139492",
          "name": "Yichan Ye"
        },
        {
          "authorId": "2265647562",
          "name": "Xuhao Shan"
        },
        {
          "authorId": "2383858348",
          "name": "Cheng Yang"
        },
        {
          "authorId": "2383583790",
          "name": "E. Bao"
        },
        {
          "authorId": "2383489501",
          "name": "Min Li"
        },
        {
          "authorId": "2336256562",
          "name": "Weili Chen"
        },
        {
          "authorId": "2336266115",
          "name": "Xuerong Huang"
        },
        {
          "authorId": "2375084258",
          "name": "Jikui Liu"
        },
        {
          "authorId": "2383588445",
          "name": "Hao Kou"
        },
        {
          "authorId": "13424201",
          "name": "Ruyue Huang"
        }
      ],
      "abstract": "Introduction Pontine infarction, a subtype of ischemic stroke, often leads to neurological deterioration (ND). Current diagnostic methods rely mainly on imaging and neglect clinical data, while existing multimodal models struggle with small lesions, heterogeneous inputs, and high computational cost. Methods We propose PI-MMNet, a cross-modal neural network combining: (i) a Multi-modal Feature Processing module with Mamba-based extractors, (ii) a Dynamic Residual Fusion module for robust feature integration, and (iii) an Adaptive Graph module for efficient relational reasoning. A multi-loss strategy jointly optimizes alignment, graph consistency, and classification. Experiments used 386 pontine infarction cases with MRI and clinical data under 5-fold cross-validation. Results PI-MMNet outperformed state-of-the-art methods, improving accuracy by 1.03%, F1 by 0.0504, and AUC by 0.0343, while using only 146 parameters and 135 memory of the strongest baseline. Ablation and visualization confirmed the contributions of all modules. Discussion PI-MMNet provides an efficient and interpretable framework for predicting ND in pontine infarction and may generalize to other multimodal medical tasks. Our code is available at https://github.com/jinhui66/PI-MMNet."
    },
    {
      "paperId": "34e6ad8c6cf6fdbd748edcc785afb5c8fe5daa6c",
      "externalIds": {
        "DOI": "10.1016/j.pce.2025.104114",
        "CorpusId": 281763459
      },
      "corpusId": 281763459,
      "title": "Post-stack Seismic Impedance Inversion Based on Sparse-coded Mamba Seismic Model",
      "venue": "Physics and Chemistry of the Earth, Parts A/B/C",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.pce.2025.104114?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.pce.2025.104114, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2384018642",
          "name": "Yijian Lin"
        },
        {
          "authorId": "2296673562",
          "name": "Suping Peng"
        },
        {
          "authorId": "2296229104",
          "name": "Xiao-lei Cui"
        },
        {
          "authorId": "115633453",
          "name": "Yongxu Lu"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "1f33829848d45567ffc40d66cd9ffb22d5690cc0",
      "externalIds": {
        "DOI": "10.26554/sti.2025.10.4.1301-1311",
        "CorpusId": 281810209
      },
      "corpusId": 281810209,
      "title": "Dynamic Modeling of Energy Data: World Crude Oil and Coal Prices 2017-2023 (A State-Space Model Analysis of Multivariate Time Series)",
      "venue": "Science and Technology Indonesia",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.26554/sti.2025.10.4.1301-1311?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.26554/sti.2025.10.4.1301-1311, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "152556593",
          "name": "E. Russel"
        },
        {
          "authorId": "2338292344",
          "name": "Wamiliana"
        },
        {
          "authorId": "2143664008",
          "name": "M. Usman"
        },
        {
          "authorId": "121243672",
          "name": "F. Elfaki"
        },
        {
          "authorId": "2383626813",
          "name": "Arisman Adnan"
        },
        {
          "authorId": "2383773642",
          "name": "Lindrianasari"
        }
      ],
      "abstract": "The analysis of global crude oil and coal prices has attracted considerable research interest, as these prices significantly affect both society and industry, making the topic highly relevant for governments and policy makers. This study examines the correlation between global coal and crude oil prices from 2017 to 2023. It analyzes the behavior of these price series using a unit root test and develops an optimal model for conducting a Granger-causality analysis. To forecast crude oil and coal prices for the next 30 periods, a state-space modeling approach is applied. The unit root test results reveal that these prices are non-stationary, suggesting that any shocks to prices will have persistent effects. The best-fitting model for the association between coal and crude oil prices is a vector autoregressive model of order two (VAR(2)). The Granger-causality results reveal that current crude oil prices are influenced by both their own past values and previous coal prices, and vice versa. Forecasts using the state-space model suggest a modest upward trend for crude oil prices over the next 30 periods, while coal prices are projected to rise more strongly."
    },
    {
      "paperId": "cc58308ff3eaa8caffe54504d6f80d32118a02c0",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-02401",
        "ArXiv": "2510.02401",
        "DOI": "10.48550/arXiv.2510.02401",
        "CorpusId": 281829659
      },
      "corpusId": 281829659,
      "title": "Linear RNNs for autoregressive generation of long music samples",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.02401, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2383974759",
          "name": "Konrad Szewczyk"
        },
        {
          "authorId": "2383303513",
          "name": "Daniel Gallo Fern'andez"
        },
        {
          "authorId": "2383974953",
          "name": "James Townsend"
        }
      ],
      "abstract": "Directly learning to generate audio waveforms in an autoregressive manner is a challenging task, due to the length of the raw sequences and the existence of important structure on many different timescales. Traditional approaches based on recurrent neural networks, as well as causal convolutions and self-attention, have only had limited success on this task. However, recent work has shown that deep state space models, also referred to as linear RNNs, can be highly efficient in this context. In this work, we push the boundaries of linear RNNs applied to raw audio modeling, investigating the effects of different architectural choices and using context-parallelism to enable training on sequences up to one minute (1M tokens) in length. We present a model, HarmonicRNN, which attains state of the art log-likelihoods and perceptual metrics on small-scale datasets."
    },
    {
      "paperId": "4c4cd968070c9482eeb82b624a28bfb8e9a32d45",
      "externalIds": {
        "DBLP": "journals/kbs/ZhangYZ25",
        "DOI": "10.1016/j.knosys.2025.114573",
        "CorpusId": 281835495
      },
      "corpusId": 281835495,
      "title": "Objectness scan for efficient vision Mamba",
      "venue": "Knowledge-Based Systems",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.knosys.2025.114573?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.knosys.2025.114573, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2265380063",
          "name": "Kai Zhang"
        },
        {
          "authorId": "2256935412",
          "name": "Xia Yuan"
        },
        {
          "authorId": "2115602155",
          "name": "Chunxia Zhao"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "752df9cea9691e09c0e652bb1447f19dc62800ce",
      "externalIds": {
        "DOI": "10.1002/mp.70002",
        "CorpusId": 281839196,
        "PubMed": "41046478"
      },
      "corpusId": 281839196,
      "title": "An edge enhanced 3D mamba U\u2010Net for pediatric brain tumor segmentation with transfer learning",
      "venue": "Medical Physics (Lancaster)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1002/mp.70002?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1002/mp.70002, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2112561237",
          "name": "Xiaoyan Sun"
        },
        {
          "authorId": "2327827059",
          "name": "Wenhan He"
        },
        {
          "authorId": "2384085078",
          "name": "Jianing Ruan"
        },
        {
          "authorId": "2282274146",
          "name": "Zhenming Yuan"
        },
        {
          "authorId": "2384457780",
          "name": "Zhexian Sun"
        },
        {
          "authorId": "2298407091",
          "name": "Jian Zhang"
        }
      ],
      "abstract": "Pediatric gliomas, particularly high\u2010grade subtypes, are highly aggressive tumors with low survival rates, and their segmentation remains challenging due to distinct imaging characteristics and data scarcity. While deep learning models perform well in adult glioma segmentation, they struggle with pediatric gliomas, particularly in segmenting complex regions such as the tumor core (TC) and enhancing tumor (ET)."
    },
    {
      "paperId": "d885638ec32378e3a89d4b14b65d6d148282799d",
      "externalIds": {
        "DBLP": "journals/kbs/YangXZZL25",
        "DOI": "10.1016/j.knosys.2025.114570",
        "CorpusId": 281933805
      },
      "corpusId": 281933805,
      "title": "Mamba with multi-frequency perception for image super-resolution",
      "venue": "Knowledge-Based Systems",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.knosys.2025.114570?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.knosys.2025.114570, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2311989216",
          "name": "Huimin Yang"
        },
        {
          "authorId": "2213679651",
          "name": "Jingzhong Xiao"
        },
        {
          "authorId": "2312005915",
          "name": "Ji Zhang"
        },
        {
          "authorId": "2312170948",
          "name": "Xuchuan Zhou"
        },
        {
          "authorId": "2231842492",
          "name": "Yuxing Liu"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "96bece453c91f25fc0b954639e24ac46038cde4c",
      "externalIds": {
        "DBLP": "journals/asc/HuangLLZZ25",
        "DOI": "10.1016/j.asoc.2025.114036",
        "CorpusId": 281984029
      },
      "corpusId": 281984029,
      "title": "A channel-independent network using patch external attention and mamba for long-term multivariate time series forecasting",
      "venue": "Applied Soft Computing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.asoc.2025.114036?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.asoc.2025.114036, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2342647377",
          "name": "Zhigen Huang"
        },
        {
          "authorId": "2359880028",
          "name": "Huiyu Li"
        },
        {
          "authorId": "2217083720",
          "name": "Yepeng Liu"
        },
        {
          "authorId": "2342872947",
          "name": "Fan Zhang"
        },
        {
          "authorId": "2296939402",
          "name": "Xiaofeng Zhang"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "8ff2a303c48a091454c92ff1238f104111203719",
      "externalIds": {
        "DOI": "10.1016/j.metrad.2025.100185",
        "CorpusId": 282033750
      },
      "corpusId": 282033750,
      "title": "Consistency-Driven State-Space Model for Incomplete Multimodal MRI Brain Tumor Segmentation",
      "venue": "Meta-Radiology",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.metrad.2025.100185?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.metrad.2025.100185, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2385739444",
          "name": "Debao Liu"
        },
        {
          "authorId": "2108094277",
          "name": "Xiaozhi Zhang"
        },
        {
          "authorId": "2244125946",
          "name": "Hong Zhou"
        },
        {
          "authorId": "2199147646",
          "name": "Kok-Lay Teo"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "301bb72f390630d1a850f5339a3241fbd5e4ec45",
      "externalIds": {
        "DBLP": "journals/asc/BarrosFSRCTGBC26",
        "DOI": "10.1016/j.asoc.2025.114039",
        "CorpusId": 282035636
      },
      "corpusId": 282035636,
      "title": "Embracing data irregularities in multivariate time series",
      "venue": "Applied Soft Computing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.asoc.2025.114039?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.asoc.2025.114039, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "11044175",
          "name": "M. Barros"
        },
        {
          "authorId": "2385277878",
          "name": "Lucas P. Fontenele"
        },
        {
          "authorId": "2385305649",
          "name": "Mariana S. Silva"
        },
        {
          "authorId": "2258712892",
          "name": "T. Rissi"
        },
        {
          "authorId": "2258712115",
          "name": "E. Cabrera"
        },
        {
          "authorId": "9437367",
          "name": "E. Tannuri"
        },
        {
          "authorId": "38198145",
          "name": "E. Gomi"
        },
        {
          "authorId": "97739994",
          "name": "Rodrigo A. Barreira"
        },
        {
          "authorId": "2260882037",
          "name": "Anna Helena Reali Costa"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "e6bac01bf2b36043fcc378c9230d26d4c12801b7",
      "externalIds": {
        "DOI": "10.1016/j.energy.2025.138857",
        "CorpusId": 282038633
      },
      "corpusId": 282038633,
      "title": "Short-term wind speed prediction based on denoising algorithm of enhanced successive variational mode decomposition and integrated parallel prediction model",
      "venue": "Energy",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.energy.2025.138857?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.energy.2025.138857, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2287891809",
          "name": "Wei Jiang"
        },
        {
          "authorId": "2390450563",
          "name": "Qi Lu"
        },
        {
          "authorId": "2385306376",
          "name": "Yanhe Xu"
        },
        {
          "authorId": "2310731164",
          "name": "Zhong Chen"
        },
        {
          "authorId": "2385215133",
          "name": "Ting Gong"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "6d811b47bb1e723c93163a05f6fc62d18072937d",
      "externalIds": {
        "DOI": "10.1016/j.frl.2025.108662",
        "CorpusId": 282042352
      },
      "corpusId": 282042352,
      "title": "ACO advanced Mamba for Adaptive Portfolio Optimization",
      "venue": "Finance Research Letters",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.frl.2025.108662?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.frl.2025.108662, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2239289355",
          "name": "D. Vukovi\u0107"
        },
        {
          "authorId": "2323613521",
          "name": "Vyacheslav Zinovev"
        },
        {
          "authorId": "2385350114",
          "name": "Fedor Ushakov"
        },
        {
          "authorId": "2385352642",
          "name": "Mohanan Moni"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "725531acd72646215152f0dba60ec56087ad27f4",
      "externalIds": {
        "DBLP": "journals/taes/XuLDWZ25",
        "DOI": "10.1109/TAES.2025.3580691",
        "CorpusId": 279533490
      },
      "corpusId": 279533490,
      "title": "SMNet: A Semantic-Guided Mamba Network for Remote Sensing Change Detection",
      "venue": "IEEE Transactions on Aerospace and Electronic Systems",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TAES.2025.3580691?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TAES.2025.3580691, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "1510763055",
          "name": "Guoxia Xu"
        },
        {
          "authorId": "2368343483",
          "name": "Yang Liu"
        },
        {
          "authorId": "47121627",
          "name": "Lizhen Deng"
        },
        {
          "authorId": "2338014990",
          "name": "Xiaokang Wang"
        },
        {
          "authorId": "48808930",
          "name": "Hu Zhu"
        }
      ],
      "abstract": "In remote sensing, a significant field of research is change detection (CD) that aims at identifying semantic contrast from different images captured in identical location under various time constraints. As deep learning continues to advance, convolutional neural network and transformer have, respectively, strong local modeling and global feature extraction capabilities, making great progress in the CD field of remote sensing imagery. Nonetheless, due to inherent receptive field limitations of the structure, current methods often provide limited CD performance and insufficient feature expression ability. To address the above issues, a remote sensing change detection model named SMNet was designed, effectively integrating multilevel feature representations for CD tasks. Particularly, the strengths of Mamba and received weighted key value (RWKV) in capturing far-reaching dependencies are employed in the proposed framework. First, SMNet utilizes a novel learnable visual state space block, which dynamically adjusts the adjustment factors obtained from temporal semantics and mask semantics through the use of learnable adapters, enabling the structure to fully utilize complex spatiotemporal details to intensify the perception capability for detecting semantic variations. Furthermore, to efficiently capture global dependencies, a spatio-temporal transfer operation and a multidirectional WKV attention mechanism are utilized in RWKV. Ultimately, the heterogeneous pixel fusion module is proposed to enhance the circulation of dual temporal attributes within the pixel-based feature semantic domain. In addition, our comprehensive tests on some CD datasets that demonstrate our suggested SMNet strategy compares favorably with current leading-edge techniques."
    },
    {
      "paperId": "0af59e025fef298805c6d5f7675f6323552248f3",
      "externalIds": {
        "PubMedCentral": "12526546",
        "DOI": "10.3390/s25196225",
        "CorpusId": 281981911,
        "PubMed": "41095046"
      },
      "corpusId": 281981911,
      "title": "MMFNet: A Mamba-Based Multimodal Fusion Network for Remote Sensing Image Semantic Segmentation",
      "venue": "Italian National Conference on Sensors",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12526546, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2390570840",
          "name": "Jingting Qiu"
        },
        {
          "authorId": "2386522147",
          "name": "Wei Chang"
        },
        {
          "authorId": "2385436735",
          "name": "Wei Ren"
        },
        {
          "authorId": "2385316743",
          "name": "Shanshan Hou"
        },
        {
          "authorId": "2115688360",
          "name": "R. Yang"
        }
      ],
      "abstract": "Accurate semantic segmentation of high-resolution remote sensing imagery is challenged by substantial intra-class variability, inter-class similarity, and the limitations of single-modality data. This paper proposes MMFNet, a novel multimodal fusion network that leverages the Mamba architecture to efficiently capture long-range dependencies for semantic segmentation tasks. MMFNet adopts a dual-encoder design, combining ResNet-18 for local detail extraction and VMamba for global contextual modelling, striking a balance between segmentation accuracy and computational efficiency. A Multimodal Feature Fusion Block (MFFB) is introduced to effectively integrate complementary information from optical imagery and digital surface models (DSMs), thereby enhancing multimodal feature interaction and improving segmentation accuracy. Furthermore, a frequency-aware upsampling module (FreqFusion) is incorporated in the decoder to enhance boundary delineation and recover fine spatial details. Extensive experiments on the ISPRS Vaihingen and Potsdam benchmarks demonstrate that MMFNet achieves mean IoU scores of 83.50% and 86.06%, outperforming eight state-of-the-art methods while maintaining relatively low computational complexity. These results highlight MMFNet\u2019s potential for efficient and accurate multimodal semantic segmentation in remote sensing applications."
    },
    {
      "paperId": "67db124e76485a25110bebbad95b9e0251ea3ff7",
      "externalIds": {
        "PubMedCentral": "12526605",
        "DOI": "10.3390/s25196033",
        "CorpusId": 281776586,
        "PubMed": "41094856"
      },
      "corpusId": 281776586,
      "title": "A Review of Multi-Sensor Fusion in Autonomous Driving",
      "venue": "Italian National Conference on Sensors",
      "year": 2025,
      "citationCount": 3,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12526605, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2383493857",
          "name": "Hui Qian"
        },
        {
          "authorId": "2349060176",
          "name": "Mingchen Wang"
        },
        {
          "authorId": "2383810353",
          "name": "Maotao Zhu"
        },
        {
          "authorId": "2216744717",
          "name": "Hai Wang"
        }
      ],
      "abstract": "Multi-modal sensor fusion has become a cornerstone of robust autonomous driving systems, enabling perception models to integrate complementary cues from cameras, LiDARs, radars, and other modalities. This survey provides a structured overview of recent advances in deep learning-based fusion methods, categorizing them by architectural paradigms (e.g., BEV-centric fusion and cross-modal attention), learning strategies, and task adaptations. We highlight two dominant architectural trends: unified BEV representation and token-level cross-modal alignment, analyzing their design trade-offs and integration challenges. Furthermore, we review a wide range of applications, from object detection and semantic segmentation to behavior prediction and planning. Despite considerable progress, real-world deployment is hindered by issues such as spatio-temporal misalignment, domain shifts, and limited interpretability. We discuss how recent developments, such as diffusion models for generative fusion, Mamba-style recurrent architectures, and large vision\u2013language models, may unlock future directions for scalable and trustworthy perception systems. Extensive comparisons, benchmark analyses, and design insights are provided to guide future research in this rapidly evolving field."
    },
    {
      "paperId": "e2f0ab715a204c7ce3ccdb0bf86aeb5b494a7f9c",
      "externalIds": {
        "DOI": "10.1016/j.eswa.2025.130064",
        "CorpusId": 282283748
      },
      "corpusId": 282283748,
      "title": "DSA Mamba: A Model for Advanced Medical Image Classification",
      "venue": "Expert systems with applications",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.eswa.2025.130064?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.eswa.2025.130064, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2370530074",
          "name": "Zhiwen Wang"
        },
        {
          "authorId": "2392915320",
          "name": "Haoyu Yin"
        },
        {
          "authorId": "2352984169",
          "name": "JiLin Yu"
        },
        {
          "authorId": "2370533733",
          "name": "Mengsi Gong"
        },
        {
          "authorId": "2387119357",
          "name": "Qi Chen"
        },
        {
          "authorId": "2388414805",
          "name": "Qiaoqiao Chen"
        },
        {
          "authorId": "2387111272",
          "name": "Zhenlin He"
        },
        {
          "authorId": "2387038899",
          "name": "Danying Wang"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "d0de1171d41a6309572c66e362975eca5c0c3f7f",
      "externalIds": {
        "PubMedCentral": "12664974",
        "DOI": "10.1016/j.patter.2025.101394",
        "CorpusId": 282295751,
        "PubMed": "41328166"
      },
      "corpusId": 282295751,
      "title": "Decoding multi-joint hand movements from brain signals by learning a synergy-based neural manifold",
      "venue": "Patterns",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBYNCND",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12664974, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2305031749",
          "name": "Huaqin Sun"
        },
        {
          "authorId": "2387526905",
          "name": "Zhengyi Wang"
        },
        {
          "authorId": "2166032992",
          "name": "Yu Qi"
        },
        {
          "authorId": "2253442058",
          "name": "Yueming Wang"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "605b751d52496e63b9e51ebc99dd86a5180f28ff",
      "externalIds": {
        "DOI": "10.1016/j.patcog.2025.112648",
        "CorpusId": 282444523
      },
      "corpusId": 282444523,
      "title": "Enhancing VMamba for Change Detection via Lightweight Feature Interaction and Selection",
      "venue": "Pattern Recognition",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.patcog.2025.112648?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.patcog.2025.112648, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2220798075",
          "name": "Mingwei Zhang"
        },
        {
          "authorId": "2388679733",
          "name": "Yuan Jiang"
        },
        {
          "authorId": "2184281630",
          "name": "Qi Wang"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "0c5ac6a7ee8c9c66c8020fc318de64260f58e5fd",
      "externalIds": {
        "PubMedCentral": "12567485",
        "DOI": "10.3390/s25206287",
        "CorpusId": 282020980,
        "PubMed": "41157341"
      },
      "corpusId": 282020980,
      "title": "Physical Layer Security Enhancement in IRS-Assisted Interweave CIoV Networks: A Heterogeneous Multi-Agent Mamba RainbowDQN Method",
      "venue": "Italian National Conference on Sensors",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12567485, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2068166155",
          "name": "Ruiquan Lin"
        },
        {
          "authorId": "2387649372",
          "name": "Shengjie Xie"
        },
        {
          "authorId": "2298204294",
          "name": "Wenchen Chen"
        },
        {
          "authorId": "2387407334",
          "name": "Tao Xu"
        }
      ],
      "abstract": "The Internet of Vehicles (IoV) relies on Vehicle-to-Everything (V2X) communications to enable cooperative perception among vehicles, infrastructures, and devices, where Vehicle-to-Infrastructure (V2I) links are crucial for reliable transmission. However, the openness of wireless channels exposes IoV to eavesdropping, threatening privacy and security. This paper investigates an Intelligent Reflecting Surface (IRS)-assisted interweave Cognitive IoV (CIoV) network to enhance physical layer security in V2I communications. A non-convex joint optimization problem involving spectrum allocation, transmit power for Vehicle Users (VUs), and IRS phase shifts is formulated. To address this challenge, a heterogeneous multi-agent (HMA) Mamba RainbowDQN algorithm is proposed, where homogeneous VUs and a heterogeneous secondary base station (SBS) act as distinct agents to simplify decision-making. Simulation results show that the proposed method significantly outperform benchmark schemes, achieving a 13.29% improvement in secrecy rate and a 54.2% reduction in secrecy outage probability (SOP). These results confirm the effectiveness of integrating IRS and deep reinforcement learning (DRL) for secure and efficient V2I communications in CIoV networks."
    },
    {
      "paperId": "392600760b9626c38f6e386a0f9404025c2562b6",
      "externalIds": {
        "DOI": "10.1002/mp.70068",
        "CorpusId": 281955569,
        "PubMed": "41068068"
      },
      "corpusId": 281955569,
      "title": "A multi\u2010source feature\u2010driven deep learning method to generate linear energy transfer distribution for proton therapy",
      "venue": "Medical Physics (Lancaster)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1002/mp.70068?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1002/mp.70068, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2384940118",
          "name": "Qian Liu"
        },
        {
          "authorId": "2390576730",
          "name": "Shangyan Wei"
        },
        {
          "authorId": "2385321981",
          "name": "Huijuan Peng"
        },
        {
          "authorId": "2320115726",
          "name": "Xiaonan Liu"
        },
        {
          "authorId": "2315065282",
          "name": "Xinyuan Chen"
        },
        {
          "authorId": "2249291072",
          "name": "J. Dai"
        },
        {
          "authorId": "33356393",
          "name": "K. Men"
        }
      ],
      "abstract": "Monte Carlo (MC)\u2010based estimation of dose\u2010averaged linear energy transfer (LETd) in proton therapy is accurate but computationally intensive. Although deep learning (DL) models offer efficient alternatives, few have been optimized for multi\u2010source inputs or validated on anatomically complex tumors, such as nasopharyngeal carcinoma (NPC)."
    },
    {
      "paperId": "e976fffa23249fde45b60c05cde8c98494b37ea7",
      "externalIds": {
        "ArXiv": "2509.26625",
        "DBLP": "journals/corr/abs-2509-26625",
        "DOI": "10.48550/arXiv.2509.26625",
        "CorpusId": 281681421
      },
      "corpusId": 281681421,
      "title": "Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 5,
      "influentialCitationCount": 1,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.26625, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2383043854",
          "name": "Junlin Han"
        },
        {
          "authorId": "2382993389",
          "name": "Shengbang Tong"
        },
        {
          "authorId": "2335871751",
          "name": "David Fan"
        },
        {
          "authorId": "2383366276",
          "name": "Yufan Ren"
        },
        {
          "authorId": "2266467601",
          "name": "Koustuv Sinha"
        },
        {
          "authorId": "2359275450",
          "name": "Philip Torr"
        },
        {
          "authorId": "2283931239",
          "name": "Filippos Kokkinos"
        }
      ],
      "abstract": "Large Language Models (LLMs), despite being trained on text alone, surprisingly develop rich visual priors. These priors allow latent visual capabilities to be unlocked for vision tasks with a relatively small amount of multimodal data, and in some cases, to perform visual tasks without ever having seen an image. Through systematic analysis, we reveal that visual priors-the implicit, emergent knowledge about the visual world acquired during language pre-training-are composed of separable perception and reasoning priors with unique scaling trends and origins. We show that an LLM's latent visual reasoning ability is predominantly developed by pre-training on reasoning-centric data (e.g., code, math, academia) and scales progressively. This reasoning prior acquired from language pre-training is transferable and universally applicable to visual reasoning. In contrast, a perception prior emerges more diffusely from broad corpora, and perception ability is more sensitive to the vision encoder and visual instruction tuning data. In parallel, text describing the visual world proves crucial, though its performance impact saturates rapidly. Leveraging these insights, we propose a data-centric recipe for pre-training vision-aware LLMs and verify it in 1T token scale pre-training. Our findings are grounded in over 100 controlled experiments consuming 500,000 GPU-hours, spanning the full MLLM construction pipeline-from LLM pre-training to visual alignment and supervised multimodal fine-tuning-across five model scales, a wide range of data categories and mixtures, and multiple adaptation setups. Along with our main findings, we propose and investigate several hypotheses, and introduce the Multi-Level Existence Bench (MLE-Bench). Together, this work provides a new way of deliberately cultivating visual priors from language pre-training, paving the way for the next generation of multimodal LLMs."
    },
    {
      "paperId": "6a499ecc97925b6f7b3b9c90e22d970ae6374734",
      "externalIds": {
        "ArXiv": "2509.26413",
        "DBLP": "journals/corr/abs-2509-26413",
        "DOI": "10.48550/arXiv.2509.26413",
        "CorpusId": 281681419
      },
      "corpusId": 281681419,
      "title": "PRISM: Progressive Rain removal with Integrated State-space Modeling",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.26413, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2382992435",
          "name": "Pengze Xue"
        },
        {
          "authorId": "2341660086",
          "name": "Shanwen Wang"
        },
        {
          "authorId": "2384421848",
          "name": "Fei Zhou"
        },
        {
          "authorId": "2383217093",
          "name": "Yan Cui"
        },
        {
          "authorId": "2384023938",
          "name": "Xin Sun"
        }
      ],
      "abstract": "Image deraining is an essential vision technique that removes rain streaks and water droplets, enhancing clarity for critical vision tasks like autonomous driving. However, current single-scale models struggle with fine-grained recovery and global consistency. To address this challenge, we propose Progressive Rain removal with Integrated State-space Modeling (PRISM), a progressive three-stage framework: Coarse Extraction Network (CENet), Frequency Fusion Network (SFNet), and Refine Network (RNet). Specifically, CENet and SFNet utilize a novel Hybrid Attention UNet (HA-UNet) for multi-scale feature aggregation by combining channel attention with windowed spatial transformers. Moreover, we propose Hybrid Domain Mamba (HDMamba) for SFNet to jointly model spatial semantics and wavelet domain characteristics. Finally, RNet recovers the fine-grained structures via an original-resolution subnetwork. Our model learns high-frequency rain characteristics while preserving structural details and maintaining global context, leading to improved image quality. Our method achieves competitive results on multiple datasets against recent deraining methods."
    },
    {
      "paperId": "8d1fd02ebeae18b26ce28b2d7cd520c16eb342a8",
      "externalIds": {
        "ArXiv": "2509.25918",
        "DBLP": "journals/corr/abs-2509-25918",
        "DOI": "10.48550/arXiv.2509.25918",
        "CorpusId": 281682271
      },
      "corpusId": 281682271,
      "title": "Bringing Emerging Architectures to Sequence Labeling in NLP",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.25918, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2248187659",
          "name": "Ana Ezquerro"
        },
        {
          "authorId": "2375389741",
          "name": "Carlos G'omez-Rodr'iguez"
        },
        {
          "authorId": "2261282212",
          "name": "David Vilares"
        }
      ],
      "abstract": "Pretrained Transformer encoders are the dominant approach to sequence labeling. While some alternative architectures-such as xLSTMs, structured state-space models, diffusion models, and adversarial learning-have shown promise in language modeling, few have been applied to sequence labeling, and mostly on flat or simplified tasks. We study how these architectures adapt across tagging tasks that vary in structural complexity, label space, and token dependencies, with evaluation spanning multiple languages. We find that the strong performance previously observed in simpler settings does not always generalize well across languages or datasets, nor does it extend to more complex structured tasks."
    },
    {
      "paperId": "15ccf77ecd201c5ef5bc8761b42cd3329d37c84c",
      "externalIds": {
        "ArXiv": "2509.26645",
        "DBLP": "journals/corr/abs-2509-26645",
        "DOI": "10.48550/arXiv.2509.26645",
        "CorpusId": 281682896
      },
      "corpusId": 281682896,
      "title": "TTT3R: 3D Reconstruction as Test-Time Training",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 11,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.26645, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2335297076",
          "name": "Xingyu Chen"
        },
        {
          "authorId": "2335112926",
          "name": "Yue Chen"
        },
        {
          "authorId": "2334863452",
          "name": "Yuliang Xiu"
        },
        {
          "authorId": "2267724284",
          "name": "Andreas Geiger"
        },
        {
          "authorId": "2268728872",
          "name": "Anpei Chen"
        }
      ],
      "abstract": "Modern Recurrent Neural Networks have become a competitive architecture for 3D reconstruction due to their linear-time complexity. However, their performance degrades significantly when applied beyond the training context length, revealing limited length generalization. In this work, we revisit the 3D reconstruction foundation models from a Test-Time Training perspective, framing their designs as an online learning problem. Building on this perspective, we leverage the alignment confidence between the memory state and incoming observations to derive a closed-form learning rate for memory updates, to balance between retaining historical information and adapting to new observations. This training-free intervention, termed TTT3R, substantially improves length generalization, achieving a $2\\times$ improvement in global pose estimation over baselines, while operating at 20 FPS with just 6 GB of GPU memory to process thousands of images. Code available in https://rover-xingyu.github.io/TTT3R"
    },
    {
      "paperId": "896061a2a65a701f74c3cda9a00a7a85d4a1aba8",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-26626",
        "ArXiv": "2509.26626",
        "DOI": "10.48550/arXiv.2509.26626",
        "CorpusId": 281682326
      },
      "corpusId": 281682326,
      "title": "Recursive Self-Aggregation Unlocks Deep Thinking in Large Language Models",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.26626, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "1598370068",
          "name": "S. Venkatraman"
        },
        {
          "authorId": "2061365264",
          "name": "Vineet Jain"
        },
        {
          "authorId": "51519654",
          "name": "Sarthak Mittal"
        },
        {
          "authorId": "2268399428",
          "name": "Vedant Shah"
        },
        {
          "authorId": "1403752231",
          "name": "Johan Obando-Ceron"
        },
        {
          "authorId": "1865800402",
          "name": "Y. Bengio"
        },
        {
          "authorId": "41053241",
          "name": "Brian R. Bartoldson"
        },
        {
          "authorId": "1749353",
          "name": "B. Kailkhura"
        },
        {
          "authorId": "2253462182",
          "name": "Guillaume Lajoie"
        },
        {
          "authorId": "2253652688",
          "name": "Glen Berseth"
        },
        {
          "authorId": "2067020770",
          "name": "Nikolay Malkin"
        },
        {
          "authorId": "1383135665",
          "name": "Moksh Jain"
        }
      ],
      "abstract": "Test-time scaling methods improve the capabilities of large language models (LLMs) by increasing the amount of compute used during inference to make a prediction. Inference-time compute can be scaled in parallel by choosing among multiple independent solutions or sequentially through self-refinement. We propose Recursive Self-Aggregation (RSA), a test-time scaling method inspired by evolutionary methods that combines the benefits of both parallel and sequential scaling. Each step of RSA refines a population of candidate reasoning chains through aggregation of subsets to yield a population of improved solutions, which are then used as the candidate pool for the next iteration. RSA exploits the rich information embedded in the reasoning chains -- not just the final answers -- and enables bootstrapping from partially correct intermediate steps within different chains of thought. Empirically, RSA delivers substantial performance gains with increasing compute budgets across diverse tasks, model families and sizes. Notably, RSA enables Qwen3-4B-Instruct-2507 to achieve competitive performance with larger reasoning models, including DeepSeek-R1 and o3-mini (high), while outperforming purely parallel and sequential scaling strategies across AIME-25, HMMT-25, Reasoning Gym, LiveCodeBench-v6, and SuperGPQA. We further demonstrate that training the model to combine solutions via a novel aggregation-aware reinforcement learning approach yields significant performance gains. Code available at https://github.com/HyperPotatoNeo/RSA."
    },
    {
      "paperId": "e249ed7bff85ce308cf401bf4c7284d37df5c6c3",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-00258",
        "ArXiv": "2510.00258",
        "DOI": "10.48550/arXiv.2510.00258",
        "CorpusId": 281706317
      },
      "corpusId": 281706317,
      "title": "Delayed Attention Training Improves Length Generalization in Transformer-RNN Hybrids",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.00258, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "51280131",
          "name": "Buu Phan"
        },
        {
          "authorId": "2383171065",
          "name": "Reza Ebrahimi"
        },
        {
          "authorId": "2330587187",
          "name": "Sanjay Haresh"
        },
        {
          "authorId": "2264497937",
          "name": "Roland Memisevic"
        }
      ],
      "abstract": "We study length generalization in sequence models on a composite problem involving both state tracking and associative recall. Prior work finds that recurrent networks handle state tracking well but struggle with recall, whereas Transformers excel at recall yet fail to extend state-tracking capabilities to longer sequences. Motivated by the complementary strengths of these architectures, we construct hybrid models integrating recurrent and attention-based components, and train them on the combined task to evaluate whether both capabilities can be preserved. Our results reveal that, in such hybrids, the Transformer component tends to exploit shortcut solutions, leading to poor length generalization. We identify this shortcut reliance as a key obstacle and propose a simple yet effective training strategy -- delaying the training of the attention layers -- that mitigates this effect and significantly improves length generalization performance. Our experiments show that this approach enables hybrid models to achieve near-perfect accuracy ($>90\\%$) on hybrid sequences three times longer than those used during training."
    },
    {
      "paperId": "35a922194adae17d7bf408183a80ae5c63b5573d",
      "externalIds": {
        "ArXiv": "2509.24318",
        "DBLP": "journals/corr/abs-2509-24318",
        "DOI": "10.48550/arXiv.2509.24318",
        "CorpusId": 281675067
      },
      "corpusId": 281675067,
      "title": "Similarity-Aware Selective State-Space Modeling for Semantic Correspondence",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.24318, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2119244714",
          "name": "Seungwook Kim"
        },
        {
          "authorId": "2286208576",
          "name": "Minsu Cho"
        }
      ],
      "abstract": "Establishing semantic correspondences between images is a fundamental yet challenging task in computer vision. Traditional feature-metric methods enhance visual features but may miss complex inter-correlation relationships, while recent correlation-metric approaches are hindered by high computational costs due to processing 4D correlation maps. We introduce MambaMatcher, a novel method that overcomes these limitations by efficiently modeling high-dimensional correlations using selective state-space models (SSMs). By implementing a similarity-aware selective scan mechanism adapted from Mamba's linear-complexity algorithm, MambaMatcher refines the 4D correlation map effectively without compromising feature map resolution or receptive field. Experiments on standard semantic correspondence benchmarks demonstrate that MambaMatcher achieves state-of-the-art performance."
    },
    {
      "paperId": "11b0a35b0584b2d412713142669e751513ffe930",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-24334",
        "ArXiv": "2509.24334",
        "DOI": "10.1109/TGRS.2025.3616324",
        "CorpusId": 281674802
      },
      "corpusId": 281674802,
      "title": "Wavelet-Assisted Mamba for Satellite-Derived Sea Surface Temperature Super-Resolution",
      "venue": "IEEE Transactions on Geoscience and Remote Sensing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.24334, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2304517178",
          "name": "Wankun Chen"
        },
        {
          "authorId": "2304892191",
          "name": "Feng Gao"
        },
        {
          "authorId": "3393223",
          "name": "Yanhai Gan"
        },
        {
          "authorId": "2263575848",
          "name": "Jingchao Cao"
        },
        {
          "authorId": "2304713982",
          "name": "Junyu Dong"
        },
        {
          "authorId": "2072520954",
          "name": "Q. Du"
        }
      ],
      "abstract": "Sea surface temperature (SST) is an essential indicator of global climate change and one of the most intuitive factors reflecting ocean conditions. Obtaining high-resolution SST data remains challenging due to limitations in physical imaging, and super-resolution (SR) via deep neural networks is a promising solution. Recently, Mamba-based approaches leveraging state-space models (SSMs) have demonstrated significant potential for long-range dependence modeling with linear complexity. However, their application to SST data SR remains largely unexplored. To this end, we propose the wavelet-assisted Mamba SR (WMSR) framework for satellite-derived SST data. The WMSR includes two key components: the low-frequency state-space module (LFSSM) and the high-frequency enhancement module (HFEM). The LFSSM uses a 2-D selective scan module (2D-SSM) to capture global information of the input data, and the robust global modeling capabilities of SSM are exploited to preserve the critical temperature information in the low-frequency component. The HFEM employs the pixel difference convolution to match and correct the high-frequency feature, achieving accurate and clear textures. Through comprehensive experiments on three SST datasets, our WMSR demonstrated superior performance over state-of-the-art methods. Our codes and datasets will be made publicly available at https://github.com/oucailab/WMSR"
    },
    {
      "paperId": "2bee05f5ad785364eb2fd8197fea4245eab3b7a3",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-25155",
        "ArXiv": "2509.25155",
        "DOI": "10.48550/arXiv.2509.25155",
        "CorpusId": 281676311
      },
      "corpusId": 281676311,
      "title": "Context-Driven Performance Modeling for Causal Inference Operators on Neural Processing Units",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.25155, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2276489527",
          "name": "Neelesh Gupta"
        },
        {
          "authorId": "2322438555",
          "name": "Rakshith Jayanth"
        },
        {
          "authorId": "2243178300",
          "name": "Dhruv Parikh"
        },
        {
          "authorId": "2322448526",
          "name": "Viktor K. Prasanna"
        }
      ],
      "abstract": "The proliferation of large language models has driven demand for long-context inference on resource-constrained edge platforms. However, deploying these models on Neural Processing Units (NPUs) presents significant challenges due to architectural mismatch: the quadratic complexity of standard attention conflicts with NPU memory and compute patterns. This paper presents a comprehensive performance analysis of causal inference operators on a modern NPU, benchmarking quadratic attention against sub-quadratic alternatives including structured state-space models and causal convolutions. Our analysis reveals a spectrum of critical bottlenecks: quadratic attention becomes severely memory-bound with catastrophic cache inefficiency, while sub-quadratic variants span from compute-bound on programmable vector cores to memory-bound by data movement. These findings provide essential insights for co-designing hardware-aware models and optimization strategies to enable efficient long-context inference on edge platforms."
    },
    {
      "paperId": "4781826d42a0f09ae5ca731e43f6403b0f504324",
      "externalIds": {
        "ArXiv": "2509.24765",
        "DBLP": "journals/corr/abs-2509-24765",
        "DOI": "10.48550/arXiv.2509.24765",
        "CorpusId": 281674912
      },
      "corpusId": 281674912,
      "title": "From Ambiguity to Verdict: A Semiotic-Grounded Multi-Perspective Agent for LLM Logical Reasoning",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.24765, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2365327804",
          "name": "Yunyao Zhang"
        },
        {
          "authorId": "2383944382",
          "name": "Xinglang Zhang"
        },
        {
          "authorId": "2382923647",
          "name": "Junxi Sheng"
        },
        {
          "authorId": "2303611300",
          "name": "Wenbing Li"
        },
        {
          "authorId": "2237947276",
          "name": "Junqing Yu"
        },
        {
          "authorId": "2243650023",
          "name": "Wei Yang"
        },
        {
          "authorId": "7576095",
          "name": "Zikai Song"
        }
      ],
      "abstract": "Logical reasoning is a fundamental capability of large language models. However, existing studies often overlook the interaction between logical complexity and semantic complexity, leading to systems that struggle with abstract propositions, ambiguous contexts, and conflicting stances that are central to human reasoning. We propose LogicAgent, a semiotic-square-guided framework that jointly addresses these two axes of difficulty. The semiotic square provides a principled structure for multi-perspective semantic analysis, and LogicAgent integrates automated deduction with reflective verification to manage logical complexity across deeper reasoning chains. To support evaluation under these conditions, we introduce RepublicQA, a benchmark that couples semantic complexity with logical depth. RepublicQA reaches college-level semantic difficulty (FKGL 11.94), contains philosophically grounded abstract propositions with systematically constructed contrary and contradictory forms, and offers a semantically rich setting for assessing logical reasoning in large language models. Experiments show that LogicAgent achieves state-of-the-art performance on RepublicQA with a 6.25 percent average improvement over strong baselines, and generalizes effectively to mainstream logical reasoning benchmarks including ProntoQA, ProofWriter, FOLIO, and ProverQA, achieving an additional 7.05 percent average gain. These results demonstrate the effectiveness of semiotic-grounded multi-perspective reasoning in enhancing logical performance."
    },
    {
      "paperId": "ee2cf3b7267f7b1e545d6acba8ce3fa8d3cb66ca",
      "externalIds": {
        "ArXiv": "2509.24435",
        "DBLP": "journals/corr/abs-2509-24435",
        "DOI": "10.48550/arXiv.2509.24435",
        "CorpusId": 281676424
      },
      "corpusId": 281676424,
      "title": "Alternatives To Next Token Prediction In Text Generation - A Survey",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.24435, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2375396051",
          "name": "Charlie Wyatt"
        },
        {
          "authorId": "2262446587",
          "name": "Aditya Joshi"
        },
        {
          "authorId": "2303256737",
          "name": "Flora D. Salim"
        }
      ],
      "abstract": "The paradigm of Next Token Prediction (NTP) has driven the unprecedented success of Large Language Models (LLMs), but is also the source of their most persistent weaknesses such as poor long-term planning, error accumulation, and computational inefficiency. Acknowledging the growing interest in exploring alternatives to NTP, the survey describes the emerging ecosystem of alternatives to NTP. We categorise these approaches into five main families: (1) Multi-Token Prediction, which targets a block of future tokens instead of a single one; (2) Plan-then-Generate, where a global, high-level plan is created upfront to guide token-level decoding; (3) Latent Reasoning, which shifts the autoregressive process itself into a continuous latent space; (4) Continuous Generation Approaches, which replace sequential generation with iterative, parallel refinement through diffusion, flow matching, or energy-based methods; and (5) Non-Transformer Architectures, which sidestep NTP through their inherent model structure. By synthesizing insights across these methods, this survey offers a taxonomy to guide research into models that address the known limitations of token-level generation to develop new transformative models for natural language processing."
    },
    {
      "paperId": "fe7ad449b6ca304b4d837ea3214c923a06016cb6",
      "externalIds": {
        "ArXiv": "2509.25582",
        "DBLP": "journals/corr/abs-2509-25582",
        "DOI": "10.48550/arXiv.2509.25582",
        "CorpusId": 281681987
      },
      "corpusId": 281681987,
      "title": "Safe In-Context Reinforcement Learning",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.25582, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2345003669",
          "name": "Amir Moeini"
        },
        {
          "authorId": "2335562348",
          "name": "Minjae Kwon"
        },
        {
          "authorId": "144229259",
          "name": "A. Bozkurt"
        },
        {
          "authorId": "2382995160",
          "name": "Yuichi Motai"
        },
        {
          "authorId": "2345011304",
          "name": "Rohan Chandra"
        },
        {
          "authorId": "2362321338",
          "name": "Lu Feng"
        },
        {
          "authorId": "2350229245",
          "name": "Shangtong Zhang"
        }
      ],
      "abstract": "In-context reinforcement learning (ICRL) is an emerging RL paradigm where the agent, after some pretraining procedure, is able to adapt to out-of-distribution test tasks without any parameter updates. The agent achieves this by continually expanding the input (i.e., the context) to its policy neural networks. For example, the input could be all the history experience that the agent has access to until the current time step. The agent's performance improves as the input grows, without any parameter updates. In this work, we propose the first method that promotes the safety of ICRL's adaptation process in the framework of constrained Markov Decision Processes. In other words, during the parameter-update-free adaptation process, the agent not only maximizes the reward but also minimizes an additional cost function. We also demonstrate that our agent actively reacts to the threshold (i.e., budget) of the cost tolerance. With a higher cost budget, the agent behaves more aggressively, and with a lower cost budget, the agent behaves more conservatively."
    },
    {
      "paperId": "6dfed9b93ad3e84d43f542eeabc9713a89def8bf",
      "externalIds": {
        "PubMedCentral": "12480594",
        "DOI": "10.1038/s41598-025-18769-2",
        "CorpusId": 281678651,
        "PubMed": "41023194"
      },
      "corpusId": 281678651,
      "title": "MambaOVD: a Mamba-based open-vocabulary object detection method",
      "venue": "Scientific Reports",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBYNCND",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12480594, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2249110753",
          "name": "Kunhua Liu"
        },
        {
          "authorId": "2334897922",
          "name": "Longyan Ma"
        },
        {
          "authorId": "2335460796",
          "name": "Tao Lu"
        }
      ],
      "abstract": "Open-vocabulary object detection (OVD) is a critical research area in computer vision, particularly for applications in autonomous driving and robotics. Many existing OVD methods adopt transformer architectures for image-text fusion, utilizing self-attention mechanisms to model complex dependencies. However, transformer-based approaches are often computationally demanding, limiting their practical deployment. To address this issue, we propose MambaOVD, a novel open-vocabulary object detection method based on the Mamba architecture. MambaOVD consists of four key modules: an image encoder, a text encoder, a Mamba-based image-text fusion module, and a detection head. The image encoder extracts visual features, the text encoder generates text embeddings, the fusion module integrates multimodal information using Mamba layers, and the detection head performs object localization and classification. To evaluate the effectiveness of MambaOVD, we trained the model on the Objects365 (V1) and GoldG datasets, and conducted testing on the LVIS minival and AutoMine datasets. Experimental results show that MambaOVD achieves superior performance compared to state-of-the-art (SOTA) models, including YOLO-World-S, GLIPv2_T, and DetCLIP_T, demonstrating advantages in both qualitative and quantitative evaluations."
    },
    {
      "paperId": "bfa196a10d7119b57a3902b992be7f8dbce839c1",
      "externalIds": {
        "PubMedCentral": "12480981",
        "DOI": "10.1038/s41598-025-15920-x",
        "CorpusId": 281680725,
        "PubMed": "41022936"
      },
      "corpusId": 281680725,
      "title": "Mixed prototype correction for causal inference in medical image classification",
      "venue": "Scientific Reports",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBYNCND",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12480981, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2329049314",
          "name": "Zhiliang Hong"
        },
        {
          "authorId": "2169322896",
          "name": "Jian-chuan Yang"
        },
        {
          "authorId": "2383253601",
          "name": "Xiao-Rui Peng"
        },
        {
          "authorId": "2126149307",
          "name": "Songbo Wu"
        }
      ],
      "abstract": "The heterogeneity of medical images poses significant challenges to accurate disease diagnosis. To tackle this issue, the impact of such heterogeneity on the causal relationship between image features and diagnostic labels should be incorporated into model design, which however remains under explored. In this paper, we propose a mixed prototype correction for causal inference (MPCCI) method, aimed at mitigating the impact of unseen confounding factors on the causal relationships between medical images and disease labels, so as to enhance the diagnostic accuracy of deep learning models. The MPCCI comprises a causal inference component based on front-door adjustment and an adaptive training strategy. The causal inference component employs a multi-view feature extraction (MVFE) module to establish mediators, and a mixed prototype correction (MPC) module to execute causal interventions. Moreover, the adaptive training strategy incorporates both information purity and maturity metrics to maintain stable model training. Experimental evaluations on four medical image datasets, encompassing CT and ultrasound modalities, demonstrate the superior diagnostic accuracy and reliability of the proposed MPCCI. The code will be available at https://github.com/Yajie-Zhang/MPCCI."
    },
    {
      "paperId": "e94a66eb32a14a42fae80e0d6d37dc9d33217629",
      "externalIds": {
        "PubMedCentral": "12482534",
        "DBLP": "journals/bmcmi/AtabansiWLNXZLZL25",
        "DOI": "10.1186/s12880-025-01942-4",
        "CorpusId": 281677706,
        "PubMed": "41023997"
      },
      "corpusId": 281677706,
      "title": "DCM-Net: dual-encoder CNN-Mamba network with cross-branch fusion for robust medical image segmentation",
      "venue": "BMC Medical Imaging",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBYNCND",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12482534, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2100710054",
          "name": "C. C. Atabansi"
        },
        {
          "authorId": "2309886511",
          "name": "Sheng Wang"
        },
        {
          "authorId": "2356551652",
          "name": "Hui Li"
        },
        {
          "authorId": "2244637653",
          "name": "Jing Nie"
        },
        {
          "authorId": "2309695375",
          "name": "Lei Xiang"
        },
        {
          "authorId": "2383592296",
          "name": "Cheng Zhang"
        },
        {
          "authorId": "2283816076",
          "name": "Haijun Liu"
        },
        {
          "authorId": "2157263490",
          "name": "Xichuan Zhou"
        },
        {
          "authorId": "1967666153",
          "name": "D. Li"
        }
      ],
      "abstract": "Medical image segmentation is a critical task for the early detection and diagnosis of various conditions, such as skin cancer, polyps, thyroid nodules, and pancreatic tumors. Recently, deep learning architectures have achieved significant success in this field. However, they face a critical trade-off between local feature extraction and global context modeling. To address this limitation, we present DCM-Net, a dual-encoder architecture that integrates pretrained CNN layers with Visual State Space (VSS) blocks through a Cross-Branch Feature Fusion Module (CBFFM). A Decoder Feature Enhancement Module (DFEM) combines depth-wise separable convolutions with MLP-based semantic rectification to extract enhanced decoded features and improve the segmentation performance. Additionally, we present a new 2D pancreas and pancreatic tumor dataset (CCH-PCT-CT) collected from Chongqing University Cancer Hospital, comprising 3,547 annotated CT slices, which is used to validate the proposed model. The proposed DCM-Net architecture achieves competitive performance across all datasets investigated in this study. We develop a novel DCM-Net architecture that generates robust features for tumor and organ segmentation in medical images. DCM-Net significantly outperforms all baseline models in segmentation tasks, with higher Dice Similarity Coefficient (DSC) and mean Intersection over Union (mIoU) scores. Its robustness confirms strong potential for clinical use."
    },
    {
      "paperId": "b3122c20e400e37e42fa94f29c6046a4e891a7f7",
      "externalIds": {
        "DOI": "10.54097/108jrv85",
        "CorpusId": 281739582
      },
      "corpusId": 281739582,
      "title": "A Sequential Recommendation Model Based on Convolutional Neural Networks and State Space Models",
      "venue": "Journal of Computing and Electronic Information Management",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.54097/108jrv85?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.54097/108jrv85, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2385446898",
          "name": "Chenfei Feng"
        },
        {
          "authorId": "2383377775",
          "name": "Haitao Wang"
        },
        {
          "authorId": "2383331196",
          "name": "Yuguo Wang"
        }
      ],
      "abstract": "Sequential recommendation aims to predict users' future preferences by analyzing their historical behavior sequences. In recent years, deep learning techniques have been widely applied to sequential recommendation tasks, achieving remarkable improvements in recommendation accuracy. However, existing methods often suffer from structural imbalance in modeling the rapidly changing short-term interests and the relatively stable long-term preferences of users. Moreover, these methods face limitations such as high computational costs and low inference efficiency when dealing with long behavior sequences. To address these challenges, this paper proposes a novel sequential recommendation model named CNN-Mamba, which integrates Convolutional Neural Networks (CNN) with State Space Models (SSM). Specifically, the CNN component leverages local receptive fields to efficiently extract short-term interest features from recent user interactions, thereby enhancing the model\u2019s ability to capture local behavior patterns. Meanwhile, the SSM component is introduced as a long-term interest modeling module to capture global dependencies within long sequences. Furthermore, an adaptive fusion layer is designed to dynamically integrate the short- and long-term modeling outputs, thereby improving the model's generalization ability. In addition, the implicit recurrence mechanism in the state space model effectively reduces computational complexity and enhances the efficiency of long-sequence modeling. Experimental results on three real-world datasets demonstrate that the proposed CNN-Mamba model outperforms state-of-the-art baselines in both recommendation accuracy and inference efficiency, validating its effectiveness and practicality."
    },
    {
      "paperId": "a398a628f188d08ee0c6e33660c3984151a42484",
      "externalIds": {
        "PubMedCentral": "12621852",
        "DOI": "10.1101/2025.09.25.678635",
        "CorpusId": 282315011,
        "PubMed": "41256441"
      },
      "corpusId": 282315011,
      "title": "Edit Distance Embedding with Genomic Large Language Model",
      "venue": "bioRxiv",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12621852, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2305000719",
          "name": "Xiang Li"
        },
        {
          "authorId": "2304827956",
          "name": "Ke Chen"
        },
        {
          "authorId": "2387352148",
          "name": "Yijia Zhang"
        },
        {
          "authorId": "2283938841",
          "name": "Mingfu Shao"
        }
      ],
      "abstract": "Edit distance is a fundamental metric in genomic sequence analysis, yet it is computationally expensive to calculate. A practical approach for large-scale sequence analysis involves mapping sequences into a normed space and approximating the edit distance using the more efficiently computed distance in that space. This process, known as edit distance embedding, has been extensively studied both theoretically and in practice. Recently, embedding methods based on machine learning have gained popularity, where the mapping is represented as a neural network whose parameters are learned from data. However, the accuracy of these methods remains un-satisfactory, leaving much room for improvement. Recent advancements in genomic language models have shown remarkable performance in various sequence analysis applications. We investigate if improved embeddings can be achieved using DNA language models. We introduce LLMED, a model designed to produce sequence embeddings approximating the edit distance. LLMED is trained via contrastive learning based on a pretrained genomic large language model. Through extensive experimental comparisons, we show that LLMED surpasses leading machine learning and rule-based embedding methods in approximating the edit distance; LLMED also achieved significantly improved accuracy in a critical application, similar sequence search."
    },
    {
      "paperId": "6a571805216ef673ef8afb877bf58178f2f88cf9",
      "externalIds": {
        "PubMedCentral": "12480901",
        "DOI": "10.1038/s42004-025-01683-z",
        "CorpusId": 281677814,
        "PubMed": "41023138"
      },
      "corpusId": 281677814,
      "title": "MolGraph-xLSTM as a graph-based dual-level xLSTM framework for enhanced molecular representation and interpretability",
      "venue": "Communications Chemistry",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBYNCND",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12480901, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2196446380",
          "name": "Y. Sun"
        },
        {
          "authorId": "2342985659",
          "name": "Yutong Lu"
        },
        {
          "authorId": "2292974681",
          "name": "Yan Yi Li"
        },
        {
          "authorId": "2342913448",
          "name": "Zihao Jing"
        },
        {
          "authorId": "2252336309",
          "name": "C. K. Leung"
        },
        {
          "authorId": "2344046267",
          "name": "Pingzhao Hu"
        }
      ],
      "abstract": "Predicting molecular properties is essential for drug discovery, and computational methods can greatly enhance this process. Molecular graphs have become a focus for representation learning, with Graph Neural Networks (GNNs) widely used. However, GNNs often struggle with capturing long-range dependencies. To address this, we propose MolGraph-xLSTM, a novel graph-based xLSTM model that enhances feature extraction and effectively models molecule long-range interactions. Our approach processes molecular graphs at two scales: atom-level and motif-level. For atom-level graphs, a GNN-based xLSTM framework with jumping knowledge extracts local features and aggregates multilayer information to capture both local and global patterns effectively. Motif-level graphs provide complementary structural information for a broader molecular view. Embeddings from both scales are refined via a multi-head mixture of experts (MHMoE), further enhancing expressiveness and performance. We validate MolGraph-xLSTM on 21 datasets from the MoleculeNet and Therapeutics Data Commons (TDC) benchmarks, covering both classification and regression tasks. On the MoleculeNet benchmark, our model achieves an average AUROC improvement of 3.18% for classification tasks and an RMSE reduction of 3.83% for regression tasks compared to baseline methods. On the TDC benchmark, MolGraph-xLSTM improves AUROC by 2.56%, while reducing RMSE by 3.71% on average. These results confirm the effectiveness of our model in learning generalizable molecular representations for drug discovery. Predicting molecular properties is crucial for drug discovery, yet graph neural networks often fail to capture long-range dependencies. Here, the authors introduce MolGraph-xLSTM, a hierarchical graph-based model that integrates atom-level and motif-level representations with xLSTM to capture long-range intra-molecular dependencies, achieving improved performance across multiple molecular property benchmarks."
    },
    {
      "paperId": "4e1bf3d8991a29154e6defa852f7e12b3d52428c",
      "externalIds": {
        "DBLP": "journals/apin/ChenZHYKZ25",
        "DOI": "10.1007/s10489-025-06912-5",
        "CorpusId": 281729155
      },
      "corpusId": 281729155,
      "title": "TransMambaCC: Integrating Transformer and Pyramid Mamba Network for RGB-T Crowd Counting",
      "venue": "Applied intelligence (Boston)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10489-025-06912-5?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10489-025-06912-5, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2281302829",
          "name": "Yangjian Chen"
        },
        {
          "authorId": "2383362441",
          "name": "Huailin Zhao"
        },
        {
          "authorId": "2109047488",
          "name": "Liangjun Huang"
        },
        {
          "authorId": "2381915025",
          "name": "Yubo Yang"
        },
        {
          "authorId": "2256990831",
          "name": "Wencan Kang"
        },
        {
          "authorId": "2163677542",
          "name": "Jianwei Zhang"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "cf4f38ca2aece95db14762b83a0dfccb27378c60",
      "externalIds": {
        "PubMedCentral": "12562045",
        "DOI": "10.3390/bioengineering12101051",
        "CorpusId": 281735697,
        "PubMed": "41155051"
      },
      "corpusId": 281735697,
      "title": "LG-UNet Based Segmentation and Survival Prediction of Nasopharyngeal Carcinoma Using Multimodal MRI Imaging",
      "venue": "Bioengineering",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12562045, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2233435407",
          "name": "Yuhao Yang"
        },
        {
          "authorId": "2300256015",
          "name": "Junhao Wen"
        },
        {
          "authorId": "2383419148",
          "name": "Tianyi Wu"
        },
        {
          "authorId": "2384115205",
          "name": "Jinrang Dong"
        },
        {
          "authorId": "2358630590",
          "name": "Yunfei Xia"
        },
        {
          "authorId": "2300219368",
          "name": "Yu Zhang"
        }
      ],
      "abstract": "Image segmentation and survival prediction for nasopharyngeal carcinoma (NPC) are crucial for clinical diagnosis and treatment decisions. This study presents an improved 3D-UNet-based model for NPC GTV segmentation, referred to as LG-UNet. The encoder introduces deep strip convolution and channel attention mechanisms to enhance feature extraction while avoiding spatial feature loss and anisotropic constraints. The decoder incorporates Dynamic Large Convolutional Kernel (DLCK) and Global Feature Fusion (GFF) modules to capture multi-scale features and integrate global contextual information, enabling precise segmentation of the tumor GTV in NPC MRI images. Risk prediction is performed on the segmented multi-modal MRI images using the Lung-Net model, with output risk factors combined with clinical data in the Cox model to predict metastatic probabilities for NPC lesions. Experimental results on 442 NPC MRI scans from Sun Yat-sen University Cancer Center showed DSC of 0.8223, accuracy of 0.8235, recall of 0.8297, and HD95 of 1.6807 mm. Compared to the baseline model, the DSC improved by 7.73%, accuracy increased by 4.52%, and recall improved by 3.40%. The combined model\u2019s risk prediction showed C-index values of 0.756, with a 5-year AUC value of 0.789. This model can serve as an auxiliary tool for clinical decision-making in NPC."
    },
    {
      "paperId": "33d089dc2053e03731f565cce8e4fb1ddae27ec3",
      "externalIds": {
        "ArXiv": "2509.24204",
        "DBLP": "journals/corr/abs-2509-24204",
        "DOI": "10.48550/arXiv.2509.24204",
        "CorpusId": 281675028
      },
      "corpusId": 281675028,
      "title": "BALR-SAM: Boundary-Aware Low-Rank Adaptation of SAM for Resource-Efficient Medical Image Segmentation",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.24204, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2383172937",
          "name": "Zelin Liu"
        },
        {
          "authorId": "2384402272",
          "name": "Sicheng Dong"
        },
        {
          "authorId": "2382945831",
          "name": "Bocheng Li"
        },
        {
          "authorId": "2267688628",
          "name": "Yixuan Yang"
        },
        {
          "authorId": "2052357524",
          "name": "Jiacheng Ruan"
        },
        {
          "authorId": "2384313731",
          "name": "Chenxu Zhou"
        },
        {
          "authorId": "9357059",
          "name": "Suncheng Xiang"
        }
      ],
      "abstract": "Vision foundation models like the Segment Anything Model (SAM), pretrained on large-scale natural image datasets, often struggle in medical image segmentation due to a lack of domain-specific adaptation. In clinical practice, fine-tuning such models efficiently for medical downstream tasks with minimal resource demands, while maintaining strong performance, is challenging. To address these issues, we propose BALR-SAM, a boundary-aware low-rank adaptation framework that enhances SAM for medical imaging. It combines three tailored components: (1) a Complementary Detail Enhancement Network (CDEN) using depthwise separable convolutions and multi-scale fusion to capture boundary-sensitive features essential for accurate segmentation; (2) low-rank adapters integrated into SAM's Vision Transformer blocks to optimize feature representation and attention for medical contexts, while simultaneously significantly reducing the parameter space; and (3) a low-rank tensor attention mechanism in the mask decoder, cutting memory usage by 75% and boosting inference speed. Experiments on standard medical segmentation datasets show that BALR-SAM, without requiring prompts, outperforms several state-of-the-art (SOTA) methods, including fully fine-tuned MedSAM, while updating just 1.8% (11.7M) of its parameters."
    },
    {
      "paperId": "9a842663a267406acb013e27b4d6d6416ee8ffc7",
      "externalIds": {
        "DOI": "10.23919/OCEANS59106.2025.11245027",
        "CorpusId": 283270237
      },
      "corpusId": 283270237,
      "title": "Robust Underwater Localization via Sensor Fusion and Transformer-Based Visual Odometry",
      "venue": "Oceans",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.23919/OCEANS59106.2025.11245027?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.23919/OCEANS59106.2025.11245027, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2394396344",
          "name": "Muhammad Waqar Mughal"
        },
        {
          "authorId": "2394378495",
          "name": "Shixin Sun"
        },
        {
          "authorId": "2394396723",
          "name": "Syed Atif Mehdi"
        },
        {
          "authorId": "2239286516",
          "name": "Francesco Maurelli"
        }
      ],
      "abstract": "Precise localization is integral to autonomous underwater navigation, where traditional GPS-based solutions cannot be used. This research presents a robust underwater localization system that integrates transformer-based visual odometry (VO) with inertial and Doppler Velocity Log (DVL) sensor fusion, providing resilient performance in complex and GPS-denied aquatic environments. Our proposed architecture combines recent breakthroughs in deep learning with domain-specific sensor fusion methods, giving a new platform for near-real-time underwater location in divergent depths, lighting intensities, and clarity levels. The system, equipped with the MAMBA model, extracts temporally aware visual features that are resilient to underwater turbidity, dynamic lighting, and texture scarcity. These features are fused with inertial and velocity data to generate the global pose estimates in GPS-denied environments. This research demonstrates that the proposed system increases pose estimation accuracy, consistency, and reliability across diverse aquatic environments by comparing the original trajectory and predicting one by the system."
    },
    {
      "paperId": "1b439c3fd6b97b4dfac29461d5d3b2344597a224",
      "externalIds": {
        "DOI": "10.3390/rs17193328",
        "CorpusId": 281732906
      },
      "corpusId": 281732906,
      "title": "MambaSegNet: A Fast and Accurate High-Resolution Remote Sensing Imagery Ship Segmentation Network",
      "venue": "Remote Sensing",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/rs17193328?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/rs17193328, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2383364502",
          "name": "Runke Wen"
        },
        {
          "authorId": "2383384382",
          "name": "Yongjie Yuan"
        },
        {
          "authorId": "2385495145",
          "name": "Xingyuan Xu"
        },
        {
          "authorId": "2383429723",
          "name": "Shi Yin"
        },
        {
          "authorId": "2383329611",
          "name": "Zegang Chen"
        },
        {
          "authorId": "2224411230",
          "name": "Haibo Zeng"
        },
        {
          "authorId": "2108060265",
          "name": "Zhipan Wang"
        }
      ],
      "abstract": "High-resolution remote sensing imagery is crucial for ship extraction in ocean-related applications. Existing object detection and semantic segmentation methods for ship extraction have limitations: the former cannot precisely obtain ship shapes, while the latter struggles with small targets and complex backgrounds. This study addresses these issues by constructing two datasets, DIOR_SHIP and LEVIR_SHIP, using the SAM model and morphological operations. A novel MambaSegNet is then designed based on the advanced Mamba architecture. It is an encoder\u2013decoder network with MambaLayer and ResMambaBlock for effective multi-scale feature processing. The experiments conducted with seven mainstream models show that the IOU of MambaSegNet is 0.8208, the Accuracy is 0.9176, the Precision is 0.9276, the Recall is 0.9076, and the F1-score is 0.9176. Compared with other models, it acquired the best performance. This research offers a valuable dataset and a novel model for ship extraction, with potential cross-domain application prospects."
    },
    {
      "paperId": "e04f125c82f8889963ebe76f0ca5f95de276d4c4",
      "externalIds": {
        "DOI": "10.5753/eniac.2025.14516",
        "CorpusId": 283583604
      },
      "corpusId": 283583604,
      "title": "ModBERTBr: A ModernBERT-based Model for Brazilian Portuguese",
      "venue": "Anais do XXII Encontro Nacional de Intelig\u00eancia Artificial e Computacional (ENIAC 2025)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.5753/eniac.2025.14516?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.5753/eniac.2025.14516, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2396665578",
          "name": "Wallace Ben Teng Lin Wu"
        },
        {
          "authorId": "2398742938",
          "name": "Luis Paulo Faina Garcia"
        }
      ],
      "abstract": "A key model in the Large Language Model (LLM) field is the Bidirectional Encoder Representations from Transformers (BERT), known for its effectiveness and versatility. The current state-of-the-art variant of BERT is ModernBERT, and despite excelling in efficiency and performance, it is limited to English. This paper addresses this notable gap by introducing ModBERTBr, a novel pre-trained model based on the ModernBERT architecture, which is explicitly tailored for Brazilian Portuguese and incorporates cutting-edge research and technologies. Through both intrinsic and extrinsic evaluations, ModBERTBr was assessed against multiple baseline models, showing consistent improvements and competitive performance compared to its predecessors."
    },
    {
      "paperId": "40cb5778cf56ef71c35da5cd4e6893288e3e43ab",
      "externalIds": {
        "DOI": "10.1101/2025.09.25.25336636",
        "CorpusId": 281666826
      },
      "corpusId": 281666826,
      "title": "Semantic Segmentation of Sleep Events for High-Resolution Sleep Decoding",
      "venue": "medRxiv",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1101/2025.09.25.25336636?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1101/2025.09.25.25336636, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2223200259",
          "name": "Xiaoyu Bao"
        },
        {
          "authorId": "2355642925",
          "name": "Man Li"
        },
        {
          "authorId": "2355784352",
          "name": "Di Chen"
        },
        {
          "authorId": "2302888702",
          "name": "Zijian Wang"
        },
        {
          "authorId": "2350137736",
          "name": "Qiyun Huang"
        },
        {
          "authorId": "2387424834",
          "name": "Zhenfu Wen"
        },
        {
          "authorId": "2340230292",
          "name": "Yuanqing Li"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "276b7ce28c5f2334224225a8dbe8307d7dc78967",
      "externalIds": {
        "ArXiv": "2509.23640",
        "DBLP": "journals/corr/abs-2509-23640",
        "DOI": "10.48550/arXiv.2509.23640",
        "CorpusId": 281676024
      },
      "corpusId": 281676024,
      "title": "EfficientMIL: Efficient Linear-Complexity MIL Method for WSI Classification",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.23640, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2382929788",
          "name": "Chengying She"
        },
        {
          "authorId": "2384800943",
          "name": "Ben Wang"
        },
        {
          "authorId": "2382924068",
          "name": "Xinran Zhang"
        },
        {
          "authorId": "2382929441",
          "name": "Dongjie Fan"
        },
        {
          "authorId": "2298729742",
          "name": "Jialu Zhang"
        },
        {
          "authorId": "2317507789",
          "name": "Chengwei Chen"
        },
        {
          "authorId": "2280885175",
          "name": "Lizhuang Liu"
        }
      ],
      "abstract": "Whole slide images (WSIs) classification represents a fundamental challenge in computational pathology, where multiple instance learning (MIL) has emerged as the dominant paradigm. Current state-of-the-art (SOTA) MIL methods rely on attention mechanisms, achieving good performance but requiring substantial computational resources due to quadratic complexity when processing hundreds of thousands of patches. To address this computational bottleneck, we introduce EfficientMIL, a novel linear-complexity MIL approach for WSIs classification with the patches selection module Adaptive Patch Selector (APS) that we designed, replacing the quadratic-complexity self-attention mechanisms in Transformer-based MIL methods with efficient sequence models including RNN-based GRU, LSTM, and State Space Model (SSM) Mamba. EfficientMIL achieves significant computational efficiency improvements while outperforming other MIL methods across multiple histopathology datasets. On TCGA-Lung dataset, EfficientMIL-Mamba achieved AUC of 0.976 and accuracy of 0.933, while on CAMELYON16 dataset, EfficientMIL-GRU achieved AUC of 0.990 and accuracy of 0.975, surpassing previous state-of-the-art methods. Extensive experiments demonstrate that APS is also more effective for patches selection than conventional selection strategies."
    },
    {
      "paperId": "e23885d06cd2864134aaa7aa8520170a4aab14af",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-24118",
        "ArXiv": "2509.24118",
        "DOI": "10.1145/3765612.3767245",
        "CorpusId": 281674239
      },
      "corpusId": 281674239,
      "title": "HyMaTE: A Hybrid Mamba and Transformer Model for EHR Representation Learning",
      "venue": "Proceedings of the 16th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.24118, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "71104706",
          "name": "Md Mozaharul Mottalib"
        },
        {
          "authorId": "2316970422",
          "name": "Thao-Ly T. Phan"
        },
        {
          "authorId": "2281794002",
          "name": "R. Beheshti"
        }
      ],
      "abstract": "Electronic health Records (EHRs) have become a cornerstone in modern-day healthcare. They are a crucial part for analyzing the progression of patient health; however, their complexity, characterized by long, multivariate sequences, sparsity, and missing values-poses significant challenges in traditional deep learning modeling. While Transformer-based models have demonstrated success in modeling EHR data and predicting clinical outcomes, their quadratic computational complexity and limited context length hinder their efficiency and practical applications. On the other hand, State Space Models (SSMs) like Mamba present a promising alternative offering linear-time sequence modeling and improved efficiency for handling long sequences, but focus mostly on mixing sequence-level information rather than channel-level data. To overcome these challenges, we propose HyMaTE (A Hybrid Mamba and Transformer Model for EHR Representation Learning), a novel hybrid model tailored for representing longitudinal data, combining the strengths of SSMs with advanced attention mechanisms. By testing the model on predictive tasks on multiple clinical datasets, we demonstrate HyMaTE's ability to capture an effective, richer, and more nuanced unified representation of EHR data. Additionally, the interpretability of the outcomes achieved by self-attention illustrates the effectiveness of our model as a scalable and generalizable solution for real-world healthcare applications. Codes are available at: https://github.com/healthylaife/HyMaTE."
    },
    {
      "paperId": "ceec5d52c7cc86d214f17d0e47a2dcab9a7dd20e",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-23601",
        "ArXiv": "2509.23601",
        "DOI": "10.48550/arXiv.2509.23601",
        "CorpusId": 281674040
      },
      "corpusId": 281674040,
      "title": "VAMamba: An Efficient Visual Adaptive Mamba for Image Restoration",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.23601, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2382988617",
          "name": "Han Hu"
        },
        {
          "authorId": "2382988255",
          "name": "Zhuoran Zheng"
        },
        {
          "authorId": "2383317805",
          "name": "Liang Li"
        },
        {
          "authorId": "2355862757",
          "name": "Chen Lyu"
        }
      ],
      "abstract": "Recent Mamba-based image restoration methods have achieved promising results but remain limited by fixed scanning patterns and inefficient feature utilization. Conventional Mamba architectures rely on predetermined paths that cannot adapt to diverse degradations, constraining both restoration performance and computational efficiency. To overcome these limitations, we propose VAMamba, a Visual Adaptive Mamba framework with two key innovations. First, QCLAM(Queue-basedCacheLow-rankAdaptiveMemory)enhancesfeaturelearningthrougha FIFO cache that stores historical representations. Similarity between current LoRA-adapted and cached features guides intelligent fusion, enabling dynamic reuse while effectively controlling memorygrowth.Second, GPS-SS2D(GreedyPathScanSS2D)introducesadaptive scanning. A Vision Transformer generates score maps to estimate pixel importance, and a greedy strategy de termines optimal forward and backward scanning paths. These learned trajectories replace rigid patterns, enabling SS2D to perform targeted feature extraction. The integration of QCLAM and GPS-SS2D allows VAMamba to adaptively focus on degraded regions while maintaining high computational efficiency. Extensive experiments across diverse restoration tasks demonstrate that VAMamba consistently outperforms existing approaches in both restoration quality and efficiency, establishing new benchmarks for adaptive image restoration. Our code is available at https://github.com/WaterHQH/VAMamba."
    },
    {
      "paperId": "7fa35da89b968600eb8072759006b5eefb211a9f",
      "externalIds": {
        "ArXiv": "2509.24074",
        "DBLP": "journals/corr/abs-2509-24074",
        "DOI": "10.48550/arXiv.2509.24074",
        "CorpusId": 281676418
      },
      "corpusId": 281676418,
      "title": "ResFormer: All-Time Reservoir Memory for Long Sequence Classification",
      "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.24074, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2325544427",
          "name": "Hongbo Liu"
        },
        {
          "authorId": "2382929056",
          "name": "Jia Xu"
        }
      ],
      "abstract": "Sequence classification is essential in NLP for understanding and categorizing language patterns in tasks like sentiment analysis, intent detection, and topic classification. Transformer-based models, despite achieving state-of-the-art performance, have inherent limitations due to quadratic time and memory complexity, restricting their input length. Although extensive efforts have aimed at reducing computational demands, processing extensive contexts remains challenging. To overcome these limitations, we propose ResFormer, a novel neural network architecture designed to model varying context lengths efficiently through a cascaded methodology. ResFormer integrates an reservoir computing network featuring a nonlinear readout to effectively capture long-term contextual dependencies in linear time. Concurrently, short-term dependencies within sentences are modeled using a conventional Transformer architecture with fixed-length inputs. Experiments demonstrate that ResFormer significantly outperforms baseline models of DeepSeek-Qwen and ModernBERT, delivering an accuracy improvement of up to +22.3% on the EmoryNLP dataset and consistent gains on MultiWOZ, MELD, and IEMOCAP. In addition, ResFormer exhibits reduced memory consumption, underscoring its effectiveness and efficiency in modeling extensive contextual information."
    },
    {
      "paperId": "b7d6bc0c1ff8a959f106556afad6dca2592275ce",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-23867",
        "ArXiv": "2509.23867",
        "DOI": "10.48550/arXiv.2509.23867",
        "CorpusId": 281676257
      },
      "corpusId": 281676257,
      "title": "Sim-DETR: Unlock DETR for Temporal Sentence Grounding",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.23867, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2224559104",
          "name": "Jiajin Tang"
        },
        {
          "authorId": "2371146782",
          "name": "Zhengxuan Wei"
        },
        {
          "authorId": "2311419127",
          "name": "Yuchen Zhu"
        },
        {
          "authorId": "2113917364",
          "name": "Cheng Shi"
        },
        {
          "authorId": "2255423082",
          "name": "Guanbin Li"
        },
        {
          "authorId": "2304638252",
          "name": "Liang Lin"
        },
        {
          "authorId": "2297336257",
          "name": "Sibei Yang"
        }
      ],
      "abstract": "Temporal sentence grounding aims to identify exact moments in a video that correspond to a given textual query, typically addressed with detection transformer (DETR) solutions. However, we find that typical strategies designed to enhance DETR do not improve, and may even degrade, its performance in this task. We systematically analyze and identify the root causes of this abnormal behavior: (1) conflicts between queries from similar target moments and (2) internal query conflicts due to the tension between global semantics and local localization. Building on these insights, we propose a simple yet powerful baseline, Sim-DETR, which extends the standard DETR with two minor modifications in the decoder layers: (1) constraining self-attention between queries based on their semantic and positional overlap and (2) adding query-to-frame alignment to bridge the global and local contexts. Experiments demonstrate that Sim-DETR unlocks the full potential of DETR for temporal sentence grounding, offering a strong baseline for future research."
    },
    {
      "paperId": "e090629258afb667d7c5064311df7cc9917da603",
      "externalIds": {
        "ArXiv": "2509.23722",
        "DBLP": "journals/corr/abs-2509-23722",
        "DOI": "10.48550/arXiv.2509.23722",
        "CorpusId": 281675179
      },
      "corpusId": 281675179,
      "title": "AdaPtis: Reducing Pipeline Bubbles with Adaptive Pipeline Parallelism on Heterogeneous Models",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.23722, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2382934309",
          "name": "Jihu Guo"
        },
        {
          "authorId": "2385491276",
          "name": "Tenghui Ma"
        },
        {
          "authorId": "2383450890",
          "name": "Wei Gao"
        },
        {
          "authorId": "2300809135",
          "name": "Peng Sun"
        },
        {
          "authorId": "2293571092",
          "name": "Jiaxing Li"
        },
        {
          "authorId": "2293554318",
          "name": "Xun Chen"
        },
        {
          "authorId": "2383044267",
          "name": "Yuyang Jin"
        },
        {
          "authorId": "2384411602",
          "name": "Dahua Lin"
        }
      ],
      "abstract": "Pipeline parallelism is widely used to train large language models (LLMs). However, increasing heterogeneity in model architectures exacerbates pipeline bubbles, thereby reducing training efficiency. Existing approaches overlook the co-optimization of model partition, model placement, and workload scheduling, resulting in limited efficiency improvement or even performance degradation. To respond, we propose AdaPtis, an LLM training system that supports adaptive pipeline parallelism. First, we develop a pipeline performance model to accurately estimate training throughput. Second, AdaPtis jointly optimizes model partition, model placement, and workload scheduling policies guided by this performance model. Third, we design a unified pipeline executor that efficiently supports the execution of diverse pipeline strategies. Extensive experiments show that AdaPtis achieves an average speedup of 1.42x (up to 2.14x) over Megatron-LM I-1F1B across various LLM architectures and scales."
    },
    {
      "paperId": "67be475b984dff7b46577c67b1b8722351333f33",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-23677",
        "ArXiv": "2509.23677",
        "DOI": "10.48550/arXiv.2509.23677",
        "CorpusId": 281676057
      },
      "corpusId": 281676057,
      "title": "MSD-KMamba: Bidirectional Spatial-Aware Multi-Modal 3D Brain Segmentation via Multi-scale Self-Distilled Fusion Strategy",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.23677, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2262049506",
          "name": "Dayu Tan"
        },
        {
          "authorId": "2383089804",
          "name": "Ziwei Zhang"
        },
        {
          "authorId": "2383422477",
          "name": "Yansan Su"
        },
        {
          "authorId": "2382396170",
          "name": "Xin Peng"
        },
        {
          "authorId": "2387134361",
          "name": "Yike Dai"
        },
        {
          "authorId": "2262092122",
          "name": "Chunhou Zheng"
        },
        {
          "authorId": "2151195907",
          "name": "Weimin Zhong"
        }
      ],
      "abstract": "Numerous CNN-Transformer hybrid models rely on high-complexity global attention mechanisms to capture long-range dependencies, which introduces non-linear computational complexity and leads to significant resource consumption. Although knowledge distillation and sparse attention mechanisms can improve efficiency, they often fall short of delivering the high segmentation accuracy necessary for complex tasks. Balancing model performance with computational efficiency remains a critical challenge. In this work, we propose a novel 3D multi-modal image segmentation framework, termed MSD-KMamba, which integrates bidirectional spatial perception with multi-scale self-distillation. The bidirectional spatial aware branch effectively captures long-range spatial context dependencies across brain regions, while also incorporating a powerful nonlinear feature extraction mechanism that further enhances the model's ability to learn complex and heterogeneous patterns. In addition, the proposed multi-scale self-distilled fusion strategy strengthens hierarchical feature representations and improves the transfer of semantic information at different resolution levels. By jointly leveraging the bidirectional spatial perception branch and the multi-scale self-distilled fusion strategy, our framework effectively mitigates the bottleneck of quadratic computational complexity in volumetric segmentation, while simultaneously addressing the limitation of insufficient global perception. Extensive experiments on multiple standard benchmark datasets demonstrate that MSD-KMamba consistently outperforms state-of-the-art methods in segmentation accuracy, robustness, and generalization, while maintaining high computational efficiency and favorable scalability. The source code of MSD-KMamba is publicly available at https://github.com/daimao-zhang/MSD-KMamba."
    },
    {
      "paperId": "cc55306fc5d10b30f92a6036b7b91f14dc550063",
      "externalIds": {
        "ArXiv": "2510.03279",
        "DBLP": "journals/corr/abs-2510-03279",
        "DOI": "10.48550/arXiv.2510.03279",
        "CorpusId": 281842790
      },
      "corpusId": 281842790,
      "title": "MemMamba: Rethinking Memory Patterns in State Space Model",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.03279, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2384411877",
          "name": "Youjin Wang"
        },
        {
          "authorId": "2384763277",
          "name": "Yangjingyi Chen"
        },
        {
          "authorId": "2384505502",
          "name": "Jiahao Yan"
        },
        {
          "authorId": "2385108455",
          "name": "Jiaxuan Lu"
        },
        {
          "authorId": "2384334955",
          "name": "Xiao Sun"
        }
      ],
      "abstract": "With the explosive growth of data, long-sequence modeling has become increasingly important in tasks such as natural language processing and bioinformatics. However, existing methods face inherent trade-offs between efficiency and memory. Recurrent neural networks suffer from gradient vanishing and explosion, making them hard to scale. Transformers can model global dependencies but are constrained by quadratic complexity. Recently, selective state-space models such as Mamba have demonstrated high efficiency with O(n) time and O(1) recurrent inference, yet their long-range memory decays exponentially. In this work, we conduct mathematical derivations and information-theoretic analysis to systematically uncover the memory decay mechanism of Mamba, answering a fundamental question: what is the nature of Mamba's long-range memory and how does it retain information? To quantify key information loss, we further introduce horizontal-vertical memory fidelity metrics that capture degradation both within and across layers. Inspired by how humans distill and retain salient information when reading long documents, we propose MemMamba, a novel architectural framework that integrates state summarization mechanism together with cross-layer and cross-token attention, which alleviates long-range forgetting while preserving linear complexity. MemMamba achieves significant improvements over existing Mamba variants and Transformers on long-sequence benchmarks such as PG19 and Passkey Retrieval, while delivering a 48% speedup in inference efficiency. Both theoretical analysis and empirical results demonstrate that MemMamba achieves a breakthrough in the complexity-memory trade-off, offering a new paradigm for ultra-long sequence modeling."
    },
    {
      "paperId": "65e2cfa605d95642d40dc3336f7c8c8b094eec2f",
      "externalIds": {
        "DOI": "10.53941/tai.2025.100015",
        "CorpusId": 282113040
      },
      "corpusId": 282113040,
      "title": "SUGAR: A Sequence Unfolding Based Transformer Model for Group Activity Recognition",
      "venue": "Transactions on Artificial Intelligence",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.53941/tai.2025.100015?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.53941/tai.2025.100015, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2385883968",
          "name": "Yash Gondkar"
        },
        {
          "authorId": "2342473557",
          "name": "Chengjie Zheng"
        },
        {
          "authorId": "2399736485",
          "name": "Yumeng Yang"
        },
        {
          "authorId": "48004993",
          "name": "Shiqian Shen"
        },
        {
          "authorId": "2113831236",
          "name": "Wei Ding"
        },
        {
          "authorId": "2261898455",
          "name": "Ping Chen"
        }
      ],
      "abstract": "Deep learning models built upon Transformer architectures have led to substantial advancements in sequential data analysis. Nevertheless, their direct application to video-based tasks, such as Group Activity Recognition (GAR), remains constrained by the quadratic computational complexity and excessive memory requirements of global self-attention, especially when handling long video sequences. To overcome these limitations, we propose SUGAR: A Sequence Unfolding Based Transformer Model for Group Activity Recognition. Our approach introduces a novel sequence unfolding and folding mechanism that partitions long video sequences into overlapping local windows, enabling the model to concentrate attention within compact temporal regions. This local attention design dramatically reduces computational cost and memory footprint while maintaining high recognition accuracy. Within the Bi-Causal framework, SUGAR replaces conventional Transformer blocks, and experimental results on the Volleyball dataset demonstrate that our model achieves state-of-the-art performance, consistently exceeding 93% accuracy, with significantly improved efficiency. In addition, we investigate Lightning Attention 2 as an alternative linear-complexity attention module, identifying practical challenges such as increased memory usage and unstable convergence. To ensure robustness and training stability, we incorporate a dedicated safety mechanism that mitigates these issues. In summary, SUGAR offers a scalable, resource-efficient solution for group activity analysis in videos and exhibits strong potential for broader applications involving lengthy sequential data in computer vision and bioinformatics."
    },
    {
      "paperId": "ba620823850dbf7d823746fc9395aa06df19f005",
      "externalIds": {
        "DOI": "10.15625/1813-9663/22770",
        "CorpusId": 281858090
      },
      "corpusId": 281858090,
      "title": "Mamba-MHAR: An efficient multimodal framework for human action recognition",
      "venue": "Journal of Computer Science and Cybernetics",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.15625/1813-9663/22770?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.15625/1813-9663/22770, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2067953701",
          "name": "Trung-Hieu Le"
        },
        {
          "authorId": "2267747797",
          "name": "Thai-Khanh Nguyen"
        },
        {
          "authorId": "2384332886",
          "name": "Tuan-Anh Le"
        },
        {
          "authorId": "2384219449",
          "name": "Mathieu Delalandre"
        },
        {
          "authorId": "2384225253",
          "name": "Kien Tran Trung"
        },
        {
          "authorId": "2158145150",
          "name": "T. Tran"
        },
        {
          "authorId": "2269733398",
          "name": "Cuong Pham"
        }
      ],
      "abstract": "Human Action Recognition (HAR) has emerged as an active research domain in recent years with wide-ranging applications in healthcare monitoring, smart home systems, and hu- man\u2013robot interaction. This paper introduces a method, namely Mamba-MHAR (Mamba based Multimodal Human Action Recognition), a lightweight multimodal architecture aimed at improv- ing HAR performance by effectively integrating data from inertial sensors and egocentric videos. Mamba-MHAR consists of double Mamba-based branches, one for visual feature extraction - VideoMamba, and the other for motion feature extraction - MAMC. Both branches are built upon recently introduced Selective State Space Models (SSMs) to optimize the computational cost, and they are lately fused for final human activity classification. Mamba-MHAR achieves significant efficiency gains in terms of GPU usage, making it highly suitable for real-time deployment on edge and mobile devices. Extensive experiments were conducted on two challenging multimodal datasets UESTC-MMEA-CL and MuWiGes, which contain synchronized IMU and video data recorded in natural settings. The proposed Mamba-MHAR achieves 98.00% accuracy on UESTC-MMEA-CL and 98.58% on MuWiGes, surpassing state-of-the-art baselines. These results demonstrate that a simple yet efficient fusion of multimodal lightweight Mamba-based models provides a promising solution for scalable and low-power applications in pervasive computing environments."
    },
    {
      "paperId": "9db88a11830f397bfc5603f6275d5c6823a6915c",
      "externalIds": {
        "ArXiv": "2509.23310",
        "DBLP": "journals/corr/abs-2509-23310",
        "DOI": "10.48550/arXiv.2509.23310",
        "CorpusId": 281676448
      },
      "corpusId": 281676448,
      "title": "Balanced Diffusion-Guided Fusion for Multimodal Remote Sensing Classification",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.23310, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2280484156",
          "name": "Hao Liu"
        },
        {
          "authorId": "1391155448",
          "name": "Yongjie Zheng"
        },
        {
          "authorId": "2325113883",
          "name": "Yuhan Kang"
        },
        {
          "authorId": "2108147319",
          "name": "Mingyang Zhang"
        },
        {
          "authorId": "2284983858",
          "name": "Maoguo Gong"
        },
        {
          "authorId": "2382933689",
          "name": "Lorenzo Bruzzone"
        }
      ],
      "abstract": "Deep learning-based techniques for the analysis of multimodal remote sensing data have become popular due to their ability to effectively integrate complementary spatial, spectral, and structural information from different sensors. Recently, denoising diffusion probabilistic models (DDPMs) have attracted attention in the remote sensing community due to their powerful ability to capture robust and complex spatial-spectral distributions. However, pre-training multimodal DDPMs may result in modality imbalance, and effectively leveraging diffusion features to guide complementary diversity feature extraction remains an open question. To address these issues, this paper proposes a balanced diffusion-guided fusion (BDGF) framework that leverages multimodal diffusion features to guide a multi-branch network for land-cover classification. Specifically, we propose an adaptive modality masking strategy to encourage the DDPMs to obtain a modality-balanced rather than spectral image-dominated data distribution. Subsequently, these diffusion features hierarchically guide feature extraction among CNN, Mamba, and transformer networks by integrating feature fusion, group channel attention, and cross-attention mechanisms. Finally, a mutual learning strategy is developed to enhance inter-branch collaboration by aligning the probability entropy and feature similarity of individual subnetworks. Extensive experiments on four multimodal remote sensing datasets demonstrate that the proposed method achieves superior classification performance. The code is available at https://github.com/HaoLiu-XDU/BDGF."
    },
    {
      "paperId": "4e7b5a975ca648c9abb5f4bbce2762be0c7a881c",
      "externalIds": {
        "ArXiv": "2509.23040",
        "DBLP": "journals/corr/abs-2509-23040",
        "DOI": "10.48550/arXiv.2509.23040",
        "CorpusId": 281676451
      },
      "corpusId": 281676451,
      "title": "Look Back to Reason Forward: Revisitable Memory for Long-Context LLM Agents",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.23040, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2261094053",
          "name": "Yaorui Shi"
        },
        {
          "authorId": "2258784735",
          "name": "Yuxin Chen"
        },
        {
          "authorId": "2383937007",
          "name": "Siyuan Wang"
        },
        {
          "authorId": "2261249035",
          "name": "Sihang Li"
        },
        {
          "authorId": "2290177128",
          "name": "Hengxing Cai"
        },
        {
          "authorId": "2383008230",
          "name": "Qi Gu"
        },
        {
          "authorId": "2317277586",
          "name": "Xiang Wang"
        },
        {
          "authorId": "2322619190",
          "name": "An Zhang"
        }
      ],
      "abstract": "Large language models face challenges in long-context question answering, where key evidence of a query may be dispersed across millions of tokens. Existing works equip large language models with a memory corpus that is dynamically updated during a single-pass document scan, also known as the\"memorize while reading\"methods. While this approach scales efficiently, it suffers from irreversible forward-only processing, information loss through overwriting, and sparse reinforcement learning signals. To tackle these challenges, we present ReMemR1, a memory-augmented agent with callback-enhanced memory that allows selective retrieval from the entire memory history and allows non-linear reasoning and revisiting of early evidence. To further strengthen training, we propose Reinforcement Learning with Multi-Level Rewards (RLMLR), which combines final-answer rewards with dense, step-level signals that guide effective memory use. Together, these contributions mitigate information degradation, improve supervision, and support multi-hop memory utilizing. Experiments on long-document QA show significant gains over existing memory-based approaches, which validates ReMemR1 as an effective solution for long-context reasoning agents."
    },
    {
      "paperId": "5edc0932da4b00ed0bd6fdbfd1af8efc1f846c75",
      "externalIds": {
        "ArXiv": "2510.03272",
        "DBLP": "journals/corr/abs-2510-03272",
        "DOI": "10.48550/arXiv.2510.03272",
        "CorpusId": 281842932
      },
      "corpusId": 281842932,
      "title": "PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.03272, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2363597542",
          "name": "Yukun Zhang"
        },
        {
          "authorId": "2363598820",
          "name": "Xueqing Zhou"
        }
      ],
      "abstract": "We propose PDE-Transformer, a novel sequence modeling paradigm that casts the forward pass of a Transformer as the numerical discretization of a continuous reaction-diffusion system derived from a variational energy functional. In our framework, token embeddings evolve under a partial differential equation whose nonlocal integral term models self-attention, local reaction term models feed-forward layers, diffusion term encodes positional smoothing, and a stability control term corresponds to layer normalization. From this unifying perspective, we design an Adaptive PDE Diffusion Layer-an efficient, learnable finite-difference stencil that enforces local smoothness in feature space with linear time complexity and complements self-attention's global routing. Through a systematic theoretical analysis based on four pillars:stability, diffusion geometry, multi-scale dynamics, and component coupling, we derive principled guidelines for integrating the PDE layer at seven candidate points in the Transformer. Empirically, on the Long Range Arena benchmark, placing the layer immediately after embedding yields a 4.1 pp average accuracy gain over a strong baseline, and an adaptive multi-scale variant delivers further improvements. Our work thus offers a principled, lightweight mechanism to bolster long-range dependency modeling by harmonizing continuous PDE smoothing with discrete self-attention."
    },
    {
      "paperId": "146677a3e91c233acaa0d237f2a4ca02fa9dc647",
      "externalIds": {
        "ArXiv": "2509.23184",
        "CorpusId": 281674416
      },
      "corpusId": 281674416,
      "title": "PonderLM-2: Pretraining LLM with Latent Thoughts in Continuous Space",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.23184, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2272843041",
          "name": "Boyi Zeng"
        },
        {
          "authorId": "2363741783",
          "name": "He Li"
        },
        {
          "authorId": "2363672366",
          "name": "Shixiang Song"
        },
        {
          "authorId": "2358508797",
          "name": "Yixuan Wang"
        },
        {
          "authorId": "2165399443",
          "name": "Ziwei He"
        },
        {
          "authorId": "2363687606",
          "name": "Xinbing Wang"
        },
        {
          "authorId": "2257801558",
          "name": "Zhouhan Lin"
        }
      ],
      "abstract": "The remarkable success of Chain-of-Thought (CoT), which enhances performance by scaling generation steps at test-time, inspires us to ask: can we leverage a similar scaling of computational steps during pretraining to improve the generation of each individual token? To address this, we propose a novel pre-training methodology: Pretraining Language Models with Latent Thoughts (PonderLM-2). Our approach pretrains a language model (LM) to first generate an intermediate latent thought-the last hidden state of the current position-which is then used as input to predict the actual subsequent token. This additional computational step enables the LM to refine its prediction within unconstrained continuous space. Our experiments demonstrate that, at an identical inference cost, a LM that generates one additional latent thought per token outperforms a standard model with double the parameters. For instance, our PonderLM-2-Pythia-1.4B, pretrained on 300B tokens from the Pile, significantly surpasses the vanilla Pythia-2.8B trained on the same data on both language modeling and a range of general downstream tasks. Furthermore, increasing the number of latent thoughts generated before each actual token-forming a chain analogous to CoT-consistently improves the model's performance."
    },
    {
      "paperId": "a8b382e80d6b0486e04fef225ba5ae864f283d60",
      "externalIds": {
        "DBLP": "journals/tjs/ZhaoDZYJ25",
        "DOI": "10.1007/s11227-025-07888-2",
        "CorpusId": 281690330
      },
      "corpusId": 281690330,
      "title": "Dsrn-svmamba: a dual-stream recursive network base on SVMamba for scene text recognition",
      "venue": "Journal of Supercomputing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11227-025-07888-2?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11227-025-07888-2, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2292975837",
          "name": "Yingnan Zhao"
        },
        {
          "authorId": "2367924324",
          "name": "Fangqi Ding"
        },
        {
          "authorId": "2383249604",
          "name": "Dewen Zhang"
        },
        {
          "authorId": "2378718369",
          "name": "Zuguo Yang"
        },
        {
          "authorId": "2383069232",
          "name": "Jielin Jiang"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "6e9b10bb88846acd86cde8ea7f8c797c4cd3a7bb",
      "externalIds": {
        "ArXiv": "2509.21716",
        "DBLP": "journals/corr/abs-2509-21716",
        "DOI": "10.48550/arXiv.2509.21716",
        "CorpusId": 281658957
      },
      "corpusId": 281658957,
      "title": "A Unifying Framework for Parallelizing Sequential Models with Linear Dynamical Systems",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.21716, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2313640658",
          "name": "Xavier Gonzalez"
        },
        {
          "authorId": "2336877105",
          "name": "E. K. Buchanan"
        },
        {
          "authorId": "2336917675",
          "name": "Hyun Dong Lee"
        },
        {
          "authorId": "2382829179",
          "name": "Jerry Weihong Liu"
        },
        {
          "authorId": "2382922484",
          "name": "Ke Alexander Wang"
        },
        {
          "authorId": "2344839607",
          "name": "David M. Zoltowski"
        },
        {
          "authorId": "2324791413",
          "name": "Christopher R'e"
        },
        {
          "authorId": "2342841",
          "name": "Scott W. Linderman"
        }
      ],
      "abstract": "Harnessing parallelism in seemingly sequential models is a central challenge for modern machine learning. Several approaches have been proposed for evaluating sequential processes in parallel using fixed-point methods, like Newton, Picard, and Jacobi iterations. In this work, we show that these methods can be understood within a common framework based on linear dynamical systems (LDSs), where different iteration schemes arise naturally as approximate linearizations of a nonlinear recursion. This unifying view highlights shared principles behind these techniques and clarifies when particular fixed-point methods are most likely to be effective. By bridging diverse algorithms through the language of LDSs, our framework provides a clearer theoretical foundation for parallelizing sequential models and points toward new opportunities for efficient and scalable computation."
    },
    {
      "paperId": "ba7d2126093968a055e8de70b623d2d3470c6b49",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-22284",
        "ArXiv": "2509.22284",
        "DOI": "10.48550/arXiv.2509.22284",
        "CorpusId": 281659371
      },
      "corpusId": 281659371,
      "title": "Structured Sparse Transition Matrices to Enable State Tracking in State-Space Models",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.22284, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2337691331",
          "name": "Aleksandar Terzi'c"
        },
        {
          "authorId": "2269732701",
          "name": "Nicolas Menet"
        },
        {
          "authorId": "50828564",
          "name": "Michael Hersche"
        },
        {
          "authorId": "2275082093",
          "name": "Thomas Hofmann"
        },
        {
          "authorId": "2259963752",
          "name": "Abbas Rahimi"
        }
      ],
      "abstract": "Modern state-space models (SSMs) often utilize transition matrices which enable efficient computation but pose restrictions on the model's expressivity, as measured in terms of the ability to emulate finite-state automata (FSA). While unstructured transition matrices are optimal in terms of expressivity, they come at a prohibitively high compute and memory cost even for moderate state sizes. We propose a structured sparse parametrization of transition matrices in SSMs that enables FSA state tracking with optimal state size and depth, while keeping the computational cost of the recurrence comparable to that of diagonal SSMs. Our method, PD-SSM, parametrizes the transition matrix as the product of a column one-hot matrix ($P$) and a complex-valued diagonal matrix ($D$). Consequently, the computational cost of parallel scans scales linearly with the state size. Theoretically, the model is BIBO-stable and can emulate any $N$-state FSA with one layer of dimension $N$ and a linear readout of size $N \\times N$, significantly improving on all current structured SSM guarantees. Experimentally, the model significantly outperforms a wide collection of modern SSM variants on various FSA state tracking tasks. On multiclass time-series classification, the performance is comparable to that of neural controlled differential equations, a paradigm explicitly built for time-series analysis. Finally, we integrate PD-SSM into a hybrid Transformer-SSM architecture and demonstrate that the model can effectively track the states of a complex FSA in which transitions are encoded as a set of variable-length English sentences. The code is available at https://github.com/IBM/expressive-sparse-state-space-model"
    },
    {
      "paperId": "6c6596e9a8a796bc6b9f54f1d606ed00b99b9ee1",
      "externalIds": {
        "ArXiv": "2509.22307",
        "DBLP": "journals/corr/abs-2509-22307",
        "DOI": "10.48550/arXiv.2509.22307",
        "CorpusId": 281658244
      },
      "corpusId": 281658244,
      "title": "Johnson-Lindenstrauss Lemma Guided Network for Efficient 3D Medical Segmentation",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.22307, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2239169173",
          "name": "Jinpeng Lu"
        },
        {
          "authorId": "2239154076",
          "name": "Linghan Cai"
        },
        {
          "authorId": "2183975210",
          "name": "Yinda Chen"
        },
        {
          "authorId": "2376273285",
          "name": "Guo Tang"
        },
        {
          "authorId": "2293655391",
          "name": "Songhan Jiang"
        },
        {
          "authorId": "2378207964",
          "name": "Haoyuan Shi"
        },
        {
          "authorId": "2293298085",
          "name": "Zhiwei Xiong"
        }
      ],
      "abstract": "Lightweight 3D medical image segmentation remains constrained by a fundamental\"efficiency / robustness conflict\", particularly when processing complex anatomical structures and heterogeneous modalities. In this paper, we study how to redesign the framework based on the characteristics of high-dimensional 3D images, and explore data synergy to overcome the fragile representation of lightweight methods. Our approach, VeloxSeg, begins with a deployable and extensible dual-stream CNN-Transformer architecture composed of Paired Window Attention (PWA) and Johnson-Lindenstrauss lemma-guided convolution (JLC). For each 3D image, we invoke a\"glance-and-focus\"principle, where PWA rapidly retrieves multi-scale information, and JLC ensures robust local feature extraction with minimal parameters, significantly enhancing the model's ability to operate with low computational budget. Followed by an extension of the dual-stream architecture that incorporates modal interaction into the multi-scale image-retrieval process, VeloxSeg efficiently models heterogeneous modalities. Finally, Spatially Decoupled Knowledge Transfer (SDKT) via Gram matrices injects the texture prior extracted by a self-supervised network into the segmentation network, yielding stronger representations than baselines at no extra inference cost. Experimental results on multimodal benchmarks show that VeloxSeg achieves a 26% Dice improvement, alongside increasing GPU throughput by 11x and CPU by 48x. Codes are available at https://github.com/JinPLu/VeloxSeg."
    },
    {
      "paperId": "929093c57e1016d249d1a8659f7646b381fe8141",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-21887",
        "ArXiv": "2509.21887",
        "DOI": "10.48550/arXiv.2509.21887",
        "CorpusId": 281658383
      },
      "corpusId": 281658383,
      "title": "StableDub: Taming Diffusion Prior for Generalized and Efficient Visual Dubbing",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.21887, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2108547049",
          "name": "Liyang Chen"
        },
        {
          "authorId": "2385437726",
          "name": "Tianze Zhou"
        },
        {
          "authorId": "2290834920",
          "name": "Xu He"
        },
        {
          "authorId": "2257042466",
          "name": "Boshi Tang"
        },
        {
          "authorId": "2316992606",
          "name": "Zhiyong Wu"
        },
        {
          "authorId": "2383918439",
          "name": "Yang Huang"
        },
        {
          "authorId": "2265250105",
          "name": "Yang Wu"
        },
        {
          "authorId": "2239060530",
          "name": "Zhongqian Sun"
        },
        {
          "authorId": "2275557669",
          "name": "Wei Yang"
        },
        {
          "authorId": "2276424351",
          "name": "Helen M. Meng"
        }
      ],
      "abstract": "The visual dubbing task aims to generate mouth movements synchronized with the driving audio, which has seen significant progress in recent years. However, two critical deficiencies hinder their wide application: (1) Audio-only driving paradigms inadequately capture speaker-specific lip habits, which fail to generate lip movements similar to the target avatar; (2) Conventional blind-inpainting approaches frequently produce visual artifacts when handling obstructions (e.g., microphones, hands), limiting practical deployment. In this paper, we propose StableDub, a novel and concise framework integrating lip-habit-aware modeling with occlusion-robust synthesis. Specifically, building upon the Stable-Diffusion backbone, we develop a lip-habit-modulated mechanism that jointly models phonemic audio-visual synchronization and speaker-specific orofacial dynamics. To achieve plausible lip geometries and object appearances under occlusion, we introduce the occlusion-aware training strategy by explicitly exposing the occlusion objects to the inpainting process. By incorporating the proposed designs, the model eliminates the necessity for cost-intensive priors in previous methods, thereby exhibiting superior training efficiency on the computationally intensive diffusion-based backbone. To further optimize training efficiency from the perspective of model architecture, we introduce a hybrid Mamba-Transformer architecture, which demonstrates the enhanced applicability in low-resource research scenarios. Extensive experimental results demonstrate that StableDub achieves superior performance in lip habit resemblance and occlusion robustness. Our method also surpasses other methods in audio-lip sync, video quality, and resolution consistency. We expand the applicability of visual dubbing methods from comprehensive aspects, and demo videos can be found at https://stabledub.github.io."
    },
    {
      "paperId": "9006cca5ac357884e785a925e0be0e58c5e2438a",
      "externalIds": {
        "ArXiv": "2509.22463",
        "DBLP": "journals/corr/abs-2509-22463",
        "DOI": "10.48550/arXiv.2509.22463",
        "CorpusId": 281658808
      },
      "corpusId": 281658808,
      "title": "IIET: Efficient Numerical Transformer via Implicit Iterative Euler Method",
      "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.22463, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2380634022",
          "name": "Xinyu Liu"
        },
        {
          "authorId": "2325002673",
          "name": "Bei Li"
        },
        {
          "authorId": "2261393008",
          "name": "Jiahao Liu"
        },
        {
          "authorId": "2316399834",
          "name": "Junhao Ruan"
        },
        {
          "authorId": "2267039675",
          "name": "Kechen Jiao"
        },
        {
          "authorId": "2322083216",
          "name": "Hongyin Tang"
        },
        {
          "authorId": "2324838018",
          "name": "Jingang Wang"
        },
        {
          "authorId": "2382963289",
          "name": "Xiao Tong"
        },
        {
          "authorId": "2306387180",
          "name": "Jingbo Zhu"
        }
      ],
      "abstract": "High-order numerical methods enhance Transformer performance in tasks like NLP and CV, but introduce a performance-efficiency trade-off due to increased computational overhead. Our analysis reveals that conventional efficiency techniques, such as distillation, can be detrimental to the performance of these models, exemplified by PCformer. To explore more optimizable ODE-based Transformer architectures, we propose the Iterative Implicit Euler Transformer (IIET), which simplifies high-order methods using an iterative implicit Euler approach. This simplification not only leads to superior performance but also facilitates model compression compared to PCformer. To enhance inference efficiency, we introduce Iteration Influence-Aware Distillation (IIAD). Through a flexible threshold, IIAD allows users to effectively balance the performance-efficiency trade-off. On lm-evaluation-harness, IIET boosts average accuracy by 2.65% over vanilla Transformers and 0.8% over PCformer. Its efficient variant, E-IIET, significantly cuts inference overhead by 55% while retaining 99.4% of the original task accuracy. Moreover, the most efficient IIET variant achieves an average performance gain exceeding 1.6% over vanilla Transformer with comparable speed."
    },
    {
      "paperId": "defb06f8043d0d44aa58928d2b6777a7d193f31a",
      "externalIds": {
        "ArXiv": "2509.21900",
        "CorpusId": 281658772
      },
      "corpusId": 281658772,
      "title": "IPDnet2: an efficient and improved inter-channel phase difference estimation network for sound source localization",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.21900, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2107944312",
          "name": "Ya-Bin Wang"
        },
        {
          "authorId": "2269419049",
          "name": "Bing Yang"
        },
        {
          "authorId": "2269171001",
          "name": "Xiaofei Li"
        }
      ],
      "abstract": "IPDnet is our recently proposed real-time sound source localization network. It employs alternating full-band and narrow-band (B)LSTMs to learn the full-band correlation and narrow-band extraction of DP-IPD, respectively, which achieves superior performance. However, processing narrow-band independently incurs high computational complexity and the limited scalability of LSTM layers constrains the localization accuracy. In this work, we extend IPDnet to IPDnet2, improving both localization accuracy and efficiency. IPDnet2 adapts the oSpatialNet as the backbone to enhance spatial cues extraction and provide superior scalability. Additionally, a simple yet effective frequency-time pooling mechanism is proposed to compress frequency and time resolutions and thus reduce computational cost, and meanwhile not losing localization capability. Experimental results show that IPDnet2 achieves comparable localization performance with IPDnet while only requiring less than 2\\% of its computation cost. Moreover, the proposed network achieves state-of-the-art SSL performance by scaling up the model size while still maintaining relatively low complexity."
    },
    {
      "paperId": "e6bbcc9450a651c9f77cdddfc6b2cfc43a3a449d",
      "externalIds": {
        "ArXiv": "2509.22583",
        "DBLP": "journals/corr/abs-2509-22583",
        "DOI": "10.48550/arXiv.2509.22583",
        "CorpusId": 281658853
      },
      "corpusId": 281658853,
      "title": "Orochi: Versatile Biomedical Image Processor",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.22583, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2218469002",
          "name": "Gaole Dai"
        },
        {
          "authorId": "2384313729",
          "name": "Chenghao Zhou"
        },
        {
          "authorId": "2281170441",
          "name": "Yu Zhou"
        },
        {
          "authorId": "2268799823",
          "name": "Rongyu Zhang"
        },
        {
          "authorId": "2267853574",
          "name": "Yuan Zhang"
        },
        {
          "authorId": "2335857967",
          "name": "Chengkai Hou"
        },
        {
          "authorId": "2303860307",
          "name": "Tiejun Huang"
        },
        {
          "authorId": "2348883891",
          "name": "Jianxu Chen"
        },
        {
          "authorId": "2376781333",
          "name": "Shanghang Zhang"
        }
      ],
      "abstract": "Deep learning has emerged as a pivotal tool for accelerating research in the life sciences, with the low-level processing of biomedical images (e.g., registration, fusion, restoration, super-resolution) being one of its most critical applications. Platforms such as ImageJ (Fiji) and napari have enabled the development of customized plugins for various models. However, these plugins are typically based on models that are limited to specific tasks and datasets, making them less practical for biologists. To address this challenge, we introduce Orochi, the first application-oriented, efficient, and versatile image processor designed to overcome these limitations. Orochi is pre-trained on patches/volumes extracted from the raw data of over 100 publicly available studies using our Random Multi-scale Sampling strategy. We further propose Task-related Joint-embedding Pre-Training (TJP), which employs biomedical task-related degradation for self-supervision rather than relying on Masked Image Modelling (MIM), which performs poorly in downstream tasks such as registration. To ensure computational efficiency, we leverage Mamba's linear computational complexity and construct Multi-head Hierarchy Mamba. Additionally, we provide a three-tier fine-tuning framework (Full, Normal, and Light) and demonstrate that Orochi achieves comparable or superior performance to current state-of-the-art specialist models, even with lightweight parameter-efficient options. We hope that our study contributes to the development of an all-in-one workflow, thereby relieving biologists from the overwhelming task of selecting among numerous models."
    },
    {
      "paperId": "d73cce88a846cce083f043dd92c2c2a5b0dece95",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-21936",
        "ArXiv": "2509.21936",
        "DOI": "10.48550/arXiv.2509.21936",
        "CorpusId": 281658859
      },
      "corpusId": 281658859,
      "title": "Statistical Advantage of Softmax Attention: Insights from Single-Location Regression",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.21936, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2387391932",
          "name": "Odilon Duranthon"
        },
        {
          "authorId": "2362504752",
          "name": "Pierre Marion"
        },
        {
          "authorId": "2283847563",
          "name": "Claire Boyer"
        },
        {
          "authorId": "2326988160",
          "name": "Bruno Loureiro"
        },
        {
          "authorId": "2387391927",
          "name": "Lenka Zdeborov\u00b4a"
        }
      ],
      "abstract": "Large language models rely on attention mechanisms with a softmax activation. Yet the dominance of softmax over alternatives (e.g., component-wise or linear) remains poorly understood, and many theoretical works have focused on the easier-to-analyze linearized attention. In this work, we address this gap through a principled study of the single-location regression task, where the output depends on a linear transformation of a single input token at a random location. Building on ideas from statistical physics, we develop an analysis of attention-based predictors in the high-dimensional limit, where generalization performance is captured by a small set of order parameters. At the population level, we show that softmax achieves the Bayes risk, whereas linear attention fundamentally falls short. We then examine other activation functions to identify which properties are necessary for optimal performance. Finally, we analyze the finite-sample regime: we provide an asymptotic characterization of the test error and show that, while softmax is no longer Bayes-optimal, it consistently outperforms linear attention. We discuss the connection with optimization by gradient-based algorithms."
    },
    {
      "paperId": "208b42395280eded490364316557f7d2482bfb0a",
      "externalIds": {
        "ArXiv": "2509.22522",
        "DBLP": "journals/corr/abs-2509-22522",
        "DOI": "10.48550/arXiv.2509.22522",
        "CorpusId": 281658187
      },
      "corpusId": 281658187,
      "title": "JointDiff: Bridging Continuous and Discrete in Multi-Agent Trajectory Generation",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.22522, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2139167805",
          "name": "Guillem Capellera"
        },
        {
          "authorId": "2309004257",
          "name": "Luis Ferraz"
        },
        {
          "authorId": "2351813240",
          "name": "Antonio Rubio"
        },
        {
          "authorId": "2296992509",
          "name": "Alexandre Alahi"
        },
        {
          "authorId": "2382760119",
          "name": "Antonio Agudo"
        }
      ],
      "abstract": "Generative models often treat continuous data and discrete events as separate processes, creating a gap in modeling complex systems where they interact synchronously. To bridge this gap, we introduce JointDiff, a novel diffusion framework designed to unify these two processes by simultaneously generating continuous spatio-temporal data and synchronous discrete events. We demonstrate its efficacy in the sports domain by simultaneously modeling multi-agent trajectories and key possession events. This joint modeling is validated with non-controllable generation and two novel controllable generation scenarios: weak-possessor-guidance, which offers flexible semantic control over game dynamics through a simple list of intended ball possessors, and text-guidance, which enables fine-grained, language-driven generation. To enable the conditioning with these guidance signals, we introduce CrossGuid, an effective conditioning operation for multi-agent domains. We also share a new unified sports benchmark enhanced with textual descriptions for soccer and football datasets. JointDiff achieves state-of-the-art performance, demonstrating that joint modeling is crucial for building realistic and controllable generative models for interactive systems."
    },
    {
      "paperId": "afe7cc72ba2d9ed70ea79e2a6062829f7fc3b062",
      "externalIds": {
        "ArXiv": "2509.22841",
        "DBLP": "journals/corr/abs-2509-22841",
        "DOI": "10.48550/arXiv.2509.22841",
        "CorpusId": 281674737
      },
      "corpusId": 281674737,
      "title": "Multimodal Slice Interaction Network Enhanced by Transfer Learning for Precise Segmentation of Internal Gross Tumor Volume in Lung Cancer PET/CT Imaging",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.22841, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2326434942",
          "name": "Yi Luo"
        },
        {
          "authorId": "2383169567",
          "name": "Yike Guo"
        },
        {
          "authorId": "100788571",
          "name": "Hamed Hooshangnejad"
        },
        {
          "authorId": "2284991804",
          "name": "Rui Zhang"
        },
        {
          "authorId": "2285221959",
          "name": "Xue Feng"
        },
        {
          "authorId": "2253962592",
          "name": "Quan Chen"
        },
        {
          "authorId": "2326453524",
          "name": "Wil Ngwa"
        },
        {
          "authorId": "2253460990",
          "name": "Kai Ding"
        }
      ],
      "abstract": "Lung cancer remains the leading cause of cancerrelated deaths globally. Accurate delineation of internal gross tumor volume (IGTV) in PET/CT imaging is pivotal for optimal radiation therapy in mobile tumors such as lung cancer to account for tumor motion, yet is hindered by the limited availability of annotated IGTV datasets and attenuated PET signal intensity at tumor boundaries. In this study, we present a transfer learningbased methodology utilizing a multimodal interactive perception network with MAMBA, pre-trained on extensive gross tumor volume (GTV) datasets and subsequently fine-tuned on a private IGTV cohort. This cohort constitutes the PET/CT subset of the Lung-cancer Unified Cross-modal Imaging Dataset (LUCID). To further address the challenge of weak PET intensities in IGTV peripheral slices, we introduce a slice interaction module (SIM) within a 2.5D segmentation framework to effectively model inter-slice relationships. Our proposed module integrates channel and spatial attention branches with depthwise convolutions, enabling more robust learning of slice-to-slice dependencies and thereby improving overall segmentation performance. A comprehensive experimental evaluation demonstrates that our approach achieves a Dice of 0.609 on the private IGTV dataset, substantially surpassing the conventional baseline score of 0.385. This work highlights the potential of transfer learning, coupled with advanced multimodal techniques and a SIM to enhance the reliability and clinical relevance of IGTV segmentation for lung cancer radiation therapy planning."
    },
    {
      "paperId": "19fa28e43b6ae7c27478112cb98eea64c2e0f913",
      "externalIds": {
        "ArXiv": "2509.22813",
        "DBLP": "journals/corr/abs-2509-22813",
        "DOI": "10.48550/arXiv.2509.22813",
        "CorpusId": 281674186
      },
      "corpusId": 281674186,
      "title": "TRUST: Test-Time Refinement using Uncertainty-Guided SSM Traverses",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.22813, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2307459523",
          "name": "Sahar Dastani"
        },
        {
          "authorId": "108062243",
          "name": "Ali Bahri"
        },
        {
          "authorId": "2037886454",
          "name": "G. Hakim"
        },
        {
          "authorId": "2168705225",
          "name": "Moslem Yazdanpanah"
        },
        {
          "authorId": "1380287805",
          "name": "Mehrdad Noori"
        },
        {
          "authorId": "2188345071",
          "name": "David Osowiechi"
        },
        {
          "authorId": "2373034254",
          "name": "Samuel Barbeau"
        },
        {
          "authorId": "144019647",
          "name": "Ismail Ben Ayed"
        },
        {
          "authorId": "2358407",
          "name": "H. Lombaert"
        },
        {
          "authorId": "2260340228",
          "name": "Christian Desrosiers"
        }
      ],
      "abstract": "State Space Models (SSMs) have emerged as efficient alternatives to Vision Transformers (ViTs), with VMamba standing out as a pioneering architecture designed for vision tasks. However, their generalization performance degrades significantly under distribution shifts. To address this limitation, we propose TRUST (Test-Time Refinement using Uncertainty-Guided SSM Traverses), a novel test-time adaptation (TTA) method that leverages diverse traversal permutations to generate multiple causal perspectives of the input image. Model predictions serve as pseudo-labels to guide updates of the Mamba-specific parameters, and the adapted weights are averaged to integrate the learned information across traversal scans. Altogether, TRUST is the first approach that explicitly leverages the unique architectural properties of SSMs for adaptation. Experiments on seven benchmarks show that TRUST consistently improves robustness and outperforms existing TTA methods."
    },
    {
      "paperId": "e990be9268e56e9aea7f96dbd314f50ae4afa239",
      "externalIds": {
        "DBLP": "journals/sivp/LiLDG25",
        "DOI": "10.1007/s11760-025-04818-w",
        "CorpusId": 281639460
      },
      "corpusId": 281639460,
      "title": "LiftMamba: Mamba-based lightweight network for 3D human pose estimation",
      "venue": "Signal, Image and Video Processing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11760-025-04818-w?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11760-025-04818-w, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2383062685",
          "name": "Ma Li"
        },
        {
          "authorId": "2382808463",
          "name": "Dexiang Liu"
        },
        {
          "authorId": "2164444494",
          "name": "Xinguan Dai"
        },
        {
          "authorId": "2221760367",
          "name": "Hangbiao Gao"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "765b805e76d601b880634026980877b313c62984",
      "externalIds": {
        "PubMedCentral": "12562176",
        "DOI": "10.3390/bioengineering12101030",
        "CorpusId": 281638926,
        "PubMed": "41155029"
      },
      "corpusId": 281638926,
      "title": "DermaMamba: A Dual-Branch Vision Mamba Architecture with Linear Complexity for Efficient Skin Lesion Classification",
      "venue": "Bioengineering",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12562176, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2321410929",
          "name": "Zhongyu Yao"
        },
        {
          "authorId": "2368307871",
          "name": "Yuxuan Yan"
        },
        {
          "authorId": "2116746658",
          "name": "Zhe Liu"
        },
        {
          "authorId": "2334462926",
          "name": "Tianhang Chen"
        },
        {
          "authorId": "2382642945",
          "name": "Ling Cho"
        },
        {
          "authorId": "2382651663",
          "name": "Yat-Wah Leung"
        },
        {
          "authorId": "2356585436",
          "name": "Tianchi Lu"
        },
        {
          "authorId": "2382652631",
          "name": "Wenjin Niu"
        },
        {
          "authorId": "2385494630",
          "name": "Zhenyu Qiu"
        },
        {
          "authorId": "2289395496",
          "name": "Yuchen Wang"
        },
        {
          "authorId": "2382799957",
          "name": "Xingcheng Zhu"
        },
        {
          "authorId": "2291209480",
          "name": "Ka-chun Wong"
        }
      ],
      "abstract": "Accurate skin lesion classification is crucial for the early detection of malignant lesions, including melanoma, as well as improved patient outcomes. While convolutional neural networks (CNNs) excel at capturing local morphological features, they struggle with global context modeling essential for comprehensive lesion assessment. Vision transformers address this limitation but suffer from quadratic computational complexity O(n2), hindering deployment in resource-constrained clinical environments. We propose DermaMamba, a novel dual-branch fusion architecture that integrates CNN-based local feature extraction with Vision Mamba (VMamba) for efficient global context modeling with linear complexity O(n). Our approach introduces a state space fusion mechanism with adaptive weighting that dynamically balances local and global features based on lesion characteristics. We incorporate medical domain knowledge through multi-directional scanning strategies and ABCDE (Asymmetry, Border irregularity, Color variation, Diameter, Evolution) rule feature integration. Extensive experiments on the ISIC dataset show that DermaMamba achieves 92.1% accuracy, 91.7% precision, 91.3% recall, and 91.5% mac-F1 score, which outperforms the best baseline by 2.0% accuracy with 2.3\u00d7 inference speedup and 40% memory reduction. The improvements are statistically significant based on a significance test (p < 0.001, Cohen\u2019s d > 0.8), with greater than 79% confidence also preserved on challenging boundary cases. These results establish DermaMamba as an effective solution bridging diagnostic accuracy and computational efficiency for clinical deployment."
    },
    {
      "paperId": "d6e9bcc3b22ccea50fbc63ce4810baa69209926c",
      "externalIds": {
        "DOI": "10.1145/3769304",
        "CorpusId": 281634836
      },
      "corpusId": 281634836,
      "title": "Towards Floating Point-Based AI Acceleration: Hybrid PIM with Non-Uniform Data Format and Reduced Multiplications",
      "venue": "ACM Transactions on Design Automation of Electronic Systems",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3769304?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3769304, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2223374285",
          "name": "Lidong Guo"
        },
        {
          "authorId": "2135206587",
          "name": "Zhenhua Zhu"
        },
        {
          "authorId": "6636914",
          "name": "Xuefei Ning"
        },
        {
          "authorId": "2287971476",
          "name": "Tengxuan Liu"
        },
        {
          "authorId": "2242132627",
          "name": "Shiyao Li"
        },
        {
          "authorId": "2277327801",
          "name": "Guohao Dai"
        },
        {
          "authorId": "2193535807",
          "name": "Huazhong Yang"
        },
        {
          "authorId": "2240063716",
          "name": "Wangyang Fu"
        },
        {
          "authorId": "2156626385",
          "name": "Yu Wang"
        }
      ],
      "abstract": "Neural networks (NNs) have exhibited excellent performance in various fields of artificial intelligence. However, the primary operations in these mainstream models, including matrix-vector multiplication (MVM), element-wise multiplication (EWM), and depth-wise convolution (DWConv), require massive data movements during computation, which greatly impacts NNs\u2019 inference performance. The emerging Processing-In-Memory (PIM) architectures have shown great potential to overcome the memory wall problem. However, constrained by the supported data format and operator type, directly adopting PIM architectures for neural network acceleration faces three challenges: (1) Floating-point (FP) format has been widely adopted for ensuring high algorithm accuracy. However, Resistive Random-Access Memory (RRAM)-based analog PIM architectures perform integer (INT) MVMs in the analog domain, limiting their application to the more accurate FP format; (2) Static Random-Access Memory (SRAM)-based digital PIM architectures require additional circuits to support the FP format, and the SRAM capacity cannot satisfy the storage requirement of latest large language models (LLMs); (3) When performing the operators with few accumulation steps, such as EWMs and DWConvs, only few memory units in PIM architecture are activated, resulting in severe device under-utilization. To tackle the above challenges, this article proposes an RRAM and 3D-SRAM-based hybrid PIM architecture, achieving FP-based algorithm accuracy, high device utilization, and high energy efficiency. At the software level, we first analyze the impact of quantization errors on NN\u2019s inference accuracy. For the quantization error-insensitive MVM operations, we propose the PIM-oriented exponent-free non-uniform (PN) data format. The proposed PN format can be flexibly adjusted to fit the non-uniform distribution and approach FP-based algorithm accuracy using bit-slicing-based full INT operations. For the quantization error-sensitive EWM/DWConv operations, we introduce the multiplication-free approximated FP multiplications to reduce the additional hardware overhead. At the hardware level, we propose a hybrid PIM architecture, including an RRAM analog PIM using shift-and-add for PN-based MVMs, and a 3D-SRAM digital PIM with high utilization for DWConv/EWM operations. Extensive experiments on CNNs and attention-free LLMs validate that the proposed PIM architecture achieves up to 99.4\u00d7 and 33.9\u00d7 speedup with 5697.7\u00d7 and 8.2\u00d7 energy efficiency improvement compared to GPU and PIM-baseline, respectively. With the proposed PN format and approximated FP multiplications, the algorithm accuracy of CNNs and attention-free LLMs can be improved by up to 3.01% and 10.18%, respectively."
    },
    {
      "paperId": "ae24f98f15f7e64bcb0067fcb6eca206c814716c",
      "externalIds": {
        "DOI": "10.1109/ISCME66795.2025.11281484",
        "CorpusId": 283922159
      },
      "corpusId": 283922159,
      "title": "A Multi-step and Multivariate Prediction Model for Short-term Wind Power Forecasting",
      "venue": "2025 10th International Seminar on Computer Technology, Mechanical and Electrical Engineering (ISCME)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ISCME66795.2025.11281484?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ISCME66795.2025.11281484, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2358944133",
          "name": "Zizhen Tang"
        },
        {
          "authorId": "2390892142",
          "name": "Limei Ma"
        },
        {
          "authorId": "2358220287",
          "name": "Baochen Zhen"
        },
        {
          "authorId": null,
          "name": "Xinyu Wang"
        }
      ],
      "abstract": "As global decarbonization accelerates and fossil resources decline, wind energy is becoming a major pillar of power generation. Yet the stochastic and non-stationary nature of wind limits the reliability of ultra-short-term forecasts that are required for secure system operation and dispatch. This paper introduces a novel wind power prediction model which leverages a two-branch architecture. The design comprises a left sequence-modeling branch instantiated with standard attention layers and with state-space layers to encode global and mid-range temporal structure, together with a right lightweight residual refinement branch composed of one-dimensional convolution and linear layers to sharpen local patterns.Unique modifications are introduced to overcome three practical limitations frequently observed in prior models: high computational complexity on long contexts, weak short-horizon accuracy when local transients are under-represented, and limited adaptability under cross-farm distribution shift. The proposed model improves temporal feature extraction while reducing latency and memory usage, which facilitates deployment in supervisory control and data acquisition environments and on edge devices. Experiments across multiple wind farms show substantial reductions in mean absolute error and mean square error relative to a standard Transformer under matched training protocols, and ablation studies confirm the contribution of each architectural component."
    },
    {
      "paperId": "a3edda680ebf90aaf4c80ae3291c8cb05ae83ee1",
      "externalIds": {
        "DOI": "10.1109/EIECS67708.2025.11283472",
        "CorpusId": 284020870
      },
      "corpusId": 284020870,
      "title": "FDTG-Fuse: Fine-Grained Luminance and Target Correction in Infrared and Visible Image Fusion via Feature Distillation and Text Guidance",
      "venue": "2025 5th International Conference on Electronic Information Engineering and Computer Science (EIECS)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/EIECS67708.2025.11283472?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/EIECS67708.2025.11283472, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2382070874",
          "name": "Yihui Wang"
        },
        {
          "authorId": "2240730721",
          "name": "Dengshi Li"
        },
        {
          "authorId": "2382599471",
          "name": "Chengbo Yu"
        },
        {
          "authorId": null,
          "name": "Haiyang Ye"
        },
        {
          "authorId": "2274810298",
          "name": "Aolei Chen"
        },
        {
          "authorId": "2353800118",
          "name": "Zhiming Zhan"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "9bc940226ca2a12828c15944c90132f18393ed6d",
      "externalIds": {
        "DOI": "10.1109/ICCS67844.2025.11292334",
        "CorpusId": 284021567
      },
      "corpusId": 284021567,
      "title": "MCT-SCINet: A Soil Moisture Prediction Model for Earthen Heritage Combining Mamba and Temporal Attention",
      "venue": "International Conference on Conceptual Structures",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCS67844.2025.11292334?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCS67844.2025.11292334, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2399749483",
          "name": "Jiaqi Li"
        },
        {
          "authorId": "2399602725",
          "name": "Tao Xue"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "bd08cff48c223644c5fd9b7a1d39526a24c0cd08",
      "externalIds": {
        "DOI": "10.1109/EIECS67708.2025.11283488",
        "CorpusId": 284023545
      },
      "corpusId": 284023545,
      "title": "KMT-Net: Multi-component Decomposition Framework Integrating KAN and State Space Models for Time Series Forecasting",
      "venue": "2025 5th International Conference on Electronic Information Engineering and Computer Science (EIECS)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/EIECS67708.2025.11283488?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/EIECS67708.2025.11283488, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2399903463",
          "name": "Fengzhou Wang"
        },
        {
          "authorId": "2399627134",
          "name": "Zhiyong An"
        }
      ],
      "abstract": "Time series forecasting requires modeling long-term trends, periodic patterns, and irregular residuals, which conventional decomposition methods often fail to capture effectively. We propose KMT-Net, a framework that integrates Reversible Instance Normalization (RevIN), multi-component decomposition, a Mamba Encoder, and shared Kolmogorov-Arnold Networks (KAN). The input series is normalized with RevIN and decomposed into trend, seasonal, and residual components. The residual, containing complex non-periodic fluctuations, is modeled by the Mamba Encoder to capture long-range dependencies. All components are then refined through the shared KAN to capture nonlinear interactions before being recombined into the final forecast. Experiments on the ETT benchmark demonstrate that KMT-Net consistently improves long-term forecasting performance, showing robustness and effectiveness in modeling diverse temporal dynamics compared with state-of-the-art baselines."
    },
    {
      "paperId": "e07f5d2cc1fbb26d35c794f8088d43608ac322c9",
      "externalIds": {
        "DOI": "10.1109/EIECS67708.2025.11283524",
        "CorpusId": 284021827
      },
      "corpusId": 284021827,
      "title": "MAVMN: Mamba-Augmented Volumetric Memory Network for Interactive 3D Segmentation",
      "venue": "2025 5th International Conference on Electronic Information Engineering and Computer Science (EIECS)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/EIECS67708.2025.11283524?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/EIECS67708.2025.11283524, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2376013224",
          "name": "Lei Chen"
        },
        {
          "authorId": null,
          "name": "Jingwen Fu"
        },
        {
          "authorId": "2323903361",
          "name": "Xingpu Wei"
        },
        {
          "authorId": null,
          "name": "Shuai Liu"
        }
      ],
      "abstract": "Interactive 3D segmentation at large through-plane depth and high resolution is impeded by cross-slice long-range dependencies, low contrast, and small-voxel targets; explicit correlation (e.g., full attention) is computationally and memory prohibitive. We present MAVMN, a memory-guided framework using selective state-space (Mamba) models for content-gated axial recursion to accumulate and query evidence in linear time with a constant state, suppressing redundancy. MAVMN supports matrixized pixel-level whole-volume readout and slice-wise propagation, fusing memory and query features channel-wise before decoding. With a coarse-box ROI and a quality-aware scorer for hard-case-first, multi-round interaction, MAVMN preserves pixel-level alignment and modularity while achieving scalable, stable, cross-slice-consistent segmentation."
    },
    {
      "paperId": "ceaf60e6b9c8ba8ca874fe75d36fb698538ddae8",
      "externalIds": {
        "DOI": "10.1109/EIECS67708.2025.11283277",
        "CorpusId": 284022965
      },
      "corpusId": 284022965,
      "title": "VLTSR: Visual-Layout Multi-Modal Fusion for Table Structure Recognition",
      "venue": "2025 5th International Conference on Electronic Information Engineering and Computer Science (EIECS)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/EIECS67708.2025.11283277?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/EIECS67708.2025.11283277, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2399755939",
          "name": "Taogong Wang"
        }
      ],
      "abstract": "Table Structure Recognition (TSR) is a crucial task in document understanding to extract structured table information from images. High-quality table structure recognition models depend on three essential capabilities: extracting and utilizing visual and layout information, global context modeling, and local detail encoding. However, existing methods face challenges in incorporating all these capabilities simultaneously. Consequently, we aim to enable table structure recognition networks to efficiently utilize multi-modal table data, high-quality global context modeling, and local detail encoding. This paper introduces VLTSR, a novel table structure recognition model composed of a state space model-based table image encoder, a layout feature fusion module, and an autoregressive decoder. We evaluated the proposed methods on several public datasets, and the experimental results demonstrate the effectiveness of our approach."
    },
    {
      "paperId": "5d63add3dd27e8d01faba06c0ddfb791e3322b70",
      "externalIds": {
        "DOI": "10.1109/AIPMV67185.2025.11290593",
        "CorpusId": 284023866
      },
      "corpusId": 284023866,
      "title": "CGM: A Spectral-Spatial Contextual Collaboration Network for Hyperspectral Image Classification",
      "venue": "2025 3rd International Conference on Algorithm, Image Processing and Machine Vision (AIPMV)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/AIPMV67185.2025.11290593?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/AIPMV67185.2025.11290593, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2399890430",
          "name": "Zhiyuan Zhang"
        },
        {
          "authorId": "2399747079",
          "name": "Jiajin Zhang"
        },
        {
          "authorId": "2399635664",
          "name": "Jiewen Ning"
        },
        {
          "authorId": null,
          "name": "Shutian Tang"
        }
      ],
      "abstract": "Hyperspectral image (HSI) classification is crucial for applications such as precision agriculture, environmental monitoring, and urban mapping. However, current methods often struggle with capturing multi-scale spatial dependencies and handling spectral ambiguity, especially under scenarios involving similar spectral signatures across distinct classes and small sample availability.To address these challenges, we propose CGM\u2014a novel Spectral-Spatial Contextual Collaboration Network that synergistically integrates Convolutional Neural Networks (CNN), Graph Convolutional Networks (GCN), and Selective State Space Models (Mamba). Specifically, CNN extracts local spatial textures, GCN captures non-Euclidean spectral relationships, and Mamba efficiently models long-range dependencies via dual-pathway mechanisms (SPAM and SPEM). An adaptive weighted fusion strategy dynamically balances feature contributions across modules.Extensive experiments on three public datasets demonstrate that the CGM model consistently outperforms state-of-the-art baselines, achieving an overall accuracy of 95.04% on the Indian Pines dataset, 96.68% on the Pavia University dataset, and 95.85% on the Salinas dataset. Our model offers both accuracy and efficiency, showing strong potential for real-world HSI tasks."
    },
    {
      "paperId": "96ef81a2bbdcc0d4f71c6cfe4c520a22bd7fa42d",
      "externalIds": {
        "DOI": "10.1109/EIECS67708.2025.11283197",
        "CorpusId": 284021990
      },
      "corpusId": 284021990,
      "title": "AttentionMamba: A Hybrid Model Combining Mamba and Transformer for Time Series Anomaly Detection",
      "venue": "2025 5th International Conference on Electronic Information Engineering and Computer Science (EIECS)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/EIECS67708.2025.11283197?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/EIECS67708.2025.11283197, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": null,
          "name": "Mengyang Shen"
        },
        {
          "authorId": null,
          "name": "Gang Wang"
        }
      ],
      "abstract": "Deep learning has been widely applied to anomaly detection, yet unsupervised time series anomaly detection remains challenging due to limited temporal context capture and inefficient modeling of long sequences. Existing Transformer-based methods excel in short-term dependency modeling but suffer from quadratic time complexity, while Mamba, a state space model (SSM), efficiently handles long sequences but has not been applied to anomaly detection. We propose AttentionMamba, a hybrid framework integrating Transformer and Mamba to capture both short- and long-term dependencies. Experiments show it outperforms existing methods in unsupervised time series anomaly detection."
    },
    {
      "paperId": "13f305830a731cdabd42071fa00018559170dd81",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-21239",
        "ArXiv": "2509.21239",
        "DOI": "10.48550/arXiv.2509.21239",
        "CorpusId": 281526016
      },
      "corpusId": 281526016,
      "title": "SlideMamba: Entropy-Based Adaptive Fusion of GNN and Mamba for Enhanced Representation Learning in Digital Pathology",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.21239, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2383454874",
          "name": "Shakib Khan"
        },
        {
          "authorId": "2125377852",
          "name": "Fariba Dambandkhameneh"
        },
        {
          "authorId": "2163627207",
          "name": "Nazim Shaikh"
        },
        {
          "authorId": "2272344662",
          "name": "Yao Nie"
        },
        {
          "authorId": "2364239623",
          "name": "Raghavan Venugopal"
        },
        {
          "authorId": "2271630559",
          "name": "Xiao Li"
        }
      ],
      "abstract": "Advances in computational pathology increasingly rely on extracting meaningful representations from Whole Slide Images (WSIs) to support various clinical and biological tasks. In this study, we propose a generalizable deep learning framework that integrates the Mamba architecture with Graph Neural Networks (GNNs) for enhanced WSI analysis. Our method is designed to capture both local spatial relationships and long-range contextual dependencies, offering a flexible architecture for digital pathology analysis. Mamba modules excels in capturing long-range global dependencies, while GNNs emphasize fine-grained short-range spatial interactions. To effectively combine these complementary signals, we introduce an adaptive fusion strategy that uses an entropy-based confidence weighting mechanism. This approach dynamically balances contributions from both branches by assigning higher weight to the branch with more confident (lower-entropy) predictions, depending on the contextual importance of local versus global information for different downstream tasks. We demonstrate the utility of our approach on a representative task: predicting gene fusion and mutation status from WSIs. Our framework, SlideMamba, achieves an area under the precision recall curve (PRAUC) of 0.751 \\pm 0.05, outperforming MIL (0.491 \\pm 0.042), Trans-MIL (0.39 \\pm 0.017), Mamba-only (0.664 \\pm 0.063), GNN-only (0.748 \\pm 0.091), and a prior similar work GAT-Mamba (0.703 \\pm 0.075). SlideMamba also achieves competitive results across ROC AUC (0.738 \\pm 0.055), sensitivity (0.662 \\pm 0.083), and specificity (0.725 \\pm 0.094). These results highlight the strength of the integrated architecture, enhanced by the proposed entropy-based adaptive fusion strategy, and suggest promising potential for application of spatially-resolved predictive modeling tasks in computational pathology."
    },
    {
      "paperId": "c23668ffc0cbfc7dbd7d78f3b849305ede1dacb4",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-21265",
        "ArXiv": "2509.21265",
        "DOI": "10.48550/arXiv.2509.21265",
        "CorpusId": 281525927
      },
      "corpusId": 281525927,
      "title": "MedVSR: Medical Video Super-Resolution with Cross State-Space Propagation",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.21265, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2275332982",
          "name": "Xinyu Liu"
        },
        {
          "authorId": "2325237941",
          "name": "Guolei Sun"
        },
        {
          "authorId": "2305053410",
          "name": "Cheng Wang"
        },
        {
          "authorId": "2275284236",
          "name": "Yixuan Yuan"
        },
        {
          "authorId": "1796918",
          "name": "E. Konukoglu"
        }
      ],
      "abstract": "High-resolution (HR) medical videos are vital for accurate diagnosis, yet are hard to acquire due to hardware limitations and physiological constraints. Clinically, the collected low-resolution (LR) medical videos present unique challenges for video super-resolution (VSR) models, including camera shake, noise, and abrupt frame transitions, which result in significant optical flow errors and alignment difficulties. Additionally, tissues and organs exhibit continuous and nuanced structures, but current VSR models are prone to introducing artifacts and distorted features that can mislead doctors. To this end, we propose MedVSR, a tailored framework for medical VSR. It first employs Cross State-Space Propagation (CSSP) to address the imprecise alignment by projecting distant frames as control matrices within state-space models, enabling the selective propagation of consistent and informative features to neighboring frames for effective alignment. Moreover, we design an Inner State-Space Reconstruction (ISSR) module that enhances tissue structures and reduces artifacts with joint long-range spatial feature learning and large-kernel short-range information aggregation. Experiments across four datasets in diverse medical scenarios, including endoscopy and cataract surgeries, show that MedVSR significantly outperforms existing VSR models in reconstruction performance and efficiency. Code released at https://github.com/CUHK-AIM-Group/MedVSR."
    },
    {
      "paperId": "0a611f08a2e234e012df91c408ff210fdae4ebde",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-20789",
        "ArXiv": "2509.20789",
        "DOI": "10.48550/arXiv.2509.20789",
        "CorpusId": 281526062
      },
      "corpusId": 281526062,
      "title": "Aligning Inductive Bias for Data-Efficient Generalization in State Space Models",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.20789, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2382076945",
          "name": "Qiyu Chen"
        },
        {
          "authorId": "2382776524",
          "name": "Guozhang Chen"
        }
      ],
      "abstract": "The remarkable success of large-scale models is fundamentally tied to scaling laws, yet the finite nature of high-quality data presents a looming challenge. One of the next frontiers in modeling is data efficiency: the ability to learn more from less. A model's inductive bias is a critical lever for this, but foundational sequence models like State Space Models (SSMs) rely on a fixed bias. This fixed prior is sample-inefficient when a task's underlying structure does not match. In this work, we introduce a principled framework to solve this problem. We first formalize the inductive bias of linear time-invariant SSMs through an SSM-induced kernel, mathematically and empirically proving its spectrum is directly governed by the model's frequency response. Further, we propose a method of Task-Dependent Initialization (TDI): power spectrum matching, a fast and efficient method that aligns the model's inductive bias with the task's spectral characteristics before large-scale training. Our experiments on a diverse set of real-world benchmarks show that TDI significantly improves generalization and sample efficiency, particularly in low-data regimes. This work provides a theoretical and practical tool to create more data-efficient models, a crucial step towards sustainable scaling."
    },
    {
      "paperId": "3f612b71932587e185c3fce6a050ec5787d8cf04",
      "externalIds": {
        "ArXiv": "2509.21003",
        "CorpusId": 281526300
      },
      "corpusId": 281526300,
      "title": "TF-Restormer: Complex Spectral Prediction for Speech Restoration",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.21003, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "1646707077",
          "name": "U.H Shin"
        },
        {
          "authorId": "2363631944",
          "name": "Jaehyun Ko"
        },
        {
          "authorId": "2381990337",
          "name": "Woocheol Jeong"
        },
        {
          "authorId": "2382056901",
          "name": "Hyuing-Min Park"
        }
      ],
      "abstract": "Speech restoration in real-world conditions is challenging due to compounded distortions such as clipping, band-pass filtering, digital artifacts, noise, and reverberation, and low sampling rates. Existing systems, including vocoder-based approaches, often sacrifice signal fidelity, while diffusion models remain impractical for streaming. Moreover, most assume a fixed target sampling rate, requiring external resampling that leads to redundant computations. We present TF-Restormer, an encoder-decoder architecture that concentrates analysis on input-bandwidth with a time-frequency dual-path encoder and reconstructs missing high-frequency bands through a light decoder with frequency extension queries. It enables efficient and universal restoration across arbitrary input-output rates without redundant resampling. To support adversarial training across diverse rates, we introduce a shared sampling-frequency-independent (SFI) STFT discriminator. TF-Restormer further supports streaming with a causal time module, and improves robustness under extreme degradations by injecting spectral inductive bias into the frequency module. Finally, we propose a scaled log-spectral loss that stabilizes optimization under severe conditions while emphasizing well-predicted spectral details. As a single model across sampling rates, TF-Restormer consistently outperforms prior systems, achieving balanced gains in signal fidelity and perceptual quality, while its streaming mode maintains competitive effectiveness for real-time application. Code and demos are available at https://tf-restormer.github.io/demo."
    },
    {
      "paperId": "959d54ecb0f02d6a76468b3ff6220c9add4a8c5b",
      "externalIds": {
        "PubMedCentral": "12634262",
        "DOI": "10.1097/CM9.0000000000003790",
        "CorpusId": 281601834,
        "PubMed": "40999489"
      },
      "corpusId": 281601834,
      "title": "Computational pathology in precision oncology: Evolution from task-specific models to foundation models",
      "venue": "Chinese Medical Journal",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBYNCND",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12634262, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2351808134",
          "name": "Yuhao Wang"
        },
        {
          "authorId": "2351806057",
          "name": "Yunjie Gu"
        },
        {
          "authorId": "2260818280",
          "name": "Xueyuan Zhang"
        },
        {
          "authorId": "2351848791",
          "name": "Baizhi Wang"
        },
        {
          "authorId": "2382369288",
          "name": "Rundong Wang"
        },
        {
          "authorId": "2351791655",
          "name": "Xiaolong Li"
        },
        {
          "authorId": "2108083821",
          "name": "Yudong Liu"
        },
        {
          "authorId": "2308654242",
          "name": "Fengmei Qu"
        },
        {
          "authorId": "2260719549",
          "name": "Fei Ren"
        },
        {
          "authorId": "2382475656",
          "name": "Rui Yan"
        },
        {
          "authorId": "2364464309",
          "name": "S. K. Zhou"
        }
      ],
      "abstract": "Abstract With the rapid development of artificial intelligence, computational pathology has been seamlessly integrated into the entire clinical workflow, which encompasses diagnosis, treatment, prognosis, and biomarker discovery. This integration has significantly enhanced clinical accuracy and efficiency while reducing the workload for clinicians. Traditionally, research in this field has depended on the collection and labeling of large datasets for specific tasks, followed by the development of task-specific computational pathology models. However, this approach is labor intensive and does not scale efficiently for open-set identification or rare diseases. Given the diversity of clinical tasks, training individual models from scratch to address the whole spectrum of clinical tasks in the pathology workflow is impractical, which highlights the urgent need to transition from task-specific models to foundation models (FMs). In recent years, pathological FMs have proliferated. These FMs can be classified into three categories, namely, pathology image FMs, pathology image\u2013text FMs, and pathology image\u2013gene FMs, each of which results in distinct functionalities and application scenarios. This review provides an overview of the latest research advancements in pathological FMs, with a particular emphasis on their applications in oncology. The key challenges and opportunities presented by pathological FMs in precision oncology are also explored."
    },
    {
      "paperId": "8cd7106ee48cd2ca55568e29847dc9154922381f",
      "externalIds": {
        "DOI": "10.1038/s44287-025-00211-4",
        "CorpusId": 281644691
      },
      "corpusId": 281644691,
      "title": "Extended reality technologies for applications in the metaverse",
      "venue": "Nature Reviews Electrical Engineering",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1038/s44287-025-00211-4?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1038/s44287-025-00211-4, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2382667500",
          "name": "Hiroshi Mukawa"
        },
        {
          "authorId": "2382669511",
          "name": "Yoichi Hirota"
        },
        {
          "authorId": "2382676253",
          "name": "Hiroki Mizuno"
        },
        {
          "authorId": "2382671622",
          "name": "Makoto Murata"
        },
        {
          "authorId": "2382671702",
          "name": "Fuminori Homma"
        },
        {
          "authorId": "2260478411",
          "name": "Keita Mochizuki"
        },
        {
          "authorId": "2382668821",
          "name": "Ryo Ogawa"
        },
        {
          "authorId": "2212198160",
          "name": "Yuki Mamishin"
        },
        {
          "authorId": "2382668130",
          "name": "Hiroyuki Aga"
        },
        {
          "authorId": "2382675066",
          "name": "Jun Yokono"
        },
        {
          "authorId": "2382671739",
          "name": "Daiki Shimada"
        },
        {
          "authorId": "2382675121",
          "name": "Masaki Fukuchi"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "65fcb7fcb722e3b18c04a495014793baa0067484",
      "externalIds": {
        "ArXiv": "2509.25223",
        "DBLP": "journals/corr/abs-2509-25223",
        "DOI": "10.48550/arXiv.2509.25223",
        "CorpusId": 281682698
      },
      "corpusId": 281682698,
      "title": "Enhancing Linear Attention with Residual Learning",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.25223, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2348153752",
          "name": "Xunhao Lai"
        },
        {
          "authorId": "2383301087",
          "name": "Jialiang Kang"
        },
        {
          "authorId": "2348188080",
          "name": "Jianqiao Lu"
        },
        {
          "authorId": "2383109017",
          "name": "Tong Lin"
        },
        {
          "authorId": "2390569029",
          "name": "Pengyu Zhao"
        }
      ],
      "abstract": "Linear attention offers a linear-time alternative to self-attention but often struggles to capture long-range patterns. We revisit linear attention through a prediction-correction lens and show that prevalent variants can be written as a combination of a historical prediction and a single-token correction, which creates an expressivity bottleneck. To address this bottleneck, we introduce Residual Linear Attention (RLA), a framework that equips linear attention with an explicit residual-fitting mechanism. RLA maintains an auxiliary recurrent state that learns to accumulate residual errors over time and correct the base prediction. We further instantiate a delta-rule version, Residual Delta Net (RDN), incorporating adaptive gating and residual clipping for enhanced correction control and stability. Our implementation leverages highly optimized linear attention kernels and preserves linear time and memory. Across language modeling and recall-intensive evaluations, RLA and RDN consistently outperform their respective baselines and other modern linear-attention methods, narrowing the gap to standard Transformers while retaining linear scaling."
    },
    {
      "paperId": "9e9c09986fb4c510d4e967ba82b893ba708db767",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-20154",
        "ArXiv": "2509.20154",
        "DOI": "10.48550/arXiv.2509.20154",
        "CorpusId": 281505177
      },
      "corpusId": 281505177,
      "title": "U-Mamba2-SSL for Semi-Supervised Tooth and Pulp Segmentation in CBCT",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.20154, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2381240182",
          "name": "Zhi Qin Tan"
        },
        {
          "authorId": "2310779368",
          "name": "Xiatian Zhu"
        },
        {
          "authorId": "2380521484",
          "name": "Owen Addison"
        },
        {
          "authorId": "2310858119",
          "name": "Yunpeng Li"
        }
      ],
      "abstract": "Accurate segmentation of teeth and pulp in Cone-Beam Computed Tomography (CBCT) is vital for clinical applications like treatment planning and diagnosis. However, this process requires extensive expertise and is exceptionally time-consuming, highlighting the critical need for automated algorithms that can effectively utilize unlabeled data. In this paper, we propose U-Mamba2-SSL, a novel semi-supervised learning framework that builds on the U-Mamba2 model and employs a multi-stage training strategy. The framework first pre-trains U-Mamba2 in a self-supervised manner using a disruptive autoencoder. It then leverages unlabeled data through consistency regularization, where we introduce input and feature perturbations to ensure stable model outputs. Finally, a pseudo-labeling strategy is implemented with a reduced loss weighting to minimize the impact of potential errors. U-Mamba2-SSL achieved an average score of 0.789 and a DSC of 0.917 on the hidden test set, achieving first place in Task 1 of the STSR 2025 challenge. The code is available at https://github.com/zhiqin1998/UMamba2."
    },
    {
      "paperId": "a330ff7d17c8931fe15ab724c7c29cc4e4626af7",
      "externalIds": {
        "ArXiv": "2509.19658",
        "DBLP": "journals/corr/abs-2509-19658",
        "DOI": "10.48550/arXiv.2509.19658",
        "CorpusId": 281505665
      },
      "corpusId": 281505665,
      "title": "RoboSSM: Scalable In-context Imitation Learning via State-Space Models",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.19658, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2385542792",
          "name": "Youngju Yoo"
        },
        {
          "authorId": "81703072",
          "name": "Jiaheng Hu"
        },
        {
          "authorId": "2382024119",
          "name": "Yifeng Zhu"
        },
        {
          "authorId": "2257433986",
          "name": "Bo Liu"
        },
        {
          "authorId": "2155193246",
          "name": "Qian Liu"
        },
        {
          "authorId": "2380440732",
          "name": "Roberto Mart'in-Mart'in"
        },
        {
          "authorId": "2290916532",
          "name": "Peter Stone"
        }
      ],
      "abstract": "In-context imitation learning (ICIL) enables robots to learn tasks from prompts consisting of just a handful of demonstrations. By eliminating the need for parameter updates at deployment time, this paradigm supports few-shot adaptation to novel tasks. However, recent ICIL methods rely on Transformers, which have computational limitations and tend to underperform when handling longer prompts than those seen during training. In this work, we introduce RoboSSM, a scalable recipe for in-context imitation learning based on state-space models (SSM). Specifically, RoboSSM replaces Transformers with Longhorn -- a state-of-the-art SSM that provides linear-time inference and strong extrapolation capabilities, making it well-suited for long-context prompts. We evaluate our approach on the LIBERO benchmark and compare it against strong Transformer-based ICIL baselines. Experiments show that RoboSSM extrapolates effectively to varying numbers of in-context demonstrations, yields high performance on unseen tasks, and remains robust in long-horizon scenarios. These results highlight the potential of SSMs as an efficient and scalable backbone for ICIL. Our code is available at https://github.com/youngjuY/RoboSSM."
    },
    {
      "paperId": "1f4e490221a936e2d48f9a58053f9d5d1307bbdd",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-19873",
        "ArXiv": "2509.19873",
        "DOI": "10.1109/ICCAD66269.2025.11240945",
        "CorpusId": 281505977
      },
      "corpusId": 281505977,
      "title": "SpecMamba: Accelerating Mamba Inference on FPGA with Speculative Decoding",
      "venue": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.19873, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2302748495",
          "name": "Linfeng Zhong"
        },
        {
          "authorId": "2313044145",
          "name": "Songqiang Xu"
        },
        {
          "authorId": "2382859645",
          "name": "Huifeng Wen"
        },
        {
          "authorId": "2284765808",
          "name": "Tong Xie"
        },
        {
          "authorId": "2273322906",
          "name": "Qingyu Guo"
        },
        {
          "authorId": "2216828793",
          "name": "Yuan Wang"
        },
        {
          "authorId": "2363118373",
          "name": "Meng Li"
        }
      ],
      "abstract": "The growing demand for efficient long-sequence modeling on edge devices has propelled widespread adoption of State Space Models (SSMs) like Mamba, due to their superior computational efficiency and scalability. As its autoregressive generation process remains memory-bound, speculative decoding has been proposed that incorporates draft model generation and target model verification. However, directly applying speculative decoding to SSMs faces three key challenges: (1) hidden state backtracking difficulties, (2) tree-based parallel verification incompatibility, and (3) hardware workload mismatch. To address these challenges, we propose SpecMamba, the first FPGA-based accelerator for Mamba with speculative decoding, which features system, algorithm, and hardware co-design. At the system level, we present a memory-aware hybrid backtracking strategy to coordinate both models. At the algorithm level, we propose first-in-first-out (FIFO)-based tree verification with tiling to minimize memory access. At the hardware level, we customize a dataflow that computes linear layers in parallel and SSM layers in series to enable maximal overlapping. Implemented on AMD FPGA platforms (VHK158 and VCK190), SpecMamba achieves a 2.27\u00d7 speedup over GPU baselines and a 2.85\u00d7 improvement compared to prior FPGA solutions, while demonstrating 5.41\u00d7 and 1.26\u00d7 higher energy efficiency, respectively."
    },
    {
      "paperId": "47583dbd910d5815059f9426b21c0614c0e4544d",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-20240",
        "ArXiv": "2509.20240",
        "DOI": "10.48550/arXiv.2509.20240",
        "CorpusId": 281505520
      },
      "corpusId": 281505520,
      "title": "A HyperGraphMamba-Based Multichannel Adaptive Model for ncRNA Classification",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.20240, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2370935937",
          "name": "Xin An"
        },
        {
          "authorId": "2363818028",
          "name": "Ruijie Li"
        },
        {
          "authorId": "2342834873",
          "name": "Qiao Ning"
        },
        {
          "authorId": "2256782511",
          "name": "Hui Li"
        },
        {
          "authorId": "2261150249",
          "name": "Qian Ma"
        },
        {
          "authorId": "2364623912",
          "name": "Shikai Guo"
        }
      ],
      "abstract": "Non-coding RNAs (ncRNAs) play pivotal roles in gene expression regulation and the pathogenesis of various diseases. Accurate classification of ncRNAs is essential for functional annotation and disease diagnosis. To address existing limitations in feature extraction depth and multimodal fusion, we propose HGMamba-ncRNA, a HyperGraphMamba-based multichannel adaptive model, which integrates sequence, secondary structure, and optionally available expression features of ncRNAs to enhance classification performance. Specifically, the sequence of ncRNA is modeled using a parallel Multi-scale Convolution and LSTM architecture (MKC-L) to capture both local patterns and long-range dependencies of nucleotides. The structure modality employs a multi-scale graph transformer (MSGraphTransformer) to represent the multi-level topological characteristics of ncRNA secondary structures. The expression modality utilizes a Chebyshev Polynomial-based Kolmogorov-Arnold Network (CPKAN) to effectively model and interpret high-dimensional expression profiles. Finally, by incorporating virtual nodes to facilitate efficient and comprehensive multimodal interaction, HyperGraphMamba is proposed to adaptively align and integrate multichannel heterogeneous modality features. Experiments conducted on three public datasets demonstrate that HGMamba-ncRNA consistently outperforms state-of-the-art methods in terms of accuracy and other metrics. Extensive empirical studies further confirm the model's robustness, effectiveness, and strong transferability, offering a novel and reliable strategy for complex ncRNA functional classification. Code and datasets are available at https://anonymous.4open.science/r/HGMamba-ncRNA-94D0."
    },
    {
      "paperId": "6bfdea9dee98a85122073bba21895ad8c0c0ff99",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-19853",
        "ArXiv": "2509.19853",
        "DOI": "10.48550/arXiv.2509.19853",
        "CorpusId": 281505364
      },
      "corpusId": 281505364,
      "title": "SAGE:State-Aware Guided End-to-End Policy for Multi-Stage Sequential Tasks via Hidden Markov Decision Process",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.19853, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2382405302",
          "name": "BinXu Wu"
        },
        {
          "authorId": "2381941625",
          "name": "TengFei Zhang"
        },
        {
          "authorId": "2382105914",
          "name": "Chen Yang"
        },
        {
          "authorId": "2382451853",
          "name": "JiaHao Wen"
        },
        {
          "authorId": "2382394246",
          "name": "Haocheng Li"
        },
        {
          "authorId": "2382812927",
          "name": "Jingtian Ma"
        },
        {
          "authorId": "2381913324",
          "name": "Zhen Chen"
        },
        {
          "authorId": "2382515941",
          "name": "Jingyuan Wang"
        }
      ],
      "abstract": "Multi-stage sequential (MSS) robotic manipulation tasks are prevalent and crucial in robotics. They often involve state ambiguity, where visually similar observations correspond to different actions. We present SAGE, a state-aware guided imitation learning framework that models tasks as a Hidden Markov Decision Process (HMDP) to explicitly capture latent task stages and resolve ambiguity. We instantiate the HMDP with a state transition network that infers hidden states, and a state-aware action policy that conditions on both observations and hidden states to produce actions, thereby enabling disambiguation across task stages. To reduce manual annotation effort, we propose a semi-automatic labeling pipeline combining active learning and soft label interpolation. In real-world experiments across multiple complex MSS tasks with state ambiguity, SAGE achieved 100% task success under the standard evaluation protocol, markedly surpassing the baselines. Ablation studies further show that such performance can be maintained with manual labeling for only about 13% of the states, indicating its strong effectiveness."
    },
    {
      "paperId": "f1ed69c8602eb23c4998ca31e769b19ff4cfb95f",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-20489",
        "ArXiv": "2509.20489",
        "DOI": "10.48550/arXiv.2509.20489",
        "CorpusId": 281526161
      },
      "corpusId": 281526161,
      "title": "CoSupFormer : A Contrastive Supervised learning approach for EEG signal Classification",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.20489, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2322577991",
          "name": "D. Darankoum"
        },
        {
          "authorId": "2322580808",
          "name": "C. Habermacher"
        },
        {
          "authorId": "2322580359",
          "name": "J. Volle"
        },
        {
          "authorId": "2348097461",
          "name": "S. Grudinin"
        }
      ],
      "abstract": "Electroencephalography signals (EEGs) contain rich multi-scale information crucial for understanding brain states, with potential applications in diagnosing and advancing the drug development landscape. However, extracting meaningful features from raw EEG signals while handling noise and channel variability remains a major challenge. This work proposes a novel end-to-end deep-learning framework that addresses these issues through several key innovations. First, we designed an encoder capable of explicitly capturing multi-scale frequency oscillations covering a wide range of features for different EEG-related tasks. Secondly, to model complex dependencies and handle the high temporal resolution of EEGs, we introduced an attention-based encoder that simultaneously learns interactions across EEG channels and within localized {\\em patches} of individual channels. We integrated a dedicated gating network on top of the attention encoder to dynamically filter out noisy and non-informative channels, enhancing the reliability of EEG data. The entire encoding process is guided by a novel loss function, which leverages supervised and contrastive learning, significantly improving model generalization. We validated our approach in multiple applications, ranging from the classification of effects across multiple Central Nervous System (CNS) disorders treatments to the diagnosis of Parkinson's and Alzheimer's disease. Our results demonstrate that the proposed learning paradigm can extract biologically meaningful patterns from raw EEG signals across different species, autonomously select high-quality channels, and achieve robust generalization through innovative architectural and loss design."
    },
    {
      "paperId": "66cfb187fcccf434275d870762fa8b1cabbd5134",
      "externalIds": {
        "PubMedCentral": "12504502",
        "DOI": "10.3389/fmed.2025.1661984",
        "CorpusId": 281529989,
        "PubMed": "41070074"
      },
      "corpusId": 281529989,
      "title": "Multi-interactive feature embedding learning for medical image segmentation",
      "venue": "Frontiers in Medicine",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12504502, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2382317769",
          "name": "Yijia Huang"
        },
        {
          "authorId": "2382440237",
          "name": "Yue Luo"
        }
      ],
      "abstract": "Medical image segmentation task can provide the lesion object semantic information, but ignores edge texture details from the lesion region. Conversely, the medical image reconstruction task furnishes the object detailed information to facilitate the semantic segmentation through self-supervised learning. The two tasks are supplementary to each other. Therefore, we propose a multi-interactive feature embedding learning for medical image segmentation. In the medical image reconstruction task, we aim to generate the detailed feature representations containing rich textures, edges, and structures, thus bridging the low-level details lost from segmentation features. In particular, we propose an adaptive feature modulation module to efficiently aggregate foreground and background features to obtain a comprehensive feature representation. In the medical segmentation task, we propose a bi-directional fusion module fusing all important complementary information between the two tasks. Besides, we introduce a multi-branch visual mamba to capture structural information at different scales, thus enhancing model adaptation to different lesion regions. Extensive experiments on four datasets demonstrate the effectiveness of our framework."
    },
    {
      "paperId": "8ea6868a64eacfcf60dd3c812bf873a1fbc2c53d",
      "externalIds": {
        "DOI": "10.1017/s1748499525100110",
        "CorpusId": 281593071
      },
      "corpusId": 281593071,
      "title": "A brief review of deep learning methods in mortality forecasting",
      "venue": "Annals of Actuarial Science",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1017/s1748499525100110?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1017/s1748499525100110, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2383337879",
          "name": "Huiling Zheng"
        },
        {
          "authorId": "2262536108",
          "name": "Hai Wang"
        },
        {
          "authorId": "2248058325",
          "name": "Rui Zhu"
        },
        {
          "authorId": "2248327701",
          "name": "Jing-Hao Xue"
        }
      ],
      "abstract": "\n Accurate mortality forecasting is crucial for actuarial pricing, reserving, and capital planning, yet the traditional Lee-Carter model struggles with non-linear age and cohort patterns, coherent multi-population forecasting, and quantifying prediction uncertainties. Recent advances in deep learning provide a range of tools that can address these limitations, but actuarial surveys have not kept pace. This paper provides the first concise view of deep learning in mortality forecasting. We cover six deep network architectures, namely Recurrent Neural Networks, Convolutional Neural Networks, Transformers, Autoencoders, Locally Connected Networks, and Multi-Task Feed-Forward Networks. We discuss how these architectures tackle cohort effects, population coherence, interpretability, and uncertainty in mortality forecasting. Evidence from the literature shows that carefully calibrated deep learning models can consistently outperform the Lee-Carter baselines; however, no single architecture resolves every challenge, and open issues remain with data scarcity, interpretability, uncertainty quantification, and keeping pace with the advances of deep learning. This review is also intended to provide actuaries with a practical roadmap for adopting deep learning models in mortality forecasting."
    },
    {
      "paperId": "b5bd034da8763a36d9fe302ec4a9d8e9ee93c9f0",
      "externalIds": {
        "ArXiv": "2510.00882",
        "DBLP": "journals/corr/abs-2510-00882",
        "DOI": "10.59275/j.melba.2025-8d4c",
        "CorpusId": 281553221
      },
      "corpusId": 281553221,
      "title": "AI-CNet3D: An Anatomically-Informed Cross-Attention Network with Multi-Task Consistency Fine-tuning for 3D Glaucoma Classification",
      "venue": "Machine Learning for Biomedical Imaging",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.00882, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2350620084",
          "name": "Roshan Kenia"
        },
        {
          "authorId": "2382433849",
          "name": "Anfei Li"
        },
        {
          "authorId": "2382186148",
          "name": "Rishabh Srivastava"
        },
        {
          "authorId": "2749476",
          "name": "Kaveri A. Thakoor"
        }
      ],
      "abstract": "Glaucoma is a progressive eye disease that leads to optic nerve damage, causing irreversible vision loss if left untreated. Optical coherence tomography (OCT) has become a crucial tool for glaucoma diagnosis, offering high-resolution 3D scans of the retina and optic nerve. However, the conventional practice of condensing information from 3D OCT volumes into 2D reports often results in the loss of key structural details. To address this, we propose a novel hybrid deep learning model that integrates cross-attention mechanisms into a 3D convolutional neural network (CNN), enabling the extraction of critical features from the superior and inferior hemiretinas, as well as from the optic nerve head (ONH) and macula, within OCT volumes. We introduce Channel Attention REpresentations (CAREs) to visualize cross-attention outputs and leverage them for consistency-based multi-task fine-tuning, aligning them with Gradient-Weighted Class Activation Maps (Grad-CAMs) from the CNN\u2019s final convolutional layer to enhance performance, interpretability, and anatomical coherence. We have named this model AI-CNet3D (AI-\u2018See\u2019-Net3D) to reflect its design as an Anatomically-Informed Cross-attention Network operating on 3D data. By dividing the volume along two axes and applying cross-attention, our model enhances glaucoma classification by capturing asymmetries between the hemiretinal regions while integrating information from the optic nerve head and macula. We validate our approach on two large datasets, showing that it outperforms state-of-the-art attention and convolutional models across all key metrics. Finally, our model is computationally efficient, reducing the parameter count by one-hundred\u2013fold compared to other attention mechanisms while maintaining high diagnostic performance and comparable GFLOPS."
    },
    {
      "paperId": "ccabbbca37bd54c2add64cf3b59c64d75514ed03",
      "externalIds": {
        "DBLP": "journals/entropy/WangCK25",
        "PubMedCentral": "12563197",
        "DOI": "10.3390/e27100995",
        "CorpusId": 281533105,
        "PubMed": "41148953"
      },
      "corpusId": 281533105,
      "title": "SGFNet: Redundancy-Reduced Spectral\u2013Spatial Fusion Network for Hyperspectral Image Classification",
      "venue": "Entropy",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12563197, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2382439700",
          "name": "Boyu Wang"
        },
        {
          "authorId": "2391764909",
          "name": "Chi Cao"
        },
        {
          "authorId": "2382235970",
          "name": "Dexing Kong"
        }
      ],
      "abstract": "Hyperspectral image classification (HSIC) involves analyzing high-dimensional data that contain substantial spectral redundancy and spatial noise, which increases the entropy and uncertainty of feature representations. Reducing such redundancy while retaining informative content in spectral\u2013spatial interactions remains a fundamental challenge for building efficient and accurate HSIC models. Traditional deep learning methods often rely on redundant modules or lack sufficient spectral\u2013spatial coupling, limiting their ability to fully exploit the information content of hyperspectral data. To address these challenges, we propose SGFNet, which is a spectral-guided fusion network designed from an information\u2013theoretic perspective to reduce feature redundancy and uncertainty. First, we designed a Spectral-Aware Filtering Module (SAFM) that suppresses noisy spectral components and reduces redundant entropy, encoding the raw pixel-wise spectrum into a compact spectral representation accessible to all encoder blocks. Second, we introduced a Spectral\u2013Spatial Adaptive Fusion (SSAF) module, which strengthens spectral\u2013spatial interactions and enhances the discriminative information in the fused features. Finally, we developed a Spectral Guidance Gated CNN (SGGC), which is a lightweight gated convolutional module that uses spectral guidance to more effectively extract spatial representations while avoiding unnecessary sequence modeling overhead. We conducted extensive experiments on four widely used hyperspectral benchmarks and compared SGFNet with eight state-of-the-art models. The results demonstrate that SGFNet consistently achieves superior performance across multiple metrics. From an information\u2013theoretic perspective, SGFNet implicitly balances redundancy reduction and information preservation, providing an efficient and effective solution for HSIC."
    },
    {
      "paperId": "f37ea689f7a62df030d11975584874e2f46c7e27",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-18691",
        "ArXiv": "2509.18691",
        "DOI": "10.48550/arXiv.2509.18691",
        "CorpusId": 281496152
      },
      "corpusId": 281496152,
      "title": "An overview of neural architectures for self-supervised audio representation learning from masked spectrograms",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.18691, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "50996703",
          "name": "Sarthak Yadav"
        },
        {
          "authorId": "9261284",
          "name": "S. Theodoridis"
        },
        {
          "authorId": "2257413640",
          "name": "Zheng-Hua Tan"
        }
      ],
      "abstract": "In recent years, self-supervised learning has amassed significant interest for training deep neural representations without labeled data. One such self-supervised learning approach is masked spectrogram modeling, where the objective is to learn semantically rich contextual representations by predicting removed or hidden portions of the input audio spectrogram. With the Transformer neural architecture at its core, masked spectrogram modeling has emerged as the prominent approach for learning general purpose audio representations, a.k.a. audio foundation models. Meanwhile, addressing the issues of the Transformer architecture, in particular the underlying Scaled Dot-product Attention operation, which scales quadratically with input sequence length, has led to renewed interest in recurrent sequence modeling approaches. Among them, Selective structured state space models (such as Mamba) and extended Long Short-Term Memory (xLSTM) are the two most promising approaches which have experienced widespread adoption. While the body of work on these two topics continues to grow, there is currently a lack of an adequate overview encompassing the intersection of these topics. In this paper, we present a comprehensive overview of the aforementioned research domains, covering masked spectrogram modeling and the previously mentioned neural sequence modeling architectures, Mamba and xLSTM. Further, we compare Transformers, Mamba and xLSTM based masked spectrogram models in a unified, reproducible framework on ten diverse downstream audio classification tasks, which will help interested readers to make informed decisions regarding suitability of the evaluated approaches to adjacent applications."
    },
    {
      "paperId": "6dfccfbbddf717c0a3f06ae99bd915968def8cbc",
      "externalIds": {
        "ArXiv": "2509.18683",
        "DBLP": "journals/corr/abs-2509-18683",
        "DOI": "10.1145/3746027.3754863",
        "CorpusId": 281495973
      },
      "corpusId": 281495973,
      "title": "LEAF-Mamba: Local Emphatic and Adaptive Fusion State Space Model for RGB-D Salient Object Detection",
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.18683, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2381785514",
          "name": "Lanhu Wu"
        },
        {
          "authorId": "2292432824",
          "name": "Zilin Gao"
        },
        {
          "authorId": "2346834646",
          "name": "Hao Fei"
        },
        {
          "authorId": "2237598359",
          "name": "Mong Li Lee"
        },
        {
          "authorId": "144793401",
          "name": "W. Hsu"
        }
      ],
      "abstract": "RGB-D salient object detection (SOD) aims to identify the most conspicuous objects in a scene with the incorporation of depth cues. Existing methods mainly rely on CNNs, limited by the local receptive fields, or Vision Transformers that suffer from the cost of quadratic complexity, posing a challenge in balancing performance and computational efficiency. Recently, state space models (SSM), Mamba, have shown great potential for modeling long-range dependency with linear complexity. However, directly applying SSM to RGB-D SOD may lead to deficient local semantics as well as the inadequate cross-modality fusion. To address these issues, we propose a Local Emphatic and Adaptive Fusion state space model (LEAF-Mamba) that contains two novel components: 1) a local emphatic state space module (LE-SSM) to capture multi-scale local dependencies for both modalities. 2) an SSM-based adaptive fusion module (AFM) for complementary cross-modality interaction and reliable cross-modality integration. Extensive experiments demonstrate that the LEAF-Mamba consistently outperforms 16 state-of-the-art RGB-D SOD methods in both efficacy and efficiency. Moreover, our method can achieve excellent performance on the RGB-T SOD task, proving a powerful generalization ability. Our code is publicly available at https://github.com/LanhooNg/LEAF-Mamba."
    },
    {
      "paperId": "822ee501c1f52e2f151156917c4c0428abbb152e",
      "externalIds": {
        "ArXiv": "2509.18576",
        "DBLP": "journals/corr/abs-2509-18576",
        "DOI": "10.48550/arXiv.2509.18576",
        "CorpusId": 281496776
      },
      "corpusId": 281496776,
      "title": "LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.18576, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2381371068",
          "name": "Zeyi Kang"
        },
        {
          "authorId": "2301097267",
          "name": "Liang He"
        },
        {
          "authorId": "2381407405",
          "name": "Yanxin Zhang"
        },
        {
          "authorId": "2286899721",
          "name": "Zuheng Ming"
        },
        {
          "authorId": "2212031237",
          "name": "Kaixing Zhao"
        }
      ],
      "abstract": "Multimodal semantic learning plays a critical role in embodied intelligence, especially when robots perceive their surroundings, understand human instructions, and make intelligent decisions. However, the field faces technical challenges such as effective fusion of heterogeneous data and computational efficiency in resource-constrained environments. To address these challenges, this study proposes the lightweight LCMF cascaded attention framework, introducing a multi-level cross-modal parameter sharing mechanism into the Mamba module. By integrating the advantages of Cross-Attention and Selective parameter-sharing State Space Models (SSMs), the framework achieves efficient fusion of heterogeneous modalities and semantic complementary alignment. Experimental results show that LCMF surpasses existing multimodal baselines with an accuracy of 74.29% in VQA tasks and achieves competitive mid-tier performance within the distribution cluster of Large Language Model Agents (LLM Agents) in EQA video tasks. Its lightweight design achieves a 4.35-fold reduction in FLOPs relative to the average of comparable baselines while using only 166.51M parameters (image-text) and 219M parameters (video-text), providing an efficient solution for Human-Robot Interaction (HRI) applications in resource-constrained scenarios with strong multimodal decision generalization capabilities."
    },
    {
      "paperId": "057c2bf5723861b94f237cab78dcd98642b45615",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-19227",
        "ArXiv": "2509.19227",
        "DOI": "10.48550/arXiv.2509.19227",
        "CorpusId": 281496443
      },
      "corpusId": 281496443,
      "title": "MsFIN: Multi-scale Feature Interaction Network for Traffic Accident Anticipation",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.19227, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2377739956",
          "name": "Tongshuai Wu"
        },
        {
          "authorId": "2374825881",
          "name": "Chao Lu"
        },
        {
          "authorId": "2383275692",
          "name": "Ze Song"
        },
        {
          "authorId": "2155853904",
          "name": "Yunlong Lin"
        },
        {
          "authorId": "2382960296",
          "name": "Sizhe Fan"
        },
        {
          "authorId": "2308368222",
          "name": "Xuemei Chen"
        }
      ],
      "abstract": "With the widespread deployment of dashcams and advancements in computer vision, developing accident prediction models from the dashcam perspective has become critical for proactive safety interventions. However, two key challenges persist: modeling feature-level interactions among traffic participants (often occluded in dashcam views) and capturing complex, asynchronous multi-temporal behavioral cues preceding accidents. To deal with these two challenges, a Multi-scale Feature Interaction Network (MsFIN) is proposed for early-stage accident anticipation from dashcam videos. MsFIN has three layers for multi-scale feature aggregation, temporal feature processing and multi-scale feature post fusion, respectively. For multi-scale feature aggregation, a Multi-scale Module is designed to extract scene representations at short-term, mid-term and long-term temporal scales. Meanwhile, the Transformer architecture is leveraged to facilitate comprehensive feature interactions. Temporal feature processing captures the sequential evolution of scene and object features under causal constraints. In the multi-scale feature post fusion stage, the network fuses scene and object features across multiple temporal scales to generate a comprehensive risk representation. Experiments on DAD and DADA datasets show that MsFIN significantly outperforms state-of-the-art models with single-scale feature extraction in both prediction correctness and earliness. Ablation studies validate the effectiveness of each module in MsFIN, highlighting how the network achieves superior performance through multi-scale feature fusion and contextual interaction modeling."
    },
    {
      "paperId": "b44aed7834786add30869a5305d3a6b3512a42a4",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-19569",
        "ArXiv": "2509.19569",
        "DOI": "10.48550/arXiv.2509.19569",
        "CorpusId": 281505260
      },
      "corpusId": 281505260,
      "title": "ExPe: Exact Positional Encodings for Generative Transformer Models with Extrapolating Capabilities",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.19569, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2315669714",
          "name": "Aleksis Datseris"
        },
        {
          "authorId": "15888811",
          "name": "S. Vassileva"
        },
        {
          "authorId": "2261400997",
          "name": "Ivan Koychev"
        },
        {
          "authorId": "2142778",
          "name": "S. Boytcheva"
        }
      ],
      "abstract": "This paper introduces a novel approach to position embeddings in transformer models, named\"Exact Positional Embeddings\"(ExPE). An absolute positional embedding method that can extrapolate to sequences of lengths longer than the ones it was trained on. Traditional transformer models rely on absolute or relative position embeddings to incorporate positional information into token embeddings, which often struggle with extrapolation to sequences longer than those seen during training. Our proposed method utilizes a novel embedding strategy that encodes exact positional information by overriding specific dimensions of the embedding vectors, thereby enabling a more precise representation of token positions. The proposed approach not only maintains the integrity of the original embeddings but also enhances the model's ability to generalize to more extended sequences. In causal language modeling, our ExPE embeddings significantly reduce perplexity compared to rotary and sinusoidal embeddings, when tested on sequences longer than those used in training."
    },
    {
      "paperId": "cc78617e2468dc6eb608292841aa444690c5e755",
      "externalIds": {
        "DBLP": "journals/tip/GaoSSCXG25",
        "DOI": "10.1109/TIP.2025.3611146",
        "CorpusId": 281504961,
        "PubMed": "40986593"
      },
      "corpusId": 281504961,
      "title": "Multiscale Segmentation-Guided Fusion Network for Hyperspectral Image Classification",
      "venue": "IEEE Transactions on Image Processing",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIP.2025.3611146?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIP.2025.3611146, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2269858386",
          "name": "Hongmin Gao"
        },
        {
          "authorId": "2297565823",
          "name": "Runhua Sheng"
        },
        {
          "authorId": "27069390",
          "name": "Yuanchao Su"
        },
        {
          "authorId": "2144172680",
          "name": "Zhonghao Chen"
        },
        {
          "authorId": "2174547688",
          "name": "Shufang Xu"
        },
        {
          "authorId": "2265143125",
          "name": "Lianru Gao"
        }
      ],
      "abstract": "Convolution Neural Networks (CNNs) have demonstrated strong feature extraction capabilities in Euclidean spaces, achieving remarkable success in hyperspectral image (HSI) classification tasks. Meanwhile, Graph convolution networks (GCNs) effectively capture spatial-contextual characteristics by leveraging correlations in non-Euclidean spaces, uncovering hidden relationships to enhance the performance of HSI classification (HSIC). Methods combining GCNs with CNNs have achieved excellent results. However, existing GCN methods primarily rely on single-scale graph structures, limiting their ability to extract features across different spatial ranges. To address this issue, this paper proposes a multiscale segmentation-guided fusion network (MS2FN) for HSIC. This method constructs pixel-level graph structures based on multiscale segmentation data, enabling the GCN to extract features across various spatial ranges. Moreover, effectively utilizing features extracted from different spatial scales is crucial for improving classification performance. This paper adopts distinct processing strategies for different feature types to enhance feature representation. Comparative experiments demonstrate that the proposed method outperforms several state-of-the-art (SOTA) approaches in accuracy. The source code will be released at https://github.com/shengrunhua/MS2FN"
    },
    {
      "paperId": "61a4a53457d1cc7c008f0e363eeb48266446d3f0",
      "externalIds": {
        "ArXiv": "2509.19633",
        "DBLP": "journals/corr/abs-2509-19633",
        "DOI": "10.48550/arXiv.2509.19633",
        "CorpusId": 281505175
      },
      "corpusId": 281505175,
      "title": "Mamba Modulation: On the Length Generalization of Mamba",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.19633, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2356815993",
          "name": "Peng Lu"
        },
        {
          "authorId": "2356572823",
          "name": "Jerry Huang"
        },
        {
          "authorId": "2375162246",
          "name": "Qiuhao Zeng"
        },
        {
          "authorId": "2383160598",
          "name": "Xinyu Wang"
        },
        {
          "authorId": "2382439729",
          "name": "Boxing Wang"
        },
        {
          "authorId": "2279546420",
          "name": "Philippe Langlais"
        },
        {
          "authorId": "2353084503",
          "name": "Yufei Cui"
        }
      ],
      "abstract": "The quadratic complexity of the attention mechanism in Transformer models has motivated the development of alternative architectures with sub-quadratic scaling, such as state-space models. Among these, Mamba has emerged as a leading architecture, achieving state-of-the-art results across a range of language modeling tasks. However, Mamba's performance significantly deteriorates when applied to contexts longer than those seen during pre-training, revealing a sharp sensitivity to context length extension. Through detailed analysis, we attribute this limitation to the out-of-distribution behaviour of its state-space dynamics, particularly within the parameterization of the state transition matrix $\\mathbf{A}$. Unlike recent works which attribute this sensitivity to the vanished accumulation of discretization time steps, $\\exp(-\\sum_{t=1}^N\\Delta_t)$, we establish a connection between state convergence behavior as the input length approaches infinity and the spectrum of the transition matrix $\\mathbf{A}$, offering a well-founded explanation of its role in length extension. Next, to overcome this challenge, we propose an approach that applies spectrum scaling to pre-trained Mamba models to enable robust long-context generalization by selectively modulating the spectrum of $\\mathbf{A}$ matrices in each layer. We show that this can significantly improve performance in settings where simply modulating $\\Delta_t$ fails, validating our insights and providing avenues for better length generalization of state-space models with structured transition matrices."
    },
    {
      "paperId": "33f95e78fd474054e7bc0a338fa96bd2d0085b24",
      "externalIds": {
        "DOI": "10.1109/GCCE65946.2025.11274789",
        "CorpusId": 283747787
      },
      "corpusId": 283747787,
      "title": "SSM-based Sequential Recommendation Considering Short-term Dependencies of Time and Adjacency",
      "venue": "Global Conference on Consumer Electronics",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/GCCE65946.2025.11274789?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/GCCE65946.2025.11274789, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2397884126",
          "name": "Keito Kozaki"
        },
        {
          "authorId": "118063648",
          "name": "Keigo Sakurai"
        },
        {
          "authorId": "3470264",
          "name": "Ren Togo"
        },
        {
          "authorId": "144392699",
          "name": "Takahiro Ogawa"
        },
        {
          "authorId": "144029207",
          "name": "M. Haseyama"
        }
      ],
      "abstract": "Sequential recommendation focuses on capturing user interests based on the chronological order of user\u2019s interaction history. Recently, State-Space Model (SSM), a state-of-the-art time-series machine learning model, has attracted attention in sequential recommendation modeling. SSM-based sequential recommendation models can effectively capture long-term user preferences, but modeling short-term dependencies is difficult. In this paper, we propose a new sequential recommendation model, which complements the short-term dependencies of SSM-based models in two aspects: item adjacencies and time intervals. To capture the adjacency between items, a graph structure is constructed from the sequences, and the time interval between items is captured in the time-aware SSM layer. Extensive experiments on two real-world datasets show that our model consistently outperforms state-of-the-art recommendation models."
    },
    {
      "paperId": "09f43762d0d976db6e77ee685167347e20a85d0d",
      "externalIds": {
        "DOI": "10.1109/GCCE65946.2025.11274607",
        "CorpusId": 283747623
      },
      "corpusId": 283747623,
      "title": "An Efficient DDoS-Attack Detection System based on Multi-Entropy Clustering and the Mamba Model",
      "venue": "Global Conference on Consumer Electronics",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/GCCE65946.2025.11274607?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/GCCE65946.2025.11274607, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2397882866",
          "name": "Hung-Wei Lai"
        },
        {
          "authorId": "2270587832",
          "name": "Chih-Chiang Wang"
        }
      ],
      "abstract": "Traditional DDoS detection systems do not cope well with large-scale diverse traffic in real time. To address this issue, this paper proposes an efficient DDoS detection system based on multi-entropy clustering and the Mamba architecture. It firstly extracts traffic features using Shannon entropy, R\u00e9nyi entropy, and min-entropy, then performs high-purity anomaly filtering via a PSO-optimized Gaussian Mixture Model. For mixed traffic, our system uses the Mamba model to enable fine-grained classification with linear time complexity while balancing accuracy and efficiency. The experimental results show that our system reduced the deep-analysis workload and the memory usage by about two-third, while achieving an F1-score of 98.3%, a recall rate of 98.4%, and an inference speed-up of 4.8\u00d7."
    },
    {
      "paperId": "e93171c2c3d3d34b502c9c6fa129cf922b89759b",
      "externalIds": {
        "DBLP": "journals/evi/SoniP25",
        "DOI": "10.1007/s12065-025-01090-2",
        "CorpusId": 281563810
      },
      "corpusId": 281563810,
      "title": "Emerging deep learning approaches for urban satellite image analysis: a survey on classification, segmentation, and change detection",
      "venue": "Evolutionary Intelligence",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s12065-025-01090-2?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s12065-025-01090-2, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2382213530",
          "name": "Tannu Kumar Soni"
        },
        {
          "authorId": "9347666",
          "name": "Pushpalata Pujari"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "82130f666dc91057f309985790112082e550c32a",
      "externalIds": {
        "DOI": "10.1007/s00371-025-04178-z",
        "CorpusId": 281515605
      },
      "corpusId": 281515605,
      "title": "Enhancing light field image super-resolution through Mamba-based spatial\u2013angular correlation learning",
      "venue": "The Visual Computer",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s00371-025-04178-z?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s00371-025-04178-z, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2314729459",
          "name": "Shaorui Chen"
        },
        {
          "authorId": "2314531665",
          "name": "Liang Chen"
        },
        {
          "authorId": "2381903958",
          "name": "Defeng Wu"
        },
        {
          "authorId": "2108053052",
          "name": "Yi Wu"
        },
        {
          "authorId": "2374270632",
          "name": "Na Qi"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "bbe49e46bee1f5b4ab8ae1bdaa3f29aece6a6a15",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-17514",
        "ArXiv": "2509.17514",
        "DOI": "10.48550/arXiv.2509.17514",
        "CorpusId": 281420316
      },
      "corpusId": 281420316,
      "title": "Achilles' Heel of Mamba: Essential difficulties of the Mamba architecture demonstrated by synthetic data",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.17514, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2299909046",
          "name": "Tianyi Chen"
        },
        {
          "authorId": "2278415720",
          "name": "Pengxiao Lin"
        },
        {
          "authorId": "2279762057",
          "name": "Zhiwei Wang"
        },
        {
          "authorId": "2233234408",
          "name": "Zhi Xu"
        }
      ],
      "abstract": "State Space Models (SSMs) have emerged as promising alternatives to attention mechanisms, with the Mamba architecture demonstrating impressive performance and linear complexity for processing long sequences. However, the fundamental differences between Mamba and Transformer architectures remain incompletely understood. In this work, we use carefully designed synthetic tasks to reveal Mamba's inherent limitations. Through experiments, we identify that Mamba's nonlinear convolution introduces an asymmetry bias that significantly impairs its ability to recognize symmetrical patterns and relationships. Using composite function and inverse sequence matching tasks, we demonstrate that Mamba strongly favors compositional solutions over symmetrical ones and struggles with tasks requiring the matching of reversed sequences. We show these limitations stem not from the SSM module itself but from the nonlinear convolution preceding it, which fuses token information asymmetrically. These insights provide a new understanding of Mamba's constraints and suggest concrete architectural improvements for future sequence models."
    },
    {
      "paperId": "01aefcf828f2be3f37a19bc03ed8a81e57f6ff67",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-17651",
        "ArXiv": "2509.17651",
        "DOI": "10.48550/arXiv.2509.17651",
        "CorpusId": 281420225
      },
      "corpusId": 281420225,
      "title": "SISMA: Semantic Face Image Synthesis with Mamba",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.17651, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2319793306",
          "name": "Filippo Botti"
        },
        {
          "authorId": "2292201265",
          "name": "Alex Ergasti"
        },
        {
          "authorId": "8841047",
          "name": "Tomaso Fontanini"
        },
        {
          "authorId": "2292198868",
          "name": "Claudio Ferrari"
        },
        {
          "authorId": "2261172250",
          "name": "Massimo Bertozzi"
        },
        {
          "authorId": "2261170519",
          "name": "Andrea Prati"
        }
      ],
      "abstract": "Diffusion Models have become very popular for Semantic Image Synthesis (SIS) of human faces. Nevertheless, their training and inference is computationally expensive and their computational requirements are high due to the quadratic complexity of attention layers. In this paper, we propose a novel architecture called SISMA, based on the recently proposed Mamba. SISMA generates high quality samples by controlling their shape using a semantic mask at a reduced computational demand. We validated our approach through comprehensive experiments with CelebAMask-HQ, revealing that our architecture not only achieves a better FID score yet also operates at three times the speed of state-of-the-art architectures. This indicates that the proposed design is a viable, lightweight substitute to transformer-based models."
    },
    {
      "paperId": "d46b3d852c4872c5fecb811ca4cd9e9c09378ff2",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-17711",
        "ArXiv": "2509.17711",
        "DOI": "10.48550/arXiv.2509.17711",
        "CorpusId": 281421097
      },
      "corpusId": 281421097,
      "title": "DA-Mamba: Dialogue-aware selective state-space model for multimodal engagement estimation",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.17711, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2381691840",
          "name": "Shenwei Kang"
        },
        {
          "authorId": "2381368410",
          "name": "Xin Zhang"
        },
        {
          "authorId": "2381464978",
          "name": "Wen Liu"
        },
        {
          "authorId": "2381865745",
          "name": "Bin Li"
        },
        {
          "authorId": "2330218989",
          "name": "Yujie Liu"
        },
        {
          "authorId": "2383509102",
          "name": "Bo Gao"
        }
      ],
      "abstract": "Human engagement estimation in conversational scenarios is essential for applications such as adaptive tutoring, remote healthcare assessment, and socially aware human--computer interaction. Engagement is a dynamic, multimodal signal conveyed by facial expressions, speech, gestures, and behavioral cues over time. In this work we introduce DA-Mamba, a dialogue-aware multimodal architecture that replaces attention-heavy dialogue encoders with Mamba-based selective state-space processing to achieve linear time and memory complexity while retaining expressive cross-modal reasoning. We design a Mamba dialogue-aware selective state-space model composed of three core modules: a Dialogue-Aware Encoder, and two Mamba-based fusion mechanisms: Modality-Group Fusion and Partner-Group Fusion, these modules achieve expressive dialogue understanding. Extensive experiments on three standard benchmarks (NoXi, NoXi-Add, and MPIIGI) show that DA-Mamba surpasses prior state-of-the-art (SOTA) methods in concordance correlation coefficient (CCC), while reducing training time and peak memory; these gains enable processing much longer sequences and facilitate real-time deployment in resource-constrained, multi-party conversational settings. The source code will be available at: https://github.com/kksssssss-ssda/MMEA."
    },
    {
      "paperId": "2f76b9c6aed01eef8556fda5a3102303aba7d694",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-17397",
        "ArXiv": "2509.17397",
        "DOI": "10.48550/arXiv.2509.17397",
        "CorpusId": 281421801
      },
      "corpusId": 281421801,
      "title": "Diff-GNSS: Diffusion-based Pseudorange Error Estimation",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.17397, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2275784907",
          "name": "Jiaqi Zhu"
        },
        {
          "authorId": "2237512756",
          "name": "Shouyi Lu"
        },
        {
          "authorId": "2326776082",
          "name": "Ziyao Li"
        },
        {
          "authorId": "2237326710",
          "name": "Guirong Zhuo"
        },
        {
          "authorId": "2265512532",
          "name": "Lu Xiong"
        }
      ],
      "abstract": "Global Navigation Satellite Systems (GNSS) are vital for reliable urban positioning. However, multipath and non-line-of-sight reception often introduce large measurement errors that degrade accuracy. Learning-based methods for predicting and compensating pseudorange errors have gained traction, but their performance is limited by complex error distributions. To address this challenge, we propose Diff-GNSS, a coarse-to-fine GNSS measurement (pseudorange) error estimation framework that leverages a conditional diffusion model to capture such complex distributions. Firstly, a Mamba-based module performs coarse estimation to provide an initial prediction with appropriate scale and trend. Then, a conditional denoising diffusion layer refines the estimate, enabling fine-grained modeling of pseudorange errors. To suppress uncontrolled generative diversity and achieve controllable synthesis, three key features related to GNSS measurement quality are used as conditions to precisely guide the reverse denoising process. We further incorporate per-satellite uncertainty modeling within the diffusion stage to assess the reliability of the predicted errors. We have collected and publicly released a real-world dataset covering various scenes. Experiments on public and self-collected datasets show that DiffGNSS consistently outperforms state-of-the-art baselines across multiple metrics. To the best of our knowledge, this is the first application of diffusion models to pseudorange error estimation. The proposed diffusion-based refinement module is plug-and-play and can be readily integrated into existing networks to markedly improve estimation accuracy."
    },
    {
      "paperId": "8237f2fc77f3c4b21d3e5c85acb9ee70ed1ba2b8",
      "externalIds": {
        "ArXiv": "2509.18467",
        "DBLP": "journals/corr/abs-2509-18467",
        "DOI": "10.48550/arXiv.2509.18467",
        "CorpusId": 281496900
      },
      "corpusId": 281496900,
      "title": "LAWCAT: Efficient Distillation from Quadratic to Linear Attention with Convolution across Tokens for Long Context Modeling",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.18467, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2243367696",
          "name": "Zeyu Liu"
        },
        {
          "authorId": "2965493",
          "name": "Souvik Kundu"
        },
        {
          "authorId": "2292670887",
          "name": "Lianghao Jiang"
        },
        {
          "authorId": "2269117149",
          "name": "Anni Li"
        },
        {
          "authorId": "3404365",
          "name": "S. Ronanki"
        },
        {
          "authorId": "40696276",
          "name": "S. Bodapati"
        },
        {
          "authorId": "1382640493",
          "name": "G. Datta"
        },
        {
          "authorId": "2658716",
          "name": "P. Beerel"
        }
      ],
      "abstract": "Although transformer architectures have achieved state-of-the-art performance across diverse domains, their quadratic computational complexity with respect to sequence length remains a significant bottleneck, particularly for latency-sensitive long-context applications. While recent linear-complexity alternatives are increasingly powerful, effectively training them from scratch is still resource-intensive. To overcome these limitations, we propose LAWCAT (Linear Attention with Convolution Across Time), a novel linearization framework designed to efficiently transfer the capabilities of pre-trained transformers into a performant linear attention architecture. LAWCAT integrates causal Conv1D layers to enhance local dependency modeling and employs normalized gated linear attention to improve generalization across varying context lengths. Our comprehensive evaluations demonstrate that, distilling Mistral-7B with only 1K-length sequences yields over 90\\% passkey retrieval accuracy up to 22K tokens, significantly extending its effective context window. Similarly, Llama3.2-1B LAWCAT variant achieves competitive performance on S-NIAH 1\\&2\\&3 tasks (1K-8K context length) and BABILong benchmark (QA2\\&QA3, 0K-16K context length), requiring less than 0.1\\% pre-training tokens compared with pre-training models. Furthermore, LAWCAT exhibits faster prefill speeds than FlashAttention-2 for sequences exceeding 8K tokens. LAWCAT thus provides an efficient pathway to high-performance, long-context linear models suitable for edge deployment, reducing reliance on extensive long-sequence training data and computational resources. Code is released at: https://github.com/zeyuliu1037/LAWCAT"
    },
    {
      "paperId": "059c7be744e8f636e5f730bf1032e3f2fd3d1e58",
      "externalIds": {
        "DBLP": "conf/recsys/ZhangZSXW25",
        "DOI": "10.1145/3705328.3748060",
        "CorpusId": 281196938
      },
      "corpusId": 281196938,
      "title": "Test-Time Alignment with State Space Model for Tracking User Interest Shifts in Sequential Recommendation",
      "venue": "ACM Conference on Recommender Systems",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3705328.3748060?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3705328.3748060, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2219692939",
          "name": "Changshuo Zhang"
        },
        {
          "authorId": "2305623872",
          "name": "Xiao Zhang"
        },
        {
          "authorId": "2296718641",
          "name": "Teng Shi"
        },
        {
          "authorId": "2305587350",
          "name": "Jun Xu"
        },
        {
          "authorId": "2260701602",
          "name": "Ji-rong Wen"
        }
      ],
      "abstract": "Sequential recommendation is essential in modern recommender systems, aiming to predict the next item a user may interact with based on their historical behaviors. However, real-world scenarios are often dynamic and subject to shifts in user interests. Conventional sequential recommendation models are typically trained on static historical data, limiting their ability to adapt to such shifts and resulting in significant performance degradation during testing. Recently, Test-Time Training (TTT) has emerged as a promising paradigm, enabling pre-trained models to dynamically adapt to test data by leveraging unlabeled examples during testing. However, applying TTT to effectively track and address user interest shifts in recommender systems remains an open and challenging problem. Key challenges include how to capture temporal information effectively and explicitly identifying shifts in user interests during the testing phase. To address these issues, we propose T2ARec, a novel model leveraging state space model for TTT by introducing two Test-Time Alignment modules tailored for sequential recommendation, effectively capturing the distribution shifts in user interest patterns over time. Specifically, T2ARec aligns absolute time intervals with model-adaptive learning intervals to capture temporal dynamics and introduce an interest state alignment mechanism to effectively and explicitly identify the user interest shifts with theoretical guarantees. These two alignment modules enable efficient and incremental updates to model parameters in a self-supervised manner during testing, enhancing predictions for online recommendation. Extensive evaluations on three benchmark datasets demonstrate that T2ARec achieves state-of-the-art performance and robustly mitigates the challenges posed by user interest shifts."
    },
    {
      "paperId": "a4dbe09d8d22e60b31745767cecf7bed5451f81c",
      "externalIds": {
        "DBLP": "journals/sivp/HeYWCMCT25",
        "DOI": "10.1007/s11760-025-04756-7",
        "CorpusId": 281581040
      },
      "corpusId": 281581040,
      "title": "MambaCA-Net: a hybrid dual-stream architecture integrating temporal and spatial features for breast ultrasound analysis",
      "venue": "Signal, Image and Video Processing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11760-025-04756-7?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11760-025-04756-7, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2382623435",
          "name": "Xu He"
        },
        {
          "authorId": "2351777923",
          "name": "Yu Yan"
        },
        {
          "authorId": "2351265750",
          "name": "Man Wang"
        },
        {
          "authorId": "8005964",
          "name": "Runqiu Cai"
        },
        {
          "authorId": "2351241459",
          "name": "Jingwu Ma"
        },
        {
          "authorId": "2351219238",
          "name": "Xiaowei Cai"
        },
        {
          "authorId": "2384409866",
          "name": "Ying Tong"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "03b46aed274d3246c3a0e7831809a39648e81538",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-17063",
        "ArXiv": "2509.17063",
        "DOI": "10.48550/arXiv.2509.17063",
        "CorpusId": 281421740
      },
      "corpusId": 281421740,
      "title": "TSGym: Design Choices for Deep Multivariate Time-Series Forecasting",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.17063, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2269799058",
          "name": "Shuang Liang"
        },
        {
          "authorId": "2204929921",
          "name": "Chaochuan Hou"
        },
        {
          "authorId": "2271170377",
          "name": "Xu Yao"
        },
        {
          "authorId": "2381404168",
          "name": "Shiping Wang"
        },
        {
          "authorId": "2249727206",
          "name": "Minqi Jiang"
        },
        {
          "authorId": "2004577591",
          "name": "Songqiao Han"
        },
        {
          "authorId": "2146285145",
          "name": "Hailiang Huang"
        }
      ],
      "abstract": "Recently, deep learning has driven significant advancements in multivariate time series forecasting (MTSF) tasks. However, much of the current research in MTSF tends to evaluate models from a holistic perspective, which obscures the individual contributions and leaves critical issues unaddressed. Adhering to the current modeling paradigms, this work bridges these gaps by systematically decomposing deep MTSF methods into their core, fine-grained components like series-patching tokenization, channel-independent strategy, attention modules, or even Large Language Models and Time-series Foundation Models. Through extensive experiments and component-level analysis, our work offers more profound insights than previous benchmarks that typically discuss models as a whole. Furthermore, we propose a novel automated solution called TSGym for MTSF tasks. Unlike traditional hyperparameter tuning, neural architecture searching or fixed model selection, TSGym performs fine-grained component selection and automated model construction, which enables the creation of more effective solutions tailored to diverse time series data, therefore enhancing model transferability across different data sources and robustness against distribution shifts. Extensive experiments indicate that TSGym significantly outperforms existing state-of-the-art MTSF and AutoML methods. All code is publicly available on https://github.com/SUFE-AILAB/TSGym."
    },
    {
      "paperId": "c6b58205104aae91112af57f3e6751f9a4a4803c",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-17141",
        "ArXiv": "2509.17141",
        "DOI": "10.48550/arXiv.2509.17141",
        "CorpusId": 281420301
      },
      "corpusId": 281420301,
      "title": "History-Aware Visuomotor Policy Learning via Point Tracking",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.17141, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2323513308",
          "name": "Jingjing Chen"
        },
        {
          "authorId": "2152115958",
          "name": "Hongjie Fang"
        },
        {
          "authorId": "2109436791",
          "name": "Chenxi Wang"
        },
        {
          "authorId": "2135571875",
          "name": "Shiquan Wang"
        },
        {
          "authorId": "2281998765",
          "name": "Cewu Lu"
        }
      ],
      "abstract": "Many manipulation tasks require memory beyond the current observation, yet most visuomotor policies rely on the Markov assumption and thus struggle with repeated states or long-horizon dependencies. Existing methods attempt to extend observation horizons but remain insufficient for diverse memory requirements. To this end, we propose an object-centric history representation based on point tracking, which abstracts past observations into a compact and structured form that retains only essential task-relevant information. Tracked points are encoded and aggregated at the object level, yielding a compact history representation that can be seamlessly integrated into various visuomotor policies. Our design provides full history-awareness with high computational efficiency, leading to improved overall task performance and decision accuracy. Through extensive evaluations on diverse manipulation tasks, we show that our method addresses multiple facets of memory requirements - such as task stage identification, spatial memorization, and action counting, as well as longer-term demands like continuous and pre-loaded memory - and consistently outperforms both Markovian baselines and prior history-based approaches. Project website: http://tonyfang.net/history"
    },
    {
      "paperId": "cdd48c02eeb7f6f1340e7310b21ec02073f157fc",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-17172",
        "ArXiv": "2509.17172",
        "DOI": "10.48550/arXiv.2509.17172",
        "CorpusId": 281421660
      },
      "corpusId": 281421660,
      "title": "SynergyNet: Fusing Generative Priors and State-Space Models for Facial Beauty Prediction",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.17172, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2267648129",
          "name": "D. E. Boukhari"
        }
      ],
      "abstract": "The automated prediction of facial beauty is a benchmark task in affective computing that requires a sophisticated understanding of both local aesthetic details (e.g., skin texture) and global facial harmony (e.g., symmetry, proportions). Existing models, based on either Convolutional Neural Networks (CNNs) or Vision Transformers (ViTs), exhibit inherent architectural biases that limit their performance; CNNs excel at local feature extraction but struggle with long-range dependencies, while ViTs model global relationships at a significant computational cost. This paper introduces the \\textbf{Mamba-Diffusion Network (MD-Net)}, a novel dual-stream architecture that resolves this trade-off by delegating specialized roles to state-of-the-art models. The first stream leverages a frozen U-Net encoder from a pre-trained latent diffusion model, providing a powerful generative prior for fine-grained aesthetic qualities. The second stream employs a Vision Mamba (Vim), a modern state-space model, to efficiently capture global facial structure with linear-time complexity. By synergistically integrating these complementary representations through a cross-attention mechanism, MD-Net creates a holistic and nuanced feature space for prediction. Evaluated on the SCUT-FBP5500 benchmark, MD-Net sets a new state-of-the-art, achieving a Pearson Correlation of \\textbf{0.9235} and demonstrating the significant potential of hybrid architectures that fuse generative and sequential modeling paradigms for complex visual assessment tasks."
    },
    {
      "paperId": "d3d0b535febb273d07c760648be56b728a600e19",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-16618",
        "ArXiv": "2509.16618",
        "DOI": "10.1007/978-3-032-05114-1_55",
        "CorpusId": 281421658
      },
      "corpusId": 281421658,
      "title": "Surgical-MambaLLM: Mamba2-Enhanced Multimodal Large Language Model for VQLA in Robotic Surgery",
      "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.16618, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2345389215",
          "name": "Pengfei Hao"
        },
        {
          "authorId": "2274928570",
          "name": "Hongqiu Wang"
        },
        {
          "authorId": "2370987247",
          "name": "Shuaibo Li"
        },
        {
          "authorId": "153107262",
          "name": "Zhaohu Xing"
        },
        {
          "authorId": "2310506217",
          "name": "Guang Yang"
        },
        {
          "authorId": "2381914386",
          "name": "Kaishun Wu"
        },
        {
          "authorId": "2348189926",
          "name": "Lei Zhu"
        }
      ],
      "abstract": "In recent years, Visual Question Localized-Answering in robotic surgery (Surgical-VQLA) has gained significant attention for its potential to assist medical students and junior doctors in understanding surgical scenes. Recently, the rapid development of Large Language Models (LLMs) has provided more promising solutions for this task. However, current methods struggle to establish complex dependencies between text and visual details, and have difficulty perceiving the spatial information of surgical scenes. To address these challenges, we propose a novel method, Surgical-MambaLLM, which is the first to combine Mamba2 with LLM in the surgical domain, that leverages Mamba2's ability to effectively capture cross-modal dependencies and perceive spatial information in surgical scenes, thereby enhancing the LLMs'understanding of surgical images. Specifically, we propose the Cross-modal Bidirectional Mamba2 Integration (CBMI) module to leverage Mamba2 for effective multimodal fusion, with its cross-modal integration capabilities. Additionally, tailored to the geometric characteristics of surgical scenes, we design the Surgical Instrument Perception (SIP) scanning mode for Mamba2 to scan the surgical images, enhancing the model's spatial understanding of the surgical scene. Extensive experiments demonstrate that our Surgical-MambaLLM model outperforms the state-of-the-art methods on the EndoVis17-VQLA and EndoVis18-VQLA datasets, significantly improving the performance of the Surgical-VQLA task."
    },
    {
      "paperId": "d173d95add13f9423854122cbddd05cf1d0305b8",
      "externalIds": {
        "ArXiv": "2509.22691",
        "DBLP": "journals/corr/abs-2509-22691",
        "DOI": "10.48550/arXiv.2509.22691",
        "CorpusId": 281674230
      },
      "corpusId": 281674230,
      "title": "Sequential Token Merging: Revisiting Hidden States",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.22691, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2365373587",
          "name": "Yan Wen"
        },
        {
          "authorId": "2352946347",
          "name": "Peng Ye"
        },
        {
          "authorId": "2344387380",
          "name": "Lin Zhang"
        },
        {
          "authorId": "2145518289",
          "name": "Baopu Li"
        },
        {
          "authorId": "2174741399",
          "name": "Jiakang Yuan"
        },
        {
          "authorId": "2344974004",
          "name": "Yaoxin Yang"
        },
        {
          "authorId": "2341974994",
          "name": "Tao Chen"
        }
      ],
      "abstract": "Vision Mambas (ViMs) achieve remarkable success with sub-quadratic complexity, but their efficiency remains constrained by quadratic token scaling with image resolution. While existing methods address token redundancy, they overlook ViMs'intrinsic Limited Directional Sequential Dependence (LDSD) - a critical information flow mechanism revealed in our analysis. We further identify Mamba's selective scan enables gradual information aggregation in hidden states. Based on these insights, we propose Sequential Token Merging (STM), featuring: 1) Bidirectional nearest neighbor merging to preserve sequential dependencies through symmetric spatial aggregation, and 2) Hidden states protection to stabilize the hidden states around the class token. STM strategically leverages Mamba's layer-wise loss convergence to convert temporal forgetfulness into stability. Experiments demonstrate STM's superiority: 1.0% accuracy drop for ViM-Ti at 20% token reduction, and only 1.4% degradation for ViM-S at 40% reduction. Our method achieves state-of-the-art efficiency with minimal complexity, while providing new insights into state-space model dynamics. Codes will be released soon."
    },
    {
      "paperId": "7746e512bc5c578f9f214756a88131276e056d39",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-16445",
        "ArXiv": "2509.16445",
        "DOI": "10.48550/arXiv.2509.16445",
        "CorpusId": 281421834
      },
      "corpusId": 281421834,
      "title": "FiLM-Nav: Efficient and Generalizable Navigation via VLM Fine-tuning",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.16445, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "49327690",
          "name": "Naoki Yokoyama"
        },
        {
          "authorId": "2241366412",
          "name": "Sehoon Ha"
        }
      ],
      "abstract": "Enabling robotic assistants to navigate complex environments and locate objects described in free-form language is a critical capability for real-world deployment. While foundation models, particularly Vision-Language Models (VLMs), offer powerful semantic understanding, effectively adapting their web-scale knowledge for embodied decision-making remains a key challenge. We present FiLM-Nav (Fine-tuned Language Model for Navigation), an approach that directly fine-tunes pre-trained VLM as the navigation policy. In contrast to methods that use foundation models primarily in a zero-shot manner or for map annotation, FiLM-Nav learns to select the next best exploration frontier by conditioning directly on raw visual trajectory history and the navigation goal. Leveraging targeted simulated embodied experience allows the VLM to ground its powerful pre-trained representations in the specific dynamics and visual patterns relevant to goal-driven navigation. Critically, fine-tuning on a diverse data mixture combining ObjectNav, OVON, ImageNav, and an auxiliary spatial reasoning task proves essential for achieving robustness and broad generalization. FiLM-Nav sets a new state-of-the-art in both SPL and success rate on HM3D ObjectNav among open-vocabulary methods, and sets a state-of-the-art SPL on the challenging HM3D-OVON benchmark, demonstrating strong generalization to unseen object categories. Our work validates that directly fine-tuning VLMs on diverse simulated embodied data is a highly effective pathway towards generalizable and efficient semantic navigation capabilities."
    },
    {
      "paperId": "130463ae56adb37938ec7646441f1d694220ede2",
      "externalIds": {
        "ArXiv": "2509.15689",
        "DBLP": "journals/corr/abs-2509-15689",
        "DOI": "10.48550/arXiv.2509.15689",
        "CorpusId": 281411089
      },
      "corpusId": 281411089,
      "title": "Interpretable Modeling of Articulatory Temporal Dynamics from real-time MRI for Phoneme Recognition",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.15689, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2381294897",
          "name": "Jay Park"
        },
        {
          "authorId": "2323202020",
          "name": "Hong Nguyen"
        },
        {
          "authorId": "2322505619",
          "name": "Sean Foley"
        },
        {
          "authorId": "2306075843",
          "name": "Jihwan Lee"
        },
        {
          "authorId": "2362630767",
          "name": "Yoonjeong Lee"
        },
        {
          "authorId": "2248267338",
          "name": "Dani Byrd"
        },
        {
          "authorId": "2269736677",
          "name": "Shrikanth S. Narayanan"
        }
      ],
      "abstract": "Real-time Magnetic Resonance Imaging (rtMRI) visualizes vocal tract action, offering a comprehensive window into speech articulation. However, its signals are high dimensional and noisy, hindering interpretation. We investigate compact representations of spatiotemporal articulatory dynamics for phoneme recognition from midsagittal vocal tract rtMRI videos. We compare three feature types: (1) raw video, (2) optical flow, and (3) six linguistically-relevant regions of interest (ROIs) for articulator movements. We evaluate models trained independently on each representation, as well as multi-feature combinations. Results show that multi-feature models consistently outperform single-feature baselines, with the lowest phoneme error rate (PER) of 0.34 obtained by combining ROI and raw video. Temporal fidelity experiments demonstrate a reliance on fine-grained articulatory dynamics, while ROI ablation studies reveal strong contributions from tongue and lips. Our findings highlight how rtMRI-derived features provide accuracy and interpretability, and establish strategies for leveraging articulatory data in speech processing."
    },
    {
      "paperId": "e4b958f82d6a7e3c8a5ab8050c88e02c44322313",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-15680",
        "ArXiv": "2509.15680",
        "DOI": "10.48550/arXiv.2509.15680",
        "CorpusId": 281411501
      },
      "corpusId": 281411501,
      "title": "Mamba-2 audio captioning: design space exploration and analysis",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.15680, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2331317933",
          "name": "TaeHan Lee"
        },
        {
          "authorId": "2381462642",
          "name": "Jaehan Jung"
        },
        {
          "authorId": "2353318057",
          "name": "Hyukjun Lee"
        }
      ],
      "abstract": "We present an audio captioning model built on the Mamba-2 large language model backbone, which is a state-of-the-art (SOTA) state-space model (SSM). We systematically explore the design space: LLM sizes, LoRA ranks, and connector designs leveraging Mamba-2's linear-time complexity with respect to sequence length. Across benchmarks, our models achieve strong captioning performance compared with larger language models trained on the same dataset, despite using fewer parameters. For the first time, we conduct an in-depth analysis of how the number of LLM parameters, audio encoder fine-tuning strategies, audio feature diversity, and different feature reduction or expansion techniques affect performance."
    },
    {
      "paperId": "9ed51929b8c00d769477cb163730081c2e5f47ff",
      "externalIds": {
        "ArXiv": "2509.16017",
        "DBLP": "journals/corr/abs-2509-16017",
        "DOI": "10.48550/arXiv.2509.16017",
        "CorpusId": 281411153
      },
      "corpusId": 281411153,
      "title": "DistillMatch: Leveraging Knowledge Distillation from Vision Foundation Model for Multimodal Image Matching",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.16017, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2381640419",
          "name": "Meng Yang"
        },
        {
          "authorId": "40513331",
          "name": "Fan Fan"
        },
        {
          "authorId": "2145275245",
          "name": "Zizhuo Li"
        },
        {
          "authorId": "2376500848",
          "name": "Songchu Deng"
        },
        {
          "authorId": "95952596",
          "name": "Yong Ma"
        },
        {
          "authorId": "2318287334",
          "name": "Jiayi Ma"
        }
      ],
      "abstract": "Multimodal image matching seeks pixel-level correspondences between images of different modalities, crucial for cross-modal perception, fusion and analysis. However, the significant appearance differences between modalities make this task challenging. Due to the scarcity of high-quality annotated datasets, existing deep learning methods that extract modality-common features for matching perform poorly and lack adaptability to diverse scenarios. Vision Foundation Model (VFM), trained on large-scale data, yields generalizable and robust feature representations adapted to data and tasks of various modalities, including multimodal matching. Thus, we propose DistillMatch, a multimodal image matching method using knowledge distillation from VFM. DistillMatch employs knowledge distillation to build a lightweight student model that extracts high-level semantic features from VFM (including DINOv2 and DINOv3) to assist matching across modalities. To retain modality-specific information, it extracts and injects modality category information into the other modality's features, which enhances the model's understanding of cross-modal correlations. Furthermore, we design V2I-GAN to boost the model's generalization by translating visible to pseudo-infrared images for data augmentation. Experiments show that DistillMatch outperforms existing algorithms on public datasets."
    },
    {
      "paperId": "459fac2863125af1f7adc6849e7f2dbdccf074df",
      "externalIds": {
        "ArXiv": "2509.15763",
        "DBLP": "journals/corr/abs-2509-15763",
        "DOI": "10.48550/arXiv.2509.15763",
        "CorpusId": 281411076
      },
      "corpusId": 281411076,
      "title": "UniGist: Towards General and Hardware-aligned Sequence-level Long Context Compression",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.15763, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2057946598",
          "name": "Chenlong Deng"
        },
        {
          "authorId": "2326349647",
          "name": "Zhisong Zhang"
        },
        {
          "authorId": "1580228663",
          "name": "Kelong Mao"
        },
        {
          "authorId": "2308037430",
          "name": "Shuaiyi Li"
        },
        {
          "authorId": "2044202073",
          "name": "Tianqing Fang"
        },
        {
          "authorId": "2254831297",
          "name": "Hongming Zhang"
        },
        {
          "authorId": "2238955130",
          "name": "Haitao Mi"
        },
        {
          "authorId": "2326519013",
          "name": "Dong Yu"
        },
        {
          "authorId": "2273086037",
          "name": "Zhicheng Dou"
        }
      ],
      "abstract": "Large language models are increasingly capable of handling long-context inputs, but the memory overhead of key-value (KV) cache remains a major bottleneck for general-purpose deployment. While various compression strategies have been explored, sequence-level compression, which drops the full KV caches for certain tokens, is particularly challenging as it can lead to the loss of important contextual information. To address this, we introduce UniGist, a sequence-level long-context compression framework that efficiently preserves context information by replacing raw tokens with special compression tokens (gists) in a fine-grained manner. We adopt a chunk-free training strategy and design an efficient kernel with a gist shift trick, enabling optimized GPU training. Our scheme also supports flexible inference by allowing the actual removal of compressed tokens, resulting in real-time memory savings. Experiments across multiple long-context tasks demonstrate that UniGist significantly improves compression quality, with especially strong performance in detail-recalling tasks and long-range dependency modeling."
    },
    {
      "paperId": "4183dcdf3b878ccc65769e164f2cf9fb2d972eee",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-15857",
        "ArXiv": "2509.15857",
        "DOI": "10.48550/arXiv.2509.15857",
        "CorpusId": 281410892
      },
      "corpusId": 281410892,
      "title": "EvoBrain: Dynamic Multi-channel EEG Graph Modeling for Time-evolving Brain Network",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.15857, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2310332113",
          "name": "Rikuto Kotoge"
        },
        {
          "authorId": "2306160786",
          "name": "Zheng Chen"
        },
        {
          "authorId": "2268876138",
          "name": "Tasuku Kimura"
        },
        {
          "authorId": "1744555",
          "name": "Yasuko Matsubara"
        },
        {
          "authorId": "2281018588",
          "name": "T. Yanagisawa"
        },
        {
          "authorId": "2304700490",
          "name": "H. Kishima"
        },
        {
          "authorId": "2268847622",
          "name": "Yasushi Sakurai"
        }
      ],
      "abstract": "Dynamic GNNs, which integrate temporal and spatial features in Electroencephalography (EEG) data, have shown great potential in automating seizure detection. However, fully capturing the underlying dynamics necessary to represent brain states, such as seizure and non-seizure, remains a non-trivial task and presents two fundamental challenges. First, most existing dynamic GNN methods are built on temporally fixed static graphs, which fail to reflect the evolving nature of brain connectivity during seizure progression. Second, current efforts to jointly model temporal signals and graph structures and, more importantly, their interactions remain nascent, often resulting in inconsistent performance. To address these challenges, we present the first theoretical analysis of these two problems, demonstrating the effectiveness and necessity of explicit dynamic modeling and time-then-graph dynamic GNN method. Building on these insights, we propose EvoBrain, a novel seizure detection model that integrates a two-stream Mamba architecture with a GCN enhanced by Laplacian Positional Encoding, following neurological insights. Moreover, EvoBrain incorporates explicitly dynamic graph structures, allowing both nodes and edges to evolve over time. Our contributions include (a) a theoretical analysis proving the expressivity advantage of explicit dynamic modeling and time-then-graph over other approaches, (b) a novel and efficient model that significantly improves AUROC by 23% and F1 score by 30%, compared with the dynamic GNN baseline, and (c) broad evaluations of our method on the challenging early seizure prediction tasks."
    },
    {
      "paperId": "720b5bd8257fecdf3feb04cdcc5545b2412b4ec5",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-16345",
        "ArXiv": "2509.16345",
        "DOI": "10.48550/arXiv.2509.16345",
        "CorpusId": 281420758
      },
      "corpusId": 281420758,
      "title": "Estimating Clinical Lab Test Result Trajectories from PPG using Physiological Foundation Model and Patient-Aware State Space Model - a UNIPHY+ Approach",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.16345, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2381580954",
          "name": "Minxiao Wang"
        },
        {
          "authorId": "2298043599",
          "name": "Runze Yan"
        },
        {
          "authorId": "2381385152",
          "name": "Carol Li"
        },
        {
          "authorId": "2345003097",
          "name": "Saurabh Kataria"
        },
        {
          "authorId": "2306519652",
          "name": "Xiao Hu"
        },
        {
          "authorId": "2306732166",
          "name": "Matthew Clark"
        },
        {
          "authorId": "2324512247",
          "name": "Timothy Ruchti"
        },
        {
          "authorId": "2246068685",
          "name": "Timothy G. Buchman"
        },
        {
          "authorId": "2343365118",
          "name": "Sivasubramanium V. Bhavani"
        },
        {
          "authorId": "2258660803",
          "name": "Randall J. Lee"
        }
      ],
      "abstract": "Clinical laboratory tests provide essential biochemical measurements for diagnosis and treatment, but are limited by intermittent and invasive sampling. In contrast, photoplethysmogram (PPG) is a non-invasive, continuously recorded signal in intensive care units (ICUs) that reflects cardiovascular dynamics and can serve as a proxy for latent physiological changes. We propose UNIPHY+Lab, a framework that combines a large-scale PPG foundation model for local waveform encoding with a patient-aware Mamba model for long-range temporal modeling. Our architecture addresses three challenges: (1) capturing extended temporal trends in laboratory values, (2) accounting for patient-specific baseline variation via FiLM-modulated initial states, and (3) performing multi-task estimation for interrelated biomarkers. We evaluate our method on the two ICU datasets for predicting the five key laboratory tests. The results show substantial improvements over the LSTM and carry-forward baselines in MAE, RMSE, and $R^2$ among most of the estimation targets. This work demonstrates the feasibility of continuous, personalized lab value estimation from routine PPG monitoring, offering a pathway toward non-invasive biochemical surveillance in critical care."
    },
    {
      "paperId": "cb5046f287cedaa460878962572a004125a26b58",
      "externalIds": {
        "PubMedCentral": "12491050",
        "DOI": "10.3389/fonc.2025.1647701",
        "CorpusId": 281490471,
        "PubMed": "41049845"
      },
      "corpusId": 281490471,
      "title": "Evaluation of recent lightweight deep learning architectures for lung cancer CT classification",
      "venue": "Frontiers in Oncology",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12491050, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2324195550",
          "name": "Mennaallah Mahmoud"
        },
        {
          "authorId": "14319990",
          "name": "Yan-hang Wen"
        },
        {
          "authorId": "17974102",
          "name": "Xiaohuan Pan"
        },
        {
          "authorId": "2212546378",
          "name": "Yuling Liufu"
        },
        {
          "authorId": "2313504990",
          "name": "Yubao Guan"
        }
      ],
      "abstract": "Introduction While numerous large and complex deep learning architectures continue to be developed for medical imaging, clinical adoption remains limited to a small number of established models. Recent lightweight architectures, despite showing promise in computer vision tasks, are underutilized or have never been applied to medical imaging applications, particularly lung cancer classification. This study evaluates the performance of recently developed lightweight models that have received limited attention in medical imaging tasks, establishing comprehensive baseline comparisons to guide evidence-based selection for clinical deployment in resource-constrained environments. Methods Using CT images, we assessed three lightweight pre-trained models\u2014MobileOne-S0, FastViT-S12, and MambaOut-Femto for lung cancer categorization. Performance measures (accuracy, AUC) and efficiency measures (inference time, number of parameters) were contrasted. Used were a public dataset (95 cases) and a private dataset (274 cases). Resampling and data augmentation constituted part of image preparation. Five-fold cross-validation helped to validate model performance. Results With the lowest inference time and modest parameters, MambaOut-Femto displayed the best efficiency. While FastViT-S12 had the largest memory usage, MobileOne-S0 used fewer parameters. On Dataset 1, MambaOut-Femto obtained a mean accuracy of 0.896 \u00b1 0.014 and an (Area under the curve) AUC of 0.972 \u00b1 0.004; on Dataset 2, accuracy was 0.916 \u00b1 0.040. When compared to traditional models like ResNet and Swin Transformer on the same datasets and under the same hyperparameters, the lightweight models outperformed them with significantly lower memory usage and fewer FLOPs. Discussion The lightweight models demonstrated superior efficiency and comparable performance to traditional models, making them ideal for deployment in low-resource settings where computational resources are limited. These findings highlight the potential for practical use in clinical workflows, overcoming barriers associated with traditional models."
    },
    {
      "paperId": "b974488f794a5057cac412d36aade4dab90d397b",
      "externalIds": {
        "DOI": "10.1101/2025.09.18.677041",
        "CorpusId": 282315763
      },
      "corpusId": 282315763,
      "title": "Short-term performance prediction in team sports using tracking data",
      "venue": "bioRxiv",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1101/2025.09.18.677041?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1101/2025.09.18.677041, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2352979003",
          "name": "Arnaud Odet"
        },
        {
          "authorId": "2352978839",
          "name": "Thomas B\u00e9chard"
        },
        {
          "authorId": "2307965440",
          "name": "S. D\u00e9jean"
        },
        {
          "authorId": "2352978728",
          "name": "Cristian Pasquaretta"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "d1b29e50775be1446cc286fbb370fde47e504cd2",
      "externalIds": {
        "DOI": "10.1117/12.3077417",
        "CorpusId": 281443083
      },
      "corpusId": 281443083,
      "title": "A deep visual odometry architecture integrating spatial convolution and selective sequence modeling",
      "venue": "International Conference on Machine Vision, Automatic Identification and Detection",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1117/12.3077417?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1117/12.3077417, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2386566357",
          "name": "hao yin"
        }
      ],
      "abstract": "This paper investigates a monocular visual odometry (VO) method based on deep learning. In recent years, deep learning techniques have demonstrated significant advantages in VO applications, effectively replacing complex engineering steps such as feature extraction and outlier rejection in traditional VO pipelines. To this end, we propose a novel network architecture that integrates ego-motion estimation with sequence-based deep neural network learning. The proposed method employs a Convolutional Neural Network (CNN) to directly extract features from raw RGB image sequences and introduces the Mamba module\u2014based on state space modeling\u2014as a replacement for conventional recurrent neural networks (RNNs) to capture inter-frame dynamics. The network estimates the relative six degrees-of-freedom (6-DoF) camera pose between each frame in the sequence and is capable of implicitly learning the absolute scale without requiring camera intrinsic parameters. This enables full trajectory reconstruction without any post-calibration. Extensive experiments conducted on the KITTI dataset demonstrate that the proposed method achieves superior accuracy and robustness compared to both traditional and other deep learning-based VO approaches."
    },
    {
      "paperId": "ceaa49f02c0ff7695ee31a9efb29a47adfc6b462",
      "externalIds": {
        "DOI": "10.1109/AIIM67611.2025.11233116",
        "CorpusId": 283100837
      },
      "corpusId": 283100837,
      "title": "SFC-Net:A Spatial Feature Correlation-Driven Pyramid Network for Medical Image Registration",
      "venue": "2025 5th International Symposium on Artificial Intelligence and Intelligent Manufacturing (AIIM)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/AIIM67611.2025.11233116?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/AIIM67611.2025.11233116, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2393145356",
          "name": "Lukai Jiang"
        },
        {
          "authorId": "2393570127",
          "name": "Shangce Wang"
        },
        {
          "authorId": "2301417542",
          "name": "Tuo Li"
        },
        {
          "authorId": "2153558789",
          "name": "Xiaofeng Zou"
        },
        {
          "authorId": "2381936298",
          "name": "Yang Yu"
        }
      ],
      "abstract": "Medical image registration, as one of the core tasks in medical image analysis, plays a crucial role in clinical diagnosis and treatment planning. Although deep learning-based methods have achieved remarkable progress in registration accuracy and efficiency, existing approaches often directly map image features to deformation fields, which lack explicit modeling of feature correspondences. This deficiency makes it challenging for models to capture complex spatial deformation patterns effectively. This study proposes a Spatial Feature Correlation-driven Pyramid Registration Network (SFC-Net), which provides driving signals for deformation field generation by explicitly calculating the correspondences between moving and fixed image features. To accurately capture cross-image correspondences, we design a Spatial Feature Correlation Module (SFC) based on an Adaptive Frequency Filter (AFF), which enhances structure-relevant features through dynamic frequency-domain feature selection and quantifies feature matching degrees using a feature correlation computation layer. Furthermore, the network employs a coarse-to-fine registration strategy to progressively transform multi-scale spatial correspondences into high-precision deformation fields. Our experimental results on medical image datasets demonstrate that the proposed SFC-Net achieves state-of-the-art registration performance."
    },
    {
      "paperId": "3bd5a335097add63dd243614868b5cedc841c7d6",
      "externalIds": {
        "DOI": "10.1109/ICPRE67300.2025.11273990",
        "CorpusId": 283747951
      },
      "corpusId": 283747951,
      "title": "EFAMamba: An Efficient Feature Aggregation Model based on Mamba for Accurate Wind Power Forecasting",
      "venue": "IEEE International Conference on Power and Renewable Energy",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICPRE67300.2025.11273990?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICPRE67300.2025.11273990, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2398726528",
          "name": "Xiangyue Liu"
        },
        {
          "authorId": "2291444946",
          "name": "Hongwei Zhao"
        },
        {
          "authorId": "2248146078",
          "name": "Lei Liu"
        },
        {
          "authorId": "2201412609",
          "name": "Xue Dong"
        },
        {
          "authorId": "2314052361",
          "name": "Qiuju Chen"
        },
        {
          "authorId": "2314068835",
          "name": "Bin Li"
        }
      ],
      "abstract": "Accurate wind power prediction is of great significance for ensuring the stable operation of power systems. Although the rapid development of deep learning technology has significantly improved the accuracy of wind power prediction, existing methods often lack sufficient capability for information interaction among variables when processing multivariate time series data, and struggle to capture long-range dependencies in longer sequences, which limits their predictive performance. To address this, this paper proposes a novel model named Efficient Feature Aggregation based on Mamba (EFAMamba) for multistep wind power prediction. This model introduces Mamba to more effectively capture long-term dependencies in time series. The EFA module aggregates all variable sequences into a global representation, which is then dispatched and fused with each variable sequence representation, significantly promoting information interaction among variables. The entire framework of EFAMamba can comprehensively consider historical data of multiple variables as well as future meteorological variable information, making it particularly suitable for multi-step wind power prediction. Extensive experiments conducted on real wind farm dataset have validated that the proposed EFAMamba demonstrates significant advantages over current state-of-the-art (SOTA) methods in terms of prediction accuracy for multi-step forecasting tasks, particularly excelling under longer prediction horizons. Specifically, when predicting 72 hours ahead, the NMAE, NRMSE, and QR are improved by 9.96%, 3.77%, and 1.39%, respectively, demonstrating the effectiveness of the proposed EFAMamba in wind power prediction. The source code and datasets are available at https://github.com/USTCAI4EEE/EFAMamba."
    },
    {
      "paperId": "abe9cdb0c02ad25a95a34d16649f86044adee95e",
      "externalIds": {
        "DOI": "10.1109/AICIT65974.2025.11282778",
        "CorpusId": 283924734
      },
      "corpusId": 283924734,
      "title": "HFE-Traffic: Frequency-Oriented Encrypted Traffic Classification with Mamba State Space Models",
      "venue": "2025 4th International Conference on Artificial Intelligence and Computer Information Technology (AICIT)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/AICIT65974.2025.11282778?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/AICIT65974.2025.11282778, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": null,
          "name": "Siyuan Liu"
        },
        {
          "authorId": "2399134892",
          "name": "Jing Yang"
        },
        {
          "authorId": "2399109517",
          "name": "Xuezheng Wei"
        },
        {
          "authorId": null,
          "name": "Zhaoshan Fan"
        },
        {
          "authorId": "2345375445",
          "name": "Bo An"
        },
        {
          "authorId": "2399104970",
          "name": "Lei Guo"
        },
        {
          "authorId": "2399072186",
          "name": "Fanjie Zeng"
        }
      ],
      "abstract": "Accurate and efficient c lassification of encrypted traffic i s c rucial f or m odern n etwork m anagement, enabling vital tasks such as Quality of Service (QoS) assurance, anomaly detection, and security enforcement. However, this task is fundamentally challenged by the absence of payload visibility and the volatility of time-domain features, which are easily distorted by network jitter and padding. Current methods often yield fragile models or rely on heavyweight architectures unsuitable for real-time deployment. To address these issues, we propose HFE-Traffic, a n ovel f ramework t hat p ivots from unstable temporal patterns to robust, frequency-oriented representations. Our approach transforms packet-length and inter-arrival-time sequences into the spectral domain using multi-scale Fast Fourier Transform (FFT) and lagged autocorrelation, effectively uncovering periodic structures resilient to obfuscation. This representation is enriched by statistical indicators and processed by a Mamba state-space model to capture long-range dependencies with linear complexity. On the ISCX-VPN2016 and USTC-TFC2016 benchmarks, HFE-Traffic a chieves state-of-the-art accuracy, outperforming strong Transformer baselines by an average of \u223c2 percentage points, and demonstrates superior robustness in an open-world setting. Coupled with a low inference cost of approximately 2.3 ms per flow, o ur work presents a practical and effective solution that balances accuracy, robustness, and deployability."
    },
    {
      "paperId": "07946112246bce20ff3a4a3c4501df8f720d658b",
      "externalIds": {
        "DOI": "10.1109/AIAHPC66801.2025.11290302",
        "CorpusId": 284024237
      },
      "corpusId": 284024237,
      "title": "A Lightweight Mamba Enhanced U-Net for Accelerated MRI Reconstruction",
      "venue": "2025 5th International Conference on Artificial Intelligence, Automation and High Performance Computing (AIAHPC)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/AIAHPC66801.2025.11290302?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/AIAHPC66801.2025.11290302, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2399807824",
          "name": "Xiangyi Chen"
        },
        {
          "authorId": null,
          "name": "Hongwei Du"
        }
      ],
      "abstract": "In recent years, deep learning-based magnetic resonance imaging reconstruction has achieved remarkable progress, particularly with convolutional neural networks and Transformer architectures. Nevertheless, these approaches still face challenges in effectively modeling long-range dependencies and maintaining computational efficiency, which limits their applicability in real-world clinical settings. To overcome these limitations, we introduce a lightweight hybrid network architecture. Built upon the U-Net framework, the proposed model incorporates a linear-complexity Mamba sequence modeling module, a multi-scale wavelet convolutional decoder, and an Inception-inspired multi-scale bottleneck design. This hybrid design not only reduces the number of model parameters and computational overhead but also enhances reconstruction quality. Experiments on public dataset demonstrate that our method outperforms existing mainstream approaches across multiple evaluation metrics, highlighting its effectiveness and potential for clinical deployment."
    },
    {
      "paperId": "a4484e580718a9d83c06983d5db7c60d1690131c",
      "externalIds": {
        "DBLP": "conf/miccai/WuXGPZ25",
        "ArXiv": "2509.14609",
        "DOI": "10.1007/978-3-032-04947-6_27",
        "CorpusId": 281394407
      },
      "corpusId": 281394407,
      "title": "HybridMamba: A Dual-Domain Mamba for 3D Medical Image Segmentation",
      "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.14609, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2327106980",
          "name": "Weitong Wu"
        },
        {
          "authorId": "153107262",
          "name": "Zhaohu Xing"
        },
        {
          "authorId": "2381104241",
          "name": "Jing Gong"
        },
        {
          "authorId": "2381711366",
          "name": "Qin Peng"
        },
        {
          "authorId": "2348189926",
          "name": "Lei Zhu"
        }
      ],
      "abstract": "In the domain of 3D biomedical image segmentation, Mamba exhibits the superior performance for it addresses the limitations in modeling long-range dependencies inherent to CNNs and mitigates the abundant computational overhead associated with Transformer-based frameworks when processing high-resolution medical volumes. However, attaching undue importance to global context modeling may inadvertently compromise critical local structural information, thus leading to boundary ambiguity and regional distortion in segmentation outputs. Therefore, we propose the HybridMamba, an architecture employing dual complementary mechanisms: 1) a feature scanning strategy that progressively integrates representations both axial-traversal and local-adaptive pathways to harmonize the relationship between local and global representations, and 2) a gated module combining spatial-frequency analysis for comprehensive contextual modeling. Besides, we collect a multi-center CT dataset related to lung cancer. Experiments on MRI and CT datasets demonstrate that HybridMamba significantly outperforms the state-of-the-art methods in 3D medical image segmentation."
    },
    {
      "paperId": "be8f6afc33751a0f6ba90cb9640324e7e02b1a64",
      "externalIds": {
        "DOI": "10.3390/aerospace12090842",
        "CorpusId": 281472917
      },
      "corpusId": 281472917,
      "title": "A Mamba-Based Hierarchical Partitioning Framework for Upper-Level Wind Field Reconstruction",
      "venue": "Aerospace",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/aerospace12090842?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/aerospace12090842, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2269016610",
          "name": "Wantong Chen"
        },
        {
          "authorId": "2382550044",
          "name": "Yifan Zhang"
        },
        {
          "authorId": "2306098554",
          "name": "Ruihua Liu"
        },
        {
          "authorId": "2115306696",
          "name": "Shuguang Sun"
        },
        {
          "authorId": "2383265227",
          "name": "Qing Feng"
        }
      ],
      "abstract": "An accurate perception of upper-level wind fields is essential for improving civil aviation safety and route optimization. However, the sparsity of observational data and the structural complexity of wind fields make reconstruction highly challenging. To address this, we propose QuadMamba-WindNet (QMW-Net), a structure-enhanced deep neural network that integrates a hierarchical state-space modeling framework with a learnable quad-tree-based regional partitioning mechanism, enabling multi-scale adaptive encoding and efficient dynamic modeling. The model is trained end-to-end on ERA5 reanalysis data and validated with simulated flight trajectory observation masks, allowing the reconstruction of complete horizontal wind fields at target altitude levels. Experimental results show that QMW-Net achieves a mean absolute error (MAE) of 1.62 m/s and a mean relative error (MRE) of 6.68% for wind speed reconstruction at 300 hPa, with a mean directional error of 4.85\u00b0 and an R2 of 0.93, demonstrating high accuracy and stable error convergence. Compared with Physics-Informed Neural Networks (PINNs) and Gaussian Process Regression (GPR), QMW-Net delivers superior predictive performance and generalization across multiple test sets. The proposed model provides refined wind field support for civil aviation forecasting and trajectory planning, and shows potential for broader applications in high-dynamic flight environments and atmospheric sensing."
    },
    {
      "paperId": "4571cd3359c982ad9946306f4635bd3f49013f7b",
      "externalIds": {
        "DBLP": "journals/jrtip/ZangSSHYMS25",
        "DOI": "10.1007/s11554-025-01765-0",
        "CorpusId": 281447809
      },
      "corpusId": 281447809,
      "title": "LMDNet: a lightweight network for infrared small target detection based on mamba and difference convolution",
      "venue": "Journal of Real-Time Image Processing",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11554-025-01765-0?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11554-025-01765-0, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2233339711",
          "name": "Dongyuan Zang"
        },
        {
          "authorId": "2281117865",
          "name": "Weihua Su"
        },
        {
          "authorId": "2233443067",
          "name": "Zijing Song"
        },
        {
          "authorId": "2381611203",
          "name": "Jiabao Huang"
        },
        {
          "authorId": "2381397236",
          "name": "Meng Yin"
        },
        {
          "authorId": "2381389910",
          "name": "Jun Ma"
        },
        {
          "authorId": "2382013789",
          "name": "Shenao Song"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "0cf4285a409fe58155ff59d1311196e6563fa664",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-13735",
        "ArXiv": "2509.13735",
        "DOI": "10.48550/arXiv.2509.13735",
        "CorpusId": 281332467
      },
      "corpusId": 281332467,
      "title": "State Space Models over Directed Graphs",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.13735, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2380686101",
          "name": "Junzhi She"
        },
        {
          "authorId": "2268429288",
          "name": "Xunkai Li"
        },
        {
          "authorId": "2312235766",
          "name": "Ronghua Li"
        },
        {
          "authorId": "2240263835",
          "name": "Guoren Wang"
        }
      ],
      "abstract": "Directed graphs are ubiquitous across numerous domains, where the directionality of edges encodes critical causal dependencies. However, existing GNNs and graph Transformers tailored for directed graphs face two major challenges: (1) effectively capturing long-range causal dependencies derived from directed edges; (2) balancing accuracy and training efficiency when processing large-scale graph datasets. In recent years, state space models (SSMs) have achieved substantial progress in causal sequence tasks, and their variants designed for graphs have demonstrated state-of-the-art accuracy while maintaining high efficiency across various graph learning benchmarks. However, existing graph state space models are exclusively designed for undirected graphs, which limits their performance in directed graph learning. To this end, we propose an innovative approach DirEgo2Token which sequentializes directed graphs via k-hop ego graphs. This marks the first systematic extension of state space models to the field of directed graph learning. Building upon this, we develop DirGraphSSM, a novel directed graph neural network architecture that implements state space models on directed graphs via the message-passing mechanism. Experimental results demonstrate that DirGraphSSM achieves state-of-the-art performance on three representative directed graph learning tasks while attaining competitive performance on two additional tasks with 1.5$\\times $ to 2$\\times $ training speed improvements compared to existing state-of-the-art models."
    },
    {
      "paperId": "07d9f535cbadc0bdfe614c803b56b1b79d966b57",
      "externalIds": {
        "ArXiv": "2509.13713",
        "DBLP": "journals/corr/abs-2509-13713",
        "DOI": "10.48550/arXiv.2509.13713",
        "CorpusId": 281332609
      },
      "corpusId": 281332609,
      "title": "UM-Depth : Uncertainty Masked Self-Supervised Monocular Depth Estimation with Visual Odometry",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.13713, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2321963946",
          "name": "Tae-Wook Um"
        },
        {
          "authorId": "2151286902",
          "name": "Ki-Hyeon Kim"
        },
        {
          "authorId": "2151705124",
          "name": "Hyun-Duck Choi"
        },
        {
          "authorId": "2380688080",
          "name": "Hyo-Sung Ahn"
        }
      ],
      "abstract": "Monocular depth estimation has been increasingly adopted in robotics and autonomous driving for its ability to infer scene geometry from a single camera. In self-supervised monocular depth estimation frameworks, the network jointly generates and exploits depth and pose estimates during training, thereby eliminating the need for depth labels. However, these methods remain challenged by uncertainty in the input data, such as low-texture or dynamic regions, which can cause reduced depth accuracy. To address this, we introduce UM-Depth, a framework that combines motion- and uncertainty-aware refinement to enhance depth accuracy at dynamic object boundaries and in textureless regions. Specifically, we develop a teacherstudent training strategy that embeds uncertainty estimation into both the training pipeline and network architecture, thereby strengthening supervision where photometric signals are weak. Unlike prior motion-aware approaches that incur inference-time overhead and rely on additional labels or auxiliary networks for real-time generation, our method uses optical flow exclusively within the teacher network during training, which eliminating extra labeling demands and any runtime cost. Extensive experiments on the KITTI and Cityscapes datasets demonstrate the effectiveness of our uncertainty-aware refinement. Overall, UM-Depth achieves state-of-the-art results in both self-supervised depth and pose estimation on the KITTI datasets."
    },
    {
      "paperId": "3c2951ef0137e5903addf6ef8341d15b13e3b4c1",
      "externalIds": {
        "ArXiv": "2509.13765",
        "DBLP": "journals/corr/abs-2509-13765",
        "DOI": "10.48550/arXiv.2509.13765",
        "CorpusId": 281333182
      },
      "corpusId": 281333182,
      "title": "TENET: An Efficient Sparsity-Aware LUT-Centric Architecture for Ternary LLM Inference On Edge",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 3,
      "influentialCitationCount": 1,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.13765, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2342864825",
          "name": "Zhirui Huang"
        },
        {
          "authorId": "2387892681",
          "name": "Rui Ma"
        },
        {
          "authorId": "2072595192",
          "name": "Shijie Cao"
        },
        {
          "authorId": "2266760794",
          "name": "Ran Shu"
        },
        {
          "authorId": "2380689803",
          "name": "Ian Wang"
        },
        {
          "authorId": "2308670666",
          "name": "Ting Cao"
        },
        {
          "authorId": "2241935225",
          "name": "Chixiao Chen"
        },
        {
          "authorId": "2237132501",
          "name": "Yongqiang Xiong"
        }
      ],
      "abstract": "Ternary quantization has emerged as a powerful technique for reducing both computational and memory footprint of large language models (LLM), enabling efficient real-time inference deployment without significantly compromising model accuracy. Conventional LLM inference platforms (e.g GPUs) cannot capitalize on its benefits, as they (i) lack native support for ternary arithmetic and memory specialization and (ii) remain severely under-utilized in low-batch, real-time scenarios. In this work, we propose TENET, a sparse-aware LUT-centric architecture that co-optimizes algorithm, compute, and memory for ternary LLM inference. To maximize the efficiency of Ternary Linear layer, TENET introduces a Sparse Ternary LUT (STL) core that optimizes ternary mixed-precision GEMM using a symmetric precompute lookup table. It also features Dynamic Activation N:M Sparsity to exploit the sparsity within the activation of each token. Additionally, we propose a LUT-based 64B:80B ternary weight decompression module to fully exploit the memory efficiency of ternary values. At the system level, we design a heterogeneous TENET accelerator with full programmability that integrates STL cores with high-precision cores. An associated Linear-Projection-aware Sparse Attention dataflow is introduced to optimize memory access and hardware utilization. We implement TENET accelerator prototype on both FPGA and ASIC platforms. Experiments across various model sizes and workloads demonstrate that TENET-FPGA and TENET-ASIC improve energy efficiency by 4.3$\\times$ and 21.1$\\times$, respectively, compared to the A100 GPU. Furthermore, TENET-ASIC achieves a 2.7$\\times$ average speedup compared to the A100 GPU in end-to-end inference latency."
    },
    {
      "paperId": "b9f7c7d0ee1479b5afb4733a3d41089005559036",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-25202",
        "ArXiv": "2509.25202",
        "DOI": "10.48550/arXiv.2509.25202",
        "CorpusId": 281683024
      },
      "corpusId": 281683024,
      "title": "VLHSA: Vision-Language Hierarchical Semantic Alignment for Jigsaw Puzzle Solving with Eroded Gaps",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.25202, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2383103578",
          "name": "Zhuoning Xu"
        },
        {
          "authorId": "2383142073",
          "name": "Xinyan Liu"
        }
      ],
      "abstract": "Jigsaw puzzle solving remains challenging in computer vision, requiring an understanding of both local fragment details and global spatial relationships. While most traditional approaches only focus on visual cues like edge matching and visual coherence, few methods explore natural language descriptions for semantic guidance in challenging scenarios, especially for eroded gap puzzles. We propose a vision-language framework that leverages textual context to enhance puzzle assembly performance. Our approach centers on the Vision-Language Hierarchical Semantic Alignment (VLHSA) module, which aligns visual patches with textual descriptions through multi-level semantic matching from local tokens to global context. Also, a multimodal architecture that combines dual visual encoders with language features for cross-modal reasoning is integrated into this module. Experiments demonstrate that our method significantly outperforms state-of-the-art models across various datasets, achieving substantial improvements, including a 14.2 percentage point gain in piece accuracy. Ablation studies confirm the critical role of the VLHSA module in driving improvements over vision-only approaches. Our work establishes a new paradigm for jigsaw puzzle solving by incorporating multimodal semantic insights."
    },
    {
      "paperId": "721ec0af2e394e4d378913d98af74c6c8371d798",
      "externalIds": {
        "DBLP": "journals/jrtip/LiTL25",
        "DOI": "10.1007/s11554-025-01764-1",
        "CorpusId": 281479920
      },
      "corpusId": 281479920,
      "title": "Real-time recognition of miner unsafe behaviors via skeleton-based spatiotemporal modeling with Mamba",
      "venue": "Journal of Real-Time Image Processing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11554-025-01764-1?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11554-025-01764-1, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2285535280",
          "name": "Biao Li"
        },
        {
          "authorId": "2287634555",
          "name": "Shoufeng Tang"
        },
        {
          "authorId": "2285463000",
          "name": "Wenyi Li"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "3614e31c08ed913c791fd3ca8675df8a1151f9ba",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-12777",
        "ArXiv": "2509.12777",
        "DOI": "10.48550/arXiv.2509.12777",
        "CorpusId": 281325249
      },
      "corpusId": 281325249,
      "title": "CECT-Mamba: a Hierarchical Contrast-enhanced-aware Model for Pancreatic Tumor Subtyping from Multi-phase CECT",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.12777, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2381281910",
          "name": "Zhifang Gong"
        },
        {
          "authorId": "2350628206",
          "name": "Shuo Gao"
        },
        {
          "authorId": "2380627739",
          "name": "Ben Zhao"
        },
        {
          "authorId": "2382703625",
          "name": "Yingjing Xu"
        },
        {
          "authorId": "2380629852",
          "name": "Yijun Yang"
        },
        {
          "authorId": "2267522312",
          "name": "Shenghong Ju"
        },
        {
          "authorId": "2381322192",
          "name": "Guangquan Zhou"
        }
      ],
      "abstract": "Contrast-enhanced computed tomography (CECT) is the primary imaging technique that provides valuable spatial-temporal information about lesions, enabling the accurate diagnosis and subclassification of pancreatic tumors. However, the high heterogeneity and variability of pancreatic tumors still pose substantial challenges for precise subtyping diagnosis. Previous methods fail to effectively explore the contextual information across multiple CECT phases commonly used in radiologists'diagnostic workflows, thereby limiting their performance. In this paper, we introduce, for the first time, an automatic way to combine the multi-phase CECT data to discriminate between pancreatic tumor subtypes, among which the key is using Mamba with promising learnability and simplicity to encourage both temporal and spatial modeling from multi-phase CECT. Specifically, we propose a dual hierarchical contrast-enhanced-aware Mamba module incorporating two novel spatial and temporal sampling sequences to explore intra and inter-phase contrast variations of lesions. A similarity-guided refinement module is also imposed into the temporal scanning modeling to emphasize the learning on local tumor regions with more obvious temporal variations. Moreover, we design the space complementary integrator and multi-granularity fusion module to encode and aggregate the semantics across different scales, achieving more efficient learning for subtyping pancreatic tumors. The experimental results on an in-house dataset of 270 clinical cases achieve an accuracy of 97.4% and an AUC of 98.6% in distinguishing between pancreatic ductal adenocarcinoma (PDAC) and pancreatic neuroendocrine tumors (PNETs), demonstrating its potential as a more accurate and efficient tool."
    },
    {
      "paperId": "c613840c8c5a71a7635902b46d719bf08c4a4968",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-12592",
        "ArXiv": "2509.12592",
        "DOI": "10.48550/arXiv.2509.12592",
        "CorpusId": 281325975
      },
      "corpusId": 281325975,
      "title": "Match Chat: Real Time Generative AI and Generative Computing for Tennis",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.12592, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2361248358",
          "name": "Aaron Baughman"
        },
        {
          "authorId": "2286871770",
          "name": "Gozde Akay"
        },
        {
          "authorId": "2286882258",
          "name": "Eduardo Morales"
        },
        {
          "authorId": "2286902311",
          "name": "Rahul Agarwal"
        },
        {
          "authorId": "2380611383",
          "name": "Preetika Srivastava"
        }
      ],
      "abstract": "We present Match Chat, a real-time, agent-driven assistant designed to enhance the tennis fan experience by delivering instant, accurate responses to match-related queries. Match Chat integrates Generative Artificial Intelligence (GenAI) with Generative Computing (GenComp) techniques to synthesize key insights during live tennis singles matches. The system debuted at the 2025 Wimbledon Championships and the 2025 US Open, where it provided about 1 million users with seamless access to streaming and static data through natural language queries. The architecture is grounded in an Agent-Oriented Architecture (AOA) combining rule engines, predictive models, and agents to pre-process and optimize user queries before passing them to GenAI components. The Match Chat system had an answer accuracy of 92.83% with an average response time of 6.25 seconds under loads of up to 120 requests per second (RPS). Over 96.08% of all queries were guided using interactive prompt design, contributing to a user experience that prioritized clarity, responsiveness, and minimal effort. The system was designed to mask architectural complexity, offering a frictionless and intuitive interface that required no onboarding or technical familiarity. Across both Grand Slam deployments, Match Chat maintained 100% uptime and supported nearly 1 million unique users, underscoring the scalability and reliability of the platform. This work introduces key design patterns for real-time, consumer-facing AI systems that emphasize speed, precision, and usability that highlights a practical path for deploying performant agentic systems in dynamic environments."
    },
    {
      "paperId": "dd720aeec5d727948ea3acdf8d783482007be0ce",
      "externalIds": {
        "ArXiv": "2509.13070",
        "DBLP": "journals/corr/abs-2509-13070",
        "DOI": "10.48550/arXiv.2509.13070",
        "CorpusId": 281325742
      },
      "corpusId": 281325742,
      "title": "TFANet: Three-Stage Image-Text Feature Alignment Network for Robust Referring Image Segmentation",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.13070, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2379931182",
          "name": "Qianqi Lu"
        },
        {
          "authorId": "2284528508",
          "name": "Yuxiang Xie"
        },
        {
          "authorId": "2381136585",
          "name": "Jing Zhang"
        },
        {
          "authorId": "2284343062",
          "name": "Shiwei Zou"
        },
        {
          "authorId": "2341688973",
          "name": "Yan Chen"
        },
        {
          "authorId": "3276061",
          "name": "Xidao Luan"
        }
      ],
      "abstract": "Referring Image Segmentation (RIS) is a task that segments image regions based on language expressions, requiring fine-grained alignment between two modalities. However, existing methods often struggle with multimodal misalignment and language semantic loss, especially in complex scenes containing multiple visually similar objects, where uniquely described targets are frequently mislocalized or incompletely segmented. To tackle these challenges, this paper proposes TFANet, a Three-stage Image-Text Feature Alignment Network that systematically enhances multimodal alignment through a hierarchical framework comprising three stages: Knowledge Plus Stage (KPS), Knowledge Fusion Stage (KFS), and Knowledge Intensification Stage (KIS). In the first stage, we design the Multiscale Linear Cross-Attention Module (MLAM), which facilitates bidirectional semantic exchange between visual features and textual representations across multiple scales. This establishes rich and efficient alignment between image regions and different granularities of linguistic descriptions. Subsequently, the KFS further strengthens feature alignment through the Cross-modal Feature Scanning Module (CFSM), which applies multimodal selective scanning to capture long-range dependencies and construct a unified multimodal representation. This is essential for modeling long-range cross-modal dependencies and enhancing alignment accuracy in complex scenes. Finally, in the KIS, we propose the Word-level Linguistic Feature-guided Semantic Deepening Module (WFDM) to compensate for semantic degradation introduced in earlier stages."
    },
    {
      "paperId": "7d376cd24a8d66b9aff96876a5ab338c677078d1",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-13576",
        "ArXiv": "2509.13576",
        "DOI": "10.48550/arXiv.2509.13576",
        "CorpusId": 281333255
      },
      "corpusId": 281333255,
      "title": "Cross-Distribution Diffusion Priors-Driven Iterative Reconstruction for Sparse-View CT",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.13576, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2379569032",
          "name": "Haodong Li"
        },
        {
          "authorId": "2260610083",
          "name": "Shuo Han"
        },
        {
          "authorId": "2254303132",
          "name": "Haiyang Mao"
        },
        {
          "authorId": "2366315452",
          "name": "Yu Shi"
        },
        {
          "authorId": "2334668807",
          "name": "Changsheng Fang"
        },
        {
          "authorId": "2254255418",
          "name": "Jianjia Zhang"
        },
        {
          "authorId": "2256069455",
          "name": "Weiwen Wu"
        },
        {
          "authorId": "2334496125",
          "name": "Hengyong Yu"
        }
      ],
      "abstract": "Sparse-View CT (SVCT) reconstruction enhances temporal resolution and reduces radiation dose, yet its clinical use is hindered by artifacts due to view reduction and domain shifts from scanner, protocol, or anatomical variations, leading to performance degradation in out-of-distribution (OOD) scenarios. In this work, we propose a Cross-Distribution Diffusion Priors-Driven Iterative Reconstruction (CDPIR) framework to tackle the OOD problem in SVCT. CDPIR integrates cross-distribution diffusion priors, derived from a Scalable Interpolant Transformer (SiT), with model-based iterative reconstruction methods. Specifically, we train a SiT backbone, an extension of the Diffusion Transformer (DiT) architecture, to establish a unified stochastic interpolant framework, leveraging Classifier-Free Guidance (CFG) across multiple datasets. By randomly dropping the conditioning with a null embedding during training, the model learns both domain-specific and domain-invariant priors, enhancing generalizability. During sampling, the globally sensitive transformer-based diffusion model exploits the cross-distribution prior within the unified stochastic interpolant framework, enabling flexible and stable control over multi-distribution-to-noise interpolation paths and decoupled sampling strategies, thereby improving adaptation to OOD reconstruction. By alternating between data fidelity and sampling updates, our model achieves state-of-the-art performance with superior detail preservation in SVCT reconstructions. Extensive experiments demonstrate that CDPIR significantly outperforms existing approaches, particularly under OOD conditions, highlighting its robustness and potential clinical value in challenging imaging scenarios."
    },
    {
      "paperId": "c35d1396f4dc58ba2fd9d90e428335f473526e81",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-01206",
        "ArXiv": "2510.01206",
        "DOI": "10.48550/arXiv.2510.01206",
        "CorpusId": 281725078
      },
      "corpusId": 281725078,
      "title": "Accelerating Long-Term Molecular Dynamics with Physics-Informed Time-Series Forecasting",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.01206, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2282535946",
          "name": "Hung Le"
        },
        {
          "authorId": "2322923295",
          "name": "Sherif M. Abbas"
        },
        {
          "authorId": "2312035148",
          "name": "Minh Hoang Nguyen"
        },
        {
          "authorId": "2373601064",
          "name": "Van Dai Do"
        },
        {
          "authorId": "2384315873",
          "name": "Huu Hiep Nguyen"
        },
        {
          "authorId": "2297995667",
          "name": "Dung Nguyen"
        }
      ],
      "abstract": "Efficient molecular dynamics (MD) simulation is vital for understanding atomic-scale processes in materials science and biophysics. Traditional density functional theory (DFT) methods are computationally expensive, which limits the feasibility of long-term simulations. We propose a novel approach that formulates MD simulation as a time-series forecasting problem, enabling advanced forecasting models to predict atomic trajectories via displacements rather than absolute positions. We incorporate a physics-informed loss and inference mechanism based on DFT-parametrised pair-wise Morse potential functions that penalize unphysical atomic proximity to enforce physical plausibility. Our method consistently surpasses standard baselines in simulation accuracy across diverse materials. The results highlight the importance of incorporating physics knowledge to enhance the reliability and precision of atomic trajectory forecasting. Remarkably, it enables stable modeling of thousands of MD steps in minutes, offering a scalable alternative to costly DFT simulations."
    },
    {
      "paperId": "f3758631a48e99e53ba5b92ee11944deefc49fa2",
      "externalIds": {
        "DOI": "10.1109/TIP.2025.3607615",
        "CorpusId": 281324298,
        "PubMed": "40953415"
      },
      "corpusId": 281324298,
      "title": "MambaDiff: Mamba-Enhanced Diffusion Model for 3D Medical Image Segmentation",
      "venue": "IEEE Transactions on Image Processing",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIP.2025.3607615?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIP.2025.3607615, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2379310122",
          "name": "Yu Liu"
        },
        {
          "authorId": "2380700857",
          "name": "Yan Feng"
        },
        {
          "authorId": "46754596",
          "name": "Juan Cheng"
        },
        {
          "authorId": "2380597320",
          "name": "Haolin Zhan"
        },
        {
          "authorId": "2141361957",
          "name": "Zhiqin Zhu"
        }
      ],
      "abstract": "Accurate 3D medical image segmentation is crucial for diagnosis and treatment. Diffusion models demonstrate promising performance in medical image segmentation tasks due to the progressive nature of the generation process and the explicit modeling of data distributions. However, the weak guidance of conditional information and insufficient feature extraction in diffusion models lead to the loss of fine-grained features and structural consistency in the segmentation results, thereby affecting the accuracy of medical image segmentation. To address this challenge, we propose a Mamba-Enhanced Diffusion Model for 3D Medical Image Segmentation. We extract multilevel semantic features from the original images using an encoder and tightly integrate them with the denoising process of the diffusion model through a Semantic Hierarchical Embedding (SHE) mechanism, to capture the intricate relationship between the noisy label and image data. Meanwhile, we design a Global-Slice Perception Mamba (GSPM) layer, which integrates multi-dimensional perception mechanisms to endow the model with comprehensive spatial reasoning and feature extraction capabilities. Experimental results show that our proposed MambaDiff achieves more competitive performance compared to prior arts with substantially fewer parameters on four public medical image segmentation datasets including BraTS 2021, BraTS 2024, LiTS and MSD Hippocampus. The source code of our method is available at https://github.com/yuliu316316/MambaDiff"
    },
    {
      "paperId": "454f640df75bc327d73e4f1e292816cf2925d417",
      "externalIds": {
        "DBLP": "journals/iotj/ChenYCLFJL25",
        "DOI": "10.1109/JIOT.2025.3582636",
        "CorpusId": 279671277
      },
      "corpusId": 279671277,
      "title": "RailVoxelDet: A Lightweight 3-D Object Detection Method for Railway Transportation Driven by Onboard LiDAR Data",
      "venue": "IEEE Internet of Things Journal",
      "year": 2025,
      "citationCount": 7,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JIOT.2025.3582636?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JIOT.2025.3582636, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2262790719",
          "name": "Zhichao Chen"
        },
        {
          "authorId": "2263258061",
          "name": "Jie Yang"
        },
        {
          "authorId": "2176603697",
          "name": "Lifang Chen"
        },
        {
          "authorId": "2283799967",
          "name": "Fan Li"
        },
        {
          "authorId": "2179411017",
          "name": "Zhicheng Feng"
        },
        {
          "authorId": "2260824209",
          "name": "Limin Jia"
        },
        {
          "authorId": "2112519624",
          "name": "Pan Li"
        }
      ],
      "abstract": "3-D perception in train operating environments presents significant challenges, as it must ensure both precise distance estimation and computational efficiency to meet stringent braking requirements. To date, existing 3-D detection architectures, which employ dense voxel or pillar representations, encounter challenges of computational inefficiency and accuracy degradation when processing large-scale railway light detection and ranging (LiDAR) data. To address this challenge, we propose RailVoxelDet, a railway-optimized 3-D detector integrating the multifactor dynamic voxel feature encoder (MDVFE) and efficient backbone. Specifically, MDVFE converts point clouds to 2-D sparse voxels, reducing computational complexity. The backbone employs residual bottlenecks with shared full connected layers and sparse convolutions, enhanced by the SimAM-Point module. Additionally, the feature query and matching module (FQMM) is proposed to establish a bottom-up multilevel feature fusion architecture. Experimental results show RailVoxelDet reaches 71.29% mAP on OSDaR23 and 61.94% mAP on AirR24, with 6.42G FLOPs and a 71.42 ms inference time. It outperforms 12 comparison models, delivering state-of-the-art results."
    },
    {
      "paperId": "82ef2b75d90d19c8265985f244e24640c7c5bec9",
      "externalIds": {
        "DBLP": "journals/iotj/ZhangZLSHKD25",
        "DOI": "10.1109/JIOT.2025.3583695",
        "CorpusId": 279688567
      },
      "corpusId": 279688567,
      "title": "VDI-Net: Viewpoint Changes and Dynamic Interference Immune Place Recognition Network for Intelligent Vehicles Based on Multiview Images",
      "venue": "IEEE Internet of Things Journal",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JIOT.2025.3583695?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JIOT.2025.3583695, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2283086404",
          "name": "Liye Zhang"
        },
        {
          "authorId": "2357018576",
          "name": "Shuo Zhang"
        },
        {
          "authorId": "2283135365",
          "name": "Zhongzheng Li"
        },
        {
          "authorId": "2283094098",
          "name": "Xiaoyu Sun"
        },
        {
          "authorId": "2146241416",
          "name": "Weiming Hu"
        },
        {
          "authorId": "2165711860",
          "name": "Dong Kong"
        },
        {
          "authorId": "2328824857",
          "name": "Hairong Dong"
        }
      ],
      "abstract": "visual place recognition (VPR) improves the localization accuracy of agents in complex environments by extracting effective environmental representations for place matching, and it does not rely on additional high-precision, high-cost sensors or digital maps. However, current VPR research still faces limitations when simultaneously addressing the challenges of viewpoint changes and dynamic target interference, where it is not easy to obtain stable and reliable viewpoint-invariant environmental representations. To address these challenges, inspired by the human ability to recognize scenes, this article customizes a place recognition network called VDI-Net based on multiview images, immune to viewpoint changes and dynamic interference. Specifically, a dynamic target filtering (DTF) module is proposed to effectively filter out dynamic targets in complex environments. To tackle the challenge of viewpoint changes, a vision-surround Mamba module (VSM) is introduced to enhance the rotational invariance of features across different viewpoints. In the comparative validation of the nuScenes dataset and our real-vehicle collection dataset, the experimental results show that our proposed method achieves satisfactory performance in addressing the challenges of viewpoint changes and dynamic target interference. The proposed method outperforms current representative VPR baselines and surpasses some classical LiDAR-based and multimodal place recognition baselines."
    },
    {
      "paperId": "c91188d2d9cd2f1727680b914ad093bd040398bd",
      "externalIds": {
        "DBLP": "journals/iotj/ChenHLZSMLC25",
        "DOI": "10.1109/JIOT.2025.3586661",
        "CorpusId": 280188027
      },
      "corpusId": 280188027,
      "title": "MHMamba: Mobile Hybrid Model for Edge-Enabled Acromegaly Auxiliary Diagnosis in Smart Healthcare",
      "venue": "IEEE Internet of Things Journal",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JIOT.2025.3586661?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JIOT.2025.3586661, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2372448680",
          "name": "Hao Chen"
        },
        {
          "authorId": "2375752997",
          "name": "Wenqiang He"
        },
        {
          "authorId": "2304917098",
          "name": "Jiayi Li"
        },
        {
          "authorId": "2148944838",
          "name": "Weihong Zhou"
        },
        {
          "authorId": "11213130",
          "name": "Chenglu Sun"
        },
        {
          "authorId": "7724284",
          "name": "Zengyi Ma"
        },
        {
          "authorId": "2248425263",
          "name": "Jingchun Luo"
        },
        {
          "authorId": "2127379146",
          "name": "Chen Chen"
        }
      ],
      "abstract": "Acromegaly, a chronic endocrine disorder, requires early diagnosis to prevent severe complications. Existing deep-learning-based diagnostic methods often prioritize accuracy at the expense of computational complexity and feature diversity, limiting their deployment in resource-constrained IoT environments. This article proposes Mobile Hybrid Mamba (MHMamba), a lightweight model integrating inverted residual attention convolution (IRAC) and VMamba, designed for efficient acromegaly screening on edge devices. By synergizing convolutional neural network\u2019s local feature extraction with VMamba\u2019s linear-complexity global modeling, MHMamba achieves state-of-the-art performance (2.8%\u20133.1% improvement in precision, recall, and F1-score) while maintaining ultralightweight parameters (13.882M) and low computational overhead (2.054G FLOPs). Crucially, MHMamba\u2019s mobile-friendly architecture enables real-time facial analysis on smartphones, making it suitable for IoT-driven telemedicine and decentralized health monitoring. Through occlusion experiments and GradCAM++ visualization, we identify key facial regions (nose, mouth, and cheekbones) critical for diagnosis, aligning with clinical biomarkers. The model\u2019s interpretability and portability position it as a pivotal tool for IoT-enabled smart healthcare systems, bridging the gap between artificial-intelligence-driven diagnostics and edge device deployment."
    },
    {
      "paperId": "e0eb2e0044645ae095a944e2f5192a1ac3bde9e6",
      "externalIds": {
        "ArXiv": "2509.11649",
        "DBLP": "journals/corr/abs-2509-11649",
        "DOI": "10.48550/arXiv.2509.11649",
        "CorpusId": 281314905
      },
      "corpusId": 281314905,
      "title": "Joint-octamamba:an octa joint segmentation network based on feature enhanced mamba",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.11649, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2380565816",
          "name": "Chuang Liu"
        },
        {
          "authorId": "2380524155",
          "name": "Nan Guo"
        }
      ],
      "abstract": "OCTA is a crucial non-invasive imaging technique for diagnosing and monitoring retinal diseases like diabetic retinopathy, age-related macular degeneration, and glaucoma. Current 2D-based methods for retinal vessel (RV) segmentation offer insufficient accuracy. To address this, we propose RVMamba, a novel architecture integrating multiple feature extraction modules with the Mamba state-space model. Moreover, existing joint segmentation models for OCTA data exhibit performance imbalance between different tasks. To simultaneously improve the segmentation of the foveal avascular zone (FAZ) and mitigate this imbalance, we introduce FAZMamba and a unified Joint-OCTAMamba framework. Experimental results on the OCTA-500 dataset demonstrate that Joint-OCTAMamba outperforms existing models across evaluation metrics.The code is available at https://github.com/lc-sfis/Joint-OCTAMamba."
    },
    {
      "paperId": "af263e1520f00588dbd4da11a32e3d37189b055f",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-12069",
        "ArXiv": "2509.12069",
        "DOI": "10.48550/arXiv.2509.12069",
        "CorpusId": 281314981
      },
      "corpusId": 281314981,
      "title": "U-Mamba2: Scaling State Space Models for Dental Anatomy Segmentation in CBCT",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.12069, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2381240182",
          "name": "Zhi Qin Tan"
        },
        {
          "authorId": "2310779368",
          "name": "Xiatian Zhu"
        },
        {
          "authorId": "2380521484",
          "name": "Owen Addison"
        },
        {
          "authorId": "2310858119",
          "name": "Yunpeng Li"
        }
      ],
      "abstract": "Cone-Beam Computed Tomography (CBCT) is a widely used 3D imaging technique in dentistry, providing volumetric information about the anatomical structures of jaws and teeth. Accurate segmentation of these anatomies is critical for clinical applications such as diagnosis and surgical planning, but remains time-consuming and challenging. In this paper, we present U-Mamba2, a new neural network architecture designed for multi-anatomy CBCT segmentation in the context of the ToothFairy3 challenge. U-Mamba2 integrates the Mamba2 state space models into the U-Net architecture, enforcing stronger structural constraints for higher efficiency without compromising performance. In addition, we integrate interactive click prompts with cross-attention blocks, pre-train U-Mamba2 using self-supervised learning, and incorporate dental domain knowledge into the model design to address key challenges of dental anatomy segmentation in CBCT. Extensive experiments, including independent tests, demonstrate that U-Mamba2 is both effective and efficient, securing first place in both tasks of the Toothfairy3 challenge. In Task 1, U-Mamba2 achieved a mean Dice of 0.84, HD95 of 38.17 with the held-out test data, with an average inference time of 40.58s. In Task 2, U-Mamba2 achieved the mean Dice of 0.87 and HD95 of 2.15 with the held-out test data. The code is publicly available at https://github.com/zhiqin1998/UMamba2."
    },
    {
      "paperId": "ed161533ef871ae0bf20269a48bca9baf2953d41",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-11884",
        "ArXiv": "2509.11884",
        "DOI": "10.1145/3746027.3755291",
        "CorpusId": 281316099
      },
      "corpusId": 281316099,
      "title": "SAM-TTT: Segment Anything Model via Reverse Parameter Configuration and Test-Time Training for Camouflaged Object Detection",
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 1,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.11884, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2301109815",
          "name": "Zhenni Yu"
        },
        {
          "authorId": "2301106683",
          "name": "Li Zhao"
        },
        {
          "authorId": "2274105084",
          "name": "Guobao Xiao"
        },
        {
          "authorId": "2311740258",
          "name": "Xiaoqin Zhang"
        }
      ],
      "abstract": "This paper introduces a new Segment Anything Model (SAM) that leverages reverse parameter configuration and test-time training to enhance its performance on Camouflaged Object Detection (COD), named SAM-TTT. While most existing SAM-based COD models primarily focus on enhancing SAM by extracting favorable features and amplifying its advantageous parameters, a crucial gap is identified: insufficient attention to adverse parameters that impair SAM's semantic understanding in downstream tasks. To tackle this issue, the Reverse SAM Parameter Configuration Module is proposed to effectively mitigate the influence of adverse parameters in a train-free manner by configuring SAM's parameters. Building on this foundation, the T-Visioner Module is unveiled to strengthen advantageous parameters by integrating Test-Time Training layers, originally developed for language tasks, into vision tasks. Test-Time Training layers represent a new class of sequence modeling layers characterized by linear complexity and an expressive hidden state. By integrating two modules, SAM-TTT simultaneously suppresses adverse parameters while reinforcing advantageous ones, significantly improving SAM's semantic understanding in COD task. Our experimental results on various COD benchmarks demonstrate that the proposed approach achieves state-of-the-art performance, setting a new benchmark in the field. The code will be available at https://github.com/guobaoxiao/SAM-TTT."
    },
    {
      "paperId": "54327499e302abac8eb74ece2fe6b3651ac0e60c",
      "externalIds": {
        "DOI": "10.1109/JSEN.2025.3592951",
        "CorpusId": 280452504
      },
      "corpusId": 280452504,
      "title": "Domain-Adaptive UAV Recognition Using IR-UWB Radar and a Lightweight Mamba-Based Network",
      "venue": "IEEE Sensors Journal",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JSEN.2025.3592951?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JSEN.2025.3592951, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2375095163",
          "name": "Shengyuan Li"
        },
        {
          "authorId": "2375094731",
          "name": "Xinyue Dong"
        },
        {
          "authorId": "2278804444",
          "name": "Yiheng Fan"
        },
        {
          "authorId": "2291572020",
          "name": "Xiangwei Zhu"
        },
        {
          "authorId": "2115563294",
          "name": "Xue-Feng Yuan"
        }
      ],
      "abstract": "Impulse radio ultrawideband (IR-UWB) radar shows great promise for uncrewed aerial vehicle (UAV) detection due to its high resolution, strong penetration, and robustness against multipath interference. However, effectively leveraging both spatially static and temporally dynamic information in radar echoes, while overcoming environmental interference, remains challenging. To address this, we propose a lightweight domain-adaptive model, AIR-Mamba. It first employs adaptive gain control and discrete wavelet decomposition to reduce amplitude sensitivity and extract target micro-Doppler features, then utilizes a Mamba backbone based on state-space models (SSMs) to capture long-term motion dynamics. We also introduce a dual-adaptation strategy that combines adversarial learning and correlation alignment (CORAL) to align cross-domain features and enhance generalization. To address real data scarcity, we constructed a multiscenario UAV echo dataset using full-wave electromagnetic simulation, which was validated by measurements in a microwave anechoic chamber. Experimental results show that AIR-Mamba achieves a cross-environment classification accuracy over 96% with only 1.55M parameters, while exhibiting strong noise resistance. This performance demonstrates clear advantages in model size and accuracy, providing a practical solution for real-time UAV detection in resource-constrained environments."
    },
    {
      "paperId": "956bebfa4ae1e74f247880110b189e16d453c67a",
      "externalIds": {
        "ArXiv": "2511.00267",
        "DBLP": "journals/corr/abs-2511-00267",
        "DOI": "10.1109/HPEC67600.2025.11196365",
        "CorpusId": 282149125
      },
      "corpusId": 282149125,
      "title": "Advancing AI Challenges for the United States Department of the Air Force*",
      "venue": "IEEE Conference on High Performance Extreme Computing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.00267, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2386138104",
          "name": "Christian Prothmann"
        },
        {
          "authorId": "74882299",
          "name": "V. Gadepally"
        },
        {
          "authorId": "3257323",
          "name": "J. Kepner"
        },
        {
          "authorId": "2386142266",
          "name": "Koley Borchard"
        },
        {
          "authorId": "2239485876",
          "name": "Luca Carlone"
        },
        {
          "authorId": "98186345",
          "name": "Z. Folcik"
        },
        {
          "authorId": "2386139746",
          "name": "J. D. Griffith"
        },
        {
          "authorId": "4121232",
          "name": "Michael Houle"
        },
        {
          "authorId": "2340541194",
          "name": "Jonathan P. How"
        },
        {
          "authorId": "2386137835",
          "name": "Nathan Hughes"
        },
        {
          "authorId": "2085710932",
          "name": "Ifueko Igbinedion"
        },
        {
          "authorId": "3422534",
          "name": "Hayden Jananthan"
        },
        {
          "authorId": "1396445187",
          "name": "Tejas Jayashankar"
        },
        {
          "authorId": "2262515233",
          "name": "Michael Jones"
        },
        {
          "authorId": "143612763",
          "name": "S. Karaman"
        },
        {
          "authorId": "101613063",
          "name": "Binoy G. Kurien"
        },
        {
          "authorId": "35542353",
          "name": "A. Lancho"
        },
        {
          "authorId": "1413548233",
          "name": "Giovanni Lavezzi"
        },
        {
          "authorId": "2110879554",
          "name": "Gary C. F. Lee"
        },
        {
          "authorId": "2064303000",
          "name": "C. E. Leiserson"
        },
        {
          "authorId": "2343375357",
          "name": "Richard Linares"
        },
        {
          "authorId": "2069965774",
          "name": "Lindsey McEvoy"
        },
        {
          "authorId": "1684116",
          "name": "P. Michaleas"
        },
        {
          "authorId": "2297847320",
          "name": "Chasen Milner"
        },
        {
          "authorId": "2288614365",
          "name": "A. Pentland"
        },
        {
          "authorId": "2237675516",
          "name": "Yury Polyanskiy"
        },
        {
          "authorId": "2386139229",
          "name": "Jovan Popovich"
        },
        {
          "authorId": "2386142930",
          "name": "Jeffrey Price"
        },
        {
          "authorId": "2386139047",
          "name": "Tim W. Reid"
        },
        {
          "authorId": "2386138079",
          "name": "Stephanie Riley"
        },
        {
          "authorId": "2331418",
          "name": "S. Samsi"
        },
        {
          "authorId": "2386142363",
          "name": "Peter Saunders"
        },
        {
          "authorId": "2840652",
          "name": "O. Simek"
        },
        {
          "authorId": "2223020086",
          "name": "Mark S. Veillette"
        },
        {
          "authorId": "2292554905",
          "name": "Amir Weiss"
        },
        {
          "authorId": "1779692",
          "name": "G. Wornell"
        },
        {
          "authorId": "2253476544",
          "name": "Daniela Rus"
        },
        {
          "authorId": "2386138688",
          "name": "Scott T. Ruppel"
        }
      ],
      "abstract": "The DAF-MIT AI Accelerator is a collaboration between the United States Department of the Air Force (DAF) and the Massachusetts Institute of Technology (MIT). This program pioneers fundamental advances in artificial intelligence (AI) to expand the competitive advantage of the United States in the defense and civilian sectors. In recent years, AI Accelerator projects have developed and launched public challenge problems aimed at advancing AI research in priority areas. Hallmarks of AI Accelerator challenges include large, publicly available, and AI-ready datasets to stimulate open-source solutions and engage the wider academic and private sector AI ecosystem. This article supplements our previous publication, which introduced AI Accelerator challenges. We provide an update on how ongoing and new challenges have successfully contributed to AI research and applications of AI technologies."
    },
    {
      "paperId": "c3d31baa61e8392f1f945d950e24fb423adad2dd",
      "externalIds": {
        "DOI": "10.1109/IUS62464.2025.11201712",
        "CorpusId": 282264806
      },
      "corpusId": 282264806,
      "title": "Particle Velocity Tracking in Shear Wave Elastography Using a Mamba-Based Spatiotemporal Network",
      "venue": "IUS",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/IUS62464.2025.11201712?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/IUS62464.2025.11201712, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2269094780",
          "name": "Ali K. Z. Tehrani"
        },
        {
          "authorId": "2244261216",
          "name": "Scott Schoen"
        },
        {
          "authorId": "2307346802",
          "name": "Ion Candel"
        },
        {
          "authorId": "2387092438",
          "name": "Guangyi Zhang"
        },
        {
          "authorId": "2265566348",
          "name": "Peng Guo"
        },
        {
          "authorId": "2354557010",
          "name": "Michael Wang"
        },
        {
          "authorId": "2448282",
          "name": "Rimon Tadross"
        },
        {
          "authorId": "2354539992",
          "name": "Mike Washburn"
        },
        {
          "authorId": "2386915987",
          "name": "Hassan Rivaz"
        },
        {
          "authorId": "2265672954",
          "name": "Anthony E. Samir"
        }
      ],
      "abstract": "In shear wave elastography (SWE), speckle tracking is employed to obtain the particle velocity from IQ or RF data. Conventional phase shift speckle tracking methods in SWE compute phase changes between adjacent firings, enabling estimation of local particle velocity. Although computationally efficient, these methods are highly sensitive to noise, as they rely on only one adjacent firing for each sample or patch. Deep learning methods have improved particle velocity estimation, with recent approaches using 3D convolutions over limited firings. However, these multi-resolution architectures are restricted to short temporal windows, limiting their ability to model long-range dependencies and generalize across varying conditions. To address this, we introduce Lorast, a long-range spatiotemporal velocity tracking network. This network disentangles spatiotemporal feature extraction by explicitly computing the phase difference between firings as a form of known operator inside the network architecture. Spatial features are extracted using 2D convolutions, while temporal dependencies are modeled with Mamba, a recent sequence modeling architecture outperforming transformers. This separation eliminates the need to learn joint spatiotemporal representations. We evaluate the performance of the compared methods on simulation data, and experimental phantom data."
    },
    {
      "paperId": "332af03ae4e1e5120171de0f5b151c1d8323bfc3",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-11940",
        "ArXiv": "2509.11940",
        "DOI": "10.1007/978-3-031-57873-1",
        "CorpusId": 269783739
      },
      "corpusId": 269783739,
      "title": "Neuromorphic Intelligence",
      "venue": "Synthesis Lectures on Engineering, Science, and Technology",
      "year": 2025,
      "citationCount": 4,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.11940, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "143799386",
          "name": "M. Gerven"
        }
      ],
      "abstract": "Neuromorphic computing seeks to replicate the remarkable efficiency, flexibility, and adaptability of the human brain in artificial systems. Unlike conventional digital approaches, which suffer from the Von Neumann bottleneck and depend on massive computational and energy resources, neuromorphic systems exploit brain-inspired principles of computation to achieve orders of magnitude greater energy efficiency. By drawing on insights from a wide range of disciplines -- including artificial intelligence, physics, chemistry, biology, neuroscience, cognitive science and materials science -- neuromorphic computing promises to deliver intelligent systems that are sustainable, transparent, and widely accessible. A central challenge, however, is to identify a unifying theoretical framework capable of bridging these diverse disciplines. We argue that dynamical systems theory provides such a foundation. Rooted in differential calculus, it offers a principled language for modeling inference, learning, and control in both natural and artificial substrates. Within this framework, noise can be harnessed as a resource for learning, while differential genetic programming enables the discovery of dynamical systems that implement adaptive behaviors. Embracing this perspective paves the way toward emergent neuromorphic intelligence, where intelligent behavior arises from the dynamics of physical substrates, advancing both the science and sustainability of AI."
    },
    {
      "paperId": "f1e076750c64ef311e2febf055f63234cf3d3cf7",
      "externalIds": {
        "ArXiv": "2509.11394",
        "DBLP": "journals/corr/abs-2509-11394",
        "DOI": "10.48550/arXiv.2509.11394",
        "CorpusId": 281316265
      },
      "corpusId": 281316265,
      "title": "MixANT: Observation-dependent Memory Propagation for Stochastic Dense Action Anticipation",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.11394, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2174126602",
          "name": "Syed Talal Wasim"
        },
        {
          "authorId": "2321583937",
          "name": "Hamid Suleman"
        },
        {
          "authorId": "2117340247",
          "name": "Olga Zatsarynna"
        },
        {
          "authorId": "40894826",
          "name": "Muzammal Naseer"
        },
        {
          "authorId": "2311882857",
          "name": "Juergen Gall"
        }
      ],
      "abstract": "We present MixANT, a novel architecture for stochastic long-term dense anticipation of human activities. While recent State Space Models (SSMs) like Mamba have shown promise through input-dependent selectivity on three key parameters, the critical forget-gate ($\\textbf{A}$ matrix) controlling temporal memory remains static. We address this limitation by introducing a mixture of experts approach that dynamically selects contextually relevant $\\textbf{A}$ matrices based on input features, enhancing representational capacity without sacrificing computational efficiency. Extensive experiments on the 50Salads, Breakfast, and Assembly101 datasets demonstrate that MixANT consistently outperforms state-of-the-art methods across all evaluation settings. Our results highlight the importance of input-dependent forget-gate mechanisms for reliable prediction of human behavior in diverse real-world scenarios."
    },
    {
      "paperId": "0e15a8f471f886d936ec8ce786aafe8d54945b48",
      "externalIds": {
        "DOI": "10.1109/icip55913.2025.11084630",
        "CorpusId": 280766938
      },
      "corpusId": 280766938,
      "title": "Mamba-SF: Monocular Scene Flow Learning with State Space Models",
      "venue": "International Conference on Information Photonics",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/icip55913.2025.11084630?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/icip55913.2025.11084630, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2292524317",
          "name": "Yiming Chen"
        },
        {
          "authorId": "2294860986",
          "name": "Xue-lian Xiang"
        },
        {
          "authorId": "2278790919",
          "name": "Xianye Ben"
        },
        {
          "authorId": "2376715452",
          "name": "I. Hassan"
        },
        {
          "authorId": "31328391",
          "name": "Mingliang Zhai"
        },
        {
          "authorId": "2333851705",
          "name": "Lei Zhang"
        },
        {
          "authorId": "2322958266",
          "name": "Xiantong Zhen"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "6406bdea4c5922f450f07ff36474ae873cd22b0b",
      "externalIds": {
        "DOI": "10.1109/icip55913.2025.11084562",
        "CorpusId": 280770260
      },
      "corpusId": 280770260,
      "title": "Mamba-Based Global Correlation Learning for Light Field Spatial Super-Resolution",
      "venue": "International Conference on Information Photonics",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/icip55913.2025.11084562?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/icip55913.2025.11084562, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2054590685",
          "name": "Bo Peng"
        },
        {
          "authorId": "2377272892",
          "name": "Ruoxi Li"
        },
        {
          "authorId": "143879230",
          "name": "Jianjun Lei"
        },
        {
          "authorId": "2177091563",
          "name": "Zhe Zhang"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "59b2dbb73b7d9b7c75f2efc54440a0f121614d69",
      "externalIds": {
        "DOI": "10.1109/icip55913.2025.11084361",
        "CorpusId": 280793965
      },
      "corpusId": 280793965,
      "title": "Exploring Effective Unfolding Covering Prompt Tuning for Vision Mamba",
      "venue": "International Conference on Information Photonics",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/icip55913.2025.11084361?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/icip55913.2025.11084361, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2376770311",
          "name": "Mingwang Wu"
        },
        {
          "authorId": "2377055653",
          "name": "Yuetong Luo"
        },
        {
          "authorId": "2108003583",
          "name": "Yankong Zhang"
        },
        {
          "authorId": "2377751910",
          "name": "Bo Zhou"
        },
        {
          "authorId": "2350969125",
          "name": "Shengeng Tang"
        },
        {
          "authorId": "2296751414",
          "name": "Lechao Cheng"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "60389cc61cfd32a4cb19a3aed5c5c26267d96e7e",
      "externalIds": {
        "DOI": "10.1109/icip55913.2025.11084623",
        "CorpusId": 280810258
      },
      "corpusId": 280810258,
      "title": "Laplacian-Mamba: Mamba-Based Laplacian Pyramid Enhancement Network for Unpaired High-Definition Images",
      "venue": "International Conference on Information Photonics",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/icip55913.2025.11084623?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/icip55913.2025.11084623, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2378211951",
          "name": "Zhiquan Mao"
        },
        {
          "authorId": "2377740576",
          "name": "Mengning Yang"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "9812be36cd27c61a4db95846b179ff8f634b897d",
      "externalIds": {
        "DOI": "10.1109/icip55913.2025.11084714",
        "CorpusId": 280854562
      },
      "corpusId": 280854562,
      "title": "GMOT-Mamba: Mamba-Based Model Prediction For Generic Multiple Object Tracking",
      "venue": "International Conference on Information Photonics",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/icip55913.2025.11084714?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/icip55913.2025.11084714, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2277543338",
          "name": "Shashikant Verma"
        },
        {
          "authorId": "1429806753",
          "name": "N. Sebe"
        },
        {
          "authorId": "2277490864",
          "name": "Shanmuganathan Raman"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "c68350d18d19b465651768fd0334af3eec4b1cc1",
      "externalIds": {
        "DOI": "10.1109/icip55913.2025.11084513",
        "CorpusId": 280859069
      },
      "corpusId": 280859069,
      "title": "Efficient Asymmetric Shared Low-Rank Adaptation Based on Selective Scanning Vision Mamba for Medical Imaging Analysis",
      "venue": "International Conference on Information Photonics",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/icip55913.2025.11084513?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/icip55913.2025.11084513, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2378211090",
          "name": "Ziqiu Dong"
        },
        {
          "authorId": "2377055653",
          "name": "Yuetong Luo"
        },
        {
          "authorId": "2377751910",
          "name": "Bo Zhou"
        },
        {
          "authorId": "2108003583",
          "name": "Yankong Zhang"
        },
        {
          "authorId": "2350969125",
          "name": "Shengeng Tang"
        },
        {
          "authorId": "2296751414",
          "name": "Lechao Cheng"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "24d35ad8e24e2e771090d24fc653cc55bf99c2ae",
      "externalIds": {
        "ArXiv": "2509.11225",
        "DBLP": "journals/corr/abs-2509-11225",
        "DOI": "10.48550/arXiv.2509.11225",
        "CorpusId": 281315794
      },
      "corpusId": 281315794,
      "title": "MEMBOT: Memory-Based Robot in Intermittent POMDP",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.11225, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2380552476",
          "name": "Youzhi Liang"
        },
        {
          "authorId": "2380527742",
          "name": "Eyan Noronha"
        }
      ],
      "abstract": "Robotic systems deployed in real-world environments often operate under conditions of partial and often intermittent observability, where sensor inputs may be noisy, occluded, or entirely unavailable due to failures or environmental constraints. Traditional reinforcement learning (RL) approaches that assume full state observability are ill-equipped for such challenges. In this work, we introduce MEMBOT, a modular memory-based architecture designed to address intermittent partial observability in robotic control tasks. MEMBOT decouples belief inference from policy learning through a two-phase training process: an offline multi-task learning pretraining stage that learns a robust task-agnostic latent belief encoder using a reconstruction losses, followed by fine-tuning of task-specific policies using behavior cloning. The belief encoder, implemented as a state-space model (SSM) and a LSTM, integrates temporal sequences of observations and actions to infer latent state representations that persist even when observations are dropped. We train and evaluate MEMBOT on 10 robotic manipulation benchmark tasks from MetaWorld and Robomimic under varying rates of observation dropout. Results show that MEMBOT consistently outperforms both memoryless and naively recurrent baselines, maintaining up to 80% of peak performance under 50% observation availability. These findings highlight the effectiveness of explicit belief modeling in achieving robust, transferable, and data-efficient policies for real-world partially observable robotic systems."
    },
    {
      "paperId": "11c52463765541d629e9aa6998fa6cda16a599b3",
      "externalIds": {
        "DBLP": "journals/corr/abs-2510-01203",
        "ArXiv": "2510.01203",
        "DOI": "10.48550/arXiv.2510.01203",
        "CorpusId": 281724890
      },
      "corpusId": 281724890,
      "title": "Mamba Outpaces Reformer in Stock Prediction with Sentiments from Top Ten LLMs",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.01203, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2383311135",
          "name": "Lokesh Antony Kadiyala"
        },
        {
          "authorId": "2383309420",
          "name": "Amir Mirzaeinia"
        }
      ],
      "abstract": "The stock market is extremely difficult to predict in the short term due to high market volatility, changes caused by news, and the non-linear nature of the financial time series. This research proposes a novel framework for improving minute-level prediction accuracy using semantic sentiment scores from top ten different large language models (LLMs) combined with minute interval intraday stock price data. We systematically constructed a time-aligned dataset of AAPL news articles and 1-minute Apple Inc. (AAPL) stock prices for the dates of April 4 to May 2, 2025. The sentiment analysis was achieved using the DeepSeek-V3, GPT variants, LLaMA, Claude, Gemini, Qwen, and Mistral models through their APIs. Each article obtained sentiment scores from all ten LLMs, which were scaled to a [0, 1] range and combined with prices and technical indicators like RSI, ROC, and Bollinger Band Width. Two state-of-the-art such as Reformer and Mamba were trained separately on the dataset using the sentiment scores produced by each LLM as input. Hyper parameters were optimized by means of Optuna and were evaluated through a 3-day evaluation period. Reformer had mean squared error (MSE) or the evaluation metrics, and it should be noted that Mamba performed not only faster but also better than Reformer for every LLM across the 10 LLMs tested. Mamba performed best with LLaMA 3.3--70B, with the lowest error of 0.137. While Reformer could capture broader trends within the data, the model appeared to over smooth sudden changes by the LLMs. This study highlights the potential of integrating LLM-based semantic analysis paired with efficient temporal modeling to enhance real-time financial forecasting."
    },
    {
      "paperId": "1c838c9db0fecebd8c31e10ec45da68acaeab93c",
      "externalIds": {
        "ArXiv": "2509.11002",
        "CorpusId": 281315759
      },
      "corpusId": 281315759,
      "title": "Autonomous real-time control of turbulent dynamics",
      "venue": "",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.11002, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2380575583",
          "name": "Junjie Zhang"
        },
        {
          "authorId": "144805848",
          "name": "C. Xia"
        },
        {
          "authorId": "2380879404",
          "name": "Xianyang Jiang"
        },
        {
          "authorId": "2380525261",
          "name": "Isabella Fumarola"
        },
        {
          "authorId": "2345243357",
          "name": "Georgios Rigas"
        }
      ],
      "abstract": "Mastering turbulence remains one of physics'most intractable challenges, with its chaotic, multi-scale dynamics driving energy dissipation across transport and energy systems. Here we report REACT (Reinforcement Learning for Environmental Adaptation and Control of Turbulence), a fully autonomous reinforcement learning framework that achieves real-time, adaptive, closed-loop turbulence control in real-world environments. Deployed on a road vehicle model equipped solely with onboard sensors and servo-actuated surfaces, REACT learns directly from sparse experimental measurements in a wind tunnel environment, bypassing intractable direct numerical simulations and empirical turbulence models. The agent autonomously converges to a policy that reduces aerodynamic drag while achieving net energy savings. Without prior knowledge of flow physics, it discovers that dynamically suppressing spatio-temporally coherent flow structures in the vehicle wake maximizes energy efficiency, achieving two to four times greater performance than model-based baseline controllers. Through a physics-informed training that recasts data in terms of dimensionless physical groups and parametric input spaces, REACT synthesizes offline a single generalizable agent that transfers across speeds without retraining. These results move agentic learning beyond simulation to robust, interpretable real-world control of high-Reynolds turbulence, opening a path to self-optimizing physical systems in transport, energy and environmental flows."
    },
    {
      "paperId": "a1b5897f5427f5836af2434cf02a4356131fb587",
      "externalIds": {
        "ArXiv": "2509.12266",
        "DBLP": "journals/corr/abs-2509-12266",
        "DOI": "10.48550/arXiv.2509.12266",
        "CorpusId": 281325190
      },
      "corpusId": 281325190,
      "title": "Genome-Factory: An Integrated Library for Tuning, Deploying, and Interpreting Genomic Models",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.12266, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2289966059",
          "name": "Weimin Wu"
        },
        {
          "authorId": "2339430843",
          "name": "Xuefeng Song"
        },
        {
          "authorId": "2338241818",
          "name": "Yibo Wen"
        },
        {
          "authorId": "2387127390",
          "name": "Qinjie Lin"
        },
        {
          "authorId": "2274054723",
          "name": "Zhihan Zhou"
        },
        {
          "authorId": "2266720128",
          "name": "Jerry Yao-Chieh Hu"
        },
        {
          "authorId": "2284399128",
          "name": "Zhong Wang"
        },
        {
          "authorId": "2356840509",
          "name": "Han Liu"
        }
      ],
      "abstract": "We introduce Genome-Factory, an integrated Python library for tuning, deploying, and interpreting genomic models. Our core contribution is to simplify and unify the workflow for genomic model development: data collection, model tuning, inference, benchmarking, and interpretability. For data collection, Genome-Factory offers an automated pipeline to download genomic sequences and preprocess them. It also includes quality control, such as GC content normalization. For model tuning, Genome-Factory supports three approaches: full-parameter, low-rank adaptation, and adapter-based fine-tuning. It is compatible with a wide range of genomic models. For inference, Genome-Factory enables both embedding extraction and DNA sequence generation. For benchmarking, we include two existing benchmarks and provide a flexible interface for users to incorporate additional benchmarks. For interpretability, Genome-Factory introduces the first open-source biological interpreter based on a sparse auto-encoder. This module disentangles embeddings into sparse, near-monosemantic latent units and links them to interpretable genomic features by regressing on external readouts. To improve accessibility, Genome-Factory features both a zero-code command-line interface and a user-friendly web interface. We validate the utility of Genome-Factory across three dimensions: (i) Compatibility with diverse models and fine-tuning methods; (ii) Benchmarking downstream performance using two open-source benchmarks; (iii) Biological interpretation of learned representations with DNABERT-2. These results highlight its end-to-end usability and practical value for real-world genomic analysis."
    },
    {
      "paperId": "8d1c7c702cb8f2e999ef5b843366c1942d5ab62a",
      "externalIds": {
        "DOI": "10.1007/s00371-025-04171-6",
        "CorpusId": 283102362
      },
      "corpusId": 283102362,
      "title": "Enhancing point cloud analysis with multi-scale state space modeling and edge graph augmentation",
      "venue": "The Visual Computer",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s00371-025-04171-6?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s00371-025-04171-6, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2393460888",
          "name": "Weidong Zhu"
        },
        {
          "authorId": "2393145247",
          "name": "Qi Sun"
        },
        {
          "authorId": "9171293",
          "name": "K. Luan"
        },
        {
          "authorId": "2374101269",
          "name": "Qidi Xie"
        },
        {
          "authorId": "2373661178",
          "name": "Bangyu Peng"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "7c92d3b036f361e931ad23a5a422a090fa7f3ece",
      "externalIds": {
        "ArXiv": "2509.10417",
        "DBLP": "journals/corr/abs-2509-10417",
        "DOI": "10.48550/arXiv.2509.10417",
        "CorpusId": 281309942
      },
      "corpusId": 281309942,
      "title": "Long Context Automated Essay Scoring with Language Models",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.10417, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2380440990",
          "name": "Christopher Ormerod"
        },
        {
          "authorId": "3433384",
          "name": "Gitit Kehat"
        }
      ],
      "abstract": "Transformer-based language models are architecturally constrained to process text of a fixed maximum length. Essays written by higher-grade students frequently exceed the maximum allowed length for many popular open-source models. A common approach to addressing this issue when using these models for Automated Essay Scoring is to truncate the input text. This raises serious validity concerns as it undermines the model's ability to fully capture and evaluate organizational elements of the scoring rubric, which requires long contexts to assess. In this study, we evaluate several models that incorporate architectural modifications of the standard transformer architecture to overcome these length limitations using the Kaggle ASAP 2.0 dataset. The models considered in this study include fine-tuned versions of XLNet, Longformer, ModernBERT, Mamba, and Llama models."
    },
    {
      "paperId": "ef0a0498d1d8c59729c2dd9f301cdb14cbdd7c0e",
      "externalIds": {
        "ArXiv": "2509.10324",
        "DBLP": "journals/corr/abs-2509-10324",
        "DOI": "10.48550/arXiv.2509.10324",
        "CorpusId": 281310387
      },
      "corpusId": 281310387,
      "title": "ARMA Block: A CNN-Based Autoregressive and Moving Average Module for Long-Term Time Series Forecasting",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.10324, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2162703788",
          "name": "Myung Jin Kim"
        },
        {
          "authorId": "51120300",
          "name": "Yeonghyeon Park"
        },
        {
          "authorId": "2380444469",
          "name": "Il Dong Yun"
        }
      ],
      "abstract": "This paper proposes a simple yet effective convolutional module for long-term time series forecasting. The proposed block, inspired by the Auto-Regressive Integrated Moving Average (ARIMA) model, consists of two convolutional components: one for capturing the trend (autoregression) and the other for refining local variations (moving average). Unlike conventional ARIMA, which requires iterative multi-step forecasting, the block directly performs multi-step forecasting, making it easily extendable to multivariate settings. Experiments on nine widely used benchmark datasets demonstrate that our method ARMA achieves competitive accuracy, particularly on datasets exhibiting strong trend variations, while maintaining architectural simplicity. Furthermore, analysis shows that the block inherently encodes absolute positional information, suggesting its potential as a lightweight replacement for positional embeddings in sequential models."
    },
    {
      "paperId": "6fce7b04b5e206921c77afb027efaac76fe6babd",
      "externalIds": {
        "ArXiv": "2509.10345",
        "DBLP": "journals/corr/abs-2509-10345",
        "DOI": "10.48550/arXiv.2509.10345",
        "CorpusId": 281309710
      },
      "corpusId": 281309710,
      "title": "Towards Understanding Visual Grounding in Visual Language Models",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.10345, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2060170791",
          "name": "Georgios Pantazopoulos"
        },
        {
          "authorId": "2127606107",
          "name": "Eda Bilici Ozyigit"
        }
      ],
      "abstract": "Visual grounding refers to the ability of a model to identify a region within some visual input that matches a textual description. Consequently, a model equipped with visual grounding capabilities can target a wide range of applications in various domains, including referring expression comprehension, answering questions pertinent to fine-grained details in images or videos, caption visual context by explicitly referring to entities, as well as low and high-level control in simulated and real environments. In this survey paper, we review representative works across the key areas of research on modern general-purpose vision language models (VLMs). We first outline the importance of grounding in VLMs, then delineate the core components of the contemporary paradigm for developing grounded models, and examine their practical applications, including benchmarks and evaluation metrics for grounded multimodal generation. We also discuss the multifaceted interrelations among visual grounding, multimodal chain-of-thought, and reasoning in VLMs. Finally, we analyse the challenges inherent to visual grounding and suggest promising directions for future research."
    },
    {
      "paperId": "044464ac914285f4eba564a50320610a70f6129c",
      "externalIds": {
        "ArXiv": "2509.09971",
        "DBLP": "journals/corr/abs-2509-09971",
        "DOI": "10.48550/arXiv.2509.09971",
        "CorpusId": 281310215
      },
      "corpusId": 281310215,
      "title": "Event Camera Guided Visual Media Restoration & 3D Reconstruction: A Survey",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.09971, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "46243065",
          "name": "Aupendu Kar"
        },
        {
          "authorId": "2380445148",
          "name": "Vishnu Raj"
        },
        {
          "authorId": "2371992078",
          "name": "Guan-Ming Su"
        }
      ],
      "abstract": "Event camera sensors are bio-inspired sensors which asynchronously capture per-pixel brightness changes and output a stream of events encoding the polarity, location and time of these changes. These systems are witnessing rapid advancements as an emerging field, driven by their low latency, reduced power consumption, and ultra-high capture rates. This survey explores the evolution of fusing event-stream captured with traditional frame-based capture, highlighting how this synergy significantly benefits various video restoration and 3D reconstruction tasks. The paper systematically reviews major deep learning contributions to image/video enhancement and restoration, focusing on two dimensions: temporal enhancement (such as frame interpolation and motion deblurring) and spatial enhancement (including super-resolution, low-light and HDR enhancement, and artifact reduction). This paper also explores how the 3D reconstruction domain evolves with the advancement of event driven fusion. Diverse topics are covered, with in-depth discussions on recent works for improving visual quality under challenging conditions. Additionally, the survey compiles a comprehensive list of openly available datasets, enabling reproducible research and benchmarking. By consolidating recent progress and insights, this survey aims to inspire further research into leveraging event camera systems, especially in combination with deep learning, for advanced visual media restoration and enhancement."
    },
    {
      "paperId": "eb80336332c49ee1f208b607dbd1fd6bbca0fe44",
      "externalIds": {
        "ArXiv": "2509.09940",
        "DBLP": "journals/corr/abs-2509-09940",
        "DOI": "10.48550/arXiv.2509.09940",
        "CorpusId": 281310170
      },
      "corpusId": 281310170,
      "title": "DyKen-Hyena: Dynamic Kernel Generation via Cross-Modal Attention for Multimodal Intent Recognition",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.09940, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2380636857",
          "name": "Yifei Wang"
        },
        {
          "authorId": "2279401280",
          "name": "Wenbin Wang"
        },
        {
          "authorId": "2380463669",
          "name": "Yong Luo"
        }
      ],
      "abstract": "Though Multimodal Intent Recognition (MIR) proves effective by utilizing rich information from multiple sources (e.g., language, video, and audio), the potential for intent-irrelevant and conflicting information across modalities may hinder performance from being further improved. Most current models attempt to fuse modalities by applying mechanisms like multi-head attention to unimodal feature sequences and then adding the result back to the original representation. This process risks corrupting the primary linguistic features with noisy or irrelevant non-verbal signals, as it often fails to capture the fine-grained, token-level influence where non-verbal cues should modulate, not just augment, textual meaning. To address this, we introduce DyKen-Hyena, which reframes the problem from feature fusion to processing modulation. Our model translates audio-visual cues into dynamic, per-token convolutional kernels that directly modulate textual feature extraction. This fine-grained approach achieves state-of-the-art results on the MIntRec and MIntRec2.0 benchmarks. Notably, it yields a +10.46% F1-score improvement in out-of-scope detection, validating that our method creates a fundamentally more robust intent representation."
    },
    {
      "paperId": "721a711ede4d5178cd2bf25b590d1e21aab79cd4",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-12250",
        "ArXiv": "2509.12250",
        "DOI": "10.1145/3746027.3754848",
        "CorpusId": 281325799
      },
      "corpusId": 281325799,
      "title": "OnlineHOI: Towards Online Human-Object Interaction Generation and Perception",
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.12250, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2384828630",
          "name": "Yihong Ji"
        },
        {
          "authorId": "2380638840",
          "name": "Yunze Liu"
        },
        {
          "authorId": "2380609741",
          "name": "Yiyao Zhuo"
        },
        {
          "authorId": "2327923050",
          "name": "Weijiang Yu"
        },
        {
          "authorId": "2381238285",
          "name": "Fei Ma"
        },
        {
          "authorId": "2347392106",
          "name": "Joshua Zhexue Huang"
        },
        {
          "authorId": "2352428572",
          "name": "Fei Yu"
        }
      ],
      "abstract": "The perception and generation of Human-Object Interaction (HOI) are crucial for fields such as robotics, AR/VR, and human behavior understanding. However, current approaches model this task in an offline setting, where information at each time step can be drawn from the entire interaction sequence. In contrast, in real-world scenarios, the information available at each time step comes only from the current moment and historical data, i.e., an online setting. We find that offline methods perform poorly in an online context. Based on this observation, we propose two new tasks: Online HOI Generation and Perception. To address this task, we introduce the OnlineHOI framework, a network architecture based on the Mamba framework that employs a memory mechanism. By leveraging Mamba's powerful modeling capabilities for streaming data and the Memory mechanism's efficient integration of historical information, we achieve state-of-the-art results on the Core4D and OAKINK2 online generation tasks, as well as the online HOI4D perception task."
    },
    {
      "paperId": "9c568032ee6e0ba2fc78f586319fa3c164024bc3",
      "externalIds": {
        "DOI": "10.1109/IoTAAI66837.2025.11212995",
        "CorpusId": 282577029
      },
      "corpusId": 282577029,
      "title": "Accurate State of Health Estimation of Sodium-Ion Batteries via Mamba Architecture",
      "venue": "2025 7th International Conference on Internet of Things, Automation and Artificial Intelligence (IoTAAI)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/IoTAAI66837.2025.11212995?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/IoTAAI66837.2025.11212995, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2332883522",
          "name": "Zhongkui Du"
        },
        {
          "authorId": "2333333844",
          "name": "Rui Liu"
        },
        {
          "authorId": "2332598692",
          "name": "Jiaming Wang"
        },
        {
          "authorId": "2389048612",
          "name": "Binhao Lei"
        },
        {
          "authorId": "2391687419",
          "name": "Jingze Zhao"
        },
        {
          "authorId": "2250029228",
          "name": "Guodong Sun"
        }
      ],
      "abstract": "Accurate assessment of the State of Health (SOH) of Sodium-ion Batteries (SIBs) is important to ensure their safety and reliability in energy storage systems. Traditional deep learning models have certain limitations in establishing time series dependencies. Convolutional Neural Network (CNN) and Long Short-Term Memory network (LSTM) have difficulty capturing long-term dependencies; the self-attention mechanism in Transformers faces the problem of quadratic computational complexity (O(L2)) when dealing with long sequence scenarios. To address these problems, this paper proposed a single-cycle SOH estimation method based on the Mamba model. This method effectively combined the local feature extraction capability of convolutional neural networks with the long-term dependency modeling capability of structured state space models (SSM), and introduced a selective mechanism based on input dependencies to effectively capture features at key time points related to battery degradation while maintaining linear computational complexity. Experiments were conducted on a real-world dataset containing 31 sodium-ion batteries and 12 different charge-discharge strategies. Compared with LSTM, MLP, CNN, and Attention, the proposed method achieved better performance, resulting in lower mean absolute error (MAE), mean absolute percentage error (MAPE), and root mean square error (RMSE). Furthermore, comparative error analysis across different cycle numbers of SIBs demonstrates that the proposed method achieves more stable error performance than the aforementioned models, confirming its effectiveness and generalizability in capturing degradation trends from cycling data."
    },
    {
      "paperId": "d0472c5fcb5d7c8764eed72907ac0cd78564ce76",
      "externalIds": {
        "DOI": "10.1145/3772128.3772156",
        "CorpusId": 283691422
      },
      "corpusId": 283691422,
      "title": "A Multi-Scale Mamba Framework for Audio-Based Classification of Chinese Opera Styles",
      "venue": "Proceedings of the 2025 2nd International Conference on Virtual Reality, Image and Signal Processing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3772128.3772156?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3772128.3772156, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2325916863",
          "name": "Jiaxiang Zheng"
        },
        {
          "authorId": "2325708678",
          "name": "Moxi Cao"
        },
        {
          "authorId": "2345324227",
          "name": "Chongbin Zhang"
        }
      ],
      "abstract": "This study presents a multi-scale audio classification framework for Chinese opera genres using the Mamba State Space Model. Addressing the challenges of long-range temporal dependencies and complex spectral features in traditional opera, we extract four complementary audio representations\u2014tempogram, chromagram, mel spectrogram, and F0 contour\u2014to capture rhythm, harmony, timbre, and pitch. Each feature stream is modeled by a dedicated Mamba encoder, and an attention-based fusion module integrates the multi-scale outputs for final classification. Evaluated on a dataset of 1,034 audio samples across five major Chinese opera genres, our model achieves superior performance over CNN-, Transformer-, and MLP-based baselines, with significant gains in accuracy, precision, recall, and F1 score. The proposed approach demonstrates the effectiveness of state space modeling in non-Western musical contexts and contributes a novel solution to cross-cultural music information retrieval."
    },
    {
      "paperId": "e83c9bc5f8e9edc88089c2e4e78fa0d4a96fc0c7",
      "externalIds": {
        "DOI": "10.1145/3773365.3773419",
        "CorpusId": 283935791
      },
      "corpusId": 283935791,
      "title": "Emotion-aware Multi-modal Fusion for Human Behavior Analysis via Graph and State-space",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3773365.3773419?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3773365.3773419, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2399269654",
          "name": "Haokai Xu"
        },
        {
          "authorId": "2240730721",
          "name": "Dengshi Li"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "8bde7e47a412a6f6849935054b854f9814fb9f7c",
      "externalIds": {
        "DOI": "10.1109/ICCVDM66874.2025.11290463",
        "CorpusId": 284023427
      },
      "corpusId": 284023427,
      "title": "LIA-VMamba: Image Classification with Local Information Awareness via Bidirectional State Space Model and Adaptive Graph Convolution Enhancement",
      "venue": "2025 6th International Conference on Computer Vision and Data Mining (ICCVDM)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCVDM66874.2025.11290463?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCVDM66874.2025.11290463, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": null,
          "name": "Ming Zheng"
        },
        {
          "authorId": "2399741652",
          "name": "Yun Lin"
        },
        {
          "authorId": "2381933986",
          "name": "Binqi Li"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "7d26325c546fb7ca91a403d982130e6bca024fb4",
      "externalIds": {
        "ArXiv": "2509.09456",
        "DBLP": "journals/corr/abs-2509-09456",
        "DOI": "10.1016/j.eswa.2025.128895",
        "CorpusId": 281252061
      },
      "corpusId": 281252061,
      "title": "FlexiD-Fuse: Flexible number of inputs multi-modal medical image fusion based on diffusion model",
      "venue": "Expert systems with applications",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.09456, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2282499042",
          "name": "Yushen Xu"
        },
        {
          "authorId": "2301837453",
          "name": "Xiaosong Li"
        },
        {
          "authorId": "2108898957",
          "name": "Yuchun Wang"
        },
        {
          "authorId": "2271673122",
          "name": "Xiaoqi Cheng"
        },
        {
          "authorId": "2330373428",
          "name": "Huafeng Li"
        },
        {
          "authorId": "2279244348",
          "name": "Haishu Tan"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "55b1b25ee4fcd087f911af0193f0c4ae792c397b",
      "externalIds": {
        "ArXiv": "2509.09785",
        "DBLP": "journals/corr/abs-2509-09785",
        "DOI": "10.48550/arXiv.2509.09785",
        "CorpusId": 281310243
      },
      "corpusId": 281310243,
      "title": "Purge-Gate: Backpropagation-Free Test-Time Adaptation for Point Clouds Classification via Token Purging",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.09785, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2168705225",
          "name": "Moslem Yazdanpanah"
        },
        {
          "authorId": "108062243",
          "name": "Ali Bahri"
        },
        {
          "authorId": "1380287805",
          "name": "Mehrdad Noori"
        },
        {
          "authorId": "2307459523",
          "name": "Sahar Dastani"
        },
        {
          "authorId": "2037886454",
          "name": "G. Hakim"
        },
        {
          "authorId": "2188345071",
          "name": "David Osowiechi"
        },
        {
          "authorId": "144019647",
          "name": "Ismail Ben Ayed"
        },
        {
          "authorId": "2260340228",
          "name": "Christian Desrosiers"
        }
      ],
      "abstract": "Test-time adaptation (TTA) is crucial for mitigating performance degradation caused by distribution shifts in 3D point cloud classification. In this work, we introduce Token Purging (PG), a novel backpropagation-free approach that removes tokens highly affected by domain shifts before they reach attention layers. Unlike existing TTA methods, PG operates at the token level, ensuring robust adaptation without iterative updates. We propose two variants: PG-SP, which leverages source statistics, and PG-SF, a fully source-free version relying on CLS-token-driven adaptation. Extensive evaluations on ModelNet40-C, ShapeNet-C, and ScanObjectNN-C demonstrate that PG-SP achieves an average of +10.3\\% higher accuracy than state-of-the-art backpropagation-free methods, while PG-SF sets new benchmarks for source-free adaptation. Moreover, PG is 12.4 times faster and 5.5 times more memory efficient than our baseline, making it suitable for real-world deployment. Code is available at \\hyperlink{https://github.com/MosyMosy/Purge-Gate}{https://github.com/MosyMosy/Purge-Gate}"
    },
    {
      "paperId": "3b885b8ded3cb032a34096848a13bcadd48e9bbd",
      "externalIds": {
        "DOI": "10.3390/sci7030130",
        "CorpusId": 281296931
      },
      "corpusId": 281296931,
      "title": "Transformers and State-Space Models: Fine-Tuning Techniques for Solving Differential Equations",
      "venue": "The Scientist",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/sci7030130?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/sci7030130, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "103629820",
          "name": "V. Ignatenko"
        },
        {
          "authorId": "2277800954",
          "name": "Anton Surkov"
        },
        {
          "authorId": "2360212824",
          "name": "Vladimir Zakharov"
        },
        {
          "authorId": "3406429",
          "name": "S. Koltcov"
        }
      ],
      "abstract": "Large language models (LLMs) have recently demonstrated remarkable capabilities in natural language processing, mathematical reasoning, and code generation. However, their potential for solving differential equations\u2014fundamental to applied mathematics, physics, and engineering\u2014remains insufficiently explored. For the first time, we applied LLMs as translators from the textual form of an equation into the textual representation of its analytical solution for a broad class of equations. More precisely, we introduced a benchmark and fine-tuning protocol for differential equation solving with pre-trained LLMs. We curated a dataset of 300,000 differential equations and corresponding solutions to fine-tune T5-small, Phi-4-mini, DeepSeek-R1-Distill-Qwen, and two Mamba variants (130M and 2.8B parameters). Performance was evaluated using BLEU and TeXBLEU metrics. Phi-4-mini achieved the best results, with average BLEU > 0.9 and TeXBLEU > 0.78 across all considered equation classes, which shows the strong generalization abilities of the model. Therefore, this model should be further investigated on a broader class of differential equations and potentially can be used as a part of mathematical agents for solving more complex particular tasks, for example, from physics or engineering. Based on our results, DeepSeek-R1-Distill-Qwen consistently underperformed, while T5 showed strong results for the most frequent equation type but degraded on less common ones. Mamba models achieved the highest TeXBLEU scores despite relatively low BLEU, attributable to their production of lengthy outputs mixing correct expressions with irrelevant ones."
    },
    {
      "paperId": "b40b750603b38fca6e545eb661cb297af2296a13",
      "externalIds": {
        "DBLP": "journals/sivp/MiaorongY25",
        "DOI": "10.1007/s11760-025-04632-4",
        "CorpusId": 281300101
      },
      "corpusId": 281300101,
      "title": "The dual-encoder image forgery detection integrating convolution and mamba",
      "venue": "Signal, Image and Video Processing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11760-025-04632-4?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11760-025-04632-4, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2380195300",
          "name": "Miaorong Pan"
        },
        {
          "authorId": "2380171888",
          "name": "Wang Yi"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "e0815f25b4ca2cb009621d9210a2882ecff74907",
      "externalIds": {
        "ArXiv": "2509.08265",
        "DBLP": "journals/corr/abs-2509-08265",
        "DOI": "10.48550/arXiv.2509.08265",
        "CorpusId": 281243922
      },
      "corpusId": 281243922,
      "title": "Hyperspectral Mamba for Hyperspectral Object Tracking",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.08265, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2112439622",
          "name": "Long Gao"
        },
        {
          "authorId": "2347193899",
          "name": "Yunhe Zhang"
        },
        {
          "authorId": "2210933937",
          "name": "Yan Jiang"
        },
        {
          "authorId": "2267160567",
          "name": "Weiying Xie"
        },
        {
          "authorId": "2144421320",
          "name": "Yunsong Li"
        }
      ],
      "abstract": "Hyperspectral object tracking holds great promise due to the rich spectral information and fine-grained material distinctions in hyperspectral images, which are beneficial in challenging scenarios. While existing hyperspectral trackers have made progress by either transforming hyperspectral data into false-color images or incorporating modality fusion strategies, they often fail to capture the intrinsic spectral information, temporal dependencies, and cross-depth interactions. To address these limitations, a new hyperspectral object tracking network equipped with Mamba (HyMamba), is proposed. It unifies spectral, cross-depth, and temporal modeling through state space modules (SSMs). The core of HyMamba lies in the Spectral State Integration (SSI) module, which enables progressive refinement and propagation of spectral features with cross-depth and temporal spectral information. Embedded within each SSI, the Hyperspectral Mamba (HSM) module is introduced to learn spatial and spectral information synchronously via three directional scanning SSMs. Based on SSI and HSM, HyMamba constructs joint features from false-color and hyperspectral inputs, and enhances them through interaction with original spectral features extracted from raw hyperspectral images. Extensive experiments conducted on seven benchmark datasets demonstrate that HyMamba achieves state-of-the-art performance. For instance, it achieves 73.0\\% of the AUC score and 96.3\\% of the DP@20 score on the HOTC2020 dataset. The code will be released at https://github.com/lgao001/HyMamba."
    },
    {
      "paperId": "3ae0aed92f344237dc4486e97811f2c438ad8f59",
      "externalIds": {
        "DOI": "10.70267/cai.25v2n3.0112",
        "CorpusId": 281276473
      },
      "corpusId": 281276473,
      "title": "Frontiers in Artificial Intelligence Algorithm Optimization: A Comprehensive Review of Training-Time and Inference-Time Advances",
      "venue": "Computers and artificial intelligence",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.70267/cai.25v2n3.0112?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.70267/cai.25v2n3.0112, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2380509458",
          "name": "Juntong Lu"
        }
      ],
      "abstract": "The rapid progress of artificial intelligence (AI) has been largely driven by the scaling of deep neural networks, advances in hardware accelerators, and the availability of large-scale datasets. However, the computational, memory, and energy demands of training and deploying foundation models such as GPT-5 and LLaMA-3 have created scalability and sustainability bottlenecks. Algorithmic optimization has emerged as a central strategy to alleviate these challenges across training-time efficiency, inference-time acceleration, long-context extension, and alignment learning. This article provides a comprehensive review of the state of the art in AI algorithm optimization, systematically categorizing approaches, benchmarking them under unified metrics (memory, throughput, latency, perplexity, stability, complexity, portability), and identifying failure modes and boundary conditions. We further present reproducibility artifacts, including minimal training and inference stacks (GaLore + Sophia optimizer; vLLM + FlashAttention-3 + QServe) and standardized datasets (MMLU, GSM8K, LongBench, DCLM). Our synthesis underscores that algorithm\u2013system co-design\u2014spanning optimizer innovations, quantization-aware serving, context length generalization, and efficient preference alignment\u2014is critical to achieving both efficiency and ethical sustainability in next-generation AI systems."
    },
    {
      "paperId": "1e9a67a26dac3822ab5818ebc2f6591864db7841",
      "externalIds": {
        "DBLP": "journals/sivp/ZhangGL25a",
        "DOI": "10.1007/s11760-025-04684-6",
        "CorpusId": 281254557
      },
      "corpusId": 281254557,
      "title": "IDBNet: An Independent Dual-Branch Network for Semantic Segmentation of Remote Sensing Images",
      "venue": "Signal, Image and Video Processing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11760-025-04684-6?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11760-025-04684-6, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2380330516",
          "name": "Ting Zhang"
        },
        {
          "authorId": "2380047832",
          "name": "Chenxu Ge"
        },
        {
          "authorId": "2224006",
          "name": "Q. Leng"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "1f5590c643e0ebd726c60e908909f9d9c48bc8a4",
      "externalIds": {
        "ArXiv": "2509.07327",
        "DBLP": "journals/corr/abs-2509-07327",
        "DOI": "10.48550/arXiv.2509.07327",
        "CorpusId": 281218097
      },
      "corpusId": 281218097,
      "title": "DEPFusion: Dual-Domain Enhancement and Priority-Guided Mamba Fusion for UAV Multispectral Object Detection",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.07327, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2375856016",
          "name": "Shucong Li"
        },
        {
          "authorId": "2109128496",
          "name": "Zhenyu Liu"
        },
        {
          "authorId": "2380356177",
          "name": "Zijie Hong"
        },
        {
          "authorId": "2380083067",
          "name": "Zhiheng Zhou"
        },
        {
          "authorId": "2380134838",
          "name": "Xianghai Cao"
        }
      ],
      "abstract": "Multispectral object detection is an important application for unmanned aerial vehicles (UAVs). However, it faces several challenges. First, low-light RGB images weaken the multispectral fusion due to details loss. Second, the interference information is introduced to local target modeling during multispectral fusion. Third, computational cost poses deployment challenge on UAV platforms, such as transformer-based methods with quadratic complexity. To address these issues, a framework named DEPFusion consisting of two designed modules, Dual-Domain Enhancement (DDE) and Priority-Guided Mamba Fusion (PGMF) , is proposed for UAV multispectral object detection. Firstly, considering the adoption of low-frequency component for global brightness enhancement and frequency spectra features for texture-details recovery, DDE module is designed with Cross-Scale Wavelet Mamba (CSWM) block and Fourier Details Recovery (FDR) block. Secondly, considering guiding the scanning of Mamba from high priority score tokens, which contain local target feature, a novel Priority-Guided Serialization is proposed with theoretical proof. Based on it, PGMF module is designed for multispectral feature fusion, which enhance local modeling and reduce interference information. Experiments on DroneVehicle and VEDAI datasets demonstrate that DEPFusion achieves good performance with state-of-the-art methods."
    },
    {
      "paperId": "bad601f07ecc08548bbb01cd134bbe1642b36357",
      "externalIds": {
        "DBLP": "conf/etfa/MeyerEPGK25",
        "DOI": "10.1109/ETFA65518.2025.11205803",
        "CorpusId": 282273101
      },
      "corpusId": 282273101,
      "title": "Evaluating Advanced Anomaly Detection Models for Time Series Data of Refrigeration Systems",
      "venue": "IEEE International Conference on Emerging Technologies and Factory Automation",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ETFA65518.2025.11205803?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ETFA65518.2025.11205803, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2257858246",
          "name": "Melina Meyer"
        },
        {
          "authorId": "2047519322",
          "name": "Julian Eversheim"
        },
        {
          "authorId": "2387000308",
          "name": "Johannes Pitterle"
        },
        {
          "authorId": "2257976379",
          "name": "Martin Gergeleit"
        },
        {
          "authorId": "2257982402",
          "name": "Dirk Krechel"
        }
      ],
      "abstract": "Reliable cold chain monitoring in supermarkets is essential, yet current anomaly detection often relies on fixed expert-defined thresholds and raw data, limiting flexibility. This study evaluates different advanced anomaly detection on real-world refrigeration time series data, focusing on generalization across similar refrigeration units with varying configurations. It examines model performance on both seen and unseen units and explores calibration methods, including data scaling and individual anomaly threshold determination. Results show that Transformer-based models and state-space models outperform traditional baselines and generalize well, even across differing systems. Tailored thresholds and calibration improve detection, offering a scalable approach for industrial deployment."
    },
    {
      "paperId": "9647a92d5ab280831cad43f61cf4cc9d4905682f",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-07506",
        "ArXiv": "2509.07506",
        "DOI": "10.48550/arXiv.2509.07506",
        "CorpusId": 281095694
      },
      "corpusId": 281095694,
      "title": "Astra: A Multi-Agent System for GPU Kernel Performance Optimization",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 8,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.07506, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2028616391",
          "name": "Anjiang Wei"
        },
        {
          "authorId": "2392217334",
          "name": "Tianran Sun"
        },
        {
          "authorId": "2378862435",
          "name": "Yogesh Seenichamy"
        },
        {
          "authorId": "2373585369",
          "name": "Hang Song"
        },
        {
          "authorId": "2345816364",
          "name": "Anne Ouyang"
        },
        {
          "authorId": "1861312",
          "name": "Azalia Mirhoseini"
        },
        {
          "authorId": "2327322280",
          "name": "Ke Wang"
        },
        {
          "authorId": "2277502914",
          "name": "Alex Aiken"
        }
      ],
      "abstract": "GPU kernel optimization has long been a central challenge at the intersection of high-performance computing and machine learning. Efficient kernels are crucial for accelerating large language model (LLM) training and serving, yet attaining high performance typically requires extensive manual tuning. Compiler-based systems reduce some of this burden, but still demand substantial manual design and engineering effort. Recently, researchers have explored using LLMs for GPU kernel generation, though prior work has largely focused on translating high-level PyTorch modules into CUDA code. In this work, we introduce Astra, the first LLM-based multi-agent system for GPU kernel optimization. Unlike previous approaches, Astra starts from existing CUDA implementations extracted from SGLang, a widely deployed framework for serving LLMs, rather than treating PyTorch modules as the specification. Within Astra, specialized LLM agents collaborate through iterative code generation, testing, profiling, and planning to produce kernels that are both correct and high-performance. On kernels from SGLang, Astra achieves an average speedup of 1.32x using zero-shot prompting with OpenAI o4-mini. A detailed case study further demonstrates that LLMs can autonomously apply loop transformations, optimize memory access patterns, exploit CUDA intrinsics, and leverage fast math operations to yield substantial performance gains. Our work highlights multi-agent LLM systems as a promising new paradigm for GPU kernel optimization. Our code is publicly available at https://github.com/Anjiang-Wei/Astra."
    },
    {
      "paperId": "296f27b7c980b283467b9b7a5088daea50d29558",
      "externalIds": {
        "ArXiv": "2509.07381",
        "DBLP": "journals/corr/abs-2509-07381",
        "DOI": "10.48550/arXiv.2509.07381",
        "CorpusId": 281218623
      },
      "corpusId": 281218623,
      "title": "TransMPC: Transformer-based Explicit MPC with Variable Prediction Horizon",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.07381, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2379881454",
          "name": "Sichao Wu"
        },
        {
          "authorId": "2379843172",
          "name": "Jiang Wu"
        },
        {
          "authorId": "2378898504",
          "name": "Xingyu Cao"
        },
        {
          "authorId": "2124223824",
          "name": "Fawang Zhang"
        },
        {
          "authorId": "2313050725",
          "name": "Guangyuan Yu"
        },
        {
          "authorId": "2379769016",
          "name": "Junjie Zhao"
        },
        {
          "authorId": "2277265116",
          "name": "Yue Qu"
        },
        {
          "authorId": "2313761718",
          "name": "Fei Ma"
        },
        {
          "authorId": "2379926736",
          "name": "Jingliang Duan"
        }
      ],
      "abstract": "Traditional online Model Predictive Control (MPC) methods often suffer from excessive computational complexity, limiting their practical deployment. Explicit MPC mitigates online computational load by pre-computing control policies offline; however, existing explicit MPC methods typically rely on simplified system dynamics and cost functions, restricting their accuracy for complex systems. This paper proposes TransMPC, a novel Transformer-based explicit MPC algorithm capable of generating highly accurate control sequences in real-time for complex dynamic systems. Specifically, we formulate the MPC policy as an encoder-only Transformer leveraging bidirectional self-attention, enabling simultaneous inference of entire control sequences in a single forward pass. This design inherently accommodates variable prediction horizons while ensuring low inference latency. Furthermore, we introduce a direct policy optimization framework that alternates between sampling and learning phases. Unlike imitation-based approaches dependent on precomputed optimal trajectories, TransMPC directly optimizes the true finite-horizon cost via automatic differentiation. Random horizon sampling combined with a replay buffer provides independent and identically distributed (i.i.d.) training samples, ensuring robust generalization across varying states and horizon lengths. Extensive simulations and real-world vehicle control experiments validate the effectiveness of TransMPC in terms of solution accuracy, adaptability to varying horizons, and computational efficiency."
    },
    {
      "paperId": "76f8966054aceba57b1013ce3dc135fb06e8e5e1",
      "externalIds": {
        "ArXiv": "2509.07923",
        "DBLP": "journals/corr/abs-2509-07923",
        "DOI": "10.48550/arXiv.2509.07923",
        "CorpusId": 281218570
      },
      "corpusId": 281218570,
      "title": "Multimodal Contrastive Pretraining of CBCT and IOS for Enhanced Tooth Segmentation",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.07923, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2325148719",
          "name": "Moo Hyun Son"
        },
        {
          "authorId": "2326075231",
          "name": "Juyoung Bae"
        },
        {
          "authorId": "2319449379",
          "name": "Zelin Qiu"
        },
        {
          "authorId": "2379890709",
          "name": "Jiale Peng"
        },
        {
          "authorId": "2379960302",
          "name": "Kai Xin Li"
        },
        {
          "authorId": "2379927513",
          "name": "Yifan Lin"
        },
        {
          "authorId": "2325301458",
          "name": "Hao Chen"
        }
      ],
      "abstract": "Digital dentistry represents a transformative shift in modern dental practice. The foundational step in this transformation is the accurate digital representation of the patient's dentition, which is obtained from segmented Cone-Beam Computed Tomography (CBCT) and Intraoral Scans (IOS). Despite the growing interest in digital dental technologies, existing segmentation methodologies frequently lack rigorous validation and demonstrate limited performance and clinical applicability. To the best of our knowledge, this is the first work to introduce a multimodal pretraining framework for tooth segmentation. We present ToothMCL, a Tooth Multimodal Contrastive Learning for pretraining that integrates volumetric (CBCT) and surface-based (IOS) modalities. By capturing modality-invariant representations through multimodal contrastive learning, our approach effectively models fine-grained anatomical features, enabling precise multi-class segmentation and accurate identification of F\\'ed\\'eration Dentaire Internationale (FDI) tooth numbering. Along with the framework, we curated CBCT-IOS3.8K, the largest paired CBCT and IOS dataset to date, comprising 3,867 patients. We then evaluated ToothMCL on a comprehensive collection of independent datasets, representing the largest and most diverse evaluation to date. Our method achieves state-of-the-art performance in both internal and external testing, with an increase of 12\\% for CBCT segmentation and 8\\% for IOS segmentation in the Dice Similarity Coefficient (DSC). Furthermore, ToothMCL consistently surpasses existing approaches in tooth groups and demonstrates robust generalizability across varying imaging conditions and clinical scenarios."
    },
    {
      "paperId": "b4881ece547e401fe6291d4513940ee0926ec429",
      "externalIds": {
        "ArXiv": "2509.07593",
        "DBLP": "journals/corr/abs-2509-07593",
        "DOI": "10.48550/arXiv.2509.07593",
        "CorpusId": 281218705
      },
      "corpusId": 281218705,
      "title": "Can SSD-Mamba2 Unlock Reinforcement Learning for End-to-End Motion Control?",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.07593, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2376201400",
          "name": "Gavin Tao"
        },
        {
          "authorId": "2378185475",
          "name": "Yinuo Wang"
        },
        {
          "authorId": "2379934394",
          "name": "Jinzhao Zhou"
        }
      ],
      "abstract": "End-to-end reinforcement learning for motion control promises unified perception-action policies that scale across embodiments and tasks, yet most deployed controllers are either blind (proprioception-only) or rely on fusion backbones with unfavorable compute-memory trade-offs. Recurrent controllers struggle with long-horizon credit assignment, and Transformer-based fusion incurs quadratic cost in token length, limiting temporal and spatial context. We present a vision-driven cross-modal RL framework built on SSD-Mamba2, a selective state-space backbone that applies state-space duality (SSD) to enable both recurrent and convolutional scanning with hardware-aware streaming and near-linear scaling. Proprioceptive states and exteroceptive observations (e.g., depth tokens) are encoded into compact tokens and fused by stacked SSD-Mamba2 layers. The selective state-space updates retain long-range dependencies with markedly lower latency and memory use than quadratic self-attention, enabling longer look-ahead, higher token resolution, and stable training under limited compute. Policies are trained end-to-end under curricula that randomize terrain and appearance and progressively increase scene complexity. A compact, state-centric reward balances task progress, energy efficiency, and safety. Across diverse motion-control scenarios, our approach consistently surpasses strong state-of-the-art baselines in return, safety (collisions and falls), and sample efficiency, while converging faster at the same compute budget. These results suggest that SSD-Mamba2 provides a practical fusion backbone for scalable, foresightful, and efficient end-to-end motion control."
    },
    {
      "paperId": "11b5fe83873925fe7ecf709c450176456ee89dd6",
      "externalIds": {
        "ArXiv": "2509.07963",
        "DBLP": "journals/corr/abs-2509-07963",
        "DOI": "10.48550/arXiv.2509.07963",
        "CorpusId": 281217778
      },
      "corpusId": 281217778,
      "title": "Customizing the Inductive Biases of Softmax Attention using Structured Matrices",
      "venue": "International Conference on Machine Learning",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.07963, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2276608269",
          "name": "Yilun Kuang"
        },
        {
          "authorId": "81087552",
          "name": "Noah Amsel"
        },
        {
          "authorId": "2034179136",
          "name": "Sanae Lotfi"
        },
        {
          "authorId": "2257413018",
          "name": "Shikai Qiu"
        },
        {
          "authorId": "2220301651",
          "name": "Andres Potapczynski"
        },
        {
          "authorId": "2356857934",
          "name": "Andrew Gordon Wilson"
        }
      ],
      "abstract": "The core component of attention is the scoring function, which transforms the inputs into low-dimensional queries and keys and takes the dot product of each pair. While the low-dimensional projection improves efficiency, it causes information loss for certain tasks that have intrinsically high-dimensional inputs. Additionally, attention uses the same scoring function for all input pairs, without imposing a distance-dependent compute bias for neighboring tokens in the sequence. In this work, we address these shortcomings by proposing new scoring functions based on computationally efficient structured matrices with high ranks, including Block Tensor-Train (BTT) and Multi-Level Low Rank (MLR) matrices. On in-context regression tasks with high-dimensional inputs, our proposed scoring functions outperform standard attention for any fixed compute budget. On language modeling, a task that exhibits locality patterns, our MLR-based attention method achieves improved scaling laws compared to both standard attention and variants of sliding window attention. Additionally, we show that both BTT and MLR fall under a broader family of efficient structured matrices capable of encoding either full-rank or distance-dependent compute biases, thereby addressing significant shortcomings of standard attention. Finally, we show that MLR attention has promising results for long-range time-series forecasting."
    },
    {
      "paperId": "80e7f14590590d2511a3a8181b0683e70930ba82",
      "externalIds": {
        "ArXiv": "2509.09717",
        "DBLP": "journals/corr/abs-2509-09717",
        "DOI": "10.48550/arXiv.2509.09717",
        "CorpusId": 281309860
      },
      "corpusId": 281309860,
      "title": "Testing chatbots on the creation of encoders for audio conditioned image generation",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.09717, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2379665794",
          "name": "Jorge E. Le'on"
        },
        {
          "authorId": "2379665485",
          "name": "Miguel Carrasco"
        }
      ],
      "abstract": "On one hand, recent advances in chatbots has led to a rising popularity in using these models for coding tasks. On the other hand, modern generative image models primarily rely on text encoders to translate semantic concepts into visual representations, even when there is clear evidence that audio can be employed as input as well. Given the previous, in this work, we explore whether state-of-the-art conversational agents can design effective audio encoders to replace the CLIP text encoder from Stable Diffusion 1.5, enabling image synthesis directly from sound. We prompted five publicly available chatbots to propose neural architectures to work as these audio encoders, with a set of well-explained shared conditions. Each valid suggested encoder was trained on over two million context related audio-image-text observations, and evaluated on held-out validation and test sets using various metrics, together with a qualitative analysis of their generated images. Although almost all chatbots generated valid model designs, none achieved satisfactory results, indicating that their audio embeddings failed to align reliably with those of the original text encoder. Among the proposals, the Gemini audio encoder showed the best quantitative metrics, while the Grok audio encoder produced more coherent images (particularly, when paired with the text encoder). Our findings reveal a shared architectural bias across chatbots and underscore the remaining coding gap that needs to be bridged in future versions of these models. We also created a public demo so everyone could study and try out these audio encoders. Finally, we propose research questions that should be tackled in the future, and encourage other researchers to perform more focused and highly specialized tasks like this one, so the respective chatbots cannot make use of well-known solutions and their creativity/reasoning is fully tested."
    },
    {
      "paperId": "5223d3c3433cf6dbfac57fef628c6a823a040b03",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-06580",
        "ArXiv": "2509.06580",
        "DOI": "10.48550/arXiv.2509.06580",
        "CorpusId": 281203577
      },
      "corpusId": 281203577,
      "title": "AI for Scientific Discovery is a Social Problem",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.06580, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2325150953",
          "name": "Georgia Channing"
        },
        {
          "authorId": "2380457170",
          "name": "Avijit Ghosh"
        }
      ],
      "abstract": "Artificial intelligence promises to accelerate scientific discovery, yet its benefits remain unevenly distributed. While technical obstacles such as scarce data, fragmented standards, and unequal access to computation are significant, we argue that the primary barriers are social and institutional. Narratives that defer progress to speculative\"AI scientists,\"the undervaluing of data and infrastructure contributions, misaligned incentives, and gaps between domain experts and machine learning researchers all constrain impact. We highlight four interconnected challenges: community dysfunction, research priorities misaligned with upstream needs, data fragmentation, and infrastructure inequities. We argue that their roots lie in cultural and organizational practices. Addressing them requires not only technical innovation but also intentional community-building, cross-disciplinary education, shared benchmarks, and accessible infrastructure. We call for reframing AI for science as a collective social project, where sustainable collaboration and equitable participation are treated as prerequisites for technical progress."
    },
    {
      "paperId": "efbb90b6aa8924a6ca7406a45e43caa00a6b0637",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-06950",
        "ArXiv": "2509.06950",
        "DOI": "10.48550/arXiv.2509.06950",
        "CorpusId": 281203509
      },
      "corpusId": 281203509,
      "title": "Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.06950, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2132076159",
          "name": "Nithin Gopalakrishnan Nair"
        },
        {
          "authorId": "2332241925",
          "name": "Srinivas Kaza"
        },
        {
          "authorId": "2332307362",
          "name": "Xuan Luo"
        },
        {
          "authorId": "2282191115",
          "name": "Vishal M. Patel"
        },
        {
          "authorId": "2274101898",
          "name": "Stephen Lombardi"
        },
        {
          "authorId": "2379723620",
          "name": "Jungyeon Park"
        }
      ],
      "abstract": "Large transformer-based models have made significant progress in generalizable novel view synthesis (NVS) from sparse input views, generating novel viewpoints without the need for test-time optimization. However, these models are constrained by the limited diversity of publicly available scene datasets, making most real-world (in-the-wild) scenes out-of-distribution. To overcome this, we incorporate synthetic training data generated from diffusion models, which improves generalization across unseen domains. While synthetic data offers scalability, we identify artifacts introduced during data generation as a key bottleneck affecting reconstruction quality. To address this, we propose a token disentanglement process within the transformer architecture, enhancing feature separation and ensuring more effective learning. This refinement not only improves reconstruction quality over standard transformers but also enables scalable training with synthetic data. As a result, our method outperforms existing models on both in-dataset and cross-dataset evaluations, achieving state-of-the-art results across multiple benchmarks while significantly reducing computational costs. Project page: https://scaling3dnvs.github.io/"
    },
    {
      "paperId": "2794b32db72d81e7736a2211876cf7e82d1b26b9",
      "externalIds": {
        "ArXiv": "2509.07042",
        "DBLP": "journals/corr/abs-2509-07042",
        "DOI": "10.1007/978-3-032-05997-0_14",
        "CorpusId": 281217750
      },
      "corpusId": 281217750,
      "title": "PUUMA (Placental Patch and Whole-Uterus Dual-Branch U-Mamba-Based Architecture): Functional MRI Prediction of Gestational Age at Birth and Preterm Risk",
      "venue": "PIPPI@MICCAI",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.07042, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2284573728",
          "name": "D. Fajardo-Rojas"
        },
        {
          "authorId": "2285474186",
          "name": "Levente Baljer"
        },
        {
          "authorId": "2230346576",
          "name": "J. Verdera"
        },
        {
          "authorId": "2283546339",
          "name": "Megan Hall"
        },
        {
          "authorId": "2284573860",
          "name": "D. Cromb"
        },
        {
          "authorId": "2250223628",
          "name": "M. Rutherford"
        },
        {
          "authorId": "143976839",
          "name": "L. Story"
        },
        {
          "authorId": "2284574578",
          "name": "E. Robinson"
        },
        {
          "authorId": "2283529376",
          "name": "J. Hutter"
        }
      ],
      "abstract": "Preterm birth is a major cause of mortality and lifelong morbidity in childhood. Its complex and multifactorial origins limit the effectiveness of current clinical predictors and impede optimal care. In this study, a dual-branch deep learning architecture (PUUMA) was developed to predict gestational age (GA) at birth using T2* fetal MRI data from 295 pregnancies, encompassing a heterogeneous and imbalanced population. The model integrates both global whole-uterus and local placental features. Its performance was benchmarked against linear regression using cervical length measurements obtained by experienced clinicians from anatomical MRI and other Deep Learning architectures. The GA at birth predictions were assessed using mean absolute error. Accuracy, sensitivity, and specificity were used to assess preterm classification. Both the fully automated MRI-based pipeline and the cervical length regression achieved comparable mean absolute errors (3 weeks) and good sensitivity (0.67) for detecting preterm birth, despite pronounced class imbalance in the dataset. These results provide a proof of concept for automated prediction of GA at birth from functional MRI, and underscore the value of whole-uterus functional imaging in identifying at-risk pregnancies. Additionally, we demonstrate that manual, high-definition cervical length measurements derived from MRI, not currently routine in clinical practice, offer valuable predictive information. Future work will focus on expanding the cohort size and incorporating additional organ-specific imaging to improve generalisability and predictive performance."
    },
    {
      "paperId": "4d089b4a7f8f00c0aab74b97c0e6cec1803c1397",
      "externalIds": {
        "DOI": "10.1109/CogInfoCom66819.2025.11200625",
        "CorpusId": 282148401
      },
      "corpusId": 282148401,
      "title": "Kernel-Based and Learning-Based Interior-Point Method for Linear Complementarity Problems",
      "venue": "IEEE International Conference on Cognitive Infocommunications",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CogInfoCom66819.2025.11200625?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CogInfoCom66819.2025.11200625, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2386134028",
          "name": "Goran Lesaja"
        },
        {
          "authorId": "2386133183",
          "name": "Srdjan Lesaja"
        }
      ],
      "abstract": "This work introduces a theoretical framework of kernel-based and learning-based interior-point methods (IPMs) for $P_{*}(\\kappa)$-linear complementarity problems (LCPs). We begin by presenting the traditional kernel-based IPM for LCP. Next, we integrate L2O option that uses Long Short-Term Memory (LSTM) neural networks to approximate linear system solutions at each iteration of the original algorithm. The proposed IPM-LSTM algorithmic framework is an alternative to previously developed approaches of inexact and iterative IPMs. The proposed methodology may also be computationally beneficial, which will be the topic of forthcoming research."
    },
    {
      "paperId": "20bbdf02f1101428d41c21c20464d55bf9ee2f26",
      "externalIds": {
        "DOI": "10.1117/12.3076562",
        "CorpusId": 281239736
      },
      "corpusId": 281239736,
      "title": "3D object detection method based on selected state space",
      "venue": "International Conference on Signal Image Processing and Communication (ICSIPC 2021)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1117/12.3076562?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1117/12.3076562, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2325888437",
          "name": "Hailong Yu"
        },
        {
          "authorId": "2325918080",
          "name": "Xin Shi"
        }
      ],
      "abstract": "3D object detection is critical for autonomous driving perception, directly influencing the reliability of downstream tasks. A multi-sensor dual-stream BEV fusion algorithm based on selective state spaces is proposed to address low efficiency in multi-modal feature fusion and high computational costs in existing methods. The framework independently processes camera and LiDAR features, optimizes camera view feature extraction with a dynamic sparse scanning mechanism, and introduces a frequency-adaptive multi-modal fusion module to enhance feature alignment accuracy. The camera stream employs a selective state space-based Vmamba backbone, reducing memory and time overhead through lightweight attention-guided dynamic scanning. The LiDAR stream improves small object detection via optimized voxel encoding and sparse convolution. The fusion module treats multi-modal BEV features as distinct frequency signals, leveraging spectral weighting to preserve spatial details. Experiments on the nuScenes dataset demonstrate the proposed method outperforms baseline models by 1.7% and 2.1% in key metrics, with the dynamic sparse scanning mechanism reducing training time and memory consumption by approximately 10% and 30%, achieving a balance between accuracy and efficiency."
    },
    {
      "paperId": "7415e191f774e8416b41be8fed441b47488d1902",
      "externalIds": {
        "DOI": "10.3390/app15179805",
        "CorpusId": 281278131
      },
      "corpusId": 281278131,
      "title": "Modulation Recognition Algorithm for Long-Sequence, High-Order Modulated Signals Based on Mamba Architecture",
      "venue": "Applied Sciences",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/app15179805?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/app15179805, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "3048940",
          "name": "Enguo Zhu"
        },
        {
          "authorId": "2377280496",
          "name": "Ran Li"
        },
        {
          "authorId": "2115243314",
          "name": "Yi Ren"
        },
        {
          "authorId": "51050321",
          "name": "Jizhe Lu"
        },
        {
          "authorId": "2386792010",
          "name": "Lu Tang"
        },
        {
          "authorId": "153652817",
          "name": "Tiancong Huang"
        }
      ],
      "abstract": "This paper investigates modulation recognition technology for high-order modulated signals. Addressing the issue that existing deep learning-based modulation recognition methods struggle to effectively capture the features of long sequence signals in high-order modulation, we propose a ConvMamba model that integrates convolutional neural networks (CNNs) with the Mamba2 architecture. By employing a selective state-space model, the ConvMamba effectively captures the temporal dependencies in long sequence signals. It also combines the local feature extraction capability of CNNs with a soft-thresholding denoising module, forming a hybrid structure that possesses both global modeling and noise resistance capabilities. The evaluation results on the Sig53 dataset, which contains a rich variety of high-order modulations, demonstrate that compared to traditional CNN- or Transformer-based architectures, ConvMamba achieves a better balance between computational efficiency and recognition accuracy. Compared to Transformer models with similar performance, ConvMamba reduces computational complexity by over 60%. Compared to CNN models with comparable computational resource consumption, ConvMamba significantly improves recognition accuracy. Therefore, ConvMamba shows a distinct advantage in processing high-order modulated signals with long sequences."
    },
    {
      "paperId": "837bfdded23da8b4b35a62d56575dd3dd6df090b",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-05786",
        "ArXiv": "2509.05786",
        "DOI": "10.48550/arXiv.2509.05786",
        "CorpusId": 281203999
      },
      "corpusId": 281203999,
      "title": "Effectively obtaining acoustic, visual and textual data from videos",
      "venue": "Applied Sciences",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.05786, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2379665794",
          "name": "Jorge E. Le'on"
        },
        {
          "authorId": "2379665485",
          "name": "Miguel Carrasco"
        }
      ],
      "abstract": "The increasing use of machine learning models has amplified the demand for high-quality, large-scale multimodal datasets. However, the availability of such datasets, especially those combining acoustic, visual, and textual data, remains limited. This paper addresses this gap by proposing a method of extracting related audio\u2013image\u2013text observations from videos. We detail the process of selecting suitable videos, extracting relevant data pairs, and generating descriptive texts using image-to-text models. Our approach ensures a robust semantic connection between modalities, enhancing the utility of the created datasets for various applications. We also explore the obtained data, discuss the challenges encountered, and propose solutions to improve data quality. The resulting datasets, which are publicly available, aim to support and advance research in multimodal data analysis and machine learning."
    },
    {
      "paperId": "e0f93529f1bd750de157cecf3f33f5870468d838",
      "externalIds": {
        "ArXiv": "2509.05550",
        "DBLP": "journals/corr/abs-2509-05550",
        "DOI": "10.48550/arXiv.2509.05550",
        "CorpusId": 281204198
      },
      "corpusId": 281204198,
      "title": "TreeGPT: Pure TreeFFN Encoder-Decoder Architecture for Structured Reasoning Without Attention Mechanisms",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 1,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.05550, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2380063296",
          "name": "Zixi Li"
        }
      ],
      "abstract": "We present TreeGPT, an attention-free neural architecture that explores the potential of pure TreeFFN encoder-decoder design for structured reasoning tasks. Unlike traditional transformer approaches that rely on attention mechanisms, TreeGPT employs bidirectional TreeFFN components that process sequences through adjacent connections in parallel, aiming to achieve computational efficiency while maintaining reasoning capabilities. Our approach centers on a TreeFFN Encoder-Decoder mechanism: $$\\text{Encoder TreeFFN (L} \\rightarrow \\text{R)} + \\text{Decoder TreeFFN (R} \\leftarrow \\text{L)} \\rightarrow \\text{Parallel Processing}$$ where the encoder processes left-to-right dependencies while the decoder handles right-to-left patterns, both using simple neighbor-to-neighbor connections. This design eliminates attention computation while maintaining sequence modeling capabilities. We evaluate our approach on the ARC Prize 2025 dataset, where TreeGPT achieves 99\\% validation accuracy using 3.16M parameters. The model converges within 1500 training steps and demonstrates 100\\% token-level accuracy on selected evaluation samples. Our preliminary results suggest that for certain structured reasoning tasks, specialized TreeFFN architectures may offer advantages over attention-based approaches. While these findings are encouraging, we acknowledge that further investigation across diverse tasks and datasets would be valuable to establish the broader applicability of attention-free designs."
    },
    {
      "paperId": "24a7dc0295051d4444348f81662e56c42968339a",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-05757",
        "ArXiv": "2509.05757",
        "DOI": "10.48550/arXiv.2509.05757",
        "CorpusId": 281202976
      },
      "corpusId": 281202976,
      "title": "Hyperbolic Large Language Models",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.05757, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2336555917",
          "name": "Sarang Patil"
        },
        {
          "authorId": "2379700659",
          "name": "Zeyong Zhang"
        },
        {
          "authorId": "2375403283",
          "name": "Yiran Huang"
        },
        {
          "authorId": "2385490994",
          "name": "Tengfei Ma"
        },
        {
          "authorId": "2336151081",
          "name": "Mengjia Xu"
        }
      ],
      "abstract": "Large language models (LLMs) have achieved remarkable success and demonstrated superior performance across various tasks, including natural language processing (NLP), weather forecasting, biological protein folding, text generation, and solving mathematical problems. However, many real-world data exhibit highly non-Euclidean latent hierarchical anatomy, such as protein networks, transportation networks, financial networks, brain networks, and linguistic structures or syntactic trees in natural languages. Effectively learning intrinsic semantic entailment and hierarchical relationships from these raw, unstructured input data using LLMs remains an underexplored area. Due to its effectiveness in modeling tree-like hierarchical structures, hyperbolic geometry -- a non-Euclidean space -- has rapidly gained popularity as an expressive latent representation space for complex data modeling across domains such as graphs, images, languages, and multi-modal data. Here, we provide a comprehensive and contextual exposition of recent advancements in LLMs that leverage hyperbolic geometry as a representation space to enhance semantic representation learning and multi-scale reasoning. Specifically, the paper presents a taxonomy of the principal techniques of Hyperbolic LLMs (HypLLMs) in terms of four main categories: (1) hyperbolic LLMs through exp/log maps; (2) hyperbolic fine-tuned models; (3) fully hyperbolic LLMs, and (4) hyperbolic state-space models. We also explore crucial potential applications and outline future research directions. A repository of key papers, models, datasets, and code implementations is available at https://github.com/sarangp2402/Hyperbolic-LLM-Models."
    },
    {
      "paperId": "6aca981316bee38e26f840262326af0e825fa75f",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-04824",
        "ArXiv": "2509.04824",
        "DOI": "10.48550/arXiv.2509.04824",
        "CorpusId": 281195071
      },
      "corpusId": 281195071,
      "title": "Exploring Non-Local Spatial-Angular Correlations with a Hybrid Mamba-Transformer Framework for Light Field Super-Resolution",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.04824, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2374273027",
          "name": "Haosong Liu"
        },
        {
          "authorId": "2372157293",
          "name": "Xiancheng Zhu"
        },
        {
          "authorId": "72910910",
          "name": "Huanqiang Zeng"
        },
        {
          "authorId": "1739258",
          "name": "Jianqing Zhu"
        },
        {
          "authorId": "2377536210",
          "name": "Jiuwen Cao"
        },
        {
          "authorId": "2374194686",
          "name": "Junhui Hou"
        }
      ],
      "abstract": "Recently, Mamba-based methods, with its advantage in long-range information modeling and linear complexity, have shown great potential in optimizing both computational cost and performance of light field image super-resolution (LFSR). However, current multi-directional scanning strategies lead to inefficient and redundant feature extraction when applied to complex LF data. To overcome this challenge, we propose a Subspace Simple Scanning (Sub-SS) strategy, based on which we design the Subspace Simple Mamba Block (SSMB) to achieve more efficient and precise feature extraction. Furthermore, we propose a dual-stage modeling strategy to address the limitation of state space in preserving spatial-angular and disparity information, thereby enabling a more comprehensive exploration of non-local spatial-angular correlations. Specifically, in stage I, we introduce the Spatial-Angular Residual Subspace Mamba Block (SA-RSMB) for shallow spatial-angular feature extraction; in stage II, we use a dual-branch parallel structure combining the Epipolar Plane Mamba Block (EPMB) and Epipolar Plane Transformer Block (EPTB) for deep epipolar feature refinement. Building upon meticulously designed modules and strategies, we introduce a hybrid Mamba-Transformer framework, termed LFMT. LFMT integrates the strengths of Mamba and Transformer models for LFSR, enabling comprehensive information exploration across spatial, angular, and epipolar-plane domains. Experimental results demonstrate that LFMT significantly outperforms current state-of-the-art methods in LFSR, achieving substantial improvements in performance while maintaining low computational complexity on both real-word and synthetic LF datasets."
    },
    {
      "paperId": "b5fb8b48d18bda80fc7a68d9cdfdfb8252db61a3",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-05282",
        "ArXiv": "2509.05282",
        "DOI": "10.48550/arXiv.2509.05282",
        "CorpusId": 281195018
      },
      "corpusId": 281195018,
      "title": "Elucidating the Design Space of Decay in Linear Attention",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.05282, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2171650015",
          "name": "Zhen Qin"
        },
        {
          "authorId": "2116517206",
          "name": "Xuyang Shen"
        },
        {
          "authorId": "2266275708",
          "name": "Yiran Zhong"
        }
      ],
      "abstract": "This paper presents a comprehensive investigation into the decay mechanisms inherent in linear complexity sequence models. We systematically delineate the design space of decay mechanisms across four pivotal dimensions: parameterization strategy, which refers to the computational methodology for decay; parameter sharing, which involves the utilization of supplementary parameters for decay computation; decay granularity, comparing scalar versus vector-based decay; and compatibility with relative positional encoding methods, such as Rotary Position Embedding (RoPE). Through an extensive series of experiments conducted on diverse language modeling tasks, we uncovered several critical insights. Firstly, the design of the parameterization strategy for decay requires meticulous consideration. Our findings indicate that effective configurations are typically confined to a specific range of parameters. Secondly, parameter sharing cannot be used arbitrarily, as it may cause decay values to be too large or too small, thereby significantly impacting performance. Thirdly, under identical parameterization strategies, scalar decay generally underperforms compared to its vector-based counterpart. However, in certain scenarios with alternative parameterization strategies, scalar decay may unexpectedly surpass vector decay in efficacy. Lastly, our analysis reveals that RoPE, a commonly employed relative positional encoding method, typically fails to provide tangible benefits to the majority of linear attention mechanisms."
    },
    {
      "paperId": "942752924e5743d6557cc9a53c2a05fb11e3b50b",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-04897",
        "ArXiv": "2509.04897",
        "DOI": "10.48550/arXiv.2509.04897",
        "CorpusId": 281195328
      },
      "corpusId": 281195328,
      "title": "PLaMo 2 Technical Report",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.04897, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2379548780",
          "name": "Preferred Networks Kaizaburo Chubachi"
        },
        {
          "authorId": "2325151903",
          "name": "Yasuhiro Fujita"
        },
        {
          "authorId": "2379546075",
          "name": "Shinichi Hemmi"
        },
        {
          "authorId": "2325160806",
          "name": "Yuta Hirokawa"
        },
        {
          "authorId": "34300210",
          "name": "Kentaro Imajo"
        },
        {
          "authorId": "2056971870",
          "name": "Toshiki Kataoka"
        },
        {
          "authorId": "2379545963",
          "name": "Goro Kobayashi"
        },
        {
          "authorId": "2379547171",
          "name": "Kenichi Maehashi"
        },
        {
          "authorId": "2379547178",
          "name": "Calvin Metzger"
        },
        {
          "authorId": "2325156665",
          "name": "Hiroaki Mikami"
        },
        {
          "authorId": "32012282",
          "name": "Shogo Murai"
        },
        {
          "authorId": "2325152777",
          "name": "Daisuke Nishino"
        },
        {
          "authorId": "2379545960",
          "name": "Kento Nozawa"
        },
        {
          "authorId": "2325152905",
          "name": "Toru Ogawa"
        },
        {
          "authorId": "2379549250",
          "name": "Shintarou Okada"
        },
        {
          "authorId": "2325154871",
          "name": "Daisuke Okanohara"
        },
        {
          "authorId": "2379627214",
          "name": "Shunta Saito"
        },
        {
          "authorId": "2019759",
          "name": "Shotaro Sano"
        },
        {
          "authorId": "2346568554",
          "name": "Shuji Suzuki"
        },
        {
          "authorId": "2293777225",
          "name": "Kuniyuki Takahashi"
        },
        {
          "authorId": "2379547156",
          "name": "Daisuke Tanaka"
        },
        {
          "authorId": "118164308",
          "name": "Avinash Ummadisingu"
        },
        {
          "authorId": "2379949868",
          "name": "Hanqin Wang"
        },
        {
          "authorId": "2381111421",
          "name": "Sixue Wang"
        },
        {
          "authorId": "2325705757",
          "name": "Tianqi Xu"
        }
      ],
      "abstract": "In this report, we introduce PLaMo 2, a series of Japanese-focused large language models featuring a hybrid Samba-based architecture that transitions to full attention via continual pre-training to support 32K token contexts. Training leverages extensive synthetic corpora to overcome data scarcity, while computational efficiency is achieved through weight reuse and structured pruning. This efficient pruning methodology produces an 8B model that achieves performance comparable to our previous 100B model. Post-training further refines the models using a pipeline of supervised fine-tuning (SFT) and direct preference optimization (DPO), enhanced by synthetic Japanese instruction data and model merging techniques. Optimized for inference using vLLM and quantization with minimal accuracy loss, the PLaMo 2 models achieve state-of-the-art results on Japanese benchmarks, outperforming similarly-sized open models in instruction-following, language fluency, and Japanese-specific knowledge."
    },
    {
      "paperId": "2bb47d7a9cafeff591b19840d429b376ed8c9921",
      "externalIds": {
        "ArXiv": "2509.04903",
        "DBLP": "journals/corr/abs-2509-04903",
        "DOI": "10.48550/arXiv.2509.04903",
        "CorpusId": 281194749
      },
      "corpusId": 281194749,
      "title": "ACE-RL: Adaptive Constraint-Enhanced Reward for Long-form Generation Reinforcement Learning",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 4,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.04903, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2264139789",
          "name": "Jianghao Chen"
        },
        {
          "authorId": "2348617595",
          "name": "Wei Sun"
        },
        {
          "authorId": "2323575101",
          "name": "Qixiang Yin"
        },
        {
          "authorId": "2379614623",
          "name": "Lingxing Kong"
        },
        {
          "authorId": "2380033806",
          "name": "Zhixing Tan"
        },
        {
          "authorId": "2347171134",
          "name": "Jiajun Zhang"
        }
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable progress in long-context understanding, yet they face significant challenges in high-quality long-form generation. Existing studies primarily suffer from two limitations: (1) A heavy reliance on scarce, high-quality long-form response data for supervised fine-tuning (SFT) or for pairwise preference reward in reinforcement learning (RL). (2) Focus on coarse-grained quality optimization dimensions, such as relevance, coherence, and helpfulness, overlooking the fine-grained specifics inherent to diverse long-form generation scenarios. To address this issue, we propose a framework using Adaptive Constraint-Enhanced reward for long-form generation Reinforcement Learning (ACE-RL). ACE-RL first automatically deconstructs each instruction into a set of fine-grained, adaptive constraint criteria by identifying its underlying intents and demands. Subsequently, we design a reward mechanism that quantifies the quality of long-form responses based on their satisfaction over corresponding constraints, converting subjective quality evaluation into constraint verification. Finally, we utilize reinforcement learning to guide models toward superior long-form generation capabilities. Experimental results demonstrate that our ACE-RL framework significantly outperforms existing SFT and RL baselines by 20.70% and 7.32% on WritingBench, and our top-performing model even surpasses proprietary systems like GPT-4o by 7.10%, providing a more effective training paradigm for LLMs to generate high-quality content across diverse long-form generation scenarios."
    },
    {
      "paperId": "29ea44d56b8394385b19e4a35c9b3e1fe15391d7",
      "externalIds": {
        "ArXiv": "2509.05488",
        "DBLP": "journals/corr/abs-2509-05488",
        "DOI": "10.48550/arXiv.2509.05488",
        "CorpusId": 281204232
      },
      "corpusId": 281204232,
      "title": "MambaLite-Micro: Memory-Optimized Mamba Inference on MCUs",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.05488, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2382045680",
          "name": "Hongjun Xu"
        },
        {
          "authorId": "2292245016",
          "name": "Junxi Xia"
        },
        {
          "authorId": "2344202741",
          "name": "Weisi Yang"
        },
        {
          "authorId": "2299326272",
          "name": "Yueyuan Sui"
        },
        {
          "authorId": "2332091419",
          "name": "Stephen Xia"
        }
      ],
      "abstract": "Deploying Mamba models on microcontrollers (MCUs) remains challenging due to limited memory, the lack of native operator support, and the absence of embedded-friendly toolchains. We present, to our knowledge, the first deployment of a Mamba-based neural architecture on a resource-constrained MCU, a fully C-based runtime-free inference engine: MambaLite-Micro. Our pipeline maps a trained PyTorch Mamba model to on-device execution by (1) exporting model weights into a lightweight format, and (2) implementing a handcrafted Mamba layer and supporting operators in C with operator fusion and memory layout optimization. MambaLite-Micro eliminates large intermediate tensors, reducing 83.0% peak memory, while maintaining an average numerical error of only 1.7x10-5 relative to the PyTorch Mamba implementation. When evaluated on keyword spotting(KWS) and human activity recognition (HAR) tasks, MambaLite-Micro achieved 100% consistency with the PyTorch baselines, fully preserving classification accuracy. We further validated portability by deploying on both ESP32S3 and STM32H7 microcontrollers, demonstrating consistent operation across heterogeneous embedded platforms and paving the way for bringing advanced sequence models like Mamba to real-world resource-constrained applications."
    },
    {
      "paperId": "8bd15f9f63530b59944beefb88ad84902f425055",
      "externalIds": {
        "DBLP": "journals/vc/LiuL25",
        "DOI": "10.1007/s00371-025-04162-7",
        "CorpusId": 281184591
      },
      "corpusId": 281184591,
      "title": "CMCNet: enhancing face image super-resolution through CNN-Mamba collaboration",
      "venue": "The Visual Computer",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s00371-025-04162-7?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s00371-025-04162-7, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2379681651",
          "name": "Zhaoci Liu"
        },
        {
          "authorId": "2379897645",
          "name": "Huayong Liu"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "60e274a3c5c0a1a09cd76eaebdaece8565f156c3",
      "externalIds": {
        "ArXiv": "2509.04669",
        "DBLP": "journals/corr/abs-2509-04669",
        "DOI": "10.48550/arXiv.2509.04669",
        "CorpusId": 281194891
      },
      "corpusId": 281194891,
      "title": "VCMamba: Bridging Convolutions with Multi-Directional Mamba for Efficient Visual Representation",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.04669, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2301155029",
          "name": "Mustafa Munir"
        },
        {
          "authorId": "2372104539",
          "name": "Alex Zhang"
        },
        {
          "authorId": "1702532",
          "name": "R. Marculescu"
        }
      ],
      "abstract": "Recent advances in Vision Transformers (ViTs) and State Space Models (SSMs) have challenged the dominance of Convolutional Neural Networks (CNNs) in computer vision. ViTs excel at capturing global context, and SSMs like Mamba offer linear complexity for long sequences, yet they do not capture fine-grained local features as effectively as CNNs. Conversely, CNNs possess strong inductive biases for local features but lack the global reasoning capabilities of transformers and Mamba. To bridge this gap, we introduce \\textit{VCMamba}, a novel vision backbone that integrates the strengths of CNNs and multi-directional Mamba SSMs. VCMamba employs a convolutional stem and a hierarchical structure with convolutional blocks in its early stages to extract rich local features. These convolutional blocks are then processed by later stages incorporating multi-directional Mamba blocks designed to efficiently model long-range dependencies and global context. This hybrid design allows for superior feature representation while maintaining linear complexity with respect to image resolution. We demonstrate VCMamba's effectiveness through extensive experiments on ImageNet-1K classification and ADE20K semantic segmentation. Our VCMamba-B achieves 82.6% top-1 accuracy on ImageNet-1K, surpassing PlainMamba-L3 by 0.3% with 37% fewer parameters, and outperforming Vision GNN-B by 0.3% with 64% fewer parameters. Furthermore, VCMamba-B obtains 47.1 mIoU on ADE20K, exceeding EfficientFormer-L7 by 2.0 mIoU while utilizing 62% fewer parameters. Code is available at https://github.com/Wertyuui345/VCMamba."
    },
    {
      "paperId": "94c08e981e49b6e9f5f27bc77328be6a71e29903",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-04422",
        "ArXiv": "2509.04422",
        "DOI": "10.48550/arXiv.2509.04422",
        "CorpusId": 281103242
      },
      "corpusId": 281103242,
      "title": "Echo State Networks as State-Space Models: A Systems Perspective",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.04422, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2218146908",
          "name": "Pradeep Singh"
        },
        {
          "authorId": "2283013123",
          "name": "Balasubramanian Raman"
        }
      ],
      "abstract": "Echo State Networks (ESNs) are typically presented as efficient, readout-trained recurrent models, yet their dynamics and design are often guided by heuristics rather than first principles. We recast ESNs explicitly as state-space models (SSMs), providing a unified systems-theoretic account that links reservoir computing with classical identification and modern kernelized SSMs. First, we show that the echo-state property is an instance of input-to-state stability for a contractive nonlinear SSM and derive verifiable conditions in terms of leak, spectral scaling, and activation Lipschitz constants. Second, we develop two complementary mappings: (i) small-signal linearizations that yield locally valid LTI SSMs with interpretable poles and memory horizons; and (ii) lifted/Koopman random-feature expansions that render the ESN a linear SSM in an augmented state, enabling transfer-function and convolutional-kernel analyses. This perspective yields frequency-domain characterizations of memory spectra and clarifies when ESNs emulate structured SSM kernels. Third, we cast teacher forcing as state estimation and propose Kalman/EKF-assisted readout learning, together with EM for hyperparameters (leak, spectral radius, process/measurement noise) and a hybrid subspace procedure for spectral shaping under contraction constraints."
    },
    {
      "paperId": "0b0bb2eec0ad6e66b8022ff2e0cd01cdd485761a",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-03872",
        "ArXiv": "2509.03872",
        "DOI": "10.48550/arXiv.2509.03872",
        "CorpusId": 281103786
      },
      "corpusId": 281103786,
      "title": "Focus Through Motion: RGB-Event Collaborative Token Sparsification for Efficient Object Detection",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.03872, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2241538849",
          "name": "Nan Yang"
        },
        {
          "authorId": "2256978554",
          "name": "Yang Wang"
        },
        {
          "authorId": "2242975868",
          "name": "Zhanwen Liu"
        },
        {
          "authorId": "2358778277",
          "name": "Yuchao Dai"
        },
        {
          "authorId": "2379061903",
          "name": "Yang Liu"
        },
        {
          "authorId": "2263881345",
          "name": "Xiangmo Zhao"
        }
      ],
      "abstract": "Existing RGB-Event detection methods process the low-information regions of both modalities (background in images and non-event regions in event data) uniformly during feature extraction and fusion, resulting in high computational costs and suboptimal performance. To mitigate the computational redundancy during feature extraction, researchers have respectively proposed token sparsification methods for the image and event modalities. However, these methods employ a fixed number or threshold for token selection, hindering the retention of informative tokens for samples with varying complexity. To achieve a better balance between accuracy and efficiency, we propose FocusMamba, which performs adaptive collaborative sparsification of multimodal features and efficiently integrates complementary information. Specifically, an Event-Guided Multimodal Sparsification (EGMS) strategy is designed to identify and adaptively discard low-information regions within each modality by leveraging scene content changes perceived by the event camera. Based on the sparsification results, a Cross-Modality Focus Fusion (CMFF) module is proposed to effectively capture and integrate complementary features from both modalities. Experiments on the DSEC-Det and PKU-DAVIS-SOD datasets demonstrate that the proposed method achieves superior performance in both accuracy and efficiency compared to existing methods. The code will be available at https://github.com/Zizzzzzzz/FocusMamba."
    },
    {
      "paperId": "cf5c7133ce5d18afb4b5e4a166335b914f7f6ed4",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-04226",
        "ArXiv": "2509.04226",
        "DOI": "10.48550/arXiv.2509.04226",
        "CorpusId": 281103537
      },
      "corpusId": 281103537,
      "title": "Rethinking the long-range dependency in Mamba/SSM and transformer models",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.04226, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2376521395",
          "name": "Cong Ma"
        },
        {
          "authorId": "1713273",
          "name": "K. Najarian"
        }
      ],
      "abstract": "Long-range dependency is one of the most desired properties of recent sequence models such as state-space models (particularly Mamba) and transformer models. New model architectures are being actively developed and benchmarked for prediction tasks requiring long-range dependency. However, the capability of modeling long-range dependencies of these models has not been investigated from a theoretical perspective, which hinders a systematic improvement on this aspect. In this work, we mathematically define long-range dependency using the derivative of hidden states with respect to past inputs and compare the capability of SSM and transformer models of modeling long-range dependency based on this definition. We showed that the long-range dependency of SSM decays exponentially with the sequence length, which aligns with the exponential decay of memory function in RNN. But the attention mechanism used in transformers is more flexible and is not constrained to exponential decay, which could in theory perform better at modeling long-range dependency with sufficient training data, computing resources, and proper training. To combine the flexibility of long-range dependency of attention mechanism and computation efficiency of SSM, we propose a new formulation for hidden state update in SSM and prove its stability under a standard Gaussian distribution of the input data."
    },
    {
      "paperId": "e5d165df2617d09753d5b3bea0ca058a596f61cf",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-03973",
        "ArXiv": "2509.03973",
        "DOI": "10.48550/arXiv.2509.03973",
        "CorpusId": 281103113
      },
      "corpusId": 281103113,
      "title": "SAC-MIL: Spatial-Aware Correlated Multiple Instance Learning for Histopathology Whole Slide Image Classification",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.03973, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2262095105",
          "name": "Yu Bai"
        },
        {
          "authorId": "2379173814",
          "name": "Zitong Yu"
        },
        {
          "authorId": "2380637755",
          "name": "Haowen Tian"
        },
        {
          "authorId": "2378992716",
          "name": "Xijing Wang"
        },
        {
          "authorId": "2281169930",
          "name": "Shuo Yan"
        },
        {
          "authorId": "2379500995",
          "name": "Lin Wang"
        },
        {
          "authorId": "2379008148",
          "name": "Honglin Li"
        },
        {
          "authorId": "2378953102",
          "name": "Xitong Ling"
        },
        {
          "authorId": "2156622606",
          "name": "Bo Zhang"
        },
        {
          "authorId": "2261779707",
          "name": "Zheng Zhang"
        },
        {
          "authorId": "2374973626",
          "name": "Wufan Wang"
        },
        {
          "authorId": "2261889114",
          "name": "Hui Gao"
        },
        {
          "authorId": "8122021",
          "name": "Xiangyang Gong"
        },
        {
          "authorId": "2280850726",
          "name": "Wendong Wang"
        }
      ],
      "abstract": "We propose Spatial-Aware Correlated Multiple Instance Learning (SAC-MIL) for performing WSI classification. SAC-MIL consists of a positional encoding module to encode position information and a SAC block to perform full instance correlations. The positional encoding module utilizes the instance coordinates within the slide to encode the spatial relationships instead of the instance index in the input WSI sequence. The positional encoding module can also handle the length extrapolation issue where the training and testing sequences have different lengths. The SAC block is an MLP-based method that performs full instance correlation in linear time complexity with respect to the sequence length. Due to the simple structure of MLP, it is easy to deploy since it does not require custom CUDA kernels, compared to Transformer-based methods for WSI classification. SAC-MIL has achieved state-of-the-art performance on the CAMELYON-16, TCGA-LUNG, and TCGA-BRAC datasets. The code will be released upon acceptance."
    },
    {
      "paperId": "ce27d7e406102ad361b4652aa61c594726eadfb7",
      "externalIds": {
        "DBLP": "journals/cluster/ChenYHSDW25",
        "DOI": "10.1007/s10586-025-05344-7",
        "CorpusId": 281114800
      },
      "corpusId": 281114800,
      "title": "CMFuse: a hierarchical feature fusion model combining convolution and Mamba for medical image classification",
      "venue": "Cluster Computing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10586-025-05344-7?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10586-025-05344-7, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2379014982",
          "name": "Xu Chen"
        },
        {
          "authorId": "2295620066",
          "name": "Xuesong Yin"
        },
        {
          "authorId": "2111041597",
          "name": "Qi Huang"
        },
        {
          "authorId": "2295234710",
          "name": "Ting Shu"
        },
        {
          "authorId": "2036378",
          "name": "Jianhao Ding"
        },
        {
          "authorId": "2301740916",
          "name": "Yigang Wang"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "16a776a11c9a8f8dd39b8909a69e0d7ce0da44cf",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-03409",
        "ArXiv": "2509.03409",
        "DOI": "10.1145/3746027.3754568",
        "CorpusId": 281091741
      },
      "corpusId": 281091741,
      "title": "Multi-level SSL Feature Gating for Audio Deepfake Detection",
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.03409, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2319450659",
          "name": "Hoan My Tran"
        },
        {
          "authorId": "2255493375",
          "name": "Damien Lolive"
        },
        {
          "authorId": "2915629",
          "name": "Aghilas Sini"
        },
        {
          "authorId": "2319368615",
          "name": "Arnaud Delhay"
        },
        {
          "authorId": "1794352",
          "name": "P. Marteau"
        },
        {
          "authorId": "1830176",
          "name": "David Guennec"
        }
      ],
      "abstract": "Recent advancements in generative AI, particularly in speech synthesis, have enabled the generation of highly natural-sounding synthetic speech that closely mimics human voices. While these innovations hold promise for applications like assistive technologies, they also pose significant risks, including misuse for fraudulent activities, identity theft, and security threats. Current research on spoofing detection countermeasures remains limited by generalization to unseen deepfake attacks and languages. To address this, we propose a gating mechanism extracting relevant feature from the speech foundation XLS-R model as a front-end feature extractor. For downstream back-end classifier, we employ Multi-kernel gated Convolution (MultiConv) to capture both local and global speech artifacts. Additionally, we introduce Centered Kernel Alignment (CKA) as a similarity metric to enforce diversity in learned features across different MultiConv layers. By integrating CKA with our gating mechanism, we hypothesize that each component helps improving the learning of distinct synthetic speech patterns. Experimental results demonstrate that our approach achieves state-of-the-art performance on in-domain benchmarks while generalizing robustly to out-of-domain datasets, including multilingual speech samples. This underscores its potential as a versatile solution for detecting evolving speech deepfake threats."
    },
    {
      "paperId": "d1213aeb4ab310458a649b02e477233d85442878",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-03214",
        "ArXiv": "2509.03214",
        "DOI": "10.48550/arXiv.2509.03214",
        "CorpusId": 281091785
      },
      "corpusId": 281091785,
      "title": "RTGMFF: Enhanced fMRI-based Brain Disorder Diagnosis via ROI-driven Text Generation and Multimodal Feature Fusion",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.03214, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2375304030",
          "name": "Junhao Jia"
        },
        {
          "authorId": "2257321428",
          "name": "Yifei Sun"
        },
        {
          "authorId": "2379309896",
          "name": "Yunyou Liu"
        },
        {
          "authorId": "2379807103",
          "name": "Cheng Yang"
        },
        {
          "authorId": "2266432277",
          "name": "Changmiao Wang"
        },
        {
          "authorId": "2257056735",
          "name": "Feiwei Qin"
        },
        {
          "authorId": "2277508344",
          "name": "Yong Peng"
        },
        {
          "authorId": "2268398671",
          "name": "Wenwen Min"
        }
      ],
      "abstract": "Functional magnetic resonance imaging (fMRI) is a powerful tool for probing brain function, yet reliable clinical diagnosis is hampered by low signal-to-noise ratios, inter-subject variability, and the limited frequency awareness of prevailing CNN- and Transformer-based models. Moreover, most fMRI datasets lack textual annotations that could contextualize regional activation and connectivity patterns. We introduce RTGMFF, a framework that unifies automatic ROI-level text generation with multimodal feature fusion for brain-disorder diagnosis. RTGMFF consists of three components: (i) ROI-driven fMRI text generation deterministically condenses each subject's activation, connectivity, age, and sex into reproducible text tokens; (ii) Hybrid frequency-spatial encoder fuses a hierarchical wavelet-mamba branch with a cross-scale Transformer encoder to capture frequency-domain structure alongside long-range spatial dependencies; and (iii) Adaptive semantic alignment module embeds the ROI token sequence and visual features in a shared space, using a regularized cosine-similarity loss to narrow the modality gap. Extensive experiments on the ADHD-200 and ABIDE benchmarks show that RTGMFF surpasses current methods in diagnostic accuracy, achieving notable gains in sensitivity, specificity, and area under the ROC curve. Code is available at https://github.com/BeistMedAI/RTGMFF."
    },
    {
      "paperId": "82207e19d8a722f19cb6eb1f7b2c03c499a0c023",
      "externalIds": {
        "ArXiv": "2509.03066",
        "DBLP": "journals/corr/abs-2509-03066",
        "DOI": "10.48550/arXiv.2509.03066",
        "CorpusId": 281091906
      },
      "corpusId": 281091906,
      "title": "S2M2ECG: Spatio-temporal bi-directional State Space Model Enabled Multi-branch Mamba for ECG",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.03066, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2193129514",
          "name": "Huaicheng Zhang"
        },
        {
          "authorId": "2379787090",
          "name": "Ruoxin Wang"
        },
        {
          "authorId": "2378901954",
          "name": "Chenlian Zhou"
        },
        {
          "authorId": "2269416711",
          "name": "Jiguang Shi"
        },
        {
          "authorId": "2305332805",
          "name": "Yue Ge"
        },
        {
          "authorId": "35339084",
          "name": "Zhoutong Li"
        },
        {
          "authorId": "144825910",
          "name": "Sheng Chang"
        },
        {
          "authorId": "144236892",
          "name": "Hao Wang"
        },
        {
          "authorId": "145709228",
          "name": "Jin He"
        },
        {
          "authorId": "2226142423",
          "name": "Qijun Huang"
        }
      ],
      "abstract": "As one of the most effective methods for cardiovascular disease (CVD) diagnosis, multi-lead Electrocardiogram (ECG) signals present a characteristic multi-sensor information fusion challenge that has been continuously researched in deep learning domains. Despite the numerous algorithms proposed with different DL architectures, maintaining a balance among performance, computational complexity, and multi-source ECG feature fusion remains challenging. Recently, state space models (SSMs), particularly Mamba, have demonstrated remarkable effectiveness across various fields. Their inherent design for high-efficiency computation and linear complexity makes them particularly suitable for low-dimensional data like ECGs. This work proposes S2M2ECG, an SSM architecture featuring three-level fusion mechanisms: (1) Spatio-temporal bi-directional SSMs with segment tokenization for low-level signal fusion, (2) Intra-lead temporal information fusion with bi-directional scanning to enhance recognition accuracy in both forward and backward directions, (3) Cross-lead feature interaction modules for spatial information fusion. To fully leverage the ECG-specific multi-lead mechanisms inherent in ECG signals, a multi-branch design and lead fusion modules are incorporated, enabling individual analysis of each lead while ensuring seamless integration with others. Experimental results reveal that S2M2ECG achieves superior performance in the rhythmic, morphological, and clinical scenarios. Moreover, its lightweight architecture ensures it has nearly the fewest parameters among existing models, making it highly suitable for efficient inference and convenient deployment. Collectively, S2M2ECG offers a promising alternative that strikes an excellent balance among performance, computational complexity, and ECG-specific characteristics, paving the way for high-performance, lightweight computations in CVD diagnosis."
    },
    {
      "paperId": "d95ebd5d95107c8570dbe50e866133f510150b92",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-03426",
        "ArXiv": "2509.03426",
        "DOI": "10.48550/arXiv.2509.03426",
        "CorpusId": 281092527
      },
      "corpusId": 281092527,
      "title": "Time-Scaling State-Space Models for Dense Video Captioning",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.03426, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "8797855",
          "name": "A. Piergiovanni"
        },
        {
          "authorId": "2354179184",
          "name": "Ganesh Mallya"
        },
        {
          "authorId": "2237957528",
          "name": "Dahun Kim"
        },
        {
          "authorId": "145426908",
          "name": "A. Angelova"
        }
      ],
      "abstract": "Dense video captioning is a challenging video understanding task which aims to simultaneously segment the video into a sequence of meaningful consecutive events and to generate detailed captions to accurately describe each event. Existing methods often encounter difficulties when working with the long videos associated with dense video captioning, due to the computational complexity and memory limitations. Furthermore, traditional approaches require the entire video as input, in order to produce an answer, which precludes online processing of the video. We address these challenges by time-scaling State-Space Models (SSMs) to even longer sequences than before. Our approach, State-Space Models with Transfer State, combines both the long-sequence and recurrent properties of SSMs and addresses the main limitation of SSMs which are otherwise not able to sustain their state for very long contexts, effectively scaling SSMs further in time. The proposed model is particularly suitable for generating captions on-the-fly, in an online or streaming manner, without having to wait for the full video to be processed, which is more beneficial in practice. When applied to dense video captioning, our approach scales well with video lengths and uses 7x fewer FLOPs."
    },
    {
      "paperId": "9255828a4cacb06e255d369f0a730cb194562397",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-02967",
        "ArXiv": "2509.02967",
        "DOI": "10.48550/arXiv.2509.02967",
        "CorpusId": 281092537
      },
      "corpusId": 281092537,
      "title": "AR-KAN: Autoregressive-Weight-Enhanced Kolmogorov-Arnold Network for Time Series Forecasting",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.02967, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2312326889",
          "name": "Chen Zeng"
        },
        {
          "authorId": "2378911087",
          "name": "Tiehang Xu"
        },
        {
          "authorId": "2378875153",
          "name": "Qiao Wang"
        }
      ],
      "abstract": "Traditional neural networks struggle to capture the spectral structure of complex signals. Fourier neural networks (FNNs) attempt to address this by embedding Fourier series components, yet many real-world signals are almost-periodic with non-commensurate frequencies, posing additional challenges. Building on prior work showing that ARIMA outperforms large language models (LLMs) for forecasting, we extend the comparison to neural predictors and find ARIMA still superior. We therefore propose the Autoregressive-Weight-Enhanced Kolmogorov-Arnold Network (AR-KAN), which integrates a pre-trained AR module for temporal memory with a KAN for nonlinear representation. The AR module preserves essential temporal features while reducing redundancy. Experiments demonstrate that AR-KAN matches ARIMA on almost-periodic functions and achieves the best results on $72\\%$ of Rdatasets series, with a clear advantage on data with periodic structure. These results highlight AR-KAN as a robust and effective framework for time series forecasting."
    },
    {
      "paperId": "e2a4115bc5ce722ce42925470baa09149c35ec81",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-03730",
        "ArXiv": "2509.03730",
        "DOI": "10.48550/arXiv.2509.03730",
        "CorpusId": 281103774
      },
      "corpusId": 281103774,
      "title": "The Personality Illusion: Revealing Dissociation Between Self-Reports & Behavior in LLMs",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 4,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.03730, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2125166410",
          "name": "P. Han"
        },
        {
          "authorId": "2352256312",
          "name": "Rafal Kocielnik"
        },
        {
          "authorId": "2297671110",
          "name": "Peiyang Song"
        },
        {
          "authorId": "2262393353",
          "name": "Ramit Debnath"
        },
        {
          "authorId": "2378956195",
          "name": "Dean Mobbs"
        },
        {
          "authorId": "2257161858",
          "name": "Anima Anandkumar"
        },
        {
          "authorId": "2257247189",
          "name": "R. M. Alvarez"
        }
      ],
      "abstract": "Personality traits have long been studied as predictors of human behavior. Recent advances in Large Language Models (LLMs) suggest similar patterns may emerge in artificial systems, with advanced LLMs displaying consistent behavioral tendencies resembling human traits like agreeableness and self-regulation. Understanding these patterns is crucial, yet prior work primarily relied on simplified self-reports and heuristic prompting, with little behavioral validation. In this study, we systematically characterize LLM personality across three dimensions: (1) the dynamic emergence and evolution of trait profiles throughout training stages; (2) the predictive validity of self-reported traits in behavioral tasks; and (3) the impact of targeted interventions, such as persona injection, on both self-reports and behavior. Our findings reveal that instructional alignment (e.g., RLHF, instruction tuning) significantly stabilizes trait expression and strengthens trait correlations in ways that mirror human data. However, these self-reported traits do not reliably predict behavior, and observed associations often diverge from human patterns. While persona injection successfully steers self-reports in the intended direction, it exerts little or inconsistent effect on actual behavior. By distinguishing surface-level trait expression from behavioral consistency, our findings challenge assumptions about LLM personality and underscore the need for deeper evaluation in alignment and interpretability."
    },
    {
      "paperId": "b274156964c3b3aa459d3a17eba707cb955d0af2",
      "externalIds": {
        "DBLP": "journals/vc/ZhuYZ25",
        "DOI": "10.1007/s00371-025-04134-x",
        "CorpusId": 281135391
      },
      "corpusId": 281135391,
      "title": "Efficient Monte Carlo denoising via state-space model with low GPU memory overhead",
      "venue": "The Visual Computer",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s00371-025-04134-x?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s00371-025-04134-x, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2191905742",
          "name": "Rui Zhu"
        },
        {
          "authorId": "2379062256",
          "name": "Jiajing Yu"
        },
        {
          "authorId": "2380437407",
          "name": "Tongzhou Zhao"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "7e711993848ddbc78f98cb6077e08668fda7ee76",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-02471",
        "ArXiv": "2509.02471",
        "DOI": "10.1109/LSP.2025.3606785",
        "CorpusId": 281079232
      },
      "corpusId": 281079232,
      "title": "ESTM: An Enhanced Dual-Branch Spectral-Temporal Mamba for Anomalous Sound Detection",
      "venue": "IEEE Signal Processing Letters",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.02471, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2378719734",
          "name": "Chengyuan Ma"
        },
        {
          "authorId": "2353381823",
          "name": "Peng Jia"
        },
        {
          "authorId": "48843269",
          "name": "Hongyue Guo"
        },
        {
          "authorId": "2255027949",
          "name": "Wenming Yang"
        }
      ],
      "abstract": "The core challenge in industrial equipment anomalous sound detection (ASD) lies in modeling the time-frequency coupling characteristics of acoustic features. Existing modeling methods are limited by local receptive fields, making it difficult to capture long-range temporal patterns and cross-band dynamic coupling effects in machine acoustic features. In this letter, we propose a novel framework, ESTM, which is based on a dual-path Mamba architecture with time-frequency decoupled modeling and utilizes Selective State-Space Models (SSM) for long-range sequence modeling. ESTM extracts rich feature representations from different time segments and frequency bands by fusing enhanced Mel spectrograms and raw audio features, while further improving sensitivity to anomalous patterns through the TriStat-Gating (TSG) module. Our experiments demonstrate that ESTM improves anomalous detection performance on the DCASE 2020 Task 2 dataset, further validating the effectiveness of the proposed method."
    },
    {
      "paperId": "7df64e63e3187fa9973e2ca96a9b239849fb3cf5",
      "externalIds": {
        "ArXiv": "2509.02261",
        "DBLP": "journals/corr/abs-2509-02261",
        "DOI": "10.48550/arXiv.2509.02261",
        "CorpusId": 281079730
      },
      "corpusId": 281079730,
      "title": "DSGC-Net: A Dual-Stream Graph Convolutional Network for Crowd Counting via Feature Correlation Mining",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.02261, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2362618987",
          "name": "Yihong Wu"
        },
        {
          "authorId": "2379154184",
          "name": "Jinqiao Wei"
        },
        {
          "authorId": "2378718999",
          "name": "Xionghui Zhao"
        },
        {
          "authorId": "2361873817",
          "name": "Yidi Li"
        },
        {
          "authorId": "2379729146",
          "name": "Shaoyi Du"
        },
        {
          "authorId": "2317011851",
          "name": "Bin Ren"
        },
        {
          "authorId": "1429806753",
          "name": "N. Sebe"
        }
      ],
      "abstract": "Deep learning-based crowd counting methods have achieved remarkable progress in recent years. However, in complex crowd scenarios, existing models still face challenges when adapting to significant density distribution differences between regions. Additionally, the inconsistency of individual representations caused by viewpoint changes and body posture differences further limits the counting accuracy of the models. To address these challenges, we propose DSGC-Net, a Dual-Stream Graph Convolutional Network based on feature correlation mining. DSGC-Net introduces a Density Approximation (DA) branch and a Representation Approximation (RA) branch. By modeling two semantic graphs, it captures the potential feature correlations in density variations and representation distributions. The DA branch incorporates a density prediction module that generates the density distribution map, and constructs a density-driven semantic graph based on density similarity. The RA branch establishes a representation-driven semantic graph by computing global representation similarity. Then, graph convolutional networks are applied to the two semantic graphs separately to model the latent semantic relationships, which enhance the model's ability to adapt to density variations and improve counting accuracy in multi-view and multi-pose scenarios. Extensive experiments on three widely used datasets demonstrate that DSGC-Net outperforms current state-of-the-art methods. In particular, we achieve MAE of 48.9 and 5.9 in ShanghaiTech Part A and Part B datasets, respectively. The released code is available at: https://github.com/Wu-eon/CrowdCounting-DSGCNet."
    },
    {
      "paperId": "d23042b46f1222a18cabc4d66f13b639bbb119aa",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-02167",
        "ArXiv": "2509.02167",
        "DOI": "10.48550/arXiv.2509.02167",
        "CorpusId": 281079742
      },
      "corpusId": 281079742,
      "title": "AudioRWKV: Efficient and Stable Bidirectional RWKV for Audio Pattern Recognition",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.02167, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2327964314",
          "name": "Jiayu Xiong"
        },
        {
          "authorId": "2327792788",
          "name": "Jun Xue"
        },
        {
          "authorId": "2378705214",
          "name": "Jianlong Kwan"
        },
        {
          "authorId": "2327002929",
          "name": "Jing Wang"
        }
      ],
      "abstract": "Recently, Transformers (e.g., Audio Spectrogram Transformers, AST) and state-space models (e.g., Audio Mamba, AuM) have achieved remarkable progress in audio modeling. However, the O(L^2) computational complexity of the Transformer architecture hinders efficient long-sequence processing, while the Mamba architecture tends to become unstable when scaling parameters and data. To address these challenges, this paper proposes AudioRWKV (A-RWKV), a highly efficient and stable architecture for audio modeling. Specifically, we inherit the stable and efficient recurrent formulation of RWKV7 and replace its 1D token-shift operation with a 2D depthwise separable convolution to better capture local spectro-temporal patterns. Furthermore, we adapt the original causal WKV kernel into a bidirectional WKV kernel (Bi-WKV), enabling global context modeling over the entire audio sequence while maintaining linear computational complexity. Benefiting from the inherent stability of the RWKV7 foundation, A-RWKV scales seamlessly to larger model sizes. Experimental results demonstrate that, under the same linear-model regime, A-RWKV-S (22M) achieves performance parity with AuM-B (92M) while exhibiting more stable throughput than AST; for long-form audio (~5 minutes 28 seconds), WKV7 achieves up to a 13.3X speedup in processing."
    },
    {
      "paperId": "c3c39d9bb26629989523cd64ec00767034133ff8",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-02054",
        "ArXiv": "2509.02054",
        "DOI": "10.48550/arXiv.2509.02054",
        "CorpusId": 281079993
      },
      "corpusId": 281079993,
      "title": "Comprehensive Analysis and Exclusion Hypothesis of \u03b1-Approximation Method for Discretizing Analog Systems",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.02054, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2320748616",
          "name": "Shengbo Chen"
        },
        {
          "authorId": "2378710299",
          "name": "Jisong Wang"
        },
        {
          "authorId": "2378959501",
          "name": "Dejun Liu"
        },
        {
          "authorId": "2378651523",
          "name": "Jiaxi Ying"
        },
        {
          "authorId": "2379612701",
          "name": "Shuai Wang"
        }
      ],
      "abstract": "A popular method for designing digital models is transforming the transfer function of the corresponding analog models from continuous domain (s-domain) into discrete domain (z-domain) using the s-to-z transformation. The alpha-approximation is a generalized form of these transformations. When alpha is set to 0.5, the result is the well-known Tustin transformation or bi-linear transformation. In this paper, we provided a comprehensive analysis of the alpha-approximation method, including mathematical interpretation, stability analysis and distortion analysis. Through mathematical interpretation, we revealed that it can be derived by numerically integrating the error function We defined this as the hexagonal approximation. We demonstrated that the stable range of alpha was [0.5, 1] by doing stability analysis. Through distortion analysis, we found that minimizing amplitude and phase distortion simultaneously seemed impossible by regulating alpha alone. Finally, We proposed an exclusion hypothesis hypothesizing that there is no single parameter alpha to minimize the amplitude distortion and phase distortion simultaneously across all frequency points within the Nyquist frequency range. This paper demonstrates that designing parameter alpha involves balancing amplitude and phase distortion."
    },
    {
      "paperId": "0e12ce237a9e53634fb5e815b76fdb55e9c62fca",
      "externalIds": {
        "DOI": "10.22761/gd.2025.0010",
        "CorpusId": 281111359
      },
      "corpusId": 281111359,
      "title": "GeoAI Dataset for Building Change Detection from CAS500-1",
      "venue": "GEO DATA",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.22761/gd.2025.0010?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.22761/gd.2025.0010, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2379040216",
          "name": "DongHyuk Jin"
        },
        {
          "authorId": "2379036245",
          "name": "Ji Sang Park"
        },
        {
          "authorId": "2379206125",
          "name": "Yuna Jang"
        },
        {
          "authorId": "2379046383",
          "name": "Seungha Lee"
        },
        {
          "authorId": "2260698129",
          "name": "Junhwa Chi"
        }
      ],
      "abstract": "Building change detection (BCD) is essential for urban planning, disaster response, and environmental monitoring. Existing datasets, such as WHU-CD, LEVIR-CD"
    },
    {
      "paperId": "c85d05961fc4d4db630a57e3bf4cd8d7f1d9b7ca",
      "externalIds": {
        "DOI": "10.1088/1361-6501/ae01c2",
        "CorpusId": 281109976
      },
      "corpusId": 281109976,
      "title": "Progress on the application of deep learning in gearbox fault diagnosis",
      "venue": "Measurement science and technology",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1088/1361-6501/ae01c2?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1088/1361-6501/ae01c2, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2383470195",
          "name": "Min Xu"
        },
        {
          "authorId": "2266283240",
          "name": "Jianbin Xiong"
        },
        {
          "authorId": "2311303055",
          "name": "Xiangjun Dong"
        },
        {
          "authorId": "2151572125",
          "name": "Qi Wang"
        },
        {
          "authorId": "2311303792",
          "name": "Jianxiang Yang"
        }
      ],
      "abstract": "As a critical component in industrial machinery systems, gearboxes demand robust fault diagnosis solutions to ensure operational safety, energy efficiency, and sustainable manufacturing practices. Deep learning (DL) has emerged as a transformative approach for intelligent fault identification, demonstrating superior capabilities in processing complex vibration signatures compared to conventional methods. This review systematically examines DL applications in gearbox diagnostics through dual perspectives of theoretical foundations and industrial implementations. Six principal DL architectures are critically analyzed, including their advanced variants optimized for mechanical signal processing. The study systematically compares these architectures across diagnostic capabilities, computational demands, and implementation constraints. This review identifies promising research directions to address the current challenges in gearbox fault diagnosis. It aims to establish strategic research pathways that bridge the existing gap between theoretical models and industrial requirements, thereby enhancing the predictive maintenance framework for next-generation intelligent manufacturing systems."
    },
    {
      "paperId": "60ed147f7e18ac3801189269f206262fd3055a36",
      "externalIds": {
        "DBLP": "journals/inffus/NiXCLL26",
        "DOI": "10.1016/j.inffus.2025.103786",
        "CorpusId": 281629994
      },
      "corpusId": 281629994,
      "title": "CSFAFormer: Category-selective feature aggregation transformer for multimodal remote sensing image semantic segmentation",
      "venue": "Information Fusion",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.inffus.2025.103786?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.inffus.2025.103786, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2200104710",
          "name": "Yue Ni"
        },
        {
          "authorId": "2382598459",
          "name": "Donglin Xue"
        },
        {
          "authorId": "2222809862",
          "name": "Weijian Chi"
        },
        {
          "authorId": "2303165617",
          "name": "Ji Luan"
        },
        {
          "authorId": "2244573720",
          "name": "Jiahang Liu"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "abb10dcc8b5ccbd567830ae0d7595c22a8a08fba",
      "externalIds": {
        "DBLP": "conf/ijcai/YangGHZW25",
        "DOI": "10.24963/ijcai.2025/392",
        "CorpusId": 281424186
      },
      "corpusId": 281424186,
      "title": "Revealing Concept Shift in Spatio-Temporal Graphs via State Learning",
      "venue": "International Joint Conference on Artificial Intelligence",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.24963/ijcai.2025/392?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.24963/ijcai.2025/392, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2221673134",
          "name": "Kuo Yang"
        },
        {
          "authorId": "2375444640",
          "name": "Yunhe Guo"
        },
        {
          "authorId": "2289391229",
          "name": "Qihe Huang"
        },
        {
          "authorId": "2278115693",
          "name": "Zhen-Qiang Zhou"
        },
        {
          "authorId": "2288901497",
          "name": "Yang Wang"
        }
      ],
      "abstract": "Dynamic graphs are ubiquitous in the real world, presenting the temporal evolution of individuals within spatial associations. Recently, dynamic graph learning research is flourishing, striving to more effectively capture evolutionary patterns and spatial correlations. However, existing methods still fail to address the issue of concept shift in dynamic graphs. Concept shift manifests as a distribution shift in the mapping pattern between historical observations and future evolution. The reason is that some environment variables in dynamic graphs exert varying effects on evolution patterns, but these variables are not effectively captured by the models, leading to the intractable concept shift issue. To tackle this issue, we propose a State-driven environment inference framework (Samen) to achieve a dynamic graph learning framework equipped with concept generalization ability. Firstly, we propose a two-stage environment inference and compression strategy. From the perspective of state space, we introduce a prefix-suffix collaborative state learning mechanism to bidirectionally model the spatio-temporal states. A hierarchical state compressor is further designed to refine the state information resulting in concept shift. Secondly, we propose a skip-connection spatio-temporal prediction module, which effectively utilizes the inferred environments to improve the model's generalization capability. Finally, we select seven datasets from different domains to validate the effectiveness of our model. By comparing the performance of different models on samples with concept shift, we verify that our Samen gains generalization capacity that existing methods fail to capture."
    },
    {
      "paperId": "632b1d76a8ab1868ef68c13608bf201cc5e85e39",
      "externalIds": {
        "PubMedCentral": "12466717",
        "DOI": "10.3390/ani15182646",
        "CorpusId": 281611548,
        "PubMed": "41007890"
      },
      "corpusId": 281611548,
      "title": "S_T_Mamba: A Novel Jinnan Calf Diarrhea Behavior Recognition Model Based on Sequence Tree Mamba",
      "venue": "Animals",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12466717, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2720733",
          "name": "Wangli Hao"
        },
        {
          "authorId": "2384423370",
          "name": "Yakui Xue"
        },
        {
          "authorId": "2326098690",
          "name": "Hao Shu"
        },
        {
          "authorId": "2382456779",
          "name": "Bingxue Lv"
        },
        {
          "authorId": "2382887994",
          "name": "Hanwei Li"
        },
        {
          "authorId": "2164762357",
          "name": "Meng Han"
        },
        {
          "authorId": "2382605392",
          "name": "Yanhong Liu"
        },
        {
          "authorId": "2242536340",
          "name": "Fuzhong Li"
        }
      ],
      "abstract": "Simple Summary Calf diarrhea behavior is a reliable indicator of health status, and accurate behavior recognition is vital for effective health surveillance and management. However, current behavior recognition techniques often struggle to distinguish between similar behaviors, leading to a decrease in performance. This research proposes a new model, named S_T_Mamba, which leverages a sequence processing strategy and a tree state space module to capture temporal dependencies and long-range pixel associations in video frames. The model can effectively recognize subtle differences in calf diarrhea behavior. Consequently, the performance of calf diarrhea behavior recognition is enhanced by the S_T_Mamba model."
    },
    {
      "paperId": "187fd0f5db44af2d2398344cf2b4bed8588a8baa",
      "externalIds": {
        "PubMedCentral": "12473509",
        "DOI": "10.3390/s25185620",
        "CorpusId": 281612525,
        "PubMed": "41012860"
      },
      "corpusId": 281612525,
      "title": "CogMamba: Multi-Task Driver Cognitive Load and Physiological Non-Contact Estimation with Multimodal Facial Features",
      "venue": "Italian National Conference on Sensors",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12473509, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2382494735",
          "name": "Yicheng Xie"
        },
        {
          "authorId": "2382868816",
          "name": "Bin Guo"
        }
      ],
      "abstract": "The cognitive load of drivers directly affects the safety and practicality of advanced driving assistant systems, especially in autonomous driving scenarios where drivers need to quickly take control of the vehicle after performing non-driving-related tasks (NDRTs). However, existing driver cognitive load detection methods have shortcomings such as the inability to deploy invasive detection equipment inside vehicles and limitations to eye movement detection, which restrict their practical application. To achieve more efficient and practical cognitive load detection, this study proposes a multi-task non-contact cognitive load and physiological state estimation model based on RGB video, named CogMamba. The model utilizes multimodal features extracted from facial video and introduces the Mamba architecture to efficiently capture local and global temporal dependencies, thereby further jointly estimating cognitive load, heart rate (HR), and respiratory rate (RR). Experimental results demonstrate that CogMamba exhibits superior performance on two public datasets and shows excellent robustness under the cross-dataset generalization test. This study provides insights for non-contact driver state monitoring in real-world driving scenarios."
    },
    {
      "paperId": "a00123edef3bb163f0fa5e9be3bec49751e6082f",
      "externalIds": {
        "DBLP": "journals/asc/ShanZL25",
        "DOI": "10.1016/j.asoc.2025.113448",
        "CorpusId": 279516443
      },
      "corpusId": 279516443,
      "title": "SANTM: A Sparse Access Neural Turing Machine with local multi-head self-attention for long-term memorization",
      "venue": "Applied Soft Computing",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.asoc.2025.113448?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.asoc.2025.113448, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2876965",
          "name": "Dongjing Shan"
        },
        {
          "authorId": "2368481166",
          "name": "Jing Zhu"
        },
        {
          "authorId": "2385954566",
          "name": "Yamei Luo"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "15568b71f8f8b569781d21dd978a8fba9b6bbcb0",
      "externalIds": {
        "DOI": "10.1016/j.autcon.2025.106342",
        "CorpusId": 279526852
      },
      "corpusId": 279526852,
      "title": "Unsupervised anomaly segmentation model for rail damage based on image-inpainting and cold diffusion",
      "venue": "Automation in Construction",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.autcon.2025.106342?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.autcon.2025.106342, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2290969387",
          "name": "Chengjia Han"
        },
        {
          "authorId": "2332723013",
          "name": "Yiqing Dong"
        },
        {
          "authorId": "2294317258",
          "name": "Maggie Y. Gao"
        },
        {
          "authorId": "2152288031",
          "name": "Liwei Dong"
        },
        {
          "authorId": "2224378018",
          "name": "Yaowen Yang"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "0e7b06467f75a77afb9b607b7a926007609d9f47",
      "externalIds": {
        "DOI": "10.1016/j.energy.2025.137225",
        "CorpusId": 279853489
      },
      "corpusId": 279853489,
      "title": "Dual-path frequency Mamba-Transformer model for wind power forecasting",
      "venue": "Energy",
      "year": 2025,
      "citationCount": 7,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.energy.2025.137225?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.energy.2025.137225, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2369858873",
          "name": "Jun-Tao Hong"
        },
        {
          "authorId": "49106992",
          "name": "Shuang Han"
        },
        {
          "authorId": "49781544",
          "name": "Jie Yan"
        },
        {
          "authorId": "2291209362",
          "name": "Yongqian Liu"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "5ab2f1b9f5744605df9360e06f239c76bb6d8280",
      "externalIds": {
        "DOI": "10.1016/j.autcon.2025.106375",
        "CorpusId": 279906016
      },
      "corpusId": 279906016,
      "title": "Automatic detection and 3D pose reconstruction of loose bolts with rotation angle quantification using a calibration-free monocular camera",
      "venue": "Automation in Construction",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.autcon.2025.106375?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.autcon.2025.106375, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2295136213",
          "name": "Chuang Cui"
        },
        {
          "authorId": "2228397487",
          "name": "Qiu-song Zheng"
        },
        {
          "authorId": "2294826081",
          "name": "Qinghua Zhang"
        },
        {
          "authorId": "1663980909",
          "name": "Yihengu Bao"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "798bf79b21e58096e53fc15c4a8d9466a4171d39",
      "externalIds": {
        "DBLP": "journals/tcasI/DongZW25",
        "DOI": "10.1109/TCSI.2025.3527541",
        "CorpusId": 275685336
      },
      "corpusId": 275685336,
      "title": "An Efficient Window-Based Vision Transformer Accelerator via Mixed-Granularity Sparsity",
      "venue": "IEEE Transactions on Circuits and Systems Part 1: Regular Papers",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSI.2025.3527541?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSI.2025.3527541, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2293481926",
          "name": "Qiwei Dong"
        },
        {
          "authorId": "2275642329",
          "name": "Siyu Zhang"
        },
        {
          "authorId": "2135360811",
          "name": "Zhongfeng Wang"
        }
      ],
      "abstract": "Vision Transformers (ViTs) have achieved excellent performance on various computer vision tasks, while their high computation and memory costs pose challenges for practical deployment. To address this issue, token-level pruning is used as an effective method to compress ViTs, discarding unimportant image tokens that contribute little to predictions. However, directly applying unstructured token pruning to window-based ViTs damages their regular feature map structure, resulting in load imbalance when deployed on mobile devices. In this work, we propose an efficient algorithm-hardware co-optimized framework to accelerate window-based ViTs via adaptive Mixed-Granularity Sparsity (MGS). At the algorithm level, a hardware-friendly MGS algorithm is developed by integrating the inherent sparsity, global window pruning, and local N:M token pruning to balance model accuracy and its computational complexity. At the hardware level, we present a dedicated accelerator equipped with a sparse computing core and two lightweight auxiliary processing units to execute window-based calculations efficiently using MGS. Additionally, we devise a dynamic pipeline interleaving dataflow to achieve on-chip layer fusion, which reduces the processing latency and maximizes data reuse. Experimental results demonstrate that, with similar computational complexity, our highly structured MGS algorithm can achieve comparable or even better accuracy than previous compression methods. Moreover, compared to existing FPGA-based accelerators for Transformers, our design can achieve $1.80~\\sim ~6.52\\times $ and $1.16~\\sim ~12.05\\times $ improvements in terms of throughput and energy efficiency, respectively."
    },
    {
      "paperId": "21d5329763c510ef35f737a3e07d4754f9b80619",
      "externalIds": {
        "ArXiv": "2509.01381",
        "DBLP": "journals/corr/abs-2509-01381",
        "DOI": "10.48550/arXiv.2509.01381",
        "CorpusId": 280992162
      },
      "corpusId": 280992162,
      "title": "Learn to Jump: Adaptive Random Walks for Long-Range Propagation through Graph Hierarchies",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.01381, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2378252282",
          "name": "Jo\u00a8el Mathys"
        },
        {
          "authorId": "2307085791",
          "name": "Federico Errica"
        }
      ],
      "abstract": "Message-passing architectures struggle to sufficiently model long-range dependencies in node and graph prediction tasks. We propose a novel approach exploiting hierarchical graph structures and adaptive random walks to address this challenge. Our method introduces learnable transition probabilities that decide whether the walk should prefer the original graph or travel across hierarchical shortcuts. On a synthetic long-range task, we demonstrate that our approach can exceed the theoretical bound that constrains traditional approaches operating solely on the original topology. Specifically, walks that prefer the hierarchy achieve the same performance as longer walks on the original graph. These preliminary findings open a promising direction for efficiently processing large graphs while effectively capturing long-range dependencies."
    },
    {
      "paperId": "beac33c55127717712172b2ca9366e6ef92ea970",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-01431",
        "ArXiv": "2509.01431",
        "DOI": "10.48550/arXiv.2509.01431",
        "CorpusId": 281080227
      },
      "corpusId": 281080227,
      "title": "Mamba-CNN: A Hybrid Architecture for Efficient and Accurate Facial Beauty Prediction",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 4,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.01431, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2267648129",
          "name": "D. E. Boukhari"
        }
      ],
      "abstract": "The computational assessment of facial attractiveness, a challenging subjective regression task, is dominated by architectures with a critical trade-off: Convolutional Neural Networks (CNNs) offer efficiency but have limited receptive fields, while Vision Transformers (ViTs) model global context at a quadratic computational cost. To address this, we propose Mamba-CNN, a novel and efficient hybrid architecture. Mamba-CNN integrates a lightweight, Mamba-inspired State Space Model (SSM) gating mechanism into a hierarchical convolutional backbone. This core innovation allows the network to dynamically modulate feature maps and selectively emphasize salient facial features and their long-range spatial relationships, mirroring human holistic perception while maintaining computational efficiency. We conducted extensive experiments on the widely-used SCUT-FBP5500 benchmark, where our model sets a new state-of-the-art. Mamba-CNN achieves a Pearson Correlation (PC) of 0.9187, a Mean Absolute Error (MAE) of 0.2022, and a Root Mean Square Error (RMSE) of 0.2610. Our findings validate the synergistic potential of combining CNNs with selective SSMs and present a powerful new architectural paradigm for nuanced visual understanding tasks."
    },
    {
      "paperId": "37ca6a8821a70aa925895d6568d58bf058a7d6e9",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-01080",
        "ArXiv": "2509.01080",
        "DOI": "10.1007/978-3-031-96628-6_1",
        "CorpusId": 280700963
      },
      "corpusId": 280700963,
      "title": "SpectMamba: Integrating Frequency and State Space Models for Enhanced Medical Image Detection",
      "venue": "Information Processing in Medical Imaging",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.01080, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2328975935",
          "name": "Yao Wang"
        },
        {
          "authorId": "2376528248",
          "name": "Dong Yang"
        },
        {
          "authorId": "2279916669",
          "name": "Zhi Qiao"
        },
        {
          "authorId": "2328944298",
          "name": "Wenjian Huang"
        },
        {
          "authorId": "2376524280",
          "name": "Liuzhi Yang"
        },
        {
          "authorId": "2328798344",
          "name": "Zhen Qian"
        }
      ],
      "abstract": "Abnormality detection in medical imaging is a critical task requiring both high efficiency and accuracy to support effective diagnosis. While convolutional neural networks (CNNs) and Transformer-based models are widely used, both face intrinsic challenges: CNNs have limited receptive fields, restricting their ability to capture broad contextual information, and Transformers encounter prohibitive computational costs when processing high-resolution medical images. Mamba, a recent innovation in natural language processing, has gained attention for its ability to process long sequences with linear complexity, offering a promising alternative. Building on this foundation, we present SpectMamba, the first Mamba-based architecture designed for medical image detection. A key component of SpectMamba is the Hybrid Spatial-Frequency Attention (HSFA) block, which separately learns high- and low-frequency features. This approach effectively mitigates the loss of high-frequency information caused by frequency bias and correlates frequency-domain features with spatial features, thereby enhancing the model's ability to capture global context. To further improve long-range dependencies, we propose the Visual State-Space Module (VSSM) and introduce a novel Hilbert Curve Scanning technique to strengthen spatial correlations and local dependencies, further optimizing the Mamba framework. Comprehensive experiments show that SpectMamba achieves state-of-the-art performance while being both effective and efficient across various medical image detection tasks."
    },
    {
      "paperId": "d982056bbe6136f32a6a2f20ce20dd6878b6bef3",
      "externalIds": {
        "PubMedCentral": "12402081",
        "DOI": "10.1038/s41598-025-16797-6",
        "CorpusId": 281056558,
        "PubMed": "40890220"
      },
      "corpusId": 281056558,
      "title": "Marrying Perona Malik diffusion with Mamba for efficient pediatric echocardiographic left ventricular segmentation",
      "venue": "Scientific Reports",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12402081, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2284177290",
          "name": "Zi Ye"
        },
        {
          "authorId": "2284378705",
          "name": "Tianxiang Chen"
        },
        {
          "authorId": "2293354560",
          "name": "Fangyijie Wang"
        },
        {
          "authorId": "2294210425",
          "name": "Hanwei Zhang"
        },
        {
          "authorId": "2293354133",
          "name": "Lijun Zhang"
        }
      ],
      "abstract": "Segmenting echocardiographic images is a crucial step in assessing heart function, as clinical indicators can be obtained by precisely delineating the left ventricle. The success of subsequent heart analyses depends entirely on the precision of this segmentation. However, echocardiography is characterized by ambiguity and heavy background noise interference, making accurate segmentation more challenging. Present methods lack efficiency and are prone to mistakenly segmenting some background noise areas, such as the left ventricular area, due to noise disturbance. To address these issues, we introduce P-Mamba, which integrates the Mixture of Experts (MoE) concept for efficient pediatric echocardiographic left ventricular segmentation. Specifically, we utilize the recently proposed ViM layers from the vision mamba to enhance our model\u2019s computational and memory efficiency while modeling global dependencies. In the DWT-based (Discrete Wavelet Transform) Perona-Malik Diffusion (PMD) Block, we introduce a block that suppresses noise while preserving the left ventricle\u2019s local shape cues. Consequently, our proposed P-Mamba innovatively combines the PMD\u2019s noise suppression and local feature extraction capabilities with Mamba\u2019s efficient design for global dependency modeling. We conducted segmentation experiments on two pediatric ultrasound datasets and a general ultrasound dataset, namely Echonet-dynamic, and achieved state-of-the-art (SOTA) results. Specifically, on the Pediatric PSAX (8959 images) and Pediatric A4C datasets (6425 images), we achieved Dice scores of 0.922 and 0.906, respectively; on the EchoNet-Dynamic dataset (19882 images), we achieved a Dice score of 0.931. Leveraging the strengths of the P-Mamba block, our model demonstrates superior accuracy and efficiency compared to established models, including vision transformers with quadratic and linear computational complexity."
    },
    {
      "paperId": "8f971f8fb51000b3bd49475ccde5cd2efcf34d86",
      "externalIds": {
        "DBLP": "journals/cm/HeFLFNK25",
        "DOI": "10.1109/MCOM.003.2400445",
        "CorpusId": 276804217
      },
      "corpusId": 276804217,
      "title": "The Road Toward General Edge Intelligence: Standing on the Shoulders of Foundation Models",
      "venue": "IEEE Communications Magazine",
      "year": 2025,
      "citationCount": 7,
      "influentialCitationCount": 1,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/MCOM.003.2400445?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/MCOM.003.2400445, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2112469568",
          "name": "Le He"
        },
        {
          "authorId": "2259988648",
          "name": "Lisheng Fan"
        },
        {
          "authorId": "2301538",
          "name": "Xianfu Lei"
        },
        {
          "authorId": "145563835",
          "name": "Pingzhi Fan"
        },
        {
          "authorId": "1709760",
          "name": "A. Nallanathan"
        },
        {
          "authorId": "144015029",
          "name": "G. Karagiannidis"
        }
      ],
      "abstract": "Foundation models, pre-trained on vast datasets, offer a robust backbone for developing artificial intelligence (AI) applications more efficiently and cost-effectively. Standing on the shoulders of foundation models, that is, by integrating foundation models into edge systems, the system intelligence is poised for significant evolution. This evolution ultimately aims to achieve artificial general intelligence (AGI) at the edge, where edge intelligence (EI) would attain high-level capabilities like reasoning, planning, and learning from experience, potentially reaching or even surpassing human-level proficiency. We refer to this ultimate vision of EI as general edge intelligence (GEI). In this article, we explore the road toward GEI, highlighting the critical role of foundation models in this transformation and offering a glimpse into future application scenarios of GEI. Despite its promising potential, the path is challenged by resource constraints, high energy consumption, inference latency, and privacy concerns. We conducted a survey on recent research directions and efforts aimed at tackling the above challenges, particularly in efficient and safe federated fine-tuning, novel model architectures, and edge system designs for naturally integrating foundation models. By summarizing these advancements and identifying research directions, we aim to provide a broad overview of the progress toward achieving intelligent and responsive edge systems."
    },
    {
      "paperId": "27548045b1a04000537d11fd5d7fc8ae0651cb03",
      "externalIds": {
        "DBLP": "journals/tcsv/LeiFSSXAFLGY25a",
        "DOI": "10.1109/TCSVT.2025.3558732",
        "CorpusId": 277645463
      },
      "corpusId": 277645463,
      "title": "Structured Light Image Planar-Topography Feature Decomposition for Generalizable 3D Shape Measurement",
      "venue": "IEEE transactions on circuits and systems for video technology (Print)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2025.3558732?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2025.3558732, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2292458558",
          "name": "Mingyang Lei"
        },
        {
          "authorId": "2829698",
          "name": "Jingfan Fan"
        },
        {
          "authorId": "2106585128",
          "name": "Long Shao"
        },
        {
          "authorId": "2192180749",
          "name": "Hong Song"
        },
        {
          "authorId": "2244034841",
          "name": "Deqiang Xiao"
        },
        {
          "authorId": "2261751010",
          "name": "Danni Ai"
        },
        {
          "authorId": "1500409356",
          "name": "Tianyu Fu"
        },
        {
          "authorId": "150349905",
          "name": "Yucong Lin"
        },
        {
          "authorId": "2112579377",
          "name": "Ying Gu"
        },
        {
          "authorId": "2237372740",
          "name": "Jian Yang"
        }
      ],
      "abstract": "The application of structured light (SL) techniques has achieved remarkable success in three-dimensional (3D) measurements. Traditional methods generally calculate SL information pixel by pixel to obtain the measurement results. Recently, the rise of deep learning (DL) has led to significant developments in this task. However, existing DL-based methods generally learn all features within the image in an end-to-end manner, ignoring the distinction between SL and non-SL information. Therefore, these methods may encounter difficulties in focusing on subtle variations in SL patterns across different scenes, thereby degrading measurement precision. To overcome this challenge, we propose a novel SL Image Planar-Topography Feature Decomposition Network (SIDNet). To fully utilize the information from different SL modality images (fringe and speckle), we decompose different modalities into topography features (modality-specific) and planar features (modality-shared). A physics-driven decomposition loss is proposed to make the topography/planar features dissimilar/similar, which guides the network to distinguish between SL and non-SL information. Moreover, to obtain modality-fused features with global overview and local detail information, we propose a wrapped phase-driven feature fusion module. Specifically, a novel Tri-modality Mamba block is designed to integrate different sources with the guidance of the wrapped phase features. Extensive experiments demonstrate the superiority of our SIDNet in multiple simulated 3D measurement scenes. Moreover, our method shows better generalization ability than other DL models and can be directly applicable to unseen real-world scenes."
    },
    {
      "paperId": "4f0dfb4f903983a6c53b507f9e2f08c317f4b40f",
      "externalIds": {
        "DBLP": "journals/tii/WangZ25",
        "DOI": "10.1109/TII.2025.3574403",
        "CorpusId": 279564316
      },
      "corpusId": 279564316,
      "title": "Optimal Transport With Mamba for Multimodal Inertial Signal Enhancement",
      "venue": "IEEE Transactions on Industrial Informatics",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TII.2025.3574403?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TII.2025.3574403, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2238539686",
          "name": "Yifeng Wang"
        },
        {
          "authorId": "2279059212",
          "name": "Yi Zhao"
        }
      ],
      "abstract": "As motion-sensing components, multimodal inertial sensors composed of accelerometers and gyroscopes are recognized for their compact size, low cost, and broad applications in wearable and smart devices, but they are affected by severe noise. Wavelet transform is renowned for its flexibility in analyzing signals due to its diverse wavelet bases, allowing it to adapt to different signal characteristics. However, the diverse signal noises challenge wavelet allocation. Moreover, the modal differences between acceleration and gyroscope signals make it difficult to share the same wavelet, adding complexity to the wavelet allocation for multimodal signals. To this end, we propose an OT-Mamba framework, which leverages Mamba to extract signal features. Mamba is specifically designed to handle ultra-long sequences, allowing it to capture long-range temporal dependencies for better wavelet selection. Considering the heterogeneity and potential synergy between the accelerometer and gyroscope signals, an optimal transport interaction is proposed to mine their relationship for collaborative wavelet selection. The proposed OT-Mamba combines the reliability of wavelet-based methods and the flexibility of deep learning approaches. As a weakly supervised method, OT-Mamba achieves superior performance compared to existing methods (including fully supervised ones) and outperforms the current state-of-the-art method by an order of magnitude across all quantitative metrics and downstream tasks."
    },
    {
      "paperId": "94baa88482a54f20f0302e9ce2fb0ff266f4a718",
      "externalIds": {
        "DBLP": "journals/tcsv/LiuLDZFZ25",
        "DOI": "10.1109/TCSVT.2025.3551723",
        "CorpusId": 277079979
      },
      "corpusId": 277079979,
      "title": "Multi-Task Guided No-Reference Omnidirectional Image Quality Assessment With Feature Interaction",
      "venue": "IEEE transactions on circuits and systems for video technology (Print)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2025.3551723?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2025.3551723, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2330476626",
          "name": "Yunxiang Liu"
        },
        {
          "authorId": "2308411489",
          "name": "Sifan Li"
        },
        {
          "authorId": "19269060",
          "name": "Huiyu Duan"
        },
        {
          "authorId": "2145107744",
          "name": "Yu Zhou"
        },
        {
          "authorId": "2308285706",
          "name": "Daoxin Fan"
        },
        {
          "authorId": "2087717114",
          "name": "Guangtao Zhai"
        }
      ],
      "abstract": "Omnidirectional image quality assessment (OIQA) has become an increasingly vital problem in recent years. Most previous no-reference OIQA methods only extract local features from the distorted viewports, or extract global features from the entire distorted image, lacking the interaction and fusion between local and global features. Moreover, the lack of reference information also limits their performance. Thus, we propose a no-reference OIQA model which consists of three novel modules, including a bidirectional pseudo-reference module, a Mamba-based global feature extraction module, and a multi-scale local-global feature aggregation module. Specifically, by considering the image distortion degradation process, a bidirectional pseudo-reference module capturing the error maps on viewports is first constructed to refine the multi-scale local visual features, which can supply rich quality degradation reference information without the reference image. To well complement the local features, the VMamba module is adopted to extract the representative multi-scale global visual features. Inspired by human hierarchical visual perception characteristics, a novel multi-scale aggregation module is built to strengthen the feature interaction and effective fusion which can extract deep semantic information. Finally, motivated by the multi-task managing mechanism of human brain, a multi-task learning module is introduced to assist the main quality assessment task by digging the hidden information in compression type and distortion degree. Extensive experimental results demonstrate that our proposed method achieves the state-of-the-art performance on the no-reference OIQA task compared to other models."
    },
    {
      "paperId": "061cd954e76d870070fddafa338f998bfeb072a0",
      "externalIds": {
        "DBLP": "journals/tcsv/ShiZLHMS25",
        "DOI": "10.1109/TCSVT.2025.3557570",
        "CorpusId": 277555335
      },
      "corpusId": 277555335,
      "title": "Mamba Adapter: Efficient Multi-Modal Fusion for Vision-Language Tracking",
      "venue": "IEEE transactions on circuits and systems for video technology (Print)",
      "year": 2025,
      "citationCount": 9,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2025.3557570?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2025.3557570, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2278619368",
          "name": "Liangtao Shi"
        },
        {
          "authorId": "2273869508",
          "name": "Bineng Zhong"
        },
        {
          "authorId": "2055747023",
          "name": "Qihua Liang"
        },
        {
          "authorId": "2220701228",
          "name": "Xiantao Hu"
        },
        {
          "authorId": "2277249541",
          "name": "Zhiyi Mo"
        },
        {
          "authorId": "2292256220",
          "name": "Shuxiang Song"
        }
      ],
      "abstract": "Utilizing the high-level semantic information of language to compensate for the limitations of vision information is a highly regarded approach in single-object tracking. However, most existing vision-language (VL) trackers employ full-parameter fine-tuning, which can easily lead to catastrophic forgetting. Therefore, they fail to fully exploit the prior knowledge of pre-trained models from upstream tasks, resulting in unsatisfactory tracking performance. To alleviate the above problem, we propose a simple yet effective Vision- Language Tracking pipeline based on Mamba Adapter, named MAVLT, which adopts the idea of parameter-efficient fine-tuning (PEFT) to realize the interaction between vision-language modalities. This novel approach offers the following advantages: 1) The knowledge of the upstream pre-trained model is efficiently inherited by freezing its parameters. This ensures that the VL tracking framework only learns the modules for vision and language interaction, with a focus on the fusion between modalities. 2) The modal interaction between language and vision encoders is flexibly bridged in each encoder layer via proposed mamba adapter, enabling efficient interaction of visual and language information at multiple levels. Extensive experiments on five popular vision-language tracking benchmarks validate the effectiveness of the proposed MAVLT. Particularly, the MAVLT achieves 73.4% AUC score on the LaSOT benchmarks with only 0.18%(0.32M) of the total parameters updates. Code and models are available at https://github.com/GXNU-ZhongLab/MAVLT."
    },
    {
      "paperId": "747126bd39e53374d0c129f948b197f685321dee",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-09427",
        "ArXiv": "2509.09427",
        "DOI": "10.1016/j.inffus.2025.103146",
        "CorpusId": 277713280
      },
      "corpusId": 277713280,
      "title": "FS-Diff: Semantic guidance and clarity-aware simultaneous multimodal image fusion and super-resolution",
      "venue": "Information Fusion",
      "year": 2025,
      "citationCount": 8,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.09427, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2179512819",
          "name": "Yuchan Jie"
        },
        {
          "authorId": "2282499042",
          "name": "Yushen Xu"
        },
        {
          "authorId": "2229396399",
          "name": "Xiaosong Li"
        },
        {
          "authorId": "2258544596",
          "name": "Fuqiang Zhou"
        },
        {
          "authorId": "2355084410",
          "name": "Jianming Lv"
        },
        {
          "authorId": "2330373428",
          "name": "Huafeng Li"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "da44248c3ced760711c56032ca91d9f400ecea38",
      "externalIds": {
        "PubMedCentral": "12425018",
        "DOI": "10.1101/2025.08.27.672609",
        "CorpusId": 281259424,
        "PubMed": "40950025"
      },
      "corpusId": 281259424,
      "title": "PlantCAD2: A Long-Context DNA Language Model for Cross-Species Functional Annotation in Angiosperms",
      "venue": "bioRxiv",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBYNC",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12425018, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2305493700",
          "name": "Jingjing Zhai"
        },
        {
          "authorId": "2261493215",
          "name": "Aaron Gokaslan"
        },
        {
          "authorId": "2248484736",
          "name": "Sheng-Kai Hsu"
        },
        {
          "authorId": "2380131545",
          "name": "Szu-Ping Chen"
        },
        {
          "authorId": "2305568609",
          "name": "Zong-Yan Liu"
        },
        {
          "authorId": "2305685538",
          "name": "Edgar Marroquin"
        },
        {
          "authorId": "91124056",
          "name": "Eric Czech"
        },
        {
          "authorId": "2380102203",
          "name": "Betsy Cannon"
        },
        {
          "authorId": "2305492354",
          "name": "Ana Berthel"
        },
        {
          "authorId": "2125196927",
          "name": "M. Romay"
        },
        {
          "authorId": "2380095408",
          "name": "Matt Pennell"
        },
        {
          "authorId": "2257271109",
          "name": "Volodymyr Kuleshov"
        },
        {
          "authorId": "2241480206",
          "name": "E. Buckler"
        }
      ],
      "abstract": "Understanding how DNA sequence encodes biological function remains a fundamental challenge in biology. Flowering plants (angiosperms), the dominant terrestrial clade, exhibit maximal biochemical complexity, extraordinary species diversity (over 100,000 species), relatively recent origins (\u223c160 million years), \u223c200-fold variation in genome size and relative compact coding regions compared with other eukaryotes. These features present both a unique challenge and opportunity for pre-training DNA language models to understand plant-specific evolutionary conservation, regulatory architectures and genomic functions. Here, we introduce PlantCAD2, a long-context, plant-specific DNA language model with single-nucleotide resolution, pre-trained on 65 angiosperm genomes, together with a series of public benchmarks for evaluation. Comprehensive zero-shot testing shows that PlantCAD2 (676 million parameters) efficiently captures evolutionary conservation, surpassing the 7-billion-parameter Evo2 model in 10 of 12 tasks. With parameter-efficient fine-tuning, PlantCAD2 also outperforms the 1-billion-parameter AgroNT across seven cross-species tasks. Moreover, its 8 kb context window substantially improves accessible chromatin prediction in large genomes such as maize (AUPRC increasing from 0.587 to 0.711), underscoring the importance of long-range context for modeling distal regulation. Together, these results establish PlantCAD2 as a powerful, efficient, and versatile foundation model for plant genomics, enabling accurate genome annotation across diverse species."
    },
    {
      "paperId": "2ce8c722a8f638d2efe6e7cb37e8615266f447e8",
      "externalIds": {
        "DBLP": "journals/ijon/HuHLLJ25",
        "DOI": "10.1016/j.neucom.2025.131443",
        "CorpusId": 281269238
      },
      "corpusId": 281269238,
      "title": "A dual-domain mutual compensation network for multi-modality image fusion",
      "venue": "Neurocomputing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.neucom.2025.131443?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.neucom.2025.131443, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2144015174",
          "name": "Jiwei Hu"
        },
        {
          "authorId": "2380404622",
          "name": "Zhen Hu"
        },
        {
          "authorId": "2380172063",
          "name": "Ping Lou"
        },
        {
          "authorId": "2356667510",
          "name": "Kin-Man Lam"
        },
        {
          "authorId": "2261531868",
          "name": "Qiwen Jin"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "552708092e68a92558a9cde85248c5359bfc1571",
      "externalIds": {
        "DOI": "10.1002/mp.18104",
        "CorpusId": 281242868,
        "PubMed": "40926670"
      },
      "corpusId": 281242868,
      "title": "Lightweight hybrid Mamba2 for unsupervised medical image registration",
      "venue": "Medical Physics (Lancaster)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1002/mp.18104?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1002/mp.18104, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2248075230",
          "name": "Aobo Xu"
        },
        {
          "authorId": "2330230981",
          "name": "Shaofei Shen"
        },
        {
          "authorId": "2335147516",
          "name": "Wenkang Chen"
        },
        {
          "authorId": "2204021268",
          "name": "Xuejun Zhang"
        }
      ],
      "abstract": "Deformable medical image registration is a critical task in medical imaging\u2010assisted diagnosis and treatment. In recent years, medical image registration methods based on deep learning have made significant success by leveraging prior knowledge, and the registration accuracy and computational efficiency have been greatly improved. Models based on Transformers have achieved better performance than convolutional neural network methods (ConvNet) in image registration. However, their secondary computational complexity leads to significant computational overhead, posing substantial challenges for deployment in resource\u2010constrained medical environments. Recently, Mamba\u20102 introduced the structured state\u2010space Duality (SSD) framework to address the high computational cost of Transformer, achieving state\u2010of\u2010the\u2010art performance across multiple domains. Mamba\u20102 may be a more powerful competitor than Transformer in the field of image registration. The design of its global receptive field and linear computational complexity enable it to show substantial advantages and efficiency in accurately understanding the nonlinear spatial relationships between the moving images and fixed images."
    },
    {
      "paperId": "b09ba54e62aec6b4224b83ac9af3c8d30ad5bc66",
      "externalIds": {
        "PubMedCentral": "12431503",
        "DOI": "10.3390/s25175522",
        "CorpusId": 281180698,
        "PubMed": "40942951"
      },
      "corpusId": 281180698,
      "title": "MDEM: A Multi-Scale Damage Enhancement MambaOut for Pavement Damage Classification",
      "venue": "Italian National Conference on Sensors",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12431503, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2265870745",
          "name": "Shizheng Zhang"
        },
        {
          "authorId": "2339684004",
          "name": "Kunpeng Wang"
        },
        {
          "authorId": "2339235826",
          "name": "Pu Li"
        },
        {
          "authorId": "2167722221",
          "name": "Min Huang"
        },
        {
          "authorId": "2379358836",
          "name": "Jianxiang Guo"
        }
      ],
      "abstract": "Pavement damage classification is crucial for road maintenance and driving safety. However, restricted to the varying scales, irregular shapes, small area ratios, and frequent overlap with background noise, traditional methods struggle to achieve accurate recognition. To address these challenges, a novel pavement damage classification model is designed based on the MambaOut named Multi-scale Damage Enhancement MambaOut (MDEM). The model incorporates two key modules to improve damage classification performance. The Multi-scale Dynamic Feature Fusion Block (MDFF) adaptively integrates multi-scale information to enhance feature extraction, effectively distinguishing visually similar cracks at different scales. The Damage Detail Enhancement Block (DDE) emphasizes fine structural details while suppressing background interference, thereby improving the representation of small-scale damage regions. Experiments were conducted on multiple datasets, including CQU-BPMDD, CQU-BPDD, and Crack500-PDD. On the CQU-BPMDD dataset, MDEM outperformed the baseline model with improvements of 2.01% in accuracy, 2.64% in precision, 2.7% in F1-score, and 4.2% in AUC. The extensive experimental results demonstrate that MDEM significantly surpasses MambaOut and other comparable methods in pavement damage classification tasks. It effectively addresses challenges such as varying scales, irregular shapes, small damage areas, and background noise, enhancing inspection accuracy in real-world road maintenance."
    },
    {
      "paperId": "c614e613ec741375fe6c62dc2e621ef43ec11811",
      "externalIds": {
        "DOI": "10.1016/j.ast.2025.110910",
        "CorpusId": 281347517
      },
      "corpusId": 281347517,
      "title": "PC-Transformer: Probabilistic Flight Trajectory Prediction with Multi-Stage Predictive Coding",
      "venue": "Aerospace Science and Technology",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.ast.2025.110910?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.ast.2025.110910, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2266507054",
          "name": "Chenglong Ge"
        },
        {
          "authorId": "2303429346",
          "name": "Jing Zhang"
        },
        {
          "authorId": "2219670521",
          "name": "Linyu Wang"
        },
        {
          "authorId": "2303409887",
          "name": "Xuebin Wang"
        },
        {
          "authorId": "2115389883",
          "name": "Jiacheng Yao"
        },
        {
          "authorId": "2366152726",
          "name": "Tianhe Yang"
        },
        {
          "authorId": "2381233894",
          "name": "Jianping Du"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "4c26c5aafdd97c0a260a8fef6ee0bb0d83d5ce20",
      "externalIds": {
        "DOI": "10.1016/j.alit.2025.08.004",
        "CorpusId": 281397451,
        "PubMed": "40967982"
      },
      "corpusId": 281397451,
      "title": "Predictive and therapeutic applications of protein language models.",
      "venue": "Allergology International",
      "year": 2025,
      "citationCount": 3,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.alit.2025.08.004?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.alit.2025.08.004, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2164124637",
          "name": "Kairi Furui"
        },
        {
          "authorId": "2331507606",
          "name": "Koh Sakano"
        },
        {
          "authorId": "2291583563",
          "name": "Masahito Ohue"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "fbe1e56f6171885d5dca6d9fe3af660db02c5b24",
      "externalIds": {
        "DBLP": "conf/ijcai/LiuHWWJ0L025",
        "DOI": "10.24963/ijcai.2025/175",
        "CorpusId": 281470113
      },
      "corpusId": 281470113,
      "title": "TextMEF: Text-guided Prompt Learning for Multi-exposure Image Fusion",
      "venue": "International Joint Conference on Artificial Intelligence",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.24963/ijcai.2025/175?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.24963/ijcai.2025/175, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2108510456",
          "name": "Jinyuan Liu"
        },
        {
          "authorId": "2307131229",
          "name": "Qianjun Huang"
        },
        {
          "authorId": "2160868601",
          "name": "Guanyao Wu"
        },
        {
          "authorId": "2383247924",
          "name": "Di Wang"
        },
        {
          "authorId": "2292303526",
          "name": "Zhiying Jiang"
        },
        {
          "authorId": "1732160",
          "name": "Long Ma"
        },
        {
          "authorId": "2237951125",
          "name": "Risheng Liu"
        },
        {
          "authorId": "2117800366",
          "name": "Xin-Yue Fan"
        }
      ],
      "abstract": "Multi-exposure image fusion~(MEF) aims to integrate a set of low dynamic range images, producing a single image with a higher dynamic range than either one. Despite significant advancements, current MEF approaches still struggle to handle extremely over- or under-exposed conditions, resulting in unsatisfactory visual effects such as hallucinated details and distorted color tones. With this regard, we propose TextMEF, a prompt-driven fusion method enhanced by prompt learning, for multi-exposure image fusion. Specifically, we learn a set of prompts based on text-image similarity among negative and positive samples (over-exposed, under-exposed images, and well-exposed ones). These learned prompts are seamlessly integrated into the loss function, providing high-level guidance for constraining non-uniform exposure regions. Furthermore, we develop a attention Mamba module effectively translates over-/under- exposed regional features into exposure invariant space and ensure them to build efficient long-range dependency to high dynamic range image. Extensive experimental results on three publicly available benchmarks demonstrate that our TextMEF significantly outperforms state-of-the-art approaches in both visual inspection and objective analysis."
    },
    {
      "paperId": "a63877e8132718953ec6905dc33ce553652f3ef8",
      "externalIds": {
        "DBLP": "conf/ijcai/0008LW025",
        "DOI": "10.24963/ijcai.2025/129",
        "CorpusId": 281433285
      },
      "corpusId": 281433285,
      "title": "Omni-Dimensional State Space Model-driven SAM for Pixel-level Anomaly Detection",
      "venue": "International Joint Conference on Artificial Intelligence",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.24963/ijcai.2025/129?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.24963/ijcai.2025/129, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2381789912",
          "name": "Chao Huang"
        },
        {
          "authorId": "2362960188",
          "name": "Qianyi Li"
        },
        {
          "authorId": "2374093405",
          "name": "Jie Wen"
        },
        {
          "authorId": "2381926335",
          "name": "Bob Zhang"
        }
      ],
      "abstract": "Pixel-level anomaly detection is indispensable in industrial defect detection and medical diagnosis. Recently, Segment Anything Model (SAM) has achieved promising results in many vision tasks. However, direct application of the SAM to pixel-level anomaly detection tasks results in unsatisfactory performance, meanwhile SAM needs the manual prompt. Although some automatically prompt-based SAM has been proposed, these automated prompting approaches merely utilize partial image features as prompts and fail to incorporate crucial features such as multi-scale image features to generate more suitable prompts. In this paper, we propose a novel Omni Dimensional State Space Model-driven SAM (ODS-SAM) for pixel-level anomaly detection. Specifically, the proposed method adopts the SAM architecture, ensuring easy implementation and avoiding the need for fine-tuning. A State-Space Model-based residual Omni Dimensional module is designed to automatically generate suitable prompts. This module can effectively leverage multi-scale and global information, facilitating an iterative search for optimal prompts in the prompt space. The identified optimal prompts are then fed into SAM as high-dimensional tensors. Experimental results demonstrate that the proposed ODS-SAM outperforms state-of-the-art models on both industrial and medical image datasets."
    },
    {
      "paperId": "09d8b350c3ab43f2a7c38521299c3cf007ab28c8",
      "externalIds": {
        "DBLP": "conf/ijcai/Yang0CCNTM025",
        "DOI": "10.24963/ijcai.2025/245",
        "CorpusId": 281487733
      },
      "corpusId": 281487733,
      "title": "Unlocking Dark Vision Potential for Medical Image Segmentation",
      "venue": "International Joint Conference on Artificial Intelligence",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.24963/ijcai.2025/245?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.24963/ijcai.2025/245, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2375096057",
          "name": "Hongpeng Yang"
        },
        {
          "authorId": "2218642366",
          "name": "Xiangyu Hu"
        },
        {
          "authorId": "2382737388",
          "name": "Yingxin Chen"
        },
        {
          "authorId": "2382025693",
          "name": "Siyu Chen"
        },
        {
          "authorId": "2283315957",
          "name": "S. Nelakuditi"
        },
        {
          "authorId": "2375320479",
          "name": "Yan Tong"
        },
        {
          "authorId": "2280067914",
          "name": "Shiqiang Ma"
        },
        {
          "authorId": "2352013728",
          "name": "Fei Guo"
        }
      ],
      "abstract": "Accurate segmentation of lesions is crucial for disease diagnosis and treatment planning. However, blurring and low contrast in the imaging process can affect segmentation results. We have observed that noninvasive medical imaging shares considerable similarities with natural images under low light conditions and that nocturnal animals possess extremely strong night vision capabilities. Inspired by the dark vision of these nocturnal animals, we proposed a novel plug-and-play dark vision network (DVNet) to enhance the model's perception for low-contrast medical images. Specifically, by employing the wavelet transform, we decompose medical images into subbands of varying frequencies, mimicking the sensitivity of photoreceptor cells to different light intensities. To simulate the antagonistic receptive fields of horizontal cells and bipolar cells, we design a Mamba-Enhanced Fusion Module to achieve global information correlation and enhance contrast between lesions and surrounding healthy tissues. Extensive experiments demonstrate that the DVNet achieves SOTA performance in various medical image segmentation tasks."
    },
    {
      "paperId": "6e935bbef72e28ea16365661efc1f05d2e88c38b",
      "externalIds": {
        "DOI": "10.1109/MGRS.2025.3574685",
        "CorpusId": 279340545
      },
      "corpusId": 279340545,
      "title": "RRSECS: Referring remote sensing expression comprehension and segmentation",
      "venue": "IEEE Geoscience and Remote Sensing Magazine",
      "year": 2025,
      "citationCount": 4,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/MGRS.2025.3574685?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/MGRS.2025.3574685, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2243536097",
          "name": "Xiaoqiang Lu"
        },
        {
          "authorId": "2287797873",
          "name": "Long Sun"
        },
        {
          "authorId": "47681424",
          "name": "Lingling Li"
        },
        {
          "authorId": "2287033797",
          "name": "Licheng Jiao"
        },
        {
          "authorId": "2243308092",
          "name": "Yuting Yang"
        },
        {
          "authorId": "1388737956",
          "name": "Zhongjian Huang"
        },
        {
          "authorId": "2316949834",
          "name": "Jinming Chai"
        },
        {
          "authorId": "2110952179",
          "name": "Xu Liu"
        },
        {
          "authorId": "2261732890",
          "name": "Fang Liu"
        },
        {
          "authorId": "2238401472",
          "name": "Wenping Ma"
        },
        {
          "authorId": "2155665964",
          "name": "Shuyuan Yang"
        }
      ],
      "abstract": "Understanding and interpreting a specific object from large-scale remote sensing (RS) scenes provide basic support for various practical applications. To achieve it, visual grounding (VG) and referring image segmentation (RIS) are two main techniques that aim to localize and segment the referred object given a free-form linguistic expression. Currently, most works of VG and RIS focus on natural images, with only a few generalizing to RS images and further developing remote sensing visual grounding (RSVG) and referring remote sensing image segmentation (RRSIS). However, RSVG and RRSIS are designed to solve separate tasks, ignoring the benefits of jointly learning localization and segmentation. In this work, we introduce the task of referring remote sensing expression comprehension and segmentation (RRSECS) to explore the potential of multi-task learning in vision-language understanding. Specifically, we construct the first benchmark for this task, namely RefDIOR, which contains image-expression-box-mask quadruplets for training and evaluating different models, enabling us to advance the research of RRSECS. Then, we benchmark extensive methods across VG, RSVG, RIS, and RRSIS on RefDIOR, and give insightful analyses of their performances and limitations. Finally, we propose a novel cross-task collaborative Transformer (CCFormer) to accomplish language-guided end-to-end localization and segmentation, serving as a strong baseline for RRSECS. CCFormer consists of the multi-scale cross-modal fusion module to obtain fine-grained aligned vision-language features, the language-aware gated decoupling module to assign discriminative multi-modal features for each task, and the cross-task collaborative loss to refine multiple outputs in a co-rectify manner. Experimental results on RefDIOR demonstrate the effectiveness and generalization of CCFormer in addressing the challenges of RSVG and RRSIS. The code and dataset will be publicly accessible at https://github.com/IPIU-XDU/RSFM."
    },
    {
      "paperId": "9424a52439742314adf00eb2ed914b8b8618d1ef",
      "externalIds": {
        "DBLP": "conf/ijcai/Hou0LW025",
        "DOI": "10.24963/ijcai.2025/824",
        "CorpusId": 281445608
      },
      "corpusId": 281445608,
      "title": "RRG-Mamba: Efficient Radiology Report Generation with State Space Model",
      "venue": "International Joint Conference on Artificial Intelligence",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.24963/ijcai.2025/824?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.24963/ijcai.2025/824, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2240356137",
          "name": "Xiaodi Hou"
        },
        {
          "authorId": "2240216111",
          "name": "Xiaobo Li"
        },
        {
          "authorId": "2149493809",
          "name": "Mingyu Lu"
        },
        {
          "authorId": "2265297806",
          "name": "Simiao Wang"
        },
        {
          "authorId": "2108282310",
          "name": "Yijia Zhang"
        }
      ],
      "abstract": "Recent advancements in radiology report generation have utilized deep neural networks such as CNNs and Transformers, achieving notable improvements in generating accurate and detailed reports. However, their practical adoption is hindered by the challenge of balancing global dependency modeling with computational efficiency. The state space model, particularly its enhanced variant Mamba, offers promising linear-complexity solutions for long-range dependency modeling. Despite its strengths, Mamba\u2019s fixed positional encoding limits its ability to effectively capture complex spatial dependencies. To address this gap, we propose RRG-Mamba, an advanced framework for efficient radiology report generation. Within the RRGMamba, we enhance the vanilla Mamba by integrating rotary position encoding (RoPE), enabling dynamic modeling of relative positional information in visual feature sequences. Furthermore, we design a global dependency learning module to optimize long-range visual feature sequence modeling. Extensive experiments on publicly available datasets, including IU X-Ray and MIMIC-CXR, demonstrate that RRG-Mamba achieves a 3.7% improvement in BLEU-4 score over existing models, along with significant gains in computational and memory efficiency. Our code is available at https://github.com/Eleanorhxd/RRG-Mamba."
    },
    {
      "paperId": "99ec5caec2e6d4f9bd01b81fe47805ded3c4b5b8",
      "externalIds": {
        "ArXiv": "2511.04040",
        "DBLP": "conf/ijcai/0001C0J0LW25",
        "DOI": "10.24963/ijcai.2025/845",
        "CorpusId": 281488804
      },
      "corpusId": 281488804,
      "title": "Enhancing Multimodal Protein Function Prediction Through Dual-Branch Dynamic Selection with Reconstructive Pre-Training",
      "venue": "International Joint Conference on Artificial Intelligence",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.04040, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2311228541",
          "name": "Xiaoling Luo"
        },
        {
          "authorId": "2287850820",
          "name": "Peng Chen"
        },
        {
          "authorId": "2375099902",
          "name": "Chengliang Liu"
        },
        {
          "authorId": "2294861161",
          "name": "Xiaopeng Jin"
        },
        {
          "authorId": "2288171380",
          "name": "Jie Wen"
        },
        {
          "authorId": "2287011081",
          "name": "Yumeng Liu"
        },
        {
          "authorId": "2381405612",
          "name": "Junsong Wang"
        }
      ],
      "abstract": "Multimodal protein features play a crucial role in protein function prediction. However, these features encompass a wide range of information, ranging from structural data and sequence features to protein attributes and interaction networks, making it challenging to decipher their complex interconnections. In this work, we propose a multimodal protein function prediction method (DSRPGO) by utilizing dynamic selection and reconstructive pre-training mechanisms. To acquire complex protein information, we introduce reconstructive pre-training to mine more fine-grained information with low semantic levels. Moreover, we put forward the Bidirectional Interaction Module (BInM) to facilitate interactive learning among multimodal features. Additionally, to address the difficulty of hierarchical multi-label classification in this task, a Dynamic Selection Module (DSM) is designed to select the feature representation that is most conducive to current protein function prediction. Our proposed DSRPGO model improves significantly in BPO, MFO, and CCO on human datasets, thereby outperforming other benchmark models."
    },
    {
      "paperId": "8127170c891bf8c8f83b860bc0dbba33f96003a5",
      "externalIds": {
        "DBLP": "conf/ijcai/NieJB025",
        "DOI": "10.24963/ijcai.2025/195",
        "CorpusId": 281448174
      },
      "corpusId": 281448174,
      "title": "Towards Robust Deterministic and Probabilistic Modeling for Predictive Learning",
      "venue": "International Joint Conference on Artificial Intelligence",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.24963/ijcai.2025/195?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.24963/ijcai.2025/195, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2262444342",
          "name": "Xuesong Nie"
        },
        {
          "authorId": "2263355062",
          "name": "Haoyuan Jin"
        },
        {
          "authorId": "2275649799",
          "name": "Vijayakumar Bhagavatula"
        },
        {
          "authorId": "2333410592",
          "name": "Xiaofeng Liu"
        }
      ],
      "abstract": "Predictive modeling of unannotated spatiotemporal data presents inherent challenges, primarily due to the highly entangled visual dynamics in real-world scenes. To tackle these complexities, we introduce a novel insight through Disentangling Deterministic and Probabilistic (DDP) modeling. We note a key observation in spatiotemporal data where low-level details typically remain stable, whereas high-level motion frequently exhibits dynamic variations. The core motivation involves constructing two distinct pathways in the latent space: a deterministic path and a probabilistic path. The probabilistic path begins by defining the motion flow, which explicitly describes complex many-to-many motion patterns between patches, and models its probabilistic distribution using a motion diffuser. The deterministic path incorporates a spectral-aware enhancer to retain and amplify visual details in the frequency domain. These designs ensure visual consistency while also capturing intricate long-term motion dynamics. Extensive experiments demonstrate the superiority of DDP across diverse scenario evaluations."
    },
    {
      "paperId": "fc99a11b72189f7f0129a0e98831c98c7168928e",
      "externalIds": {
        "DBLP": "conf/ijcai/Luan0Z0YZ025",
        "DOI": "10.24963/ijcai.2025/189",
        "CorpusId": 281424686
      },
      "corpusId": 281424686,
      "title": "Rotation Invariant Spatial Networks for Single-View Point Cloud Classification",
      "venue": "International Joint Conference on Artificial Intelligence",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.24963/ijcai.2025/189?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.24963/ijcai.2025/189, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2329185698",
          "name": "Feng Luan"
        },
        {
          "authorId": "2260813951",
          "name": "Jiarui Hu"
        },
        {
          "authorId": "2280896178",
          "name": "Changshi Zhou"
        },
        {
          "authorId": "2260614650",
          "name": "Zhipeng Wang"
        },
        {
          "authorId": "2352657113",
          "name": "Jiguang Yue"
        },
        {
          "authorId": "144111479",
          "name": "Yanmin Zhou"
        },
        {
          "authorId": "2260639385",
          "name": "Bin He"
        }
      ],
      "abstract": "Point cloud classification is critical for three-dimensional scene understanding. However, in real-world scenarios, depth cameras often capture partial, single-view point clouds of objects with different poses, making their accurate classification a challenge. In this paper, we propose a novel point cloud classification network that captures the detailed spatial structure of objects by constructing tetrahedra, which is different from point-wise operations. Specifically, we propose a RISpaNet block to extract rotation-invariant features. A rotation-invariant property generation module is designed in RISpaNet for constructing rotation-invariant tetrahedron properties (RITPs). Meanwhile, a multi-scale pooling module and a hybrid encoder are used to process RITPs to generate integrated rotation-invariant features. Further, for single-view point clouds, a complete point cloud auxiliary branch and a part-whole correlation module are jointly employed to obtain complete point cloud features from partial point clouds. Experimental results show that this network performs better than other state-of-the-art methods, evaluated on four public datasets. We achieved an overall accuracy of 94.7% (+2.0%) on ModelNet40, 93.4% (+5.9%) on MVP, 94.7% (+6.3%) on PCN and 94.8% (+1.7%) on ScanObjectNN. Our project website is https://luxurylf.github.io/RISpaNet_project/."
    },
    {
      "paperId": "41fa44cfa0b1fee5b6abb6e20adbcd07ef42d5d1",
      "externalIds": {
        "DBLP": "conf/ijcai/ZhangL00025",
        "DOI": "10.24963/ijcai.2025/263",
        "CorpusId": 281458216
      },
      "corpusId": 281458216,
      "title": "Multimodal Prior Learning with Double Constraint Alignment for Snapshot Spectral Compressive Imaging",
      "venue": "International Joint Conference on Artificial Intelligence",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.24963/ijcai.2025/263?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.24963/ijcai.2025/263, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2351810939",
          "name": "Mingjin Zhang"
        },
        {
          "authorId": "2328655745",
          "name": "Longyi Li"
        },
        {
          "authorId": "2358813245",
          "name": "Fei Gao"
        },
        {
          "authorId": "2351814173",
          "name": "Qiming Zhang"
        },
        {
          "authorId": "1492038265",
          "name": "Jie-Ru Guo"
        }
      ],
      "abstract": "The objective of snapshot spectral compressive imaging reconstruction is to recover the 3D hyperspectral image (HSI) from a 2D measurement. Existing methods either focus on network architecture design or simply introduce image-level prior to the model. However, these methods lack guiding information for accurate reconstruction. Recognizing that textual description contain rich semantic information that can significantly enhance details, this paper introduces a novel framework, CAMM, which integrates text information into the model to improve the performance. The framework comprises two key components: Fine-grained Alignment Module (FAM) and Multimodal Fusion Mamba (MFM). Specifically, FAM is used to reduce the knowledge gap between the RGB domain obtained by the pre-trained vision-language model and the HSI domain. Through the double constraints of distribution similarity and entropy, the adaptive alignment of different complexity features is realized, which makes the encoded features more accurate. MFM aims to identify the guiding effect of RGB features and text features on HSI in space and channel dimensions. Instead of fusing features directly, it integrates prior at image-level and text-level prior into Mamba's state-space equation, so that each scanning step can be accurately guided. This kind of positive feedback adjustment ensures the authenticity of the guiding information. To our knowledge, this is the first text-guided model for compressive spectral imaging. Extensive experimental results the public datasets demonstrate the superior performance of CAMM, validating the effectiveness of our proposed method."
    },
    {
      "paperId": "d8c8240a3c3cdcba4ef2c1634f112c5a65940dd3",
      "externalIds": {
        "DBLP": "conf/ijcai/YangQHD25",
        "DOI": "10.24963/ijcai.2025/248",
        "CorpusId": 281449279
      },
      "corpusId": 281449279,
      "title": "DPMamba: Distillation Prompt Mamba for Multimodal Remote Sensing Image Classification with Missing Modalities",
      "venue": "International Joint Conference on Artificial Intelligence",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.24963/ijcai.2025/248?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.24963/ijcai.2025/248, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2290733255",
          "name": "Yueguang Yang"
        },
        {
          "authorId": "31123515",
          "name": "Jiahui Qu"
        },
        {
          "authorId": "2328345896",
          "name": "Ling Huang"
        },
        {
          "authorId": "2262111227",
          "name": "Wenqian Dong"
        }
      ],
      "abstract": "Multimodal remote sensing image classification (RSIC) has emerged as a key focus in Earth observation, driven by its capacity to extract complementary information from diverse sources. Existing methods struggle with modality absence caused by weather or equipment failures, leading to performance degradation. As a solution, knowledge distillation-based methods train student networks (SN) using a full-modality teacher, but they usually require training separate SN for each modality absence scenario, increasing complexity. To this end, we propose a unified Distillation Prompt Mamba (DPMamba) framework for multimodal RSIC with missing modalities. DPMamba leverages knowledge distillation in a shared text semantic space to optimize learnable prompts, transforming them from ``placeholder\" to ``adaptation\" states by enriching missing modality information with full-modality knowledge. To achieve this, we focus on two main aspects: first, we propose a new modality-aware Mamba for dynamically and hierarchically extracting cross-modality interactive features, providing richer, contextually relevant representations for backpropagation-based optimization of prompts; and second, we introduce a novel text-bridging distillation method to efficiently transfer full-modality knowledge, guiding the inclusion of missing modality information into prompts. Extensive evaluations demonstrate the effectiveness and robustness of the proposed DPMamba."
    },
    {
      "paperId": "034777dec95df8893efcc34e253692b3370bc605",
      "externalIds": {
        "PubMedCentral": "12473383",
        "DOI": "10.3390/s25185729",
        "CorpusId": 281367212,
        "PubMed": "41012967"
      },
      "corpusId": 281367212,
      "title": "Hybrid MambaVision and Transformer-Based Architecture for 3D Lane Detection",
      "venue": "Italian National Conference on Sensors",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12473383, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2380904315",
          "name": "Raul-Mihai Cap"
        },
        {
          "authorId": "2646035",
          "name": "C\u0103lin-Adrian Popa"
        }
      ],
      "abstract": "Lane detection is an essential task in the field of computer vision and autonomous driving. This involves identifying and locating road markings on the road surface. This capability not only helps drivers keep the vehicle in the correct lane, but also provides critical data for advanced driver assistance systems and autonomous vehicles. Traditional lane detection models work mainly on the 2D image plane and achieve remarkable results. However, these models often assume a flat-world scenario, which does not correspond to real-world conditions, where roads have elevation variations and road markings may be curved. Our approach solves this challenge by focusing on 3D lane detection without relying on the inverse perspective mapping technique. Instead, we introduce a new framework using the MambaVision-S-1K backbone, which combines Mamba-based processing with Transformer capabilities to capture both local detail and global contexts from monocular images. This hybrid approach allows accurate modeling of lane geometry in three dimensions, even in the presence of elevation variations. By replacing the traditional convolutional neural network backbone with MambaVision, our proposed model significantly improves the capability of 3D lane detection systems. Our method achieved state-of-the-art performance on the ONCE-3DLanes dataset, thus demonstrating its superiority in accurately capturing lane curvature and elevation variations. These results highlight the potential of integrating advanced backbones based on Vision Transformers in the field of autonomous driving for more robust and reliable lane detection. The code will be available online."
    },
    {
      "paperId": "b0960805d5c87e04cfd5aa63d758d4f5765e9584",
      "externalIds": {
        "DBLP": "journals/entropy/ChenLG25",
        "PubMedCentral": "12468563",
        "DOI": "10.3390/e27090959",
        "CorpusId": 281376622,
        "PubMed": "41008085"
      },
      "corpusId": 281376622,
      "title": "Reconstructing Hyperspectral Images from RGB Images by Multi-Scale Spectral\u2013Spatial Sequence Learning",
      "venue": "Entropy",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 1,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12468563, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "1914607085",
          "name": "Wenjing Chen"
        },
        {
          "authorId": "2380867266",
          "name": "Lang Liu"
        },
        {
          "authorId": "2157314411",
          "name": "Rong Gao"
        }
      ],
      "abstract": "With rapid advancements in transformers, the reconstruction of hyperspectral images from RGB images, also known as spectral super-resolution (SSR), has made significant breakthroughs. However, existing transformer-based methods often struggle to balance computational efficiency with long-range receptive fields. Recently, Mamba has demonstrated linear complexity in modeling long-range dependencies and shown broad applicability in vision tasks. This paper proposes a multi-scale spectral\u2013spatial sequence learning method, named MSS-Mamba, for reconstructing hyperspectral images from RGB images. First, we introduce a continuous spectral\u2013spatial scan (CS3) mechanism to improve cross-dimensional feature extraction of the foundational Mamba model. Second, we propose a sequence tokenization strategy that generates multi-scale-aware sequences to overcome Mamba\u2019s limitations in hierarchically learning multi-scale information. Specifically, we design the multi-scale information fusion (MIF) module, which tokenizes input sequences before feeding them into Mamba. The MIF employs a dual-branch architecture to process global and local information separately, dynamically fusing features through an adaptive router that generates weighting coefficients. This produces feature maps that contain both global contextual information and local details, ultimately reconstructing a high-fidelity hyperspectral image. Experimental results on the ARAD_1k, CAVE and grss_dfc_2018 dataset demonstrate the performance of MSS-Mamba."
    },
    {
      "paperId": "86dcdd7562bb2014d4fe89b8892f44fcc842fdd2",
      "externalIds": {
        "PubMedCentral": "12664985",
        "DOI": "10.1016/j.patter.2025.101373",
        "CorpusId": 281640053,
        "PubMed": "41328162"
      },
      "corpusId": 281640053,
      "title": "Pan-microalgal dark proteome mapping via interpretable deep learning and synthetic chimeras",
      "venue": "Patterns",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 1,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12664985, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2330367354",
          "name": "David R. Nelson"
        },
        {
          "authorId": "2163023490",
          "name": "Ashish Kumar Jaiswal"
        },
        {
          "authorId": "2382645310",
          "name": "Noha Samir Ismail"
        },
        {
          "authorId": "7783956",
          "name": "Alexandra Mystikou"
        },
        {
          "authorId": "1398644838",
          "name": "K. Salehi-Ashtiani"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "4b036f3ea00ecdfb3c1ac44eddc0fc9dc6bd3a15",
      "externalIds": {
        "DBLP": "conf/ijcai/SuHY025",
        "DOI": "10.24963/ijcai.2025/209",
        "CorpusId": 281461993
      },
      "corpusId": 281461993,
      "title": "CMFS: CLIP-Guided Modality Interaction for Mitigating Noise in Multi-Modal Image Fusion and Segmentation",
      "venue": "International Joint Conference on Artificial Intelligence",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.24963/ijcai.2025/209?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.24963/ijcai.2025/209, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2381585801",
          "name": "Guilin Su"
        },
        {
          "authorId": "2200064016",
          "name": "Yuqing Huang"
        },
        {
          "authorId": "2203365832",
          "name": "Chao Yang"
        },
        {
          "authorId": "2244610261",
          "name": "Zhenyu He"
        }
      ],
      "abstract": "Infrared-visible image fusion and semantic segmentation are pivotal tasks for robust scene understanding under challenging conditions such as low light. However, existing methods often struggle with high noise, modality inconsistencies, and inefficient cross-modal interactions, limiting fusion quality and segmentation accuracy. To this end, we propose CMFS, a unified framework that leverages CLIP-guided modality interaction to mitigate noise in multi-modal image fusion and segmentation. Our approach features a region-aware Modal Interaction Alignment module that combines a VMamba-based encoder with an additional shuffle layer to obtain more robust features and a CLIP-guided, regionally constrained multi-modal feature interaction block to emphasize foreground targets while suppressing low-light noise. Additionally, a Frequency-Spatial Collaboration module uses selective scanning and integrates wavelet-, spatial-, and Fourier-domain features to achieve adaptive denoising and balanced feature allocation. Furthermore, we employ a low-rank mixture-of-experts with dynamic routing to improve region-specific fusion and enhance pixel-level accuracy. Extensive experiments on several benchmarks show that, compared with state-of-the-art methods, the proposed approach demonstrates effectiveness in both image fusion quality and semantic segmentation accuracy, especially in complex environments. The source code will be released at IJCAI2025-CMFS."
    },
    {
      "paperId": "a614166eebdc48f283e6fcdabbdc90fcac71b136",
      "externalIds": {
        "DOI": "10.1088/1742-6596/3101/1/012011",
        "CorpusId": 281731754
      },
      "corpusId": 281731754,
      "title": "MAS4SAM: Multi-Template Adaptive Screening for Visual Object Tracking with SAM2",
      "venue": "Journal of Physics: Conference Series",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1088/1742-6596/3101/1/012011?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1088/1742-6596/3101/1/012011, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2383415388",
          "name": "Yuxuan Li"
        },
        {
          "authorId": "2383949376",
          "name": "Mengyuan Liu"
        }
      ],
      "abstract": "The template is paramount for visual object tracking, which is treated as an object model to recognize the tracked object in the following images. Thanks to Segment Anything Model 2 for providing the powerful capability to perform feature extraction and fusion on multi-frame images, achieving accurate object tracking. However, since it uses consecutive adjacent frames as templates, the problem of error accumulation is more likely to occur during the tracking process, which limits its long-term tracking performance. Therefore, we first demonstrate through extensive experiments that the tracking performance can be significantly improved by using a naive template filtering mechanism. Subsequently, inspired by this experimental result, based on the principle of reducing cumulative error, a multi-level memory screening structure is designed to form a dynamic template set. In the LaSOT Test dataset, without any model fine-tuning, our method ultimately achieves AUC scores of 74.7%, 72.63%, 72.18%, and 71.72% with the large, base plus, small and tiny model, respectively, surpassing the SAMURAI method and approaching the performance of DAM4SAM. Meanwhile, we also evaluated performance on the more challenging LaSOText dataset, where MAS4SAM outperforms the original SAM2 by 4.61%, 3.8%, 3.07%, and 4.59% across the four backbone scales, respectively. Furthermore, it yields more significant improvements than the fixed threshold method in both subsets of LaSOT, indicating that MAS4SAM exhibits adaptive behavior in template screening, thus improving the long-term tracking performance of SAM2 in a wide range of scenarios. The code is released at https://github.com/Cannol/MAS4SAM."
    },
    {
      "paperId": "58fea9fdc59c9d9902ef2dfee64defd86d14ad14",
      "externalIds": {
        "DBLP": "journals/kbs/BaiDWYZYQ25",
        "DOI": "10.1016/j.knosys.2025.114505",
        "CorpusId": 281812153
      },
      "corpusId": 281812153,
      "title": "Hierarchy-induced dual-channel tokenized graph learning",
      "venue": "Knowledge-Based Systems",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.knosys.2025.114505?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.knosys.2025.114505, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2384073775",
          "name": "Huiwen Bai"
        },
        {
          "authorId": "2316790611",
          "name": "Lizhong Ding"
        },
        {
          "authorId": "2168989170",
          "name": "Guoren Wang"
        },
        {
          "authorId": "14886336",
          "name": "Ye Yuan"
        },
        {
          "authorId": "2383805277",
          "name": "Junyu Zhang"
        },
        {
          "authorId": "2362296753",
          "name": "Yuwan Yang"
        },
        {
          "authorId": "2075177467",
          "name": "Lianpeng Qiao"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "69a14a0613d5d5413ca80fe13ef12ade30aca91d",
      "externalIds": {
        "DOI": "10.1101/2025.08.27.672600",
        "CorpusId": 282305529
      },
      "corpusId": 282305529,
      "title": "Multi-Scale Anti-Correlated Neural States Dominate Naturalistic Whole-Brain Activity",
      "venue": "bioRxiv",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1101/2025.08.27.672600?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1101/2025.08.27.672600, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2281834894",
          "name": "Dora G\u00f6z\u00fckara"
        },
        {
          "authorId": "117267021",
          "name": "Djamari Oetringer"
        },
        {
          "authorId": "2351411876",
          "name": "Nasir Ahmad"
        },
        {
          "authorId": "2278600992",
          "name": "Linda Geerligs"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "d91b0fc81cd728ac48073d4a088c1e24f45281b7",
      "externalIds": {
        "DBLP": "journals/apin/ChenWPCDHX25",
        "DOI": "10.1007/s10489-025-06816-4",
        "CorpusId": 281669618
      },
      "corpusId": 281669618,
      "title": "Multi-view contrastive learning with Static attributes and Dynamic interests for Sequential Recommendation",
      "venue": "Applied intelligence (Boston)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10489-025-06816-4?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10489-025-06816-4, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2108227988",
          "name": "Mukun Chen"
        },
        {
          "authorId": "2248803714",
          "name": "Jia Wu"
        },
        {
          "authorId": "2296438593",
          "name": "Shirui Pan"
        },
        {
          "authorId": "2274151429",
          "name": "Xiantao Cai"
        },
        {
          "authorId": "2248204457",
          "name": "Bo Du"
        },
        {
          "authorId": "2248467517",
          "name": "Wenbin Hu"
        },
        {
          "authorId": "2335176954",
          "name": "Huiting Xu"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "b8b3d3590689c761dff50c04d78e9a9df07ca30e",
      "externalIds": {
        "DOI": "10.1007/s12555-025-0383-0",
        "CorpusId": 282557138
      },
      "corpusId": 282557138,
      "title": "HFBTM: Hybrid Framework for Boiler Temperature Modeling via Static-dynamic Feature Decoupling and Weighted Fusion",
      "venue": "International Journal of Control, Automation and Systems",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s12555-025-0383-0?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s12555-025-0383-0, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "90186084",
          "name": "Yusen Gang"
        },
        {
          "authorId": "2267117787",
          "name": "Chen Peng"
        },
        {
          "authorId": "71659951",
          "name": "Chuanliang Cheng"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "3fbc05ad542cef433b21b80c60df0c6a068ca9d8",
      "externalIds": {
        "DOI": "10.1117/1.JMI.12.5.054003",
        "CorpusId": 281722987,
        "PubMed": "41035538"
      },
      "corpusId": 281722987,
      "title": "DMM-UNet: dual-path multi-scale Mamba UNet for medical image segmentation",
      "venue": "Journal of Medical Imaging",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1117/1.JMI.12.5.054003?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1117/1.JMI.12.5.054003, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2244775744",
          "name": "Liquan Zhao"
        },
        {
          "authorId": "2383464949",
          "name": "Mingxia Cao"
        },
        {
          "authorId": "48928376",
          "name": "Yanfei Jia"
        }
      ],
      "abstract": "Abstract. Purpose State space models have shown promise in medical image segmentation by modeling long-range dependencies with linear complexity. However, they are limited in their ability to capture local features, which hinders their capacity to extract multiscale details and integrate global and local contextual information effectively. To address these shortcomings, we propose the dual-path multi-scale Mamba UNet (DMM-UNet) model. Approach This architecture facilitates deep fusion of local and global features through multi-scale modules within a U-shaped encoder\u2013decoder framework. First, we introduce the multi-scale channel attention selective scanning block in the encoder, which combines global selective scanning with multi-scale channel attention to model both long-range and local dependencies simultaneously. Second, we design the spatial attention selective scanning block for the decoder. This block integrates global scanning with spatial attention mechanisms, enabling precise aggregation of semantic features through gated weighting. Finally, we develop the multi-dimensional collaborative attention layer to extract complementary attention weights across height, width, and channel dimensions, facilitating cross-space-channel feature interactions. Results Experiments were conducted on the ISIC17, ISIC18, Synapse, and ACDC datasets. One of the indicators, Dice similarity coefficient, achieved 89.88% on the ISIC17 dataset, 90.52% on the ISIC18 dataset, 83.07% on the Synapse dataset, and 92.60% on the ACDC dataset. There are also other indicators that perform well on this model. Conclusions The DMM-UNet model effectively addresses the shortcomings of state space models by enabling the integration of both local and global features, improving segmentation performance, and offering enhanced multiscale feature fusion for medical image segmentation tasks."
    },
    {
      "paperId": "ecfc29f4f840242b0e4f5a92358e97b55b7b221b",
      "externalIds": {
        "DBLP": "journals/tjs/LiuLHH25",
        "DOI": "10.1007/s11227-025-07815-5",
        "CorpusId": 281298539
      },
      "corpusId": 281298539,
      "title": "Enhanced road object detection with DFPD-YOLO: focusing on small and occluded targets",
      "venue": "Journal of Supercomputing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11227-025-07815-5?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11227-025-07815-5, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2380308640",
          "name": "Zifeng Liu"
        },
        {
          "authorId": "2380632070",
          "name": "Lujiao Li"
        },
        {
          "authorId": "2380212186",
          "name": "Yongbin Hu"
        },
        {
          "authorId": "2380417319",
          "name": "Shigang Hu"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "63a41ea6e555d5abb17847d7ba76ba49098d816b",
      "externalIds": {
        "DBLP": "journals/jei/LiuCZC25",
        "DOI": "10.1117/1.JEI.34.5.053030",
        "CorpusId": 282052273
      },
      "corpusId": 282052273,
      "title": "Noise and shuffle feature perturbation driven robust curve segmentation",
      "venue": "J. Electronic Imaging",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1117/1.JEI.34.5.053030?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1117/1.JEI.34.5.053030, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2385373944",
          "name": "Jinghao Liu"
        },
        {
          "authorId": "2338494657",
          "name": "Li Chen"
        },
        {
          "authorId": "2339454418",
          "name": "Kai Zhu"
        },
        {
          "authorId": "2323187103",
          "name": "Yunxiang Cao"
        }
      ],
      "abstract": "Abstract. Curve structure segmentation is a crucial field of semantic segmentation, with a wide range of applications in blood vessel segmentation, crack detection, and other fields. Current mainstream curve structure segmentation models typically adopt an integrated architecture design, and their performance improvement tends to rely on the redesign of the model structure or the increase in complexity. To address this issue, we propose a lightweight feature perturbation mechanism. This mechanism enables flexible integration with existing segmentation models and incurs no additional computational complexity overhead. By introducing noise into the feature space, which is combined with the original features of the input image, the model exhibits improved robustness by preserving essential information in the feature representation. We incorporate shuffle operations into the local self-attention mechanism, offering a way to perturb features. This method breaks the fixed associations between pixels and enhances the diversity of features. Then, it is combined with the Mamba model. As a result, the model can more accurately capture the local details and global context information of the curve structures. The experimental results show that the proposed method has made certain improvements over the existing state-of-the-art methods on multiple standard datasets, providing an effective method for curve structure segmentation tasks."
    },
    {
      "paperId": "0d61dfe515a6fe9d3d0ba159a37c3db0e3f18ec9",
      "externalIds": {
        "DBLP": "journals/jei/ChenWLW25",
        "DOI": "10.1117/1.JEI.34.5.053022",
        "CorpusId": 281689186
      },
      "corpusId": 281689186,
      "title": "KeyMamba: keyframe-enhanced state space model for efficient temporal action detection",
      "venue": "J. Electronic Imaging",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1117/1.JEI.34.5.053022?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1117/1.JEI.34.5.053022, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2383947547",
          "name": "Zikai Chen"
        },
        {
          "authorId": "2376490415",
          "name": "Dan Wei"
        },
        {
          "authorId": "2383376911",
          "name": "Peixing Li"
        },
        {
          "authorId": "2378202655",
          "name": "Xiaolan Wang"
        }
      ],
      "abstract": "Abstract. Temporal action detection (TAD) is a challenging task in the field of video understanding. We determine the semantic labels and precise boundaries of each action instance in an untrimmed video. Over the years, a variety of networks have been proposed, including convolution, graph, and transformer, which have been effectively applied in TAD tasks. Most of the methods have been able to identify the action category well; however, the accuracy of determining the action boundary is still insufficient. Because an action contains several consecutive frames of similar images, we recommend picking out the key frames in the video sequence and enhancing the TAD representation by extracting additional features of the key frames. We propose KeyMamba, a state-space model-based learnable network for TAD tasks. The proposed model applies a bidirectional Mamba block to capture global features efficiently. We also added a temporal deformable attention module to extract key frame features from video clips. These features contain the information of motion changes, and the key frame features complement the global features, which can identify the video action boundaries more accurately. In addition, to get a higher quality Token in the spatial dimension, we added an attention mask before the bidirectional Mamba block encoder. Finally, we also apply masking operations during the forward and backward scanning processes within the bidirectional Mamba block to mitigate the impact of duplicate tokens. Our experiments have achieved outstanding performance on the THUMOS14 and ActivityNet-1.3 datasets, reaching an average mAP of 70.4 on THUMOS14 and an average mAP of 38.44 on ActivityNet-1.3."
    },
    {
      "paperId": "7e3c32a4c385b33b6a1f8763047a3270e28fdd62",
      "externalIds": {
        "DBLP": "journals/jei/LongZXHWL25",
        "DOI": "10.1117/1.JEI.34.5.053029",
        "CorpusId": 281957638
      },
      "corpusId": 281957638,
      "title": "IM-YOLO: an improved Mamba-YOLO-based model for small object detection in aerial images",
      "venue": "J. Electronic Imaging",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1117/1.JEI.34.5.053029?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1117/1.JEI.34.5.053029, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2356097578",
          "name": "Fan Long"
        },
        {
          "authorId": "2284231915",
          "name": "Jing Zhang"
        },
        {
          "authorId": "2356124213",
          "name": "Rui Xia"
        },
        {
          "authorId": "2384871513",
          "name": "Jianwen Huo"
        },
        {
          "authorId": "2356227517",
          "name": "Aimin Wang"
        },
        {
          "authorId": "2311129307",
          "name": "Heng Liu"
        }
      ],
      "abstract": "Abstract. To address key challenges in small object detection for unmanned aerial vehicle (UAV) imagery, such as illumination variations, insufficient representation of shallow-layer features, and the complexity of multiscale object detection, we propose an enhanced Mamba-YOLO model, termed IM-YOLO. Built upon the conventional Mamba-YOLO architecture, IM-YOLO incorporates an image preprocessing enhancement module to adaptively optimize image quality under varying illumination conditions. In addition, a spatial-channel synergistic attention is integrated to enhance the feature representation of shallow-layer small objects while suppressing background interference. A feature complementary fusion module is designed to dynamically integrate high-resolution details with global semantic features, significantly improving detection robustness in dense multiscale scenarios. On the VisDrone2019 dataset, IM-YOLO achieves mean average precision (mAP) values of 53.7% and 33.6%, representing improvements of 3.5% and 2.7% over the baseline model, respectively, while maintaining an inference speed of 187.3 frames per second to meet real-time detection requirements. On the DIOR and UAVDT datasets, IM-YOLO achieves mAP values of 86.4% and 65.3%, and 51.1% and 31.5%, respectively. Experiments across multiple datasets demonstrate that the proposed model exhibits superior detection performance compared with existing mainstream methods. Furthermore, detection experiments using real-world UAV imagery confirm IM-YOLO\u2019s superior small object detection performance across diverse complex scenes, validating its robustness and adaptability in practical settings. These findings establish IM-YOLO as an efficient and viable solution for UAV vision tasks."
    },
    {
      "paperId": "32e56d507ac943d996ffe4500b33c97770a1c5cb",
      "externalIds": {
        "DOI": "10.23919/metar.2025.000013",
        "CorpusId": 282766625
      },
      "corpusId": 282766625,
      "title": "Controlled Modeling of Pulp Level in Copper Flotation Process on the Selective State Spaces Model",
      "venue": "MetaResource",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.23919/metar.2025.000013?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.23919/metar.2025.000013, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2351221239",
          "name": "Haowei Chen"
        },
        {
          "authorId": "2158607767",
          "name": "Xiaorui Li"
        },
        {
          "authorId": "2353208929",
          "name": "Zhaolin Yuan"
        },
        {
          "authorId": "2390875874",
          "name": "Ligang Yang"
        },
        {
          "authorId": "2390959224",
          "name": "Xizhen Yuan"
        },
        {
          "authorId": "2393994137",
          "name": "Hongning Dai"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "10092b0c552ef9a9c108253776fec228c4354b52",
      "externalIds": {
        "DOI": "10.1109/Cyber-AI66431.2025.11233569",
        "CorpusId": 283015632
      },
      "corpusId": 283015632,
      "title": "Efficient End-to-End Intrusion Detection with Packet and Flow-Level Mamba Sequence Models",
      "venue": "2025 International Conference on Cybersecurity and AI-Based Systems (Cyber-AI)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/Cyber-AI66431.2025.11233569?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/Cyber-AI66431.2025.11233569, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2392430873",
          "name": "Benedikt Pletzer"
        },
        {
          "authorId": "2326041302",
          "name": "J\u00fcrgen Mottok"
        }
      ],
      "abstract": "The increasing interconnection of critical infrastructure systems has amplified the need for robust and adaptive intrusion detection mechanisms. Traditional network-based intrusion detection systems rely on signature matching or predefined features, making them vulnerable to novel attack strategies and evasion techniques. This work proposes a two-stage model designed to classify network traffic directly from raw data without relying on manual feature engineering or packet cropping. Drawing inspiration from recent advances in natural language processing, the proposed model employs a packet encoder to extract fixed-size embeddings and a sequence model to evaluate traffic at the flow-level. The approach is computationally efficient and well-suited for operational technology environments, enabling accurate detection of attacks while maintaining inference speed. Experimental evaluation demonstrates competitive classification performance and efficient handling of long sequences without sacrificing throughput."
    },
    {
      "paperId": "4d402a77192f4d74ae3d040c60a9502e884d9540",
      "externalIds": {
        "DOI": "10.1111/mice.70059",
        "CorpusId": 281078388
      },
      "corpusId": 281078388,
      "title": "Multimodal Mamba with multitask learning for building flood damage assessment using synthetic aperture radar remote sensing imagery",
      "venue": "Computer-Aided Civil and Infrastructure Engineering",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1111/mice.70059?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1111/mice.70059, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2253675815",
          "name": "Yu-Hsuan Ho"
        },
        {
          "authorId": "2308649960",
          "name": "Ali Mostafavi"
        }
      ],
      "abstract": "Most post\u2010disaster damage classifiers perform best when destructive forces leave clear spectral or structural signatures. However, these signatures are often subtle or absent after inundation, where damage may be nonstructural and difficult to detect. Consequently, existing models perform poorly at identifying flood\u2010related building damage. The model presented in this study, Flood\u2010DamageSense, addresses this gap as the first deep learning framework purpose\u2010built for building\u2010level flood\u2010damage assessment. The architecture fuses pre\u2010 and post\u2010event synthetic aperture radar/interferometric synthetic aperture radar (SAR/InSAR) scenes with very high\u2010resolution optical basemaps and an inherent flood\u2010risk layer that encodes long\u2010term exposure probabilities, guiding the network toward plausibly affected structures even when compositional change is minimal. A multimodal Mamba backbone with a semi\u2010Siamese encoder and task\u2010specific decoders jointly predicts (1) graded building\u2010damage states, (2) floodwater extent, and (3) building footprints. Training and evaluation on Hurricane Harvey (2017) imagery from Harris County, Texas\u2014supported by insurance\u2010derived property\u2010damage extents\u2014show a mean F1 improvement of up to 19 percentage points over state\u2010of\u2010the\u2010art baselines, with the largest gains in the frequently misclassified \u201cminor\u201d and \u201cmoderate\u201d damage categories. Ablation studies identify the inherent\u2010risk feature as the single most significant contributor to this performance boost. An end\u2010to\u2010end post\u2010processing pipeline converts pixel\u2010level outputs to actionable, building\u2010scale damage maps within minutes of image acquisition. By combining risk\u2010aware modeling with SAR's all\u2010weather capability, Flood\u2010DamageSense delivers faster, finer\u2010grained, and more reliable flood\u2010damage intelligence to support post\u2010disaster decision\u2010making and resource allocation."
    },
    {
      "paperId": "c85ae97a9e16c080aeb7fcc27bb42c21fe4211a9",
      "externalIds": {
        "ArXiv": "2509.00917",
        "DBLP": "journals/corr/abs-2509-00917",
        "DOI": "10.48550/arXiv.2509.00917",
        "CorpusId": 281079200
      },
      "corpusId": 281079200,
      "title": "DarkVRAI: Capture-Condition Conditioning and Burst-Order Selective Scan for Low-light RAW Video Denoising",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.00917, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2334697223",
          "name": "Youngjin Oh"
        },
        {
          "authorId": "2374119595",
          "name": "Junhyeong Kwon"
        },
        {
          "authorId": "2377240951",
          "name": "Junyoung Park"
        },
        {
          "authorId": "2376538204",
          "name": "Nam Ik Cho"
        }
      ],
      "abstract": "Low-light RAW video denoising is a fundamentally challenging task due to severe signal degradation caused by high sensor gain and short exposure times, which are inherently limited by video frame rate requirements. To address this, we propose DarkVRAI, a novel framework that achieved first place in the AIM 2025 Low-light RAW Video Denoising Challenge. Our method introduces two primary contributions: (1) a successful application of a conditioning scheme for image denoising, which explicitly leverages capture metadata, to video denoising to guide the alignment and denoising processes, and (2) a Burst-Order Selective Scan (BOSS) mechanism that effectively models long-range temporal dependencies within the noisy video sequence. By synergistically combining these components, DarkVRAI demonstrates state-of-the-art performance on a rigorous and realistic benchmark dataset, setting a new standard for low-light video denoising."
    },
    {
      "paperId": "39528bd930a4796d95e78b18d390fa73f776c00c",
      "externalIds": {
        "ArXiv": "2509.00975",
        "DBLP": "journals/corr/abs-2509-00975",
        "DOI": "10.48550/arXiv.2509.00975",
        "CorpusId": 281080434
      },
      "corpusId": 281080434,
      "title": "Self-Exploring Language Models for Explainable Link Forecasting on Temporal Graphs via Reinforcement Learning",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.00975, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2363673843",
          "name": "Zifeng Ding"
        },
        {
          "authorId": "2299748906",
          "name": "Shenyang Huang"
        },
        {
          "authorId": "2379612531",
          "name": "Zeyu Cao"
        },
        {
          "authorId": "2313365177",
          "name": "Emma Kondrup"
        },
        {
          "authorId": "2378718002",
          "name": "Zachary Yang"
        },
        {
          "authorId": "2370946595",
          "name": "Xingyue Huang"
        },
        {
          "authorId": "2378659831",
          "name": "Yuan Sui"
        },
        {
          "authorId": "2187737754",
          "name": "Moy Yuan"
        },
        {
          "authorId": "2189566506",
          "name": "Yuqicheng Zhu"
        },
        {
          "authorId": "2363678366",
          "name": "Xianglong Hu"
        },
        {
          "authorId": "2315916031",
          "name": "Yuan He"
        },
        {
          "authorId": "2282960184",
          "name": "Farimah Poursafaei"
        },
        {
          "authorId": "2315811771",
          "name": "Michael Bronstein"
        },
        {
          "authorId": "2273888020",
          "name": "Andreas Vlachos"
        }
      ],
      "abstract": "Forecasting future links is a central task in temporal graph (TG) reasoning, requiring models to leverage historical interactions to predict upcoming ones. Traditional neural approaches, such as temporal graph neural networks, achieve strong performance but lack explainability and cannot be applied to unseen graphs without retraining. Recent studies have begun to explore using large language models (LLMs) for graph reasoning, but most of them are constrained to static graphs or small synthetic TGs and lack the evaluation of the quality of reasoning traces generated by LLMs. In this work, we present Reasoning-Enhanced Learning for Temporal Graphs (ReaL-TG), a reinforcement learning framework that fine-tunes LLMs to perform explainable link forecasting on real-world TGs. ReaL-TG uses outcome-based reward to encourage models to self-explore reasoning strategies from graph structure and to produce explanations that directly justify their predictions. To enable evaluation on LLM-generated reasoning traces, we propose a new evaluation protocol combining ranking metrics with an LLM-as-a-Judge system that assesses both the quality of reasoning and the impact of hallucinations. Experiments with ReaL-TG-4B, obtained by fine-tuning Qwen3-4B under our framework, show that it outperforms much larger frontier LLMs, including GPT-5 mini, on ranking metrics, while producing high-quality explanations confirmed by both the LLM judge and human evaluation."
    },
    {
      "paperId": "5f3290f76f9c979302b853ce9744a7cf12d0d3bf",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-00677",
        "ArXiv": "2509.00677",
        "DOI": "10.48550/arXiv.2509.00677",
        "CorpusId": 281080876
      },
      "corpusId": 281080876,
      "title": "CSFMamba: Cross State Fusion Mamba Operator for Multimodal Remote Sensing Image Classification",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.00677, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2378715123",
          "name": "Qingyu Wang"
        },
        {
          "authorId": "2384799408",
          "name": "Xue Jiang"
        },
        {
          "authorId": "2261279530",
          "name": "Guozheng Xu"
        }
      ],
      "abstract": "Multimodal fusion has made great progress in the field of remote sensing image classification due to its ability to exploit the complementary spatial-spectral information. Deep learning methods such as CNN and Transformer have been widely used in these domains. State Space Models recently highlighted that prior methods suffer from quadratic computational complexity. As a result, modeling longer-range dependencies of spatial-spectral features imposes an overwhelming burden on the network. Mamba solves this problem by incorporating time-varying parameters into ordinary SSM and performing hardware optimization, but it cannot perform feature fusion directly. In order to make full use of Mamba's low computational burden and explore the potential of internal structure in multimodal feature fusion, we propose Cross State Fusion Mamba (CSFMamba) Network. Specifically, we first design the preprocessing module of remote sensing image information for the needs of Mamba structure, and combine it with CNN to extract multi-layer features. Secondly, a cross-state module based on Mamba operator is creatively designed to fully fuse the feature of the two modalities. The advantages of Mamba and CNN are combined by designing a more powerful backbone. We capture the fusion relationship between HSI and LiDAR modalities with stronger full-image understanding. The experimental results on two datasets of MUUFL and Houston2018 show that the proposed method outperforms the experimental results of Transformer under the premise of reducing the network training burden."
    },
    {
      "paperId": "475b1e6491fe4c23f47abfcaa5bbf92d22aaf034",
      "externalIds": {
        "ArXiv": "2509.00935",
        "DBLP": "journals/corr/abs-2509-00935",
        "DOI": "10.48550/arXiv.2509.00935",
        "CorpusId": 281079817
      },
      "corpusId": 281079817,
      "title": "SCOUT: Toward Sub-Quadratic Attention via Segment Compression for Optimized Utility in Transformers",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.00935, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "31036999",
          "name": "A. Jafari"
        },
        {
          "authorId": "2379173936",
          "name": "Yuhe Fan"
        },
        {
          "authorId": "2329091811",
          "name": "Benyamin Jamialahmadi"
        },
        {
          "authorId": "2140491231",
          "name": "Parsa Farinneya"
        },
        {
          "authorId": "2237517964",
          "name": "Boxing Chen"
        },
        {
          "authorId": "1996315",
          "name": "Marzieh S. Tahaei"
        }
      ],
      "abstract": "Transformers have demonstrated strong performance across a wide range of sequence modeling tasks, but their quadratic attention complexity limits scalability to long sequences. Linear models such as Mamba and sliding-window attention (SWA) address this by mixing tokens through recurrent or localized operations with fixed-size memory, achieving efficient inference. However, these methods risk degrading performance on long sequences due to their inability to retain detailed information from distant tokens. We propose SCOUT (Segment Compression for Optimized Utility in Transformers), a hybrid architecture that compresses tokens locally within fixed-size segments and applies attention only over these compressed representations. Each token embedding is first enriched via a linear local mixer, Mamba or SWA, that integrates recent context. Then, instead of attending to all previous tokens, each token sparsely attends to a small number of compressed checkpoint tokens that summarize the input history. This design retains much of the expressivity of full attention while substantially reducing the computational and memory cost. By attending to compressed history rather than all previous tokens, SCOUT incurs slightly higher memory than purely linear models, but its growth rate remains sub-quadratic and far more scalable than that of full Transformers. We analyze SCOUT's computational and memory efficiency and evaluate it empirically on long-context language modeling and reasoning tasks. SCOUT with both Mamba and SWA mixers outperforms strong long-sequence baselines under the same computational budget, matches full-attention Transformers on language modeling and common-sense reasoning tasks at 400M and 1.3B scales. Moreover, our SCOUT achieves higher end-to-end throughput than SOTA models, while delivering comparable results on long sequence benchmarks."
    },
    {
      "paperId": "fca4628656be47aa8342fbc2ac9ec9e6e12253e7",
      "externalIds": {
        "ArXiv": "2509.00653",
        "DBLP": "journals/corr/abs-2509-00653",
        "DOI": "10.48550/arXiv.2509.00653",
        "CorpusId": 281079396
      },
      "corpusId": 281079396,
      "title": "IndiaWeatherBench: A Dataset and Benchmark for Data-Driven Regional Weather Forecasting over India",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.00653, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2175303602",
          "name": "Tung Nguyen"
        },
        {
          "authorId": "2274100424",
          "name": "Harkanwar Singh"
        },
        {
          "authorId": "2197483945",
          "name": "Nilay Naharas"
        },
        {
          "authorId": "2091417351",
          "name": "Lucas Bandarkar"
        },
        {
          "authorId": "2259896660",
          "name": "Aditya Grover"
        }
      ],
      "abstract": "Regional weather forecasting is a critical problem for localized climate adaptation, disaster mitigation, and sustainable development. While machine learning has shown impressive progress in global weather forecasting, regional forecasting remains comparatively underexplored. Existing efforts often use different datasets and experimental setups, limiting fair comparison and reproducibility. We introduce IndiaWeatherBench, a comprehensive benchmark for data-driven regional weather forecasting focused on the Indian subcontinent. IndiaWeatherBench provides a curated dataset built from high-resolution regional reanalysis products, along with a suite of deterministic and probabilistic metrics to facilitate consistent training and evaluation. To establish strong baselines, we implement and evaluate a range of models across diverse architectures, including UNets, Transformers, and Graph-based networks, as well as different boundary conditioning strategies and training objectives. While focused on India, IndiaWeatherBench is easily extensible to other geographic regions. We open-source all raw and preprocessed datasets, model implementations, and evaluation pipelines to promote accessibility and future development. We hope IndiaWeatherBench will serve as a foundation for advancing regional weather forecasting research. Code is available at https://github.com/tung-nd/IndiaWeatherBench."
    },
    {
      "paperId": "257a853025790a1c597a311149581f4cb909c5bc",
      "externalIds": {
        "PubMedCentral": "12477684",
        "DOI": "10.1093/bib/bbaf509",
        "CorpusId": 281668275,
        "PubMed": "41020638"
      },
      "corpusId": 281668275,
      "title": "INAB: identify nucleic acid binding domain via cross-modal protein language models and multiscale computation",
      "venue": "Briefings in Bioinformatics",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBYNC",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12477684, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2276524449",
          "name": "Jun Zhang"
        },
        {
          "authorId": "2340569009",
          "name": "Hao Zeng"
        },
        {
          "authorId": "2378724863",
          "name": "Junjie Chen"
        },
        {
          "authorId": "51315514",
          "name": "Zexuan Zhu"
        }
      ],
      "abstract": "Abstract Protein\u2013nucleic acid interactions play a crucial role in biological processes, including gene regulation and editing. Accurately identifying nucleic acid-binding domains in proteins is essential to unravel these interactions, yet traditional experimental methods like X-ray crystallography remain costly and time-intensive. Computational approaches have thus emerged as indispensable tools to complement wet-lab techniques. Here, we introduce a framework for nucleic acid-binding domain prediction by integrating cross-modal protein language models with a multiscale computational architecture. The proposed method leverages a structurally annotated benchmark dataset, which quantifies binding likelihood through hierarchical, proximity-based labels derived from experimental complexes. Evaluations demonstrate that the approach achieves state-of-the-art performance, providing a new insight into the design of multimodal learning systems in protein-nucleic acid interaction analysis and an open resource to accelerate discoveries in functional genomics and drug design."
    },
    {
      "paperId": "efbfd7b0f677db727324ae57670d1be5b5c3044a",
      "externalIds": {
        "DOI": "10.1007/s11676-025-01916-w",
        "CorpusId": 281008695
      },
      "corpusId": 281008695,
      "title": "YOLO-DS: a detection model for desert shrub identification and coverage estimation in UAV remote sensing",
      "venue": "Journal of Forest Research",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11676-025-01916-w?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11676-025-01916-w, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2346339003",
          "name": "Weifan Xu"
        },
        {
          "authorId": "2378257683",
          "name": "Huifang Zhang"
        },
        {
          "authorId": "2378504104",
          "name": "Yan Zhang"
        },
        {
          "authorId": "2378416693",
          "name": "Kangshuo Liu"
        },
        {
          "authorId": "2378242703",
          "name": "Jinglu Zhang"
        },
        {
          "authorId": "2378980138",
          "name": "Yali Zhu"
        },
        {
          "authorId": "2378338448",
          "name": "Baoerhan Dilixiati"
        },
        {
          "authorId": "2239516702",
          "name": "Jifeng Ning"
        },
        {
          "authorId": "2380567282",
          "name": "Jian Gao"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "56d1b9457872bef0b5450ddd05885402055a1368",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-00599",
        "ArXiv": "2509.00599",
        "DOI": "10.48550/arXiv.2509.00599",
        "CorpusId": 281080780
      },
      "corpusId": 281080780,
      "title": "COMET: A Framework for Modeling Compound Operation Dataflows with Explicit Collectives",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.00599, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2182565552",
          "name": "Shubham Negi"
        },
        {
          "authorId": "2378631796",
          "name": "Manik Singhal"
        },
        {
          "authorId": "3468125",
          "name": "Aayush Ankit"
        },
        {
          "authorId": "3369003",
          "name": "S. Bhoja"
        },
        {
          "authorId": "2378709223",
          "name": "Kaushik Roy"
        }
      ],
      "abstract": "Modern machine learning accelerators are designed to efficiently execute deep neural networks (DNNs) by optimizing data movement, memory hierarchy, and compute throughput. However, emerging DNN models such as large language models, state space models increasingly rely on compound operations-structured compositions of multiple basic operations-which introduce new challenges for dataflow optimization and minimizing off-chip memory traffic. Moreover, as model size continues to grow, deployment across spatially distributed compute clusters becomes essential, requiring frequent and complex collective communication. Existing dataflow optimization frameworks and performance models either focus on single operations or lack explicit modeling of collective communication cost, limiting their applicability to modern workloads. To address these limitations, we propose, a framework for modeling and optimizing dataflow for compound operations on machine learning accelerators. COMET introduces a novel representation that explicitly models collective communication across spatial clusters, along with latency and energy cost models that account for both GEMM and non-GEMM operation level dependencies within compound operations. We demonstrate COMET's capabilities to analyze and optimize dataflows for compound operations such as GEMM--Softmax, GEMM--LayerNorm, and self-attention, across both edge and cloud accelerator configurations. Our collective-aware modeling enables exploration of a broader mapping space, leading to improved performance and energy efficiency. Specifically, our optimized dataflows achieve up to 1.42$\\times$ speedup for GEMM-Softmax, 3.46$\\times$ for GEMM-LayerNorm and 1.82$\\times$ for self-attention compared to unfused baselines."
    },
    {
      "paperId": "8111bd109aa2f9e0726afc49fa1119a0470c704f",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-00141",
        "ArXiv": "2509.00141",
        "DOI": "10.48550/arXiv.2509.00141",
        "CorpusId": 281080974
      },
      "corpusId": 281080974,
      "title": "Scaling Legal AI: Benchmarking Mamba and Transformers for Statutory Classification and Case Law Retrieval",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.00141, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2374047467",
          "name": "Anuraj Maurya"
        }
      ],
      "abstract": "The rapid growth of statutory corpora and judicial decisions requires scalable legal AI systems capable of classification and retrieval over extremely long contexts. Transformer-based architectures (e.g., Longformer, DeBERTa) dominate current legal NLP benchmarks but struggle with quadratic attention costs, limiting efficiency and scalability. In this work, we present the first comprehensive benchmarking of Mamba, a state-space model (SSM) with linear-time selective mechanisms, against leading transformer models for statutory classification and case law retrieval. We evaluate models on open-source legal corpora including LexGLUE, EUR-Lex, and ILDC, covering statutory tagging, judicial outcome prediction, and case retrieval tasks. Metrics include accuracy, recall at k, mean reciprocal rank (MRR), and normalized discounted cumulative gain (nDCG), alongside throughput measured in tokens per second and maximum context length. Results show that Mamba's linear scaling enables processing of legal documents several times longer than transformers, while maintaining or surpassing retrieval and classification performance. This study introduces a new legal NLP benchmark suite for long-context modeling, along with open-source code and datasets to support reproducibility. Our findings highlight trade-offs between state-space models and transformers, providing guidance for deploying scalable legal AI in statutory analysis, judicial decision support, and policy research."
    },
    {
      "paperId": "58808fd5a4d66dde665283da6077e1ee360e0a7a",
      "externalIds": {
        "DBLP": "journals/corr/abs-2509-00259",
        "ArXiv": "2509.00259",
        "DOI": "10.48550/arXiv.2509.00259",
        "CorpusId": 281081376
      },
      "corpusId": 281081376,
      "title": "Quantum-Optimized Selective State Space Model for Efficient Time Series Prediction",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.00259, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2356550541",
          "name": "Stefan-Alexandru Jura"
        },
        {
          "authorId": "1746910",
          "name": "M. Udrescu"
        },
        {
          "authorId": "2392782",
          "name": "Alexandru Top\u00eerceanu"
        }
      ],
      "abstract": "Long-range time series forecasting remains challenging, as it requires capturing non-stationary and multi-scale temporal dependencies while maintaining noise robustness, efficiency, and stability. Transformer-based architectures such as Autoformer and Informer improve generalization but suffer from quadratic complexity and degraded performance on very long time horizons. State space models, notably S-Mamba, provide linear-time updates but often face unstable training dynamics, sensitivity to initialization, and limited robustness for multivariate forecasting. To address such challenges, we propose the Quantum-Optimized Selective State Space Model (Q-SSM), a hybrid quantum-optimized approach that integrates state space dynamics with a variational quantum gate. Instead of relying on expensive attention mechanisms, Q-SSM employs a simple parametrized quantum circuit (RY-RX ansatz) whose expectation values regulate memory updates adaptively. This quantum gating mechanism improves convergence stability, enhances the modeling of long-term dependencies, and provides a lightweight alternative to attention. We empirically validate Q-SSM on three widely used benchmarks, i.e., ETT, Traffic, and Exchange Rate. Results show that Q-SSM consistently improves over strong baselines (LSTM, TCN, Reformer), Transformer-based models, and S-Mamba. These findings demonstrate that variational quantum gating can address current limitations in long-range forecasting, leading to accurate and robust multivariate predictions."
    },
    {
      "paperId": "839461c48de068e17507e0bbdd0375b5943093a6",
      "externalIds": {
        "DOI": "10.1088/1361-6501/ae00e5",
        "CorpusId": 281001657
      },
      "corpusId": 281001657,
      "title": "SDG-CSNet: a detection network based on spatial detail guidance and clue screening for substation equipment",
      "venue": "Measurement science and technology",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1088/1361-6501/ae00e5?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1088/1361-6501/ae00e5, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2321718172",
          "name": "Zehui Zhang"
        },
        {
          "authorId": "2242954869",
          "name": "Na Dong"
        },
        {
          "authorId": "2244634414",
          "name": "Zhendong Guo"
        },
        {
          "authorId": "2150231455",
          "name": "Xiaoming Mai"
        }
      ],
      "abstract": "Substation equipment carries critical operational information. For the intelligent monitoring of power systems, the deployment of camera-equipped wheeled robots and unmanned aerial vehicles for image acquisition and real-time detection of substation equipment represents an essential routine inspection methodology. While modern detection algorithms have achieved widespread adoption in power industry applications, state-of-the-art methods still face significant performance degradation when handling dense small-target detection tasks in complex field environments. To address these limitations, this paper proposes a novel spatial detail-guided and clue-screening network (SDG-CSNet), including an efficient clue-scanning module (ECSM), visual detail retention (VDR), and a detail-guided path aggregation network (DG-PAN). To tackle the degradation of image quality induced by complex illumination conditions, the input images undergo edge detail smoothing via preprocessing, while the ECSM utilizes a rescreening mechanism to effectively integrate local connectivity with global receptive fields for feature reconstruction. Addressing the characteristics of densely distributed and small-sized objects, VDR employs pixel-adaptive downsampling strategies to maintain high-frequency information. Moreover, DG-PAN implements delayed multi-scale fusion of shallow-level features, thereby guiding feature maps to adjust dynamically. Experiments demonstrate that SDG-CSNet achieves a state-of-the-art performance with 57.3% mAP. The detection speed, model size, and computation are 59.9 frames per second, 6.25\u2009M, and 14.1\u2009G. SDG-CSNet demonstrates superior robustness in low-light and motion-blur conditions. This work provides a practical and efficient solution for substation equipment detecting applications."
    },
    {
      "paperId": "013d404d1b67592c82fc1011317025a638cae585",
      "externalIds": {
        "ArXiv": "2508.20376",
        "DBLP": "journals/corr/abs-2508-20376",
        "DOI": "10.48550/arXiv.2508.20376",
        "CorpusId": 280949902
      },
      "corpusId": 280949902,
      "title": "Enhancing Mamba Decoder with Bidirectional Interaction in Multi-Task Dense Prediction",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.20376, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2378001082",
          "name": "Mang Cao"
        },
        {
          "authorId": "2210715659",
          "name": "Sanpin Zhou"
        },
        {
          "authorId": "2267319690",
          "name": "Yizhe Li"
        },
        {
          "authorId": "2111063309",
          "name": "Ye Deng"
        },
        {
          "authorId": "2158105322",
          "name": "Wenli Huang"
        },
        {
          "authorId": "2296842533",
          "name": "Le Wang"
        }
      ],
      "abstract": "Sufficient cross-task interaction is crucial for success in multi-task dense prediction. However, sufficient interaction often results in high computational complexity, forcing existing methods to face the trade-off between interaction completeness and computational efficiency. To address this limitation, this work proposes a Bidirectional Interaction Mamba (BIM), which incorporates novel scanning mechanisms to adapt the Mamba modeling approach for multi-task dense prediction. On the one hand, we introduce a novel Bidirectional Interaction Scan (BI-Scan) mechanism, which constructs task-specific representations as bidirectional sequences during interaction. By integrating task-first and position-first scanning modes within a unified linear complexity architecture, BI-Scan efficiently preserves critical cross-task information. On the other hand, we employ a Multi-Scale Scan~(MS-Scan) mechanism to achieve multi-granularity scene modeling. This design not only meets the diverse granularity requirements of various tasks but also enhances nuanced cross-task feature interactions. Extensive experiments on two challenging benchmarks, \\emph{i.e.}, NYUD-V2 and PASCAL-Context, show the superiority of our BIM vs its state-of-the-art competitors."
    },
    {
      "paperId": "82b8bf967816f118bc8a4cbeee749913ac1a8c1d",
      "externalIds": {
        "ArXiv": "2508.21135",
        "DBLP": "journals/corr/abs-2508-21135",
        "DOI": "10.48550/arXiv.2508.21135",
        "CorpusId": 280985102
      },
      "corpusId": 280985102,
      "title": "HiddenObject: Modality-Agnostic Fusion for Multimodal Hidden Object Detection",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.21135, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2376398657",
          "name": "Harris Song"
        },
        {
          "authorId": "2376421617",
          "name": "Tuan-Anh Vu"
        },
        {
          "authorId": "2378133690",
          "name": "Ieee Sanjith Menon Member"
        },
        {
          "authorId": "2378133576",
          "name": "Member Ieee Sriram Narasimhan"
        },
        {
          "authorId": "2378134626",
          "name": "M. I. M Khalid Jawed"
        }
      ],
      "abstract": "Detecting hidden or partially concealed objects remains a fundamental challenge in multimodal environments, where factors like occlusion, camouflage, and lighting variations significantly hinder performance. Traditional RGB-based detection methods often fail under such adverse conditions, motivating the need for more robust, modality-agnostic approaches. In this work, we present HiddenObject, a fusion framework that integrates RGB, thermal, and depth data using a Mamba-based fusion mechanism. Our method captures complementary signals across modalities, enabling enhanced detection of obscured or camouflaged targets. Specifically, the proposed approach identifies modality-specific features and fuses them in a unified representation that generalizes well across challenging scenarios. We validate HiddenObject across multiple benchmark datasets, demonstrating state-of-the-art or competitive performance compared to existing methods. These results highlight the efficacy of our fusion design and expose key limitations in current unimodal and na\\\"ive fusion strategies. More broadly, our findings suggest that Mamba-based fusion architectures can significantly advance the field of multimodal object detection, especially under visually degraded or complex conditions."
    },
    {
      "paperId": "a4a8f4969180ab89fffd0d56e118ce3d11f37595",
      "externalIds": {
        "ArXiv": "2508.21052",
        "DBLP": "journals/corr/abs-2508-21052",
        "DOI": "10.48550/arXiv.2508.21052",
        "CorpusId": 280950071
      },
      "corpusId": 280950071,
      "title": "FakeParts: a New Family of AI-Generated DeepFakes",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.21052, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2182938993",
          "name": "Ga\u00ebtan Brison"
        },
        {
          "authorId": "2245712281",
          "name": "Soobash Daiboo"
        },
        {
          "authorId": "2377795026",
          "name": "Samy Aimeur"
        },
        {
          "authorId": "2376536677",
          "name": "Awais Hussain Sani"
        },
        {
          "authorId": "2297729854",
          "name": "Xi Wang"
        },
        {
          "authorId": "2377794173",
          "name": "Gianni Franchi"
        },
        {
          "authorId": "2306786839",
          "name": "Vicky Kalogeiton"
        }
      ],
      "abstract": "We introduce FakeParts, a new class of deepfakes characterized by subtle, localized manipulations to specific spatial regions or temporal segments of otherwise authentic videos. Unlike fully synthetic content, these partial manipulations, ranging from altered facial expressions to object substitutions and background modifications, blend seamlessly with real elements, making them particularly deceptive and difficult to detect. To address the critical gap in detection capabilities, we present FakePartsBench, the first large-scale benchmark dataset specifically designed to capture the full spectrum of partial deepfakes. Comprising over 25K videos with pixel-level and frame-level manipulation annotations, our dataset enables comprehensive evaluation of detection methods. Our user studies demonstrate that FakeParts reduces human detection accuracy by over 30% compared to traditional deepfakes, with similar performance degradation observed in state-of-the-art detection models. This work identifies an urgent vulnerability in current deepfake detection approaches and provides the necessary resources to develop more robust methods for partial video manipulations."
    },
    {
      "paperId": "4492c40b16f7abbdbd398087dfdcfcfb96ff48d7",
      "externalIds": {
        "ArXiv": "2508.20441",
        "DBLP": "journals/corr/abs-2508-20441",
        "DOI": "10.48550/arXiv.2508.20441",
        "CorpusId": 280950074
      },
      "corpusId": 280950074,
      "title": "Uncovering the Spectral Bias in Diagonal State Space Models",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.20441, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "41154209",
          "name": "Rub\u00e9n Solozabal"
        },
        {
          "authorId": "102802804",
          "name": "Velibor Bojkovic"
        },
        {
          "authorId": "2362630636",
          "name": "Hilal AlQuabeh"
        },
        {
          "authorId": "2346978821",
          "name": "Kentaro Inui"
        },
        {
          "authorId": "2293464604",
          "name": "Martin Tak\u00e1c"
        }
      ],
      "abstract": "Current methods for initializing state space models (SSMs) parameters mainly rely on the \\textit{HiPPO framework}, which is based on an online approximation of orthogonal polynomials. Recently, diagonal alternatives have shown to reach a similar level of performance while being significantly more efficient due to the simplification in the kernel computation. However, the \\textit{HiPPO framework} does not explicitly study the role of its diagonal variants. In this paper, we take a further step to investigate the role of diagonal SSM initialization schemes from the frequency perspective. Our work seeks to systematically understand how to parameterize these models and uncover the learning biases inherent in such diagonal state-space models. Based on our observations, we propose a diagonal initialization on the discrete Fourier domain \\textit{S4D-DFouT}. The insights in the role of pole placing in the initialization enable us to further scale them and achieve state-of-the-art results on the Long Range Arena benchmark, allowing us to train from scratch on very large datasets as PathX-256."
    },
    {
      "paperId": "9d31159da8ed08d1982a956caf6646a28f6bcbf8",
      "externalIds": {
        "DBLP": "journals/corr/abs-2508-20885",
        "ArXiv": "2508.20885",
        "DOI": "10.48550/arXiv.2508.20885",
        "CorpusId": 280950117
      },
      "corpusId": 280950117,
      "title": "SincQDR-VAD: A Noise-Robust Voice Activity Detection Framework Leveraging Learnable Filters and Ranking-Aware Optimization",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.20885, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2319655678",
          "name": "Chien-Chun Wang"
        },
        {
          "authorId": "2262219909",
          "name": "En-Lun Yu"
        },
        {
          "authorId": "1965317",
          "name": "J. Hung"
        },
        {
          "authorId": "2319607558",
          "name": "Shih-Chieh Huang"
        },
        {
          "authorId": "2157306671",
          "name": "Berlin Chen"
        }
      ],
      "abstract": "Voice activity detection (VAD) is essential for speech-driven applications, but remains far from perfect in noisy and resource-limited environments. Existing methods often lack robustness to noise, and their frame-wise classification losses are only loosely coupled with the evaluation metric of VAD. To address these challenges, we propose SincQDR-VAD, a compact and robust framework that combines a Sinc-extractor front-end with a novel quadratic disparity ranking loss. The Sinc-extractor uses learnable bandpass filters to capture noise-resistant spectral features, while the ranking loss optimizes the pairwise score order between speech and non-speech frames to improve the area under the receiver operating characteristic curve (AUROC). A series of experiments conducted on representative benchmark datasets show that our framework considerably improves both AUROC and F2-Score, while using only 69% of the parameters compared to prior arts, confirming its efficiency and practical viability."
    },
    {
      "paperId": "6d72c95d80fdf400f9667c34a3a7e2dc7a48fa51",
      "externalIds": {
        "DOI": "10.1109/ICAC65379.2025.11196120",
        "CorpusId": 282148582
      },
      "corpusId": 282148582,
      "title": "A Non-Invasive Blood Pressure Estimation Method Based on Mamba-UNet and PPG Signals",
      "venue": "International Conference on Automation and Computing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICAC65379.2025.11196120?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICAC65379.2025.11196120, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2333359042",
          "name": "Huiqun Yu"
        },
        {
          "authorId": "2387661516",
          "name": "Haonan Fan"
        },
        {
          "authorId": "2199110414",
          "name": "Qingde Li"
        },
        {
          "authorId": "2268676369",
          "name": "Fangfang Lu"
        },
        {
          "authorId": "2259876974",
          "name": "Yongqiang Cheng"
        }
      ],
      "abstract": "Continuous blood pressure monitoring is of great significance for the early diagnosis of cardiovascular diseases. To address the limitations of current machine learning and deep learning-based blood pressure (BP) prediction methods, which rely on manual feature extraction and struggle to reconstruct complete BP waveforms, this paper employs a continuous non-invasive arterial BP detection model named Mamba-UNet based on photoplethysmography (PPG) signals. The model deeply integrates the selective state space model (Selective SSM) with the U-Net architecture, achieving direct mapping from PPG signals to arterial blood pressure (ABP) waveforms through end-to-end modeling. In the encoder, the MambaConvBlock module captures long-term temporal dependencies and individual vascular characteristics of PPG signals by dynamically adjusting parameters (\u2206, B, C). The decoder employs a hybrid Mamba-convolution structure, combining the global dynamic modeling capability of SSM with the local feature extraction ability of convolution to accurately reconstruct BP waveform details. The model design balances the multi-scale feature integration advantages of U-Net with the efficient long-sequence processing capability of Mamba. Evaluated on the Sensors dataset (derived from MIMIC-III, containing 1,131 ICU patient records), Mamba-UNet achieved a mean absolute error (MAE) of 6.06 mmHg for diastolic blood pressure (DBP) and 13.11 mmHg for systolic blood pressure (SBP), outperforming models such as MLP, ResNet, and U-Net."
    },
    {
      "paperId": "7e3950e1ffd10f30f38de56d268ebfae9a7d4e14",
      "externalIds": {
        "DBLP": "conf/sigcomm/ZhaoDBVM25",
        "DOI": "10.1145/3718958.3750498",
        "CorpusId": 280675889
      },
      "corpusId": 280675889,
      "title": "The Sweet Danger of Sugar: Debunking Representation Learning for Encrypted Traffic Classification",
      "venue": "Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3718958.3750498?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3718958.3750498, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2376230500",
          "name": "Yuqi Zhao"
        },
        {
          "authorId": "2376188048",
          "name": "Giovanni Dettori"
        },
        {
          "authorId": "2172689860",
          "name": "Matteo Boffa"
        },
        {
          "authorId": "2822483",
          "name": "L. Vassio"
        },
        {
          "authorId": "2317784087",
          "name": "Marco Mellia"
        }
      ],
      "abstract": "Recently we have witnessed the explosion of proposals that, inspired by Language Models like BERT, exploit Representation Learning models to create traffic representations. All of them promise astonishing performance in encrypted traffic classification (up to 98% accuracy). In this paper, with a networking expert mindset, we critically reassess their performance. Through extensive analysis, we demonstrate that the reported successes are heavily influenced by data preparation problems, which allow these models to find easy shortcuts - spurious correlation between features and labels - during fine-tuning that unrealistically boost their performance. When such shortcuts are not present - as in real scenarios - these models perform poorly. We also introduce Pcap-Encoder, an LM-based representation learning model that we specifically design to extract features from protocol headers. Pcap-Encoder appears to be the only model that provides an instrumental representation for traffic classification. Yet, its complexity questions its applicability in practical settings. Our findings reveal flaws in dataset preparation and model training, calling for a better and more conscious test design. We propose a correct evaluation methodology and stress the need for rigorous benchmarking."
    },
    {
      "paperId": "91fd141738858eff0f31ae5db208f269e4d3f982",
      "externalIds": {
        "ArXiv": "2508.19029",
        "CorpusId": 280870037
      },
      "corpusId": 280870037,
      "title": "Revisiting associative recall in modern recurrent models",
      "venue": "",
      "year": 2025,
      "citationCount": 4,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.19029, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2328974902",
          "name": "Destiny Okpekpe"
        },
        {
          "authorId": "2326112647",
          "name": "Antonio Orvieto"
        }
      ],
      "abstract": "Despite the advantageous subquadratic complexity of modern recurrent deep learning models -- such as state-space models (SSMs) -- recent studies have highlighted their potential shortcomings compared to transformers on reasoning and memorization tasks. In this paper, we dive deeper into one of such benchmarks: associative recall (AR), which has been shown to correlate well with language modeling performance, and inspect in detail the effects of scaling and optimization issues in recently proposed token mixing strategies. We first demonstrate that, unlike standard transformers, the choice of learning rate plays a critical role in the performance of modern recurrent models: an issue that can severely affect reported performance in previous works and suggests further research is needed to stabilize training. Next, we show that recurrent and attention-based models exhibit contrasting benefits when scaling in width as opposed to depth, with attention being notably unable to solve AR when limited to a single layer. We then further inspect 1-layer transformers, revealing that despite their poor performance, their training dynamics surprisingly resemble the formation of induction heads, a phenomenon previously observed only in their 2-layer counterparts. Finally, through architectural ablations, we study how components affects Transformer and Mamba's performance and optimization stability."
    },
    {
      "paperId": "aa0d0e65faba820633bbaae2dcc8ab2eb2bad498",
      "externalIds": {
        "DBLP": "journals/corr/abs-2508-18681",
        "ArXiv": "2508.18681",
        "DOI": "10.48550/arXiv.2508.18681",
        "CorpusId": 280870186
      },
      "corpusId": 280870186,
      "title": "Hierarchical Spatio-temporal Segmentation Network for Ejection Fraction Estimation in Echocardiography Videos",
      "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.18681, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2334664747",
          "name": "Dongfang Wang"
        },
        {
          "authorId": "2146237344",
          "name": "Jian Yang"
        },
        {
          "authorId": "2268822897",
          "name": "Yizhe Zhang"
        },
        {
          "authorId": "2376292074",
          "name": "Tao Zhou"
        }
      ],
      "abstract": "Automated segmentation of the left ventricular endocardium in echocardiography videos is a key research area in cardiology. It aims to provide accurate assessment of cardiac structure and function through Ejection Fraction (EF) estimation. Although existing studies have achieved good segmentation performance, their results do not perform well in EF estimation. In this paper, we propose a Hierarchical Spatio-temporal Segmentation Network (\\ourmodel) for echocardiography video, aiming to improve EF estimation accuracy by synergizing local detail modeling with global dynamic perception. The network employs a hierarchical design, with low-level stages using convolutional networks to process single-frame images and preserve details, while high-level stages utilize the Mamba architecture to capture spatio-temporal relationships. The hierarchical design balances single-frame and multi-frame processing, avoiding issues such as local error accumulation when relying solely on single frames or neglecting details when using only multi-frame data. To overcome local spatio-temporal limitations, we propose the Spatio-temporal Cross Scan (STCS) module, which integrates long-range context through skip scanning across frames and positions. This approach helps mitigate EF calculation biases caused by ultrasound image noise and other factors."
    },
    {
      "paperId": "66763d7e7a96355952f56fd7d69d955c78f656b6",
      "externalIds": {
        "ArXiv": "2508.18834",
        "DBLP": "journals/corr/abs-2508-18834",
        "DOI": "10.1145/3746027.3762026",
        "CorpusId": 280870562
      },
      "corpusId": 280870562,
      "title": "Boosting Micro-Expression Analysis via Prior-Guided Video-Level Regression",
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.18834, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2284827186",
          "name": "Zizheng Guo"
        },
        {
          "authorId": "1486438257",
          "name": "Bochao Zou"
        },
        {
          "authorId": "2378211744",
          "name": "Yinuo Jia"
        },
        {
          "authorId": "2377337335",
          "name": "Xiangyu Li"
        },
        {
          "authorId": "2152615223",
          "name": "Huimin Ma"
        }
      ],
      "abstract": "Micro-expressions (MEs) are involuntary, low-intensity, and short-duration facial expressions that often reveal an individual's genuine thoughts and emotions. Most existing ME analysis methods rely on window-level classification with fixed window sizes and hard decisions, which limits their ability to capture the complex temporal dynamics of MEs. Although recent approaches have adopted video-level regression frameworks to address some of these challenges, interval decoding still depends on manually predefined, window-based methods, leaving the issue only partially mitigated. In this paper, we propose a prior-guided video-level regression method for ME analysis. We introduce a scalable interval selection strategy that comprehensively considers the temporal evolution, duration, and class distribution characteristics of MEs, enabling precise spotting of the onset, apex, and offset phases. In addition, we introduce a synergistic optimization framework, in which the spotting and recognition tasks share parameters except for the classification heads. This fully exploits complementary information, makes more efficient use of limited data, and enhances the model's capability. Extensive experiments on multiple benchmark datasets demonstrate the state-of-the-art performance of our method, with an STRS of 0.0562 on CAS(ME)3 and 0.2000 on SAMMLV. The code is available at https://github.com/zizheng-guo/BoostingVRME."
    },
    {
      "paperId": "3eb690df90df40e91ecea0426290b6477e0514b2",
      "externalIds": {
        "DBLP": "journals/corr/abs-2508-19242",
        "ArXiv": "2508.19242",
        "DOI": "10.48550/arXiv.2508.19242",
        "CorpusId": 280870000
      },
      "corpusId": 280870000,
      "title": "Autoregressive Universal Video Segmentation Model",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.19242, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "89615223",
          "name": "Miran Heo"
        },
        {
          "authorId": "2030931418",
          "name": "Sukjun Hwang"
        },
        {
          "authorId": "2239064269",
          "name": "Min-Hung Chen"
        },
        {
          "authorId": "2333378657",
          "name": "Yu-Chiang Frank Wang"
        },
        {
          "authorId": "2372624020",
          "name": "Albert Gu"
        },
        {
          "authorId": "2373591213",
          "name": "Seon Joo Kim"
        },
        {
          "authorId": "2307471477",
          "name": "Ryo Hachiuma"
        }
      ],
      "abstract": "Recent video foundation models such as SAM2 excel at prompted video segmentation by treating masks as a general-purpose primitive. However, many real-world settings require unprompted segmentation that aims to detect and track all objects in a video without external cues, leaving today's landscape fragmented across task-specific models and pipelines. We recast streaming video segmentation as sequential mask prediction, analogous to language modeling, and introduce the Autoregressive Universal Segmentation Model (AUSM), a single architecture that unifies both prompted and unprompted video segmentation. Built on recent state-space models, AUSM maintains a fixed-size spatial state and scales to video streams of arbitrary length. Furthermore, all components of AUSM are designed for parallel training across frames, yielding substantial speedups over iterative training. On standard benchmarks (DAVIS17, YouTube-VOS 2018&2019, MOSE, YouTube-VIS 2019&2021, and OVIS) AUSM outperforms prior universal streaming video segmentation methods and achieves up to 2.5x faster training on 16-frame sequences."
    },
    {
      "paperId": "11dc2feb8cee034147dc54e8465236c3bb0470cb",
      "externalIds": {
        "DBLP": "journals/corr/abs-2508-19361",
        "ArXiv": "2508.19361",
        "DOI": "10.48550/arXiv.2508.19361",
        "CorpusId": 280918472
      },
      "corpusId": 280918472,
      "title": "Atrial Fibrillation Prediction Using a Lightweight Temporal Convolutional and Selective State Space Architecture",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.19361, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2377561835",
          "name": "Yongbin Lee"
        },
        {
          "authorId": "2336287479",
          "name": "Ki H. Chon"
        }
      ],
      "abstract": "Atrial fibrillation (AF) is the most common arrhythmia, increasing the risk of stroke, heart failure, and other cardiovascular complications. While AF detection algorithms perform well in identifying persistent AF, early-stage progression, such as paroxysmal AF (PAF), often goes undetected due to its sudden onset and short duration. However, undetected PAF can progress into sustained AF, increasing the risk of mortality and severe complications. Early prediction of AF offers an opportunity to reduce disease progression through preventive therapies, such as catecholamine-sparing agents or beta-blockers. In this study, we propose a lightweight deep learning model using only RR Intervals (RRIs), combining a Temporal Convolutional Network (TCN) for positional encoding with Mamba, a selective state space model, to enable early prediction of AF through efficient parallel sequence modeling. In subject-wise testing results, our model achieved a sensitivity of 0.908, specificity of 0.933, F1-score of 0.930, AUROC of 0.972, and AUPRC of 0.932. Additionally, our method demonstrates high computational efficiency, with only 73.5 thousand parameters and 38.3 MFLOPs, outperforming traditional Convolutional Neural Network-Recurrent Neural Network (CNN-RNN) approaches in both accuracy and model compactness. Notably, the model can predict AF up to two hours in advance using just 30 minutes of input data, providing enough lead time for preventive interventions."
    },
    {
      "paperId": "58fb8a8bd10bf3fc875f558f26bbd2dbcd257ec1",
      "externalIds": {
        "ArXiv": "2508.19200",
        "DBLP": "journals/corr/abs-2508-19200",
        "DOI": "10.48550/arXiv.2508.19200",
        "CorpusId": 280869821
      },
      "corpusId": 280869821,
      "title": "The Ramon Llull's Thinking Machine for Automated Ideation",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.19200, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2329112407",
          "name": "Xinran Zhao"
        },
        {
          "authorId": "2378198468",
          "name": "Boyuan Zheng"
        },
        {
          "authorId": "2377311094",
          "name": "Chenglei Si"
        },
        {
          "authorId": "2377438275",
          "name": "Haofei Yu"
        },
        {
          "authorId": "2377825695",
          "name": "Ken Liu"
        },
        {
          "authorId": "2377947076",
          "name": "Runlong Zhou"
        },
        {
          "authorId": "2377272906",
          "name": "Ruochen Li"
        },
        {
          "authorId": "2377320735",
          "name": "Tong Chen"
        },
        {
          "authorId": "2377564483",
          "name": "Xiang Li"
        },
        {
          "authorId": "2377326688",
          "name": "Yiming Zhang"
        },
        {
          "authorId": "2287820683",
          "name": "Tongshuang Wu"
        }
      ],
      "abstract": "This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for generating knowledge through symbolic recombination - as a conceptual foundation for building a modern Llull's thinking machine for research ideation. Our approach defines three compositional axes: Theme (e.g., efficiency, adaptivity), Domain (e.g., question answering, machine translation), and Method (e.g., adversarial training, linear attention). These elements represent high-level abstractions common in scientific work - motivations, problem settings, and technical approaches - and serve as building blocks for LLM-driven exploration. We mine elements from human experts or conference papers and show that prompting LLMs with curated combinations produces research ideas that are diverse, relevant, and grounded in current literature. This modern thinking machine offers a lightweight, interpretable tool for augmenting scientific creativity and suggests a path toward collaborative ideation between humans and AI."
    },
    {
      "paperId": "64328d1cbba4c3a21f431769fa571e444c6e741c",
      "externalIds": {
        "PubMedCentral": "12490231",
        "DOI": "10.1016/j.xcrm.2025.102332",
        "CorpusId": 281156384,
        "PubMed": "40912256"
      },
      "corpusId": 281156384,
      "title": "Large language models enable tumor-type classification and localization of cancers of unknown primary from genomic data",
      "venue": "Cell Reports Medicine",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBYNCND",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12490231, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2124807014",
          "name": "Jilei Liu"
        },
        {
          "authorId": "2340468756",
          "name": "Meng Yang"
        },
        {
          "authorId": "2379300663",
          "name": "Yajing Bi"
        },
        {
          "authorId": "2379515038",
          "name": "Junqing Zhang"
        },
        {
          "authorId": "2108652119",
          "name": "Yichen Yang"
        },
        {
          "authorId": "2154900150",
          "name": "Yang Li"
        },
        {
          "authorId": "2023999499",
          "name": "Hong-xiu Shen"
        },
        {
          "authorId": "2286029891",
          "name": "Kexin Chen"
        },
        {
          "authorId": "2109260203",
          "name": "Xiangchun Li"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "74d4ac140c3c7d5a428c7ed819d96904e94727ca",
      "externalIds": {
        "ArXiv": "2508.17679",
        "DBLP": "journals/corr/abs-2508-17679",
        "DOI": "10.48550/arXiv.2508.17679",
        "CorpusId": 280710491
      },
      "corpusId": 280710491,
      "title": "Characterizing the Behavior of Training Mamba-based State Space Models on GPUs",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.17679, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "34587594",
          "name": "Trinayan Baruah"
        },
        {
          "authorId": "31277115",
          "name": "Kaustubh Shivdikar"
        },
        {
          "authorId": "2376533755",
          "name": "Sara Prescott"
        },
        {
          "authorId": "2262216191",
          "name": "David R. Kaeli"
        }
      ],
      "abstract": "Mamba-based State Space Models (SSM) have emerged as a promising alternative to the ubiquitous transformers. Despite the expressive power of transformers, the quadratic complexity of computing attention is a major impediment to scaling performance as we increase the sequence length. SSMs provide an alternative path that addresses this problem, reducing the computational complexity requirements of self-attention with novel model architectures for different domains and fields such as video, text generation and graphs. Thus, it is important to characterize the behavior of these emerging workloads on GPUs and understand their requirements during GPU microarchitectural design. In this work we evaluate Mamba-based SSMs and characterize their behavior during training on GPUs. We construct a workload suite that offers representative models that span different model architectures. We then use this suite to analyze the architectural implications of running Mamba-based SSMs on GPUs. Our work sheds new light on potential optimizations to continue scaling the performance for such models."
    },
    {
      "paperId": "298598ed236b5291e57dc75e89d1366a7836d698",
      "externalIds": {
        "ArXiv": "2508.18130",
        "DBLP": "journals/corr/abs-2508-18130",
        "DOI": "10.48550/arXiv.2508.18130",
        "CorpusId": 280710782
      },
      "corpusId": 280710782,
      "title": "Frozen in Time: Parameter-Efficient Time Series Transformers via Reservoir-Induced Feature Expansion and Fixed Random Dynamics",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.18130, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2218146908",
          "name": "Pradeep Singh"
        },
        {
          "authorId": "2378040478",
          "name": "Mehak Sharma"
        },
        {
          "authorId": "2376538352",
          "name": "Anupriya Dey"
        },
        {
          "authorId": "2283013123",
          "name": "Balasubramanian Raman"
        }
      ],
      "abstract": "Transformers are the de-facto choice for sequence modelling, yet their quadratic self-attention and weak temporal bias can make long-range forecasting both expensive and brittle. We introduce FreezeTST, a lightweight hybrid that interleaves frozen random-feature (reservoir) blocks with standard trainable Transformer layers. The frozen blocks endow the network with rich nonlinear memory at no optimisation cost; the trainable layers learn to query this memory through self-attention. The design cuts trainable parameters and also lowers wall-clock training time, while leaving inference complexity unchanged. On seven standard long-term forecasting benchmarks, FreezeTST consistently matches or surpasses specialised variants such as Informer, Autoformer, and PatchTST; with substantially lower compute. Our results show that embedding reservoir principles within Transformers offers a simple, principled route to efficient long-term time-series prediction."
    },
    {
      "paperId": "9b5f6a0b0db7d3ef648e4aef19558442d693ee99",
      "externalIds": {
        "ArXiv": "2508.18058",
        "CorpusId": 280710575
      },
      "corpusId": 280710575,
      "title": "Comprehensively stratifying MCIs into distinct risk subtypes based on brain imaging genetics fusion learning",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.18058, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2144460985",
          "name": "Muheng Shang"
        },
        {
          "authorId": "2108931013",
          "name": "Jin Zhang"
        },
        {
          "authorId": "2319609630",
          "name": "Junwei Han"
        },
        {
          "authorId": "2250099165",
          "name": "Lei Du"
        }
      ],
      "abstract": "Mild cognitive impairment (MCI) is the prodromal stage of Alzheimer's disease (AD) and thus enrolling MCI subjects to undergo clinical trials is worthwhile. However, MCI groups usually show significant diversity and heterogeneity in the pathology and symptom, which pose great challenge to accurately select appropriate subjects. This study aimed to stratify MCI subjects into distinct subgroups with substantial difference in the risk of transitioning to AD by fusing multimodal brain imaging genetic data. The integrated imaging genetics method comprised three modules, i.e., the whole-genome-oriented risk genetic information extraction module (RGE), the genetic-to-brain mapping module (RG2PG), and the genetic-guided pseudo-brain fusion module (CMPF). We used data from AD Neuroimaging Initiative (ADNI) and identified two MCI subtypes, called low-risk MCI (lsMCI) and high-risk MCI (hsMCI). We also validated that the two subgroups showed distinct patterns of in terms of multiple biomarkers including genetics, demographics, fluid biomarkers, brain imaging features, clinical symptoms and cognitive functioning at baseline, as well as their longitudinal developmental trajectories. Furthermore, we also identified potential biomarkers that may implicate the risk of MCIs, providing critical insights for patient stratification at early stage."
    },
    {
      "paperId": "657bce9f5ae15351f30bf2d2032beca07cc3730f",
      "externalIds": {
        "ArXiv": "2508.18162",
        "DBLP": "journals/corr/abs-2508-18162",
        "DOI": "10.48550/arXiv.2508.18162",
        "CorpusId": 280711741
      },
      "corpusId": 280711741,
      "title": "The Computational Complexity of Satisfiability in State Space Models",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.18162, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2124211806",
          "name": "Eric Alsmann"
        },
        {
          "authorId": "2283000238",
          "name": "Martin Lange"
        }
      ],
      "abstract": "We analyse the complexity of the satisfiability problem ssmSAT for State Space Models (SSM), which asks whether an input sequence can lead the model to an accepting configuration. We find that ssmSAT is undecidable in general, reflecting the computational power of SSM. Motivated by practical settings, we identify two natural restrictions under which ssmSAT becomes decidable and establish corresponding complexity bounds. First, for SSM with bounded context length, ssmSAT is NP-complete when the input length is given in unary and in NEXPTIME (and PSPACE-hard) when the input length is given in binary. Second, for quantised SSM operating over fixed-width arithmetic, ssmSAT is PSPACE-complete resp. in EXPSPACE depending on the bit-width encoding. While these results hold for diagonal gated SSM we also establish complexity bounds for time-invariant SSM. Our results establish a first complexity landscape for formal reasoning in SSM and highlight fundamental limits and opportunities for the verification of SSM-based language models."
    },
    {
      "paperId": "a13063b24d95f28e6f949e537070d72f05d4c22b",
      "externalIds": {
        "DBLP": "journals/corr/abs-2508-17634",
        "ArXiv": "2508.17634",
        "DOI": "10.48550/arXiv.2508.17634",
        "CorpusId": 280710768
      },
      "corpusId": 280710768,
      "title": "Finding Outliers in a Haystack: Anomaly Detection for Large Pointcloud Scenes",
      "venue": "Asian Conference on Pattern Recognition",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.17634, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "48627702",
          "name": "Ryan Faulkner"
        },
        {
          "authorId": "2310378424",
          "name": "Ian Reid"
        },
        {
          "authorId": "2325952829",
          "name": "Simon Ratcliffe"
        },
        {
          "authorId": "2237986716",
          "name": "Tat-Jun Chin"
        }
      ],
      "abstract": "LiDAR scanning in outdoor scenes acquires accurate distance measurements over wide areas, producing large-scale point clouds. Application examples for this data include robotics, automotive vehicles, and land surveillance. During such applications, outlier objects from outside the training data will inevitably appear. Our research contributes a novel approach to open-set segmentation, leveraging the learnings of object defect-detection research. We also draw on the Mamba architecture's strong performance in utilising long-range dependencies and scalability to large data. Combining both, we create a reconstruction based approach for the task of outdoor scene open-set segmentation. We show that our approach improves performance not only when applied to our our own open-set segmentation method, but also when applied to existing methods. Furthermore we contribute a Mamba based architecture which is competitive with existing voxel-convolution based methods on challenging, large-scale pointclouds."
    },
    {
      "paperId": "5631a5e6da0c495fa14bdf1c475aa93f8bff7539",
      "externalIds": {
        "ArXiv": "2508.18413",
        "CorpusId": 280869894
      },
      "corpusId": 280869894,
      "title": "Parallelizing MCMC Across the Sequence Length",
      "venue": "",
      "year": 2025,
      "citationCount": 3,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.18413, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2344839607",
          "name": "David M. Zoltowski"
        },
        {
          "authorId": "2377313904",
          "name": "Skyler Wu"
        },
        {
          "authorId": "2313640658",
          "name": "Xavier Gonzalez"
        },
        {
          "authorId": "2376541053",
          "name": "Leo Kozachkov"
        },
        {
          "authorId": "2342841",
          "name": "Scott W. Linderman"
        }
      ],
      "abstract": "Markov chain Monte Carlo (MCMC) methods are foundational algorithms for Bayesian inference and probabilistic modeling. However, most MCMC algorithms are inherently sequential and their time complexity scales linearly with the sequence length. Previous work on adapting MCMC to modern hardware has therefore focused on running many independent chains in parallel. Here, we take an alternative approach: we propose algorithms to evaluate MCMC samplers in parallel across the chain length. To do this, we build on recent methods for parallel evaluation of nonlinear recursions that formulate the state sequence as a solution to a fixed-point problem and solve for the fixed-point using a parallel form of Newton's method. We show how this approach can be used to parallelize Gibbs, Metropolis-adjusted Langevin, and Hamiltonian Monte Carlo sampling across the sequence length. In several examples, we demonstrate the simulation of up to hundreds of thousands of MCMC samples with only tens of parallel Newton iterations. Additionally, we develop two new parallel quasi-Newton methods to evaluate nonlinear recursions with lower memory costs and reduced runtime. We find that the proposed parallel algorithms accelerate MCMC sampling across multiple examples, in some cases by more than an order of magnitude compared to sequential evaluation."
    },
    {
      "paperId": "2b96ab295fb5907ac2a6abcec81c615f68bdc0c3",
      "externalIds": {
        "DOI": "10.1002/ima.70187",
        "CorpusId": 280888086
      },
      "corpusId": 280888086,
      "title": "VMDUnet: Advancing Glioma Segmentation Integrating With Mamba and Dual Cross\u2010Attention",
      "venue": "International journal of imaging systems and technology (Print)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1002/ima.70187?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1002/ima.70187, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2295480538",
          "name": "Zhuo Chen"
        },
        {
          "authorId": "2295481202",
          "name": "Yisong Wang"
        },
        {
          "authorId": "32581309",
          "name": "Fangfang Gou"
        }
      ],
      "abstract": "Gliomas are the most common type of primary brain tumor, characterized by their diffuse invasiveness and origin within the central nervous system. Manual identification and segmentation of tumor regions in MRI is a time\u2010consuming and subjective process, and may negatively impact diagnostic accuracy because the heterogeneity and infiltrative pattern of glioma are complex. To address these problems, we propose an automated glioma segmentation approach named IADSG (Intelligent Assistant Diagnosis System for Glioma), based on our novel VMDUnet architecture. Our method incorporates Contrast Limited Adaptive Histogram Equalization (CLAHE) as a preprocessing step to enhance image contrast and quality. Moreover, we use data augmentation techniques to improve the generalization and adaptability to complex clinical images of the model. Crucially, the integration of a Mamba module and a dual cross\u2010attention mechanism enables the model to effectively balance segmentation accuracy with computational efficiency. Experimental results show that our approach achieves a segmentation accuracy of 0.7769 DSC on the internal glioma dataset and 0.9117 DSC on the public BraTS dataset, outperforming existing segmentation methods on both benchmarks. This approach reduces the time and effort involved in manual segmentation, reduces the probabilities of misdiagnosis, and provides robust support for the diagnosis and treatment to be accurately conducted. Our code is available at https://github.com/CarioAo/VMDUnet."
    },
    {
      "paperId": "c438cbb09a31120ab3a15f36dfe7edf35c3cc944",
      "externalIds": {
        "ArXiv": "2508.17554",
        "DBLP": "journals/corr/abs-2508-17554",
        "DOI": "10.48550/arXiv.2508.17554",
        "CorpusId": 280710534
      },
      "corpusId": 280710534,
      "title": "Bridging Graph and State-Space Modeling for Intensive Care Unit Length of Stay Prediction",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.17554, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2376532031",
          "name": "Shuqi Zi"
        },
        {
          "authorId": "2117030133",
          "name": "Haitz S'aez de Oc'ariz Borde"
        },
        {
          "authorId": "82272649",
          "name": "Emma Rocheteau"
        },
        {
          "authorId": "2256065073",
          "name": "Pietro Li\u00f2"
        }
      ],
      "abstract": "Predicting a patient's length of stay (LOS) in the intensive care unit (ICU) is a critical task for hospital resource management, yet remains challenging due to the heterogeneous and irregularly sampled nature of electronic health records (EHRs). In this work, we propose S$^2$G-Net, a novel neural architecture that unifies state-space sequence modeling with multi-view Graph Neural Networks (GNNs) for ICU LOS prediction. The temporal path employs Mamba state-space models (SSMs) to capture patient trajectories, while the graph path leverages an optimized GraphGPS backbone, designed to integrate heterogeneous patient similarity graphs derived from diagnostic, administrative, and semantic features. Experiments on the large-scale MIMIC-IV cohort dataset show that S$^2$G-Net consistently outperforms sequence models (BiLSTM, Mamba, Transformer), graph models (classic GNNs, GraphGPS), and hybrid approaches across all primary metrics. Extensive ablation studies and interpretability analyses highlight the complementary contributions of each component of our architecture and underscore the importance of principled graph construction. These results demonstrate that S$^2$G-Net provides an effective and scalable solution for ICU LOS prediction with multi-modal clinical data. The code can be found at https://github.com/ShuqiZi1/S2G-Net."
    },
    {
      "paperId": "937f18ec8b166a8930c0f01bff7833f56400673a",
      "externalIds": {
        "ArXiv": "2508.17299",
        "DBLP": "journals/corr/abs-2508-17299",
        "DOI": "10.48550/arXiv.2508.17299",
        "CorpusId": 280710705
      },
      "corpusId": 280710705,
      "title": "FoundDiff: Foundational Diffusion Model for Generalizable Low-Dose CT Denoising",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.17299, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2276444843",
          "name": "Zhihao Chen"
        },
        {
          "authorId": "2337868425",
          "name": "Qi Gao"
        },
        {
          "authorId": "2188268877",
          "name": "Zilong Li"
        },
        {
          "authorId": "2265998312",
          "name": "Junping Zhang"
        },
        {
          "authorId": "2268457725",
          "name": "Yi Zhang"
        },
        {
          "authorId": "2376598827",
          "name": "Jun Zhao"
        },
        {
          "authorId": "2297771585",
          "name": "Hongming Shan"
        }
      ],
      "abstract": "Low-dose computed tomography (CT) denoising is crucial for reduced radiation exposure while ensuring diagnostically acceptable image quality. Despite significant advancements driven by deep learning (DL) in recent years, existing DL-based methods, typically trained on a specific dose level and anatomical region, struggle to handle diverse noise characteristics and anatomical heterogeneity during varied scanning conditions, limiting their generalizability and robustness in clinical scenarios. In this paper, we propose FoundDiff, a foundational diffusion model for unified and generalizable LDCT denoising across various dose levels and anatomical regions. FoundDiff employs a two-stage strategy: (i) dose-anatomy perception and (ii) adaptive denoising. First, we develop a dose- and anatomy-aware contrastive language image pre-training model (DA-CLIP) to achieve robust dose and anatomy perception by leveraging specialized contrastive learning strategies to learn continuous representations that quantify ordinal dose variations and identify salient anatomical regions. Second, we design a dose- and anatomy-aware diffusion model (DA-Diff) to perform adaptive and generalizable denoising by synergistically integrating the learned dose and anatomy embeddings from DACLIP into diffusion process via a novel dose and anatomy conditional block (DACB) based on Mamba. Extensive experiments on two public LDCT datasets encompassing eight dose levels and three anatomical regions demonstrate superior denoising performance of FoundDiff over existing state-of-the-art methods and the remarkable generalization to unseen dose levels. The codes and models are available at https://github.com/hao1635/FoundDiff."
    },
    {
      "paperId": "1c5b8379d1d55e8ed56578e4825cfdd68fa08739",
      "externalIds": {
        "DBLP": "journals/corr/abs-2508-16881",
        "ArXiv": "2508.16881",
        "DOI": "10.48550/arXiv.2508.16881",
        "CorpusId": 280710374
      },
      "corpusId": 280710374,
      "title": "AWM-Fuse: Multi-Modality Image Fusion for Adverse Weather via Global and Local Text Perception",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.16881, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2265456420",
          "name": "Xilai Li"
        },
        {
          "authorId": "2325910978",
          "name": "Huichun Liu"
        },
        {
          "authorId": "2108698591",
          "name": "Xiaosong Li"
        },
        {
          "authorId": "2265387039",
          "name": "Tao Ye"
        },
        {
          "authorId": "1720959990",
          "name": "Zhenyu Kuang"
        },
        {
          "authorId": "2330373428",
          "name": "Huafeng Li"
        }
      ],
      "abstract": "Multi-modality image fusion (MMIF) in adverse weather aims to address the loss of visual information caused by weather-related degradations, providing clearer scene representations. Although less studies have attempted to incorporate textual information to improve semantic perception, they often lack effective categorization and thorough analysis of textual content. In response, we propose AWM-Fuse, a novel fusion method for adverse weather conditions, designed to handle multiple degradations through global and local text perception within a unified, shared weight architecture. In particular, a global feature perception module leverages BLIP-produced captions to extract overall scene features and identify primary degradation types, thus promoting generalization across various adverse weather conditions. Complementing this, the local module employs detailed scene descriptions produced by ChatGPT to concentrate on specific degradation effects through concrete textual cues, thereby capturing finer details. Furthermore, textual descriptions are used to constrain the generation of fusion images, effectively steering the network learning process toward better alignment with real semantic labels, thereby promoting the learning of more meaningful visual features. Extensive experiments demonstrate that AWM-Fuse outperforms current state-of-the-art methods in complex weather conditions and downstream tasks. Our code is available at https://github.com/Feecuin/AWM-Fuse."
    },
    {
      "paperId": "bcb3580db4a00ad819262edb4ee2d3609d8c549e",
      "externalIds": {
        "ArXiv": "2508.17086",
        "CorpusId": 280711048
      },
      "corpusId": 280711048,
      "title": "Detecting Multilevel Manipulation from Limit Order Book via Cascaded Contrastive Representation Learning",
      "venue": "",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.17086, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2359165485",
          "name": "Yushi Lin"
        },
        {
          "authorId": "2308996120",
          "name": "Peng Yang"
        }
      ],
      "abstract": "Trade-based manipulation (TBM) undermines the fairness and stability of financial markets drastically. Spoofing, one of the most covert and deceptive TBM strategies, exhibits complex anomaly patterns across multilevel prices, while often being simplified as a single-level manipulation. These patterns are usually concealed within the rich, hierarchical information of the Limit Order Book (LOB), which is challenging to leverage due to high dimensionality and noise. To address this, we propose a representation learning framework combining a cascaded LOB representation architecture with supervised contrastive learning. Extensive experiments demonstrate that our framework consistently improves detection performance across diverse models, with Transformer-based architectures achieving state-of-the-art results. In addition, we conduct systematic analyses and ablation studies to investigate multilevel manipulation and the contributions of key components for detection, offering broader insights into representation learning and anomaly detection for complex time series data."
    },
    {
      "paperId": "833d19bc7e1e179fc108aeb544a74123b99a4a9e",
      "externalIds": {
        "PubMedCentral": "12375071",
        "DOI": "10.1038/s41467-025-63185-9",
        "CorpusId": 281089052,
        "PubMed": "40849313"
      },
      "corpusId": 281089052,
      "title": "SpaIM: single-cell spatial transcriptomics imputation via style transfer",
      "venue": "Nature Communications",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBYNCND",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12375071, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2283469847",
          "name": "Bo Li"
        },
        {
          "authorId": "2240513366",
          "name": "Ziyang Tang"
        },
        {
          "authorId": "2299335734",
          "name": "Aishwarya Budhkar"
        },
        {
          "authorId": "2257320386",
          "name": "Xiang Liu"
        },
        {
          "authorId": "2239401336",
          "name": "Tonglin Zhang"
        },
        {
          "authorId": "2253882013",
          "name": "Baijian Yang"
        },
        {
          "authorId": "2343727497",
          "name": "Jing Su"
        },
        {
          "authorId": "2255053858",
          "name": "Qianqian Song"
        }
      ],
      "abstract": "Spatial transcriptomics (ST) technologies have transformed our understanding of cellular organization but are limited by sparse signals and restricted gene coverage. To address these challenges, we introduce SpaIM, a style transfer learning model that leverages single-cell RNA sequencing (scRNA-seq) data to predict unmeasured gene expressions in ST profiles. By disentangling shared content and modality-specific styles, SpaIM effectively integrates scRNA-seq\u2019s rich gene expression with the spatial context of ST. Evaluated across 53 datasets spanning sequencing- and imaging-based spatial technologies in various tissue types, SpaIM consistently outperforms 12 state-of-the-art methods in improving gene coverage and expression accuracy. Furthermore, SpaIM enhances downstream analyses, including ligand-receptor interaction inference, spatial domain characterization, and differential gene expression analysis. Released as open-source software, SpaIM expands accessibility and utility in ST research. Overall, SpaIM represents a robust and generalizable framework for enriching ST data with single-cell information, enabling deeper insights into tissue architecture and cellular function. SpaIM is an open-source style transfer learning model that enriches spatial transcriptomics using single-cell RNA-seq, improving gene coverage, imputation accuracy, and downstream analyses across diverse tissues and platforms."
    },
    {
      "paperId": "21d16c0e96f907de610950344ed784551e8e4ccd",
      "externalIds": {
        "DBLP": "journals/corr/abs-2508-16745",
        "ArXiv": "2508.16745",
        "DOI": "10.48550/arXiv.2508.16745",
        "CorpusId": 280711685
      },
      "corpusId": 280711685,
      "title": "Beyond Memorization: Extending Reasoning Depth with Recurrence, Memory and Test-Time Compute Scaling",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.16745, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2306782862",
          "name": "Ivan Rodkin"
        },
        {
          "authorId": "2212398766",
          "name": "Daniil Orel"
        },
        {
          "authorId": "2376539559",
          "name": "Konstantin Smirnov"
        },
        {
          "authorId": "2376538321",
          "name": "Arman Bolatov"
        },
        {
          "authorId": "2214844109",
          "name": "Bilal Elbouardi"
        },
        {
          "authorId": "2376537800",
          "name": "Besher Hassan"
        },
        {
          "authorId": "51114080",
          "name": "Yuri Kuratov"
        },
        {
          "authorId": "2176183932",
          "name": "A. Bulatov"
        },
        {
          "authorId": "2026545715",
          "name": "Preslav Nakov"
        },
        {
          "authorId": "2266394314",
          "name": "Timothy Baldwin"
        },
        {
          "authorId": "2316488670",
          "name": "Artem Shelmanov"
        },
        {
          "authorId": "2284592236",
          "name": "M. Burtsev"
        }
      ],
      "abstract": "Reasoning is a core capability of large language models, yet understanding how they learn and perform multi-step reasoning remains an open problem. In this study, we explore how different architectures and training methods affect model multi-step reasoning capabilities within a cellular automata framework. By training on state sequences generated with random Boolean functions for random initial conditions to exclude memorization, we demonstrate that most neural architectures learn to abstract the underlying rules. While models achieve high accuracy in next-state prediction, their performance declines sharply if multi-step reasoning is required. We confirm that increasing model depth plays a crucial role for sequential computations. We demonstrate that an extension of the effective model depth with recurrence, memory, and test-time compute scaling substantially enhances reasoning capabilities."
    },
    {
      "paperId": "40ce93c1759cbf1c6130877eb2db8842f78a1dae",
      "externalIds": {
        "DBLP": "journals/corr/abs-2508-16069",
        "ArXiv": "2508.16069",
        "DOI": "10.48550/arXiv.2508.16069",
        "CorpusId": 280708543
      },
      "corpusId": 280708543,
      "title": "A Unified Voxel Diffusion Module for Point Cloud 3D Object Detection",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.16069, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2301617640",
          "name": "Qifeng Liu"
        },
        {
          "authorId": "2366089424",
          "name": "Dawei Zhao"
        },
        {
          "authorId": "2376519706",
          "name": "Yabo Dong"
        },
        {
          "authorId": "2325155043",
          "name": "Linzhi Shang"
        },
        {
          "authorId": "2300816725",
          "name": "Liang Xiao"
        },
        {
          "authorId": "2325486382",
          "name": "Juan Wang"
        },
        {
          "authorId": "2386785547",
          "name": "Kunkong Zhao"
        },
        {
          "authorId": "2240250206",
          "name": "Dongming Lu"
        },
        {
          "authorId": "2374912830",
          "name": "Qi Zhu"
        }
      ],
      "abstract": "Recent advances in point cloud object detection have increasingly adopted Transformer-based and State Space Models (SSMs), demonstrating strong performance. However, voxelbased representations in these models require strict consistency in input and output dimensions due to their serialized processing, which limits the spatial diffusion capability typically offered by convolutional operations. This limitation significantly affects detection accuracy. Inspired by CNN-based object detection architectures, we propose a novel Voxel Diffusion Module (VDM) to enhance voxel-level representation and diffusion in point cloud data. VDM is composed of sparse 3D convolutions, submanifold sparse convolutions, and residual connections. To ensure computational efficiency, the output feature maps are downsampled to one-fourth of the original input resolution. VDM serves two primary functions: (1) diffusing foreground voxel features through sparse 3D convolutions to enrich spatial context, and (2) aggregating fine-grained spatial information to strengthen voxelwise feature representation. The enhanced voxel features produced by VDM can be seamlessly integrated into mainstream Transformer- or SSM-based detection models for accurate object classification and localization, highlighting the generalizability of our method. We evaluate VDM on several benchmark datasets by embedding it into both Transformerbased and SSM-based models. Experimental results show that our approach consistently improves detection accuracy over baseline models. Specifically, VDM-SSMs achieve 74.7 mAPH (L2) on Waymo, 72.9 NDS on nuScenes, 42.3 mAP on Argoverse 2, and 67.6 mAP on ONCE, setting new stateof-the-art performance across all datasets. Our code will be made publicly available."
    },
    {
      "paperId": "20f2f6a5b3585bfb5bc1365d8b1e705f729bd37d",
      "externalIds": {
        "DBLP": "journals/corr/abs-2508-16379",
        "ArXiv": "2508.16379",
        "DOI": "10.48550/arXiv.2508.16379",
        "CorpusId": 280708872
      },
      "corpusId": 280708872,
      "title": "Agentic AI Empowered Multi-UAV Trajectory Optimization in Low-Altitude Economy Networks",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.16379, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "40540111",
          "name": "Feibo Jiang"
        },
        {
          "authorId": "2152288497",
          "name": "Li Dong"
        },
        {
          "authorId": "2387896091",
          "name": "Xitao Pan"
        },
        {
          "authorId": "2244014700",
          "name": "Kezhi Wang"
        },
        {
          "authorId": "2290889674",
          "name": "Cunhua Pan"
        }
      ],
      "abstract": "This paper proposes a novel Agentic Retrieval-augmented generation with Mamba-Attention Integrated Transformer (ARMAIT) framework for multi-Unmanned Aerial Vehicle (UAV) trajectory optimization. The framework is built upon Large Language Models (LLMs), incorporating Retrieval-Augmented Generation (RAG) empowered by Agentic AI and integrated with a UAV-specific knowledge base. Through the Agentic RAG, the LLM autonomously interprets high-level task requirements and identifies the key components necessary for trajectory optimization, including model inputs and outputs, network architecture, reward functions, and task constraints. To support efficient modeling across different system scales, we introduce the Mamba-Attention Integrated Transformer (MAIT), a hybrid neural architecture that combines the long-range dependency modeling capability of attention mechanisms with the efficient temporal dynamic representation of Mamba. Furthermore, a Trajectory-Group Relative Policy Optimization (T-GRPO) method is proposed to achieve unified policy gradient optimization in both discrete and continuous trajectory spaces for MAIT training. Extensive experimental results validate the feasibility and effectiveness of the proposed ARMAIT framework."
    },
    {
      "paperId": "9c51e133dade2a86b86297a168f156c608b79a8a",
      "externalIds": {
        "ArXiv": "2508.16817",
        "DBLP": "journals/corr/abs-2508-16817",
        "DOI": "10.48550/arXiv.2508.16817",
        "CorpusId": 280711716
      },
      "corpusId": 280711716,
      "title": "Predictability Enables Parallelization of Nonlinear State Space Models",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 3,
      "influentialCitationCount": 1,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.16817, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2313640658",
          "name": "Xavier Gonzalez"
        },
        {
          "authorId": "2376541053",
          "name": "Leo Kozachkov"
        },
        {
          "authorId": "2294292",
          "name": "D. M. Zoltowski"
        },
        {
          "authorId": "2277284277",
          "name": "Kenneth L. Clarkson"
        },
        {
          "authorId": "2342841",
          "name": "Scott W. Linderman"
        }
      ],
      "abstract": "The rise of parallel computing hardware has made it increasingly important to understand which nonlinear state space models can be efficiently parallelized. Recent advances like DEER (arXiv:2309.12252) or DeepPCR (arXiv:2309.16318) have shown that evaluating a state space model can be recast as solving a parallelizable optimization problem, and sometimes this approach can yield dramatic speed-ups in evaluation time. However, the factors that govern the difficulty of these optimization problems remain unclear, limiting the larger adoption of the technique. In this work, we establish a precise relationship between the dynamics of a nonlinear system and the conditioning of its corresponding optimization formulation. We show that the predictability of a system, defined as the degree to which small perturbations in state influence future behavior, impacts the number of optimization steps required for evaluation. In predictable systems, the state trajectory can be computed in $O((\\log T)^2)$ time, where $T$ is the sequence length, a major improvement over the conventional sequential approach. In contrast, chaotic or unpredictable systems exhibit poor conditioning, with the consequence that parallel evaluation converges too slowly to be useful. Importantly, our theoretical analysis demonstrates that for predictable systems, the optimization problem is always well-conditioned, whereas for unpredictable systems, the conditioning degrades exponentially as a function of the sequence length. We validate our claims through extensive experiments, providing practical guidance on when nonlinear dynamical systems can be efficiently parallelized, and highlighting predictability as a key design principle for parallelizable models."
    },
    {
      "paperId": "f829697b89b6b0c4c4259bb07c9f5f13ee3c1b36",
      "externalIds": {
        "DOI": "10.1088/2516-1091/adfeab",
        "CorpusId": 280802607,
        "PubMed": "40845893"
      },
      "corpusId": 280802607,
      "title": "Advancements in deep learning for image-guided tumor ablation therapies: a comprehensive review",
      "venue": "Progress in Biomedical Engineering",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1088/2516-1091/adfeab?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1088/2516-1091/adfeab, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2213120573",
          "name": "Ziqi Zhao"
        },
        {
          "authorId": "2332982500",
          "name": "Yibo Hu"
        },
        {
          "authorId": "2240739116",
          "name": "Lisa X Xu"
        },
        {
          "authorId": "2262929885",
          "name": "Jianqi Sun"
        }
      ],
      "abstract": "Image-guided tumor ablation (IGTA) has revolutionized modern oncological treatments by providing minimally invasive options that ensure precise tumor eradication with minimal patient discomfort. Traditional techniques such as ultrasound (US), computed tomography, and magnetic resonance imaging have been instrumental in the planning, execution, and evaluation of ablation therapies. However, these methods often face limitations, including poor contrast, susceptibility to artifacts, and variability in operator expertise, which can undermine the accuracy of tumor targeting and therapeutic outcomes. Incorporating deep learning (DL) into IGTA represents a significant advancement that addresses these challenges. This review explores the role and potential of DL in different phases of tumor ablation therapy: preoperative, intraoperative, and postoperative. In the preoperative stage, DL excels in advanced image segmentation, enhancement, and synthesis, facilitating precise surgical planning and optimized treatment strategies. During the intraoperative phase, DL supports image registration and fusion, and real-time surgical planning, enhancing navigation accuracy and ensuring precise ablation while safeguarding surrounding healthy tissues. In the postoperative phase, DL is pivotal in automating the monitoring of treatment responses and in the early detection of recurrences through detailed analyses of follow-up imaging. This review highlights the essential role of DL in modernizing IGTA, showcasing its significant implications for procedural safety, efficacy, and patient outcomes in oncology. As DL technologies continue to evolve, they are poised to redefine the standards of care in tumor ablation therapies, making treatments more accurate, personalized, and patient-friendly."
    },
    {
      "paperId": "5e3d04870ed3868fce9ccf5490b5a064cd675da4",
      "externalIds": {
        "DOI": "10.1109/IoTAIMA66468.2025.11212779",
        "CorpusId": 282610740
      },
      "corpusId": 282610740,
      "title": "MambaVIIT: Mamba-based Visible-to-Infrared Image Translation",
      "venue": "2025 6th International Conference on Internet of Things, Artificial Intelligence and Mechanical Automation (IoTAIMA)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/IoTAIMA66468.2025.11212779?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/IoTAIMA66468.2025.11212779, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2390888070",
          "name": "Zhichao Xu"
        },
        {
          "authorId": "2144348239",
          "name": "Chenhong Sui"
        },
        {
          "authorId": "2297177477",
          "name": "Shuyi Jia"
        },
        {
          "authorId": "2286258396",
          "name": "Tiantian Tang"
        }
      ],
      "abstract": "The rapid advancement of deep neural networks (DNN) has substantially progressed image-to-image translation, yielding numerous sophisticated methods. However, current approaches face dual limitations: CNN-based architectures struggle with long-range dependencies, while Transformer-based methods incur quadratic computational complexity-both constraining performance enhancement. To overcome these challenges, we introduce MambaVIIT, a novel visible-to-infrared aerial image translation framework inspired by the Visual State Space (VSS) block from VMamba, which possesses both global modeling capability and linear computational complexity. We uniquely combine Mamba\u2019s long-range modeling with CNNs\u2019 local feature extraction strengths to propose Global-Local Perception Block (GLPB) and Global-Local Reconstruction Block (GLRB), achieving robust cross modal mapping with higher fidelity. While pixel-level losses mitigate blurring effects, we further introduce structural similarity (SSIM) loss to preserve high-level features, guaranteeing structural and content consistency between modalities. The experimental results on public datasets demonstrate the performance of MambaVIIT. Compared to existing image-to-image methods, our approach achieves superior performance in both subjective visual quality and objective metric evaluation."
    },
    {
      "paperId": "98b969b04a22864794b97c8e7fc38ae46fb12c2a",
      "externalIds": {
        "DOI": "10.1109/IoTAIMA66468.2025.11212744",
        "CorpusId": 282611507
      },
      "corpusId": 282611507,
      "title": "Multispectral Object Detection via Cross-Modality Gated Interaction Mamba",
      "venue": "2025 6th International Conference on Internet of Things, Artificial Intelligence and Mechanical Automation (IoTAIMA)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/IoTAIMA66468.2025.11212744?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/IoTAIMA66468.2025.11212744, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2333880374",
          "name": "Jie Li"
        },
        {
          "authorId": "2144348239",
          "name": "Chenhong Sui"
        },
        {
          "authorId": "2297177477",
          "name": "Shuyi Jia"
        },
        {
          "authorId": "2391164164",
          "name": "Chen Guo"
        }
      ],
      "abstract": "Multispectral object detection leverages the complementary features of visible (RGB) and infrared (IR) images, achieving significantly better performance than single-modality detection and enhancing robustness in complex environments. Existing methods are primarily based on convolutional neural networks (CNNs) or Transformer architectures. However, CNNs are limited by their local receptive fields, while Transformers, despite their global modeling capability, suffer from efficiency bottlenecks when applied to high-resolution features due to the quadratic complexity of attention. To address these issues, we propose a novel Cross-Modality Gated Interaction Mamba module (CGIM). Within this module, multispectral features are globally modeled through the efficient Mamba framework, while a crossmodality gating mechanism is introduced: the gating signal from the IR modality regulates the update of RGB features, and conversely, the RGB gating signal regulates the IR features. This bidirectional interaction enhances feature complementarity and yields more robust fusion representations. Moreover, we introduce cross-modality fusion at the high-resolution stage and directly connect it to a new detection head, thereby strengthening the perception of small objects and fine-grained details. Experimental results on the public multispectral datasets M3FD and VEDAI demonstrate that the proposed method consistently outperforms single-modality approaches and state-of-the-art multispectral detection methods in terms of detection accuracy."
    },
    {
      "paperId": "6e637cc418eebfcecf6660e5692308be2b38dd99",
      "externalIds": {
        "DOI": "10.1109/EIT67313.2025.11232442",
        "CorpusId": 283015332
      },
      "corpusId": 283015332,
      "title": "Discrimination of neutrons and gamma rays based on Hybrid Mamba",
      "venue": "IEEE International Conference on Electro/Information Technology",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/EIT67313.2025.11232442?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/EIT67313.2025.11232442, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2194888009",
          "name": "Yihan Zhan"
        },
        {
          "authorId": "2295672403",
          "name": "Mingzhe Liu"
        },
        {
          "authorId": "2216764839",
          "name": "Haoran Liu"
        },
        {
          "authorId": "2293395574",
          "name": "Zhuo Zuo"
        }
      ],
      "abstract": "Neutron-gamma pulse shape discrimination (PSD) is critical in nuclear detection systems, enabling reliable particle identification in mixed radiation fields. This study proposes a physics-informed, label-free hybrid PSD framework that integrates the interpretability of traditional models with the adaptability of modern deep sequence architectures. Specifically, the Gatti parameter is used to extract waveform-based discrimination factors, which are then regressed using the Mamba model to enhance feature continuity and robustness. A K-Nearest Neighbors (KNN) classifier (k=5) is employed to assign class labels, avoiding threshold-induced bias. Experimental data were acquired from a TOF setup using an NE213A detector and validated via coincidence tagging. The proposed Hybrid Mamba method achieved an FoM of 0.731\u2014approximately 31% higher than the standalone Gatti method\u2014and an accuracy of 0.916, outperforming the 0.644 accuracy of GP. Importantly, since ground-truth labels are often unavailable in real-world applications, this method operates in a label-free manner. By regressing PSD factors directly from raw signals and applying KNN, it eliminates reliance on labeled data while maintaining excellent performance. In summary, the Hybrid Mamba method demonstrates both high precision on labeled datasets and strong adaptability in unlabeled environments, providing a practical and scalable solution for neutron-gamma discrimination in complex radiation fields."
    },
    {
      "paperId": "78f525270ec26c9be745408430aeb48a87600612",
      "externalIds": {
        "DOI": "10.1109/EIT67313.2025.11231855",
        "CorpusId": 283013539
      },
      "corpusId": 283013539,
      "title": "A Novel Gated Attention Based Mamba Network for Remote Sensing Change Detection",
      "venue": "IEEE International Conference on Electro/Information Technology",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/EIT67313.2025.11231855?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/EIT67313.2025.11231855, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2183710567",
          "name": "Junyi Zhang"
        },
        {
          "authorId": "2244136495",
          "name": "Renwen Chen"
        }
      ],
      "abstract": "Remote sensing change detection (RSCD) identifies surface changes over time using remote sensing (RS) images captured at different times of the same area. Conventional approaches utilizing CNNs and Transformers often face challenges in balancing computational efficiency with global modeling performance. Mamba has recently become a popular choice for RSCD tasks because of its ability to model global context while maintaining linear computational efficiency. To address the challenges of modeling change regions in existing Mamba-based RSCD methods, we propose a noval Mamba-structured network. Specifically, we design a Gated Attention Module (GAM) to dynamically adapt feature extraction, and introduce a Cross-temporal Mamba Fusion (CMF) module to integrate local differences with global features. The proposed approach demonstrates superior performance over existing state-of-the-art methods across three benchmark datasets, with F1 scores of 91.03%, 83.45%, and 92.91%, respectively."
    },
    {
      "paperId": "563ed7f4528dd298c38ff29fcfc5c64d3bb2d4fb",
      "externalIds": {
        "DOI": "10.1109/PSETC65535.2025.11239093",
        "CorpusId": 283097256
      },
      "corpusId": 283097256,
      "title": "A Hybrid Mamba-ECA-UNet Architecture for Multi-Task Non-Intrusive Load Monitoring",
      "venue": "2025 International Power and Sustainable Energy Technologies Conference (PSETC)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/PSETC65535.2025.11239093?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/PSETC65535.2025.11239093, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2370077060",
          "name": "Jiangfeng Liu"
        },
        {
          "authorId": "2276486424",
          "name": "Yanfang Fan"
        },
        {
          "authorId": "2244383539",
          "name": "Xueyan Bai"
        }
      ],
      "abstract": "Non-Intrusive Load Monitoring (NILM), vital for enhancing energy efficiency, faces key challenges: resolving appliance signal superposition and modeling complex long-term temporal dependencies. While standard U-Nets use multi-scale feature extraction to address superposition, they are less effective at capturing long-range temporal patterns and lack mechanisms for adaptive channel optimization. This paper introduces Mamba-ECA-UNet, a hybrid framework designed for accurate multi-task NILM, predicting both appliance power consumption and operational states. The architecture enhances features extracted by the U-Net encoder before utilizing them in skip connections. It first applies bidirectional Mamba modules to efficiently model long-term dependencies with linear computational complexity. Sequentially, an Efficient Channel Attention (ECA) module refines these Mamba-processed features channel-wise. These enhanced features are then transmitted to the decoder via skip connections, significantly improving its ability to perform accurate disaggregation. Experiments on the UK-DALE dataset demonstrate that Mamba-ECA-UNet outperforms the baseline U-Net, reducing Mean Absolute Error (MAE) in power estimation by 28.19%."
    },
    {
      "paperId": "ab37630fd610219f1b3faa1107b117ba27b5180a",
      "externalIds": {
        "DOI": "10.1109/ASIANCON66527.2025.11281193",
        "CorpusId": 283924835
      },
      "corpusId": 283924835,
      "title": "Evolution of Transformers in Speech Recognition",
      "venue": "2025 5th Asian Conference on Innovation in Technology (ASIANCON)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ASIANCON66527.2025.11281193?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ASIANCON66527.2025.11281193, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2399061696",
          "name": "Shabbar Adamjee"
        },
        {
          "authorId": "34915113",
          "name": "Shakti Kinger"
        },
        {
          "authorId": "9220007",
          "name": "S. Bobde"
        },
        {
          "authorId": "2399056835",
          "name": "Jatin Patil"
        },
        {
          "authorId": null,
          "name": "Harmanjit Singh"
        }
      ],
      "abstract": "By outperforming conventional Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) in terms of accuracy and efficiency, Transformer-based architectures have completely transformed Automatic Speech Recognition (ASR). Standard Transformer models, however, suffer from latency problems, memory limitations, and computational bottlenecks. The Conformer, Transformer-XL, Transformer Transducer (T-T), Multi-Channel Transformers, and Diagonal State Space (DSS) Augmented Transformers are some of the major architectural advancements in transformer-based ASR that are examined in this paper. This paper examines how these changes solve ASR issues while improving performance, scalability, and efficiency, and making real-time ASR more feasible. This paper also outlines future research directions in low-latency, energy-efficient ASR models and discusses open problems in hardware optimization, multimodal ASR, and self-supervised learning."
    },
    {
      "paperId": "1f23d5578f7334dcaa6b70aab4f11f59cc01ffb9",
      "externalIds": {
        "ArXiv": "2508.15553",
        "DBLP": "journals/corr/abs-2508-15553",
        "DOI": "10.1109/TGRS.2025.3603390",
        "CorpusId": 280700139
      },
      "corpusId": 280700139,
      "title": "Deep Equilibrium Convolutional Sparse Coding for Hyperspectral Image Denoising",
      "venue": "IEEE Transactions on Geoscience and Remote Sensing",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.15553, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2320068947",
          "name": "Jin Ye"
        },
        {
          "authorId": "2376413779",
          "name": "Jingran Wang"
        },
        {
          "authorId": "31104323",
          "name": "Fengchao Xiong"
        },
        {
          "authorId": "50763211",
          "name": "Jingzhou Chen"
        },
        {
          "authorId": "2261096891",
          "name": "Yuntao Qian"
        }
      ],
      "abstract": "Hyperspectral images (HSIs) play a crucial role in remote sensing but are often degraded by complex noise patterns. Ensuring the physical property of the denoised HSIs is vital for robust HSI denoising, giving the rise of deep unfolding-based methods. However, these methods map the optimization of a physical model to a learnable network with a predefined depth, which lacks convergence guarantees. In contrast, deep equilibrium (DEQ) models treat the hidden layers of deep networks as the solution to a fixed-point problem and models them as infinite-depth networks, naturally consistent with the optimization. Under the framework of DEQ, we propose a deep equilibrium convolutional sparse coding (DECSC) framework that unifies local spatial\u2013spectral correlations, nonlocal spatial self-similarities, and global spatial consistency for robust HSI denoising. Within the convolutional sparse coding (CSC) framework, we enforce shared 2-D convolutional sparse representation to ensure global spatial consistency across bands, while unshared 3-D convolutional sparse representation captures local spatial\u2013spectral details. To further exploit nonlocal self-similarities, a transformer block is embedded after the 2-D CSC. In addition, a detail enhancement module is integrated with the 3-D CSC to promote image detail preservation. We formulate the proximal gradient descent of the CSC model as a fixed-point problem and transform the iterative updates into a learnable network architecture within the framework of DEQ. Experimental results demonstrate that our DECSC method achieves superior denoising performance compared to state-of-the-art methods."
    },
    {
      "paperId": "414c1a99942b61aa5364fd7f8a078b8a831e57ed",
      "externalIds": {
        "DBLP": "journals/corr/abs-2508-15505",
        "ArXiv": "2508.15505",
        "DOI": "10.48550/arXiv.2508.15505",
        "CorpusId": 280699943
      },
      "corpusId": 280699943,
      "title": "Task-Generalized Adaptive Cross-Domain Learning for Multimodal Image Fusion",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.15505, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2397922960",
          "name": "Meng Wang"
        },
        {
          "authorId": "2194377751",
          "name": "Zhenyu Liu"
        },
        {
          "authorId": "2115467392",
          "name": "Kun Li"
        },
        {
          "authorId": "2187473437",
          "name": "Yu Wang"
        },
        {
          "authorId": "2343780110",
          "name": "Yuwei Wang"
        },
        {
          "authorId": "2302311769",
          "name": "Yanyan Wei"
        },
        {
          "authorId": "2355842958",
          "name": "Fei Wang"
        }
      ],
      "abstract": "Multimodal Image Fusion (MMIF) aims to integrate complementary information from different imaging modalities to overcome the limitations of individual sensors. It enhances image quality and facilitates downstream applications such as remote sensing, medical diagnostics, and robotics. Despite significant advancements, current MMIF methods still face challenges such as modality misalignment, high-frequency detail destruction, and task-specific limitations. To address these challenges, we propose AdaSFFuse, a novel framework for task-generalized MMIF through adaptive cross-domain co-fusion learning. AdaSFFuse introduces two key innovations: the Adaptive Approximate Wavelet Transform (AdaWAT) for frequency decoupling, and the Spatial-Frequency Mamba Blocks for efficient multimodal fusion. AdaWAT adaptively separates the high- and low-frequency components of multimodal images from different scenes, enabling fine-grained extraction and alignment of distinct frequency characteristics for each modality. The Spatial-Frequency Mamba Blocks facilitate cross-domain fusion in both spatial and frequency domains, enhancing this process. These blocks dynamically adjust through learnable mappings to ensure robust fusion across diverse modalities. By combining these components, AdaSFFuse improves the alignment and integration of multimodal features, reduces frequency loss, and preserves critical details. Extensive experiments on four MMIF tasks -- Infrared-Visible Image Fusion (IVF), Multi-Focus Image Fusion (MFF), Multi-Exposure Image Fusion (MEF), and Medical Image Fusion (MIF) -- demonstrate AdaSFFuse's superior fusion performance, ensuring both low computational cost and a compact network, offering a strong balance between performance and efficiency. The code will be publicly available at https://github.com/Zhen-yu-Liu/AdaSFFuse."
    },
    {
      "paperId": "4f157904d3f624c018d09dec7d9e95278a2e6f9e",
      "externalIds": {
        "DBLP": "journals/corr/abs-2508-15201",
        "ArXiv": "2508.15201",
        "DOI": "10.48550/arXiv.2508.15201",
        "CorpusId": 280700106
      },
      "corpusId": 280700106,
      "title": "Survey of Vision-Language-Action Models for Embodied Manipulation",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 3,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.15201, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2347365117",
          "name": "Haoran Li"
        },
        {
          "authorId": "2257103422",
          "name": "Yuhui Chen"
        },
        {
          "authorId": "2268397358",
          "name": "Wenbo Cui"
        },
        {
          "authorId": "2347176603",
          "name": "Weiheng Liu"
        },
        {
          "authorId": "2377763386",
          "name": "Kai Liu"
        },
        {
          "authorId": "2376443273",
          "name": "Mingcai Zhou"
        },
        {
          "authorId": "2272449740",
          "name": "Zhengtao Zhang"
        },
        {
          "authorId": "2376522592",
          "name": "Dongbin Zhao"
        }
      ],
      "abstract": "Embodied intelligence systems, which enhance agent capabilities through continuous environment interactions, have garnered significant attention from both academia and industry. Vision-Language-Action models, inspired by advancements in large foundation models, serve as universal robotic control frameworks that substantially improve agent-environment interaction capabilities in embodied intelligence systems. This expansion has broadened application scenarios for embodied AI robots. This survey comprehensively reviews VLA models for embodied manipulation. Firstly, it chronicles the developmental trajectory of VLA architectures. Subsequently, we conduct a detailed analysis of current research across 5 critical dimensions: VLA model structures, training datasets, pre-training methods, post-training methods, and model evaluation. Finally, we synthesize key challenges in VLA development and real-world deployment, while outlining promising future research directions."
    },
    {
      "paperId": "1497e52121c803dba5df900806fffe636e296ef7",
      "externalIds": {
        "DBLP": "journals/corr/abs-2508-15632",
        "ArXiv": "2508.15632",
        "DOI": "10.1109/APSIPAASC65261.2025.11249206",
        "CorpusId": 280700523
      },
      "corpusId": 280700523,
      "title": "ASCMamba: Multimodal Time-Frequency Mamba for Acoustic Scene Classification",
      "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.15632, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2376496336",
          "name": "Bochao Sun"
        },
        {
          "authorId": "2376459490",
          "name": "Dong Wang"
        },
        {
          "authorId": "2306472648",
          "name": "Zhanlong Yang"
        },
        {
          "authorId": "2377701323",
          "name": "Jun Yang"
        },
        {
          "authorId": "2374358643",
          "name": "Han Yin"
        }
      ],
      "abstract": "Acoustic Scene Classification (ASC) is a fundamental problem in computational audition, which seeks to classify environments based on the distinctive acoustic features. In the ASC task of the APSIPA ASC 2025 Grand Challenge, the organizers introduce a multimodal ASC task. Unlike traditional ASC systems that rely solely on audio inputs, this challenge provides additional textual information as inputs, including the location where the audio is recorded and the time of recording. In this paper, we present our proposed system for the ASC task in the APSIPA ASC 2025 Grand Challenge. Specifically, we propose a multimodal network, ASCMamba, which integrates audio and textual information for fine-grained acoustic scene understanding and effective multimodal ASC. The proposed ASCMamba employs a DenseEncoder to extract hierarchical spectral features from spectrograms, followed by a dual-path Mamba blocks that capture long-range temporal and frequency dependencies using Mamba-based state space models. In addition, we present a two-step pseudo-labeling mechanism to generate more reliable pseudo-labels. Results show that the proposed system outperforms all the participating teams and achieves a 6.2 % improvement over the baseline. Code, model and pre-trained checkpoints are available at https://github.com/S-Orion/ASCMamba.git"
    },
    {
      "paperId": "011c7ebb188f9378ce7aeee2449472067affe3a1",
      "externalIds": {
        "PubMedCentral": "12370145",
        "DOI": "10.1371/journal.pone.0330678",
        "CorpusId": 280703968,
        "PubMed": "40839574"
      },
      "corpusId": 280703968,
      "title": "EchoMamba: A new Mamba model for fast and efficient hyperspectral image classification",
      "venue": "PLoS ONE",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12370145, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2328252781",
          "name": "Yancong Zhang"
        },
        {
          "authorId": "2328952975",
          "name": "Xiu Jin"
        },
        {
          "authorId": "2351921261",
          "name": "Xiaodan Zhang"
        },
        {
          "authorId": "2328623424",
          "name": "Yuting Wu"
        },
        {
          "authorId": "2140177482",
          "name": "Lijing Tu"
        }
      ],
      "abstract": "The classification of hyperspectral images (HSI) is an important foundation in the field of remote sensing. Mamba architectures based on state space model (SSM) have shown great potential in the field of HSI processing due to their powerful long-range sequence modeling capabilities and the efficiency advantages of linear computing. Based on this theoretical basis, We propose a novel deep learning framework: long-sequence Mamba (EchoMamba), which combines the powerful long sequence processing capabilities of Long Short-Term Memory(LSTM) and Mamba to further explore the spectral dimension of HSI, and carry out more in-depth mining and learning of the spectral dimension of HSI. Compared with the previous HSI classification model, the experimental results show that EchoMamba can significantly reduce the training time cost of HSI and effectively improve the performance of the classification task.This study not only advances the current state of HSI classification but also provides a robust foundation for future research in spectral-spatial feature extraction and large-scale remote sensing applications."
    },
    {
      "paperId": "ef6bf1515c1f56b28af9625ddd26092b60f84ce7",
      "externalIds": {
        "DBLP": "journals/jksucis/WangFLZ25",
        "DOI": "10.1007/s44443-025-00220-1",
        "CorpusId": 280865628
      },
      "corpusId": 280865628,
      "title": "Mamba and cross-channel aggregation for efficient multispectral image compression",
      "venue": "Journal of King Saud University: Computer and Information Sciences",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s44443-025-00220-1?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s44443-025-00220-1, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2339224900",
          "name": "Jingang Wang"
        },
        {
          "authorId": "2111828330",
          "name": "Qizhi Fang"
        },
        {
          "authorId": "2284028933",
          "name": "Jiahui Liu"
        },
        {
          "authorId": "2377242391",
          "name": "Lili Zhang"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "c6bfb7317ea8d9c312e0e638149421affc9325b1",
      "externalIds": {
        "DBLP": "journals/mms/FengQSTQ25",
        "DOI": "10.1007/s00530-025-01917-z",
        "CorpusId": 280755340
      },
      "corpusId": 280755340,
      "title": "MTSMNet: a multi-scale trend-seasonal mixing network for long-term time series forecasting",
      "venue": "Multimedia Systems",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s00530-025-01917-z?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s00530-025-01917-z, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2374270033",
          "name": "Ruofan Feng"
        },
        {
          "authorId": "2320821027",
          "name": "Jiwei Qin"
        },
        {
          "authorId": "2320730922",
          "name": "Dezhi Sun"
        },
        {
          "authorId": "2374305668",
          "name": "Weilin Tang"
        },
        {
          "authorId": "2352916471",
          "name": "Xizhong Qin"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "50879de0bb3d679e556a45f86aee6915d559750f",
      "externalIds": {
        "ArXiv": "2508.14556",
        "DBLP": "journals/corr/abs-2508-14556",
        "DOI": "10.48550/arXiv.2508.14556",
        "CorpusId": 280691959
      },
      "corpusId": 280691959,
      "title": "Mamba2 Meets Silence: Robust Vocal Source Separation for Sparse Regions",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.14556, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2222018028",
          "name": "Euiyeon Kim"
        },
        {
          "authorId": "2376369445",
          "name": "Yong-Hoon Choi"
        }
      ],
      "abstract": "We introduce a new music source separation model tailored for accurate vocal isolation. Unlike Transformer-based approaches, which often fail to capture intermittently occurring vocals, our model leverages Mamba2, a recent state space model, to better capture long-range temporal dependencies. To handle long input sequences efficiently, we combine a band-splitting strategy with a dual-path architecture. Experiments show that our approach outperforms recent state-of-the-art models, achieving a cSDR of 11.03 dB-the best reported to date-and delivering substantial gains in uSDR. Moreover, the model exhibits stable and consistent performance across varying input lengths and vocal occurrence patterns. These results demonstrate the effectiveness of Mamba-based models for high-resolution audio processing and open up new directions for broader applications in audio research."
    },
    {
      "paperId": "21d22d71f759102094e58c0258afc147071f453f",
      "externalIds": {
        "ArXiv": "2508.15099",
        "CorpusId": 280700374
      },
      "corpusId": 280700374,
      "title": "Hydra: A Modular Architecture for Efficient Long-Context Reasoning",
      "venue": "",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.15099, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2376398983",
          "name": "Siddharth Chaudhary"
        },
        {
          "authorId": "2390568228",
          "name": "Dev Patel"
        },
        {
          "authorId": "2380687024",
          "name": "Maheep Chaudhary"
        },
        {
          "authorId": "2376395363",
          "name": "Bennett Browning"
        }
      ],
      "abstract": "The quadratic complexity of transformers fundamentally limits reasoning system deployment in resource-constrained and long-context settings. We introduce Hydra, a modular architecture based upon a state-space backbone which adaptively routes between complementary efficiency mechanisms: sparse global attention, mixture-of-experts, and dual memories comprising a reasoning workspace and product key memory. We evaluate a 29M parameter model measuring logical chaining accuracy and throughput on synthetic sequences, plus throughput on WikiText. Ablation studies use component-specific synthetic datasets to isolate individual mechanisms. Hydra achieves $3.01\\times$ and $3.0\\times$ throughput gains at 8K tokens for synthetic and WikiText datasets, respectively, and $10\\times$ accuracy improvements on multi-step logical composition compared to equal-sized transformers. Ablations confirm each component's contribution: sparse attention captures long-range dependencies, experts specialize to input domains, and product key memory enables selective retrieval."
    },
    {
      "paperId": "7ec5702aade89b658e2b7a7d0ba5c1699349b8cf",
      "externalIds": {
        "DBLP": "journals/corr/abs-2508-13561",
        "ArXiv": "2508.13561",
        "DOI": "10.48550/arXiv.2508.13561",
        "CorpusId": 280686723
      },
      "corpusId": 280686723,
      "title": "Prediction of Hospital Associated Infections During Continuous Hospital Stays",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.13561, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2374483262",
          "name": "Rituparna Datta"
        },
        {
          "authorId": "5093602",
          "name": "M. Kamruzzaman"
        },
        {
          "authorId": "2239743922",
          "name": "Eili Y. Klein"
        },
        {
          "authorId": "2350312517",
          "name": "Gregory R. Madden"
        },
        {
          "authorId": "2376520475",
          "name": "Xinwei Deng"
        },
        {
          "authorId": "2286739481",
          "name": "Anil Vullikanti"
        },
        {
          "authorId": "101249913",
          "name": "P. Bhattacharya"
        }
      ],
      "abstract": "The US Centers for Disease Control and Prevention (CDC), in 2019, designated Methicillin-resistant Staphylococcus aureus (MRSA) as a serious antimicrobial resistance threat. The risk of acquiring MRSA and suffering life-threatening consequences due to it remains especially high for hospitalized patients due to a unique combination of factors, including: co-morbid conditions, immuno suppression, antibiotic use, and risk of contact with contaminated hospital workers and equipment. In this paper, we present a novel generative probabilistic model, GenHAI, for modeling sequences of MRSA test results outcomes for patients during a single hospitalization. This model can be used to answer many important questions from the perspectives of hospital administrators for mitigating the risk of MRSA infections. Our model is based on the probabilistic programming paradigm, and can be used to approximately answer a variety of predictive, causal, and counterfactual questions. We demonstrate the efficacy of our model by comparing it against discriminative and generative machine learning models using two real-world datasets."
    },
    {
      "paperId": "8766edd7510e8c6cd056fd10f920548f4c376541",
      "externalIds": {
        "PubMedCentral": "12364526",
        "DOI": "10.34133/research.0804",
        "CorpusId": 280688220,
        "PubMed": "40837875"
      },
      "corpusId": 280688220,
      "title": "Generative Artificial Intelligence in the Metaverse Era: A Review on Models and Applications",
      "venue": "Research",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12364526, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2377895014",
          "name": "Han Zhou"
        },
        {
          "authorId": "2376374437",
          "name": "Xinyi Chen"
        },
        {
          "authorId": "2323943325",
          "name": "Jin Li"
        },
        {
          "authorId": "2376373804",
          "name": "Zichen Zhang"
        },
        {
          "authorId": "2376304876",
          "name": "Yao Fu"
        },
        {
          "authorId": "72340450",
          "name": "M. P. Liva"
        },
        {
          "authorId": "2322830056",
          "name": "Dov Greenbaum"
        },
        {
          "authorId": "2376299778",
          "name": "Pan Hui"
        }
      ],
      "abstract": "The Metaverse is a decentralized, immersive 3-dimensional virtual environment that merges the physical and virtual worlds, fundamentally transforming digital interaction and garnering widespread attention. However, its primary platforms face challenges such as low-quality content and underdeveloped virtual environments, leading to a subpar user experience. Artificial intelligence-generated content (AIGC) has emerged as a key driver in Metaverse development, enabling the efficient and cost-effective creation of digital content. AIGC also promotes personalized content, further enhancing the appeal of the Metaverse. Although AIGC holds great promise, comprehensive investigations into its underlying models and applications remain limited. This study begins with the core neural network architectures of generative AI and examines the relationship between the Metaverse and AIGC. It delves into the deep learning technologies that support AIGC, providing both qualitative and quantitative analyses of their advantages, limitations, and hardware constraints. We also review existing practical applications of the Metaverse, highlighting the challenges and future opportunities in key domains such as healthcare and education. The research concludes that while AIGC can markedly accelerate the development of the Metaverse, its technology must be more closely aligned with development needs to deliver a truly immersive experience. The integration of AIGC and the Metaverse represents the convergence of artificial intelligence, computer graphics, and human\u2013computer interaction. This interdisciplinary synergy has the potential to redefine the way we create, interact with, and experience digital environments, pushing the boundaries of creativity and immersion."
    },
    {
      "paperId": "a8fd06e37ce5081cefa0cf8d0c268c9e5b3a55ec",
      "externalIds": {
        "DBLP": "journals/corr/abs-2508-13921",
        "ArXiv": "2508.13921",
        "DOI": "10.1145/3746027.3755351",
        "CorpusId": 280686607
      },
      "corpusId": 280686607,
      "title": "DIME-Net: A Dual-Illumination Adaptive Enhancement Network Based on Retinex and Mixture-of-Experts",
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.13921, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2369273562",
          "name": "Ziang Wang"
        },
        {
          "authorId": "2289215538",
          "name": "Xiaoqin Wang"
        },
        {
          "authorId": "2145384996",
          "name": "Dingyi Wang"
        },
        {
          "authorId": "2338298932",
          "name": "Qiang Li"
        },
        {
          "authorId": "2139902800",
          "name": "Shushan Qiao"
        }
      ],
      "abstract": "Image degradation caused by complex lighting conditions such as low-light and backlit scenarios is commonly encountered in real-world environments, significantly affecting image quality and downstream vision tasks. Most existing methods focus on a single type of illumination degradation and lack the ability to handle diverse lighting conditions in a unified manner. To address this issue, we propose a dual-illumination enhancement framework called DIME-Net. The core of our method is a Mixture-of-Experts illumination estimator module, where a sparse gating mechanism adaptively selects suitable S-curve expert networks based on the illumination characteristics of the input image. By integrating Retinex theory, this module effectively performs enhancement tailored to both low-light and backlit images. To further correct illumination-induced artifacts and color distortions, we design a damage restoration module equipped with Illumination-Aware Cross Attention and Sequential-State Global Attention mechanisms. In addition, we construct a hybrid illumination dataset, MixBL, by integrating existing datasets, allowing our model to achieve robust illumination adaptability through a single training process. Experimental results show that DIME-Net achieves competitive performance on both synthetic and real-world low-light and backlit datasets without any retraining. These results demonstrate its generalization ability and potential for practical multimedia applications under diverse and complex illumination conditions."
    },
    {
      "paperId": "d0c1552663b9d705ca3721b2ffac2d2c503cc9ec",
      "externalIds": {
        "ArXiv": "2508.13907",
        "DBLP": "journals/corr/abs-2508-13907",
        "DOI": "10.48550/arXiv.2508.13907",
        "CorpusId": 280686233
      },
      "corpusId": 280686233,
      "title": "Learning to See Through Flare",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.13907, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2376396659",
          "name": "Xiaopeng Peng"
        },
        {
          "authorId": "26429802",
          "name": "H. Gemar"
        },
        {
          "authorId": "2286867981",
          "name": "Erin F. Fleet"
        },
        {
          "authorId": "2376265914",
          "name": "Kyle Novak"
        },
        {
          "authorId": "13103553",
          "name": "A. Watnik"
        },
        {
          "authorId": "30108972",
          "name": "Grover A. Swartzlander"
        }
      ],
      "abstract": "Machine vision systems are susceptible to laser flare, where unwanted intense laser illumination blinds and distorts its perception of the environment through oversaturation or permanent damage to sensor pixels. We introduce NeuSee, the first computational imaging framework for high-fidelity sensor protection across the full visible spectrum. It jointly learns a neural representation of a diffractive optical element (DOE) and a frequency-space Mamba-GAN network for image restoration. NeuSee system is adversarially trained end-to-end on 100K unique images to suppress the peak laser irradiance as high as $10^6$ times the sensor saturation threshold $I_{\\textrm{sat}}$, the point at which camera sensors may experience damage without the DOE. Our system leverages heterogeneous data and model parallelism for distributed computing, integrating hyperspectral information and multiple neural networks for realistic simulation and image restoration. NeuSee takes into account open-world scenes with dynamically varying laser wavelengths, intensities, and positions, as well as lens flare effects, unknown ambient lighting conditions, and sensor noises. It outperforms other learned DOEs, achieving full-spectrum imaging and laser suppression for the first time, with a 10.1\\% improvement in restored image quality."
    },
    {
      "paperId": "64e97185e721e9b6aeddd4d0e8a831fa3f4f67d4",
      "externalIds": {
        "ArXiv": "2508.13712",
        "DBLP": "journals/corr/abs-2508-13712",
        "DOI": "10.1109/tmi.2025.3601450",
        "CorpusId": 280686525,
        "PubMed": "40839505"
      },
      "corpusId": 280686525,
      "title": "Diversity-enhanced Collaborative Mamba for Semi-supervised Medical Image Segmentation",
      "venue": "IEEE Transactions on Medical Imaging",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.13712, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2292057004",
          "name": "Shumeng Li"
        },
        {
          "authorId": "2151812616",
          "name": "Jian Zhang"
        },
        {
          "authorId": "1785352346",
          "name": "Lei Qi"
        },
        {
          "authorId": "2357218838",
          "name": "Luping Zhou"
        },
        {
          "authorId": "2278812416",
          "name": "Yinghuan Shi"
        },
        {
          "authorId": "2256635942",
          "name": "Yang Gao"
        }
      ],
      "abstract": "Acquiring high-quality annotated data for medical image segmentation is tedious and costly. Semi-supervised segmentation techniques alleviate this burden by leveraging unlabeled data to generate pseudo labels. Recently, advanced state space models, represented by Mamba, have shown efficient handling of long-range dependencies. This drives us to explore their potential in semi-supervised medical image segmentation. In this paper, we propose a novel Diversity-enhanced Collaborative Mamba framework (namely DCMamba) for semi-supervised medical image segmentation, which explores and utilizes the diversity from data, network, and feature perspectives. Firstly, from the data perspective, we develop patch-level weak-strong mixing augmentation with Mamba's scanning modeling characteristics. Moreover, from the network perspective, we introduce a diverse-scan collaboration module, which could benefit from the prediction discrepancies arising from different scanning directions. Furthermore, from the feature perspective, we adopt an uncertainty-weighted contrastive learning mechanism to enhance the diversity of feature representation. Experiments demonstrate that our DCMamba significantly outperforms other semi-supervised medical image segmentation methods, e.g., yielding the latest SSM-based method by 6.69% on the Synapse dataset with 20% labeled data. The code is available at https://github.com/ShumengLI/DCMamba."
    },
    {
      "paperId": "f833dbf8a4a45ccff7a5569c78283927b7c4eefb",
      "externalIds": {
        "ArXiv": "2508.13599",
        "DBLP": "journals/corr/abs-2508-13599",
        "DOI": "10.48550/arXiv.2508.13599",
        "CorpusId": 280686291
      },
      "corpusId": 280686291,
      "title": "Towards Efficient Vision State Space Models via Token Merging",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.13599, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2281955282",
          "name": "Jinyoung Park"
        },
        {
          "authorId": "2189758507",
          "name": "Minseok Son"
        },
        {
          "authorId": "2276574578",
          "name": "Changick Kim"
        }
      ],
      "abstract": "State Space Models (SSMs) have emerged as powerful architectures in computer vision, yet improving their computational efficiency remains crucial for practical and scalable deployment.While token reduction serves as an effective approach for model efficiency, applying it to SSMs requires careful consideration of their unique sequential modeling capabilities.In this work, we propose MaMe, a token-merging strategy tailored for SSM-based vision models.MaMe addresses two key challenges: quantifying token importance and preserving sequential properties. Our approach leverages the state transition parameter $\\mathbf{\\Delta}$ as an informativeness measure and introduces strategic token arrangements to preserve sequential information flow.Extensive experiments demonstrate that MaMe achieves superior efficiency-performance trade-offs for both fine-tuned and off-the-shelf models. Particularly, our approach maintains robustness even under aggressive token reduction where existing methods undergo significant performance degradation.Beyond image classification, MaMe shows strong generalization capabilities across video and audio domains, establishing an effective approach for enhancing efficiency in diverse SSM applications."
    },
    {
      "paperId": "2dd89664211440997a5c5531cd5689288254bae2",
      "externalIds": {
        "PubMedCentral": "12453675",
        "DOI": "10.1093/bioinformatics/btaf456",
        "CorpusId": 272683927,
        "PubMed": "40824067"
      },
      "corpusId": 272683927,
      "title": "The impact of tokenizer selection in genomic language models",
      "venue": "Bioinformatics",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12453675, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2261142643",
          "name": "LeAnn M. Lindsey"
        },
        {
          "authorId": "4292373",
          "name": "N. L. Pershing"
        },
        {
          "authorId": "2299183021",
          "name": "Anisa Habib"
        },
        {
          "authorId": "1401831638",
          "name": "K. Dufault-Thompson"
        },
        {
          "authorId": "2321354453",
          "name": "W. Z. Stephens"
        },
        {
          "authorId": "4549022",
          "name": "A. Blaschke"
        },
        {
          "authorId": "2277846468",
          "name": "Xiaofang Jiang"
        },
        {
          "authorId": "2261141461",
          "name": "Hari Sundar"
        }
      ],
      "abstract": "Abstract Motivation Genomic language models have recently emerged as a new method to decode, interpret, and generate genetic sequences. Existing genomic language models have utilized various tokenization methods, including character tokenization, overlapping and nonoverlapping k-mer tokenization, and byte-pair encoding, a method widely used in natural language models. Genomic sequences differ from natural language because of their low character variability, complex and overlapping features, and inconsistent directionality. These features make subword tokenization in genomic language models significantly different from both traditional language models and protein language models. Results This study explores the impact of tokenization in genomic language models by evaluating their downstream performance on 44 classification fine-tuning tasks. We also perform a direct comparison of byte pair encoding and character tokenization in Mamba, a state-space model. Our results indicate that character tokenization outperforms subword tokenization methods on tasks that rely on nucleotide-level resolution, such as splice site prediction and promoter detection. While byte-pair tokenization had stronger performance on the SARS-CoV-2 variant classification task, we observed limited statistically significant differences between tokenization methods on the remaining downstream tasks. Availability and implementation Detailed results of all benchmarking experiments are available in https://github.com/leannmlindsey/DNAtokenization. Training datasets and pretrained models are available at https://huggingface.co/datasets/leannmlindsey. Datasets and processing scripts are available at doi: 10.5281/zenodo.16287401 and doi: 10.5281/zenodo.16287130."
    },
    {
      "paperId": "cc4c521daa2871e16e7a7aadf839c697b4b6a463",
      "externalIds": {
        "ArXiv": "2508.12986",
        "DBLP": "journals/corr/abs-2508-12986",
        "DOI": "10.48550/arXiv.2508.12986",
        "CorpusId": 280677612
      },
      "corpusId": 280677612,
      "title": "Point upsampling networks for single-photon sensing",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.12986, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2317451463",
          "name": "Jinyi Liu"
        },
        {
          "authorId": "2323747258",
          "name": "Guoyang Zhao"
        },
        {
          "authorId": "2116183973",
          "name": "Lijun Liu"
        },
        {
          "authorId": "2376217148",
          "name": "Yiguang Hong"
        },
        {
          "authorId": "2317140641",
          "name": "Weiping Zhang"
        },
        {
          "authorId": "2267929532",
          "name": "Shuming Cheng"
        }
      ],
      "abstract": "Single-photon sensing has generated great interest as a prominent technique of long-distance and ultra-sensitive imaging, however, it tends to yield sparse and spatially biased point clouds, thus limiting its practical utility. In this work, we propose using point upsampling networks to increase point density and reduce spatial distortion in single-photon point cloud. Particularly, our network is built on the state space model which integrates a multi-path scanning mechanism to enrich spatial context, a bidirectional Mamba backbone to capture global geometry and local details, and an adaptive upsample shift module to correct offset-induced distortions. Extensive experiments are implemented on commonly-used datasets to confirm its high reconstruction accuracy and strong robustness to the distortion noise, and also on real-world data to demonstrate that our model is able to generate visually consistent, detail-preserving, and noise suppressed point clouds. Our work is the first to establish the upsampling framework for single-photon sensing, and hence opens a new avenue for single-photon sensing and its practical applications in the downstreaming tasks."
    },
    {
      "paperId": "2062464e8bb52dd96240bfa3df007f0471002b0e",
      "externalIds": {
        "ArXiv": "2508.12931",
        "DBLP": "journals/corr/abs-2508-12931",
        "DOI": "10.48550/arXiv.2508.12931",
        "CorpusId": 280677198
      },
      "corpusId": 280677198,
      "title": "Towards High-Resolution Industrial Image Anomaly Detection",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.12931, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2154545700",
          "name": "Ximiao Zhang"
        },
        {
          "authorId": "2267002220",
          "name": "Min Xu"
        },
        {
          "authorId": "2254873267",
          "name": "Xiuzhuang Zhou"
        }
      ],
      "abstract": "Current anomaly detection methods primarily focus on low-resolution scenarios. For high-resolution images, conventional downsampling often results in missed detections of subtle anomalous regions due to the loss of fine-grained discriminative information. Despite some progress, recent studies have attempted to improve detection resolution by employing lightweight networks or using simple image tiling and ensemble methods. However, these approaches still struggle to meet the practical demands of industrial scenarios in terms of detection accuracy and efficiency. To address the above issues, we propose HiAD, a general framework for high-resolution anomaly detection. HiAD is capable of detecting anomalous regions of varying sizes in high-resolution images under limited computational resources. Specifically, HiAD employs a dual-branch architecture that integrates anomaly cues across different scales to comprehensively capture both subtle and large-scale anomalies. Furthermore, it incorporates a multi-resolution feature fusion strategy to tackle the challenges posed by fine-grained texture variations in high-resolution images. To enhance both adaptability and efficiency, HiAD utilizes a detector pool in conjunction with various detector assignment strategies, enabling detectors to be adaptively assigned based on patch features, ensuring detection performance while effectively controlling computational costs. We conduct extensive experiments on our specifically constructed high-resolution anomaly detection benchmarks, including MVTec-HD, VisA-HD, and the real-world benchmark RealIAD-HD, demonstrating the superior performance of HiAD. The code is available at https://github.com/cnulab/HiAD."
    },
    {
      "paperId": "1535e0af4e51810e07b6aa4bf413796d406336c9",
      "externalIds": {
        "DBLP": "journals/corr/abs-2508-12839",
        "ArXiv": "2508.12839",
        "DOI": "10.48550/arXiv.2508.12839",
        "CorpusId": 280676789
      },
      "corpusId": 280676789,
      "title": "HRS: Hybrid Representation Framework with Scheduling Awareness for Time Series Forecasting in Crowdsourced Cloud-Edge Platforms",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.12839, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2257036893",
          "name": "Tiancheng Zhang"
        },
        {
          "authorId": "2257333879",
          "name": "Cheng Zhang"
        },
        {
          "authorId": "2355448833",
          "name": "Shuren Liu"
        },
        {
          "authorId": "2285033732",
          "name": "Xiaofei Wang"
        },
        {
          "authorId": "15634370",
          "name": "Shaoyuan Huang"
        },
        {
          "authorId": "2108498302",
          "name": "Wenyu Wang"
        }
      ],
      "abstract": "With the rapid proliferation of streaming services, network load exhibits highly time-varying and bursty behavior, posing serious challenges for maintaining Quality of Service (QoS) in Crowdsourced Cloud-Edge Platforms (CCPs). While CCPs leverage Predict-then-Schedule architecture to improve QoS and profitability, accurate load forecasting remains challenging under traffic surges. Existing methods either minimize mean absolute error, resulting in underprovisioning and potential Service Level Agreement (SLA) violations during peak periods, or adopt conservative overprovisioning strategies, which mitigate SLA risks at the expense of increased resource expenditure. To address this dilemma, we propose HRS, a hybrid representation framework with scheduling awareness that integrates numerical and image-based representations to better capture extreme load dynamics. We further introduce a Scheduling-Aware Loss (SAL) that captures the asymmetric impact of prediction errors, guiding predictions that better support scheduling decisions. Extensive experiments on four real-world datasets demonstrate that HRS consistently outperforms ten baselines and achieves state-of-the-art performance, reducing SLA violation rates by 63.1% and total profit loss by 32.3%."
    },
    {
      "paperId": "dce821cddbf6a3eb435de04d9353681925aaaecf",
      "externalIds": {
        "ArXiv": "2508.13073",
        "DBLP": "journals/corr/abs-2508-13073",
        "DOI": "10.48550/arXiv.2508.13073",
        "CorpusId": 280677209
      },
      "corpusId": 280677209,
      "title": "Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 25,
      "influentialCitationCount": 1,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.13073, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2267332946",
          "name": "Rui Shao"
        },
        {
          "authorId": "2348652837",
          "name": "Wei Li"
        },
        {
          "authorId": "2374285556",
          "name": "Lingsen Zhang"
        },
        {
          "authorId": "2312273317",
          "name": "Renshan Zhang"
        },
        {
          "authorId": "2320472261",
          "name": "Zhiyang Liu"
        },
        {
          "authorId": "2376230061",
          "name": "Ran Chen"
        },
        {
          "authorId": "2258952251",
          "name": "Liqiang Nie"
        }
      ],
      "abstract": "Robotic manipulation, a key frontier in robotics and embodied AI, requires precise motor control and multimodal understanding, yet traditional rule-based methods fail to scale or generalize in unstructured, novel environments. In recent years, Vision-Language-Action (VLA) models, built upon Large Vision-Language Models (VLMs) pretrained on vast image-text datasets, have emerged as a transformative paradigm. This survey provides the first systematic, taxonomy-oriented review of large VLM-based VLA models for robotic manipulation. We begin by clearly defining large VLM-based VLA models and delineating two principal architectural paradigms: (1) monolithic models, encompassing single-system and dual-system designs with differing levels of integration; and (2) hierarchical models, which explicitly decouple planning from execution via interpretable intermediate representations. Building on this foundation, we present an in-depth examination of large VLM-based VLA models: (1) integration with advanced domains, including reinforcement learning, training-free optimization, learning from human videos, and world model integration; (2) synthesis of distinctive characteristics, consolidating architectural traits, operational strengths, and the datasets and benchmarks that support their development; (3) identification of promising directions, including memory mechanisms, 4D perception, efficient adaptation, multi-agent cooperation, and other emerging capabilities. This survey consolidates recent advances to resolve inconsistencies in existing taxonomies, mitigate research fragmentation, and fill a critical gap through the systematic integration of studies at the intersection of large VLMs and robotic manipulation. We provide a regularly updated project page to document ongoing progress: https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation"
    },
    {
      "paperId": "4b5b36aa95d58ca367d6caa4da7655994cf06a09",
      "externalIds": {
        "DBLP": "journals/corr/abs-2508-12650",
        "ArXiv": "2508.12650",
        "DOI": "10.48550/arXiv.2508.12650",
        "CorpusId": 280677657
      },
      "corpusId": 280677657,
      "title": "Score-informed Neural Operator for Enhancing Ordering-based Causal Discovery",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.12650, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2376361530",
          "name": "Jiyeon Kang"
        },
        {
          "authorId": "2376210474",
          "name": "Songseong Kim"
        },
        {
          "authorId": "2300028707",
          "name": "Chanhui Lee"
        },
        {
          "authorId": "2381985408",
          "name": "Doyeong Hwang"
        },
        {
          "authorId": "2376421176",
          "name": "Joanie Hayoun Chung"
        },
        {
          "authorId": "2376194371",
          "name": "Yunkyung Ko"
        },
        {
          "authorId": "2376206739",
          "name": "Sumin Lee"
        },
        {
          "authorId": "2323795343",
          "name": "Sungwoong Kim"
        },
        {
          "authorId": "2267387269",
          "name": "Sungbin Lim"
        }
      ],
      "abstract": "Ordering-based approaches to causal discovery identify topological orders of causal graphs, providing scalable alternatives to combinatorial search methods. Under the Additive Noise Model (ANM) assumption, recent causal ordering methods based on score matching require an accurate estimation of the Hessian diagonal of the log-densities. In this paper, we aim to improve the approximation of the Hessian diagonal of the log-densities, thereby enhancing the performance of ordering-based causal discovery algorithms. Existing approaches that rely on Stein gradient estimators are computationally expensive and memory-intensive, while diffusion-model-based methods remain unstable due to the second-order derivatives of score models. To alleviate these problems, we propose Score-informed Neural Operator (SciNO), a probabilistic generative model in smooth function spaces designed to stably approximate the Hessian diagonal and to preserve structural information during the score modeling. Empirical results show that SciNO reduces order divergence by 42.7% on synthetic graphs and by 31.5% on real-world datasets on average compared to DiffAN, while maintaining memory efficiency and scalability. Furthermore, we propose a probabilistic control algorithm for causal reasoning with autoregressive models that integrates SciNO's probability estimates with autoregressive model priors, enabling reliable data-driven causal ordering informed by semantic information. Consequently, the proposed method enhances causal reasoning abilities of LLMs without additional fine-tuning or prompt engineering."
    },
    {
      "paperId": "12169316b29be5c15857b73979a3713c7545774f",
      "externalIds": {
        "ArXiv": "2508.12410",
        "DBLP": "journals/corr/abs-2508-12410",
        "DOI": "10.48550/arXiv.2508.12410",
        "CorpusId": 280676924
      },
      "corpusId": 280676924,
      "title": "SRMA-Mamba: Spatial Reverse Mamba Attention Network for Pathological Liver Segmentation in MRI Volumes",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.12410, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2349557296",
          "name": "Jun Zeng"
        },
        {
          "authorId": "2376352832",
          "name": "Yannan Huang"
        },
        {
          "authorId": "2166046270",
          "name": "Elif Keles"
        },
        {
          "authorId": "2327049418",
          "name": "Halil Ertugrul Aktas"
        },
        {
          "authorId": "2275241127",
          "name": "Gorkem Durak"
        },
        {
          "authorId": "40023276",
          "name": "Nikhil Kumar Tomar"
        },
        {
          "authorId": "2000457754",
          "name": "Quoc-Huy Trinh"
        },
        {
          "authorId": "2356400545",
          "name": "Deepak Rajan Nayak"
        },
        {
          "authorId": "2237423187",
          "name": "Ulas Bagci"
        },
        {
          "authorId": "34665941",
          "name": "Debesh Jha"
        }
      ],
      "abstract": "Liver Cirrhosis plays a critical role in the prognosis of chronic liver disease. Early detection and timely intervention are critical in significantly reducing mortality rates. However, the intricate anatomical architecture and diverse pathological changes of liver tissue complicate the accurate detection and characterization of lesions in clinical settings. Existing methods underutilize the spatial anatomical details in volumetric MRI data, thereby hindering their clinical effectiveness and explainability. To address this challenge, we introduce a novel Mamba-based network, SRMA-Mamba, designed to model the spatial relationships within the complex anatomical structures of MRI volumes. By integrating the Spatial Anatomy-Based Mamba module (SABMamba), SRMA-Mamba performs selective Mamba scans within liver cirrhotic tissues and combines anatomical information from the sagittal, coronal, and axial planes to construct a global spatial context representation, enabling efficient volumetric segmentation of pathological liver structures. Furthermore, we introduce the Spatial Reverse Attention module (SRMA), designed to progressively refine cirrhotic details in the segmentation map, utilizing both the coarse segmentation map and hierarchical encoding features. Extensive experiments demonstrate that SRMA-Mamba surpasses state-of-the-art methods, delivering exceptional performance in 3D pathological liver segmentation. Our code is available for public: https://github.com/JunZengz/SRMA-Mamba."
    },
    {
      "paperId": "b377783fb655964795afb80eb6a0136cfa7d2cb4",
      "externalIds": {
        "ArXiv": "2508.12411",
        "DBLP": "journals/corr/abs-2508-12411",
        "DOI": "10.48550/arXiv.2508.12411",
        "CorpusId": 280677347
      },
      "corpusId": 280677347,
      "title": "The Cultural Gene of Large Language Models: A Study on the Impact of Cross-Corpus Training on Model Values and Biases",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.12411, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2376196812",
          "name": "Emanuel Z. Fenech-Borg"
        },
        {
          "authorId": "2376195549",
          "name": "Tilen P. Meznaric-Kos"
        },
        {
          "authorId": "2376196549",
          "name": "Milica D. Lekovic-Bojovic"
        },
        {
          "authorId": "2376196297",
          "name": "Arni J. Hentze-Djurhuus"
        }
      ],
      "abstract": "Large language models (LLMs) are deployed globally, yet their underlying cultural and ethical assumptions remain underexplored. We propose the notion of a\"cultural gene\"-- a systematic value orientation that LLMs inherit from their training corpora -- and introduce a Cultural Probe Dataset (CPD) of 200 prompts targeting two classic cross-cultural dimensions: Individualism-Collectivism (IDV) and Power Distance (PDI). Using standardized zero-shot prompts, we compare a Western-centric model (GPT-4) and an Eastern-centric model (ERNIE Bot). Human annotation shows significant and consistent divergence across both dimensions. GPT-4 exhibits individualistic and low-power-distance tendencies (IDV score approx 1.21; PDI score approx -1.05), while ERNIE Bot shows collectivistic and higher-power-distance tendencies (IDV approx -0.89; PDI approx 0.76); differences are statistically significant (p<0.001). We further compute a Cultural Alignment Index (CAI) against Hofstede's national scores and find GPT-4 aligns more closely with the USA (e.g., IDV CAI approx 0.91; PDI CAI approx 0.88) whereas ERNIE Bot aligns more closely with China (IDV CAI approx 0.85; PDI CAI approx 0.81). Qualitative analyses of dilemma resolution and authority-related judgments illustrate how these orientations surface in reasoning. Our results support the view that LLMs function as statistical mirrors of their cultural corpora and motivate culturally aware evaluation and deployment to avoid algorithmic cultural hegemony."
    },
    {
      "paperId": "fd491170a8e29b59b9fedbb609aba8b616ca9cd3",
      "externalIds": {
        "DBLP": "journals/corr/abs-2508-12491",
        "ArXiv": "2508.12491",
        "DOI": "10.48550/arXiv.2508.12491",
        "CorpusId": 280677802
      },
      "corpusId": 280677802,
      "title": "Cost-Aware Contrastive Routing for LLMs",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.12491, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2055028725",
          "name": "Reza Shirkavand"
        },
        {
          "authorId": "9355577",
          "name": "Shangqian Gao"
        },
        {
          "authorId": "2313920324",
          "name": "Peiran Yu"
        },
        {
          "authorId": "2243268584",
          "name": "Heng Huang"
        }
      ],
      "abstract": "We study cost-aware routing for large language models across diverse and dynamic pools of models. Existing approaches often overlook prompt-specific context, rely on expensive model profiling, assume a fixed set of experts, or use inefficient trial-and-error strategies. We introduce Cost-Spectrum Contrastive Routing (CSCR), a lightweight framework that maps both prompts and models into a shared embedding space to enable fast, cost-sensitive selection. CSCR uses compact, fast-to-compute logit footprints for open-source models and perplexity fingerprints for black-box APIs. A contrastive encoder is trained to favor the cheapest accurate expert within adaptive cost bands. At inference time, routing reduces to a single k-NN lookup via a FAISS index, requiring no retraining when the expert pool changes and enabling microsecond latency. Across multiple benchmarks, CSCR consistently outperforms baselines, improving the accuracy-cost tradeoff by up to 25%, while generalizing robustly to unseen LLMs and out-of-distribution prompts."
    },
    {
      "paperId": "1b30773cac8955d8f0e73329c73727d61367f50a",
      "externalIds": {
        "ArXiv": "2508.12247",
        "DBLP": "journals/corr/abs-2508-12247",
        "DOI": "10.48550/arXiv.2508.12247",
        "CorpusId": 280676484
      },
      "corpusId": 280676484,
      "title": "STM3: Mixture of Multiscale Mamba for Long-Term Spatio-Temporal Time-Series Prediction",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.12247, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2321484859",
          "name": "Haolong Chen"
        },
        {
          "authorId": "2290883726",
          "name": "Liangyin Zhang"
        },
        {
          "authorId": "2376204315",
          "name": "Zhengyuan Xin"
        },
        {
          "authorId": "2330382982",
          "name": "Guangxu Zhu"
        }
      ],
      "abstract": "Recently, spatio-temporal time-series prediction has developed rapidly, yet existing deep learning methods struggle with learning complex long-term spatio-temporal dependencies efficiently. The long-term spatio-temporal dependency learning brings two new challenges: 1) The long-term temporal sequence includes multiscale information naturally which is hard to extract efficiently; 2) The multiscale temporal information from different nodes is highly correlated and hard to model. To address these challenges, we propose an efficient \\textit{\\textbf{S}patio-\\textbf{T}emporal \\textbf{M}ultiscale \\textbf{M}amba} (STM2) that includes a multiscale Mamba architecture to capture the multiscale information efficiently and simultaneously, and an adaptive graph causal convolution network to learn the complex multiscale spatio-temporal dependency. STM2 includes hierarchical information aggregation for different-scale information that guarantees their distinguishability. To capture diverse temporal dynamics across all spatial nodes more efficiently, we further propose an enhanced version termed \\textit{\\textbf{S}patio-\\textbf{T}emporal \\textbf{M}ixture of \\textbf{M}ultiscale \\textbf{M}amba} (STM3) that employs a special Mixture-of-Experts architecture, including a more stable routing strategy and a causal contrastive learning strategy to enhance the scale distinguishability. We prove that STM3 has much better routing smoothness and guarantees the pattern disentanglement for each expert successfully. Extensive experiments on real-world benchmarks demonstrate STM2/STM3's superior performance, achieving state-of-the-art results in long-term spatio-temporal time-series prediction."
    },
    {
      "paperId": "362dc1b7b32c47550fbd7eaf3fe096b7729ca59f",
      "externalIds": {
        "DBLP": "conf/interspeech/ZevallosGSMLH25",
        "DOI": "10.21437/interspeech.2025-1624",
        "CorpusId": 281218961
      },
      "corpusId": 281218961,
      "title": "Assessing the Performance and Efficiency of Mamba ASR in Low-Resource Scenarios",
      "venue": "Interspeech",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.21437/interspeech.2025-1624?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.21437/interspeech.2025-1624, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2372260950",
          "name": "Rodolfo Zevallos"
        },
        {
          "authorId": "2365357443",
          "name": "Mart\u00ed Cortada Garcia"
        },
        {
          "authorId": "2329457930",
          "name": "Sarah Solito"
        },
        {
          "authorId": "2372264818",
          "name": "Carlos Mena"
        },
        {
          "authorId": "3160983",
          "name": "Alex Peir\u00f3 Lilja"
        },
        {
          "authorId": "2329457598",
          "name": "Javier Hernando"
        }
      ],
      "abstract": "Mamba, a state space model-based architecture, is emerging as a strong alternative to Transformer models, showing equal or superior performance in sequence generation, including speech. However, analyses have focused mainly on high-resource scenarios. This paper explores Mamba\u2019s potential in ASR for low-resource scenarios. We compare the Transformer-based Conformer and its state-space counterpart, ConMamba, across nine languages with varying training data. Our results show that ConMamba achieves similar WER to Conformer for short-context inputs but significantly improves performance on long-context inputs, reducing WER by up to 50% on average. Additionally, ConMamba enhances efficiency, requiring 40\u201345% less training time, using 50% less memory, and accelerating inference by 63\u201370%, making it a more effective ASR solution across different data availability scenarios."
    },
    {
      "paperId": "2c7f1ee2fd3285ff78660dad67370f711753ccc8",
      "externalIds": {
        "DBLP": "conf/interspeech/SongXZFL25",
        "DOI": "10.21437/interspeech.2025-1343",
        "CorpusId": 281219402
      },
      "corpusId": 281219402,
      "title": "REB-former: RWKV-enhanced E-branchformer for Speech Recognition",
      "venue": "Interspeech",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.21437/interspeech.2025-1343?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.21437/interspeech.2025-1343, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2379894095",
          "name": "Jie Song"
        },
        {
          "authorId": "2247792062",
          "name": "Wang Xiang"
        },
        {
          "authorId": "2312525015",
          "name": "Jian Zhou"
        },
        {
          "authorId": "2298729994",
          "name": "Cunhang Fan"
        },
        {
          "authorId": "2265104670",
          "name": "Zhao Lv"
        }
      ],
      "abstract": "Transformer-based architectures have achieved significant success in automatic speech recognition (ASR). However, the quadratic complexity of their self-attention mechanisms limits processing efficiency for speech sequences. To address this issue, this paper proposes the Receptance Weighted Key Value (RWKV)-enhanced E-Branchformer (REB-former). Specifically, the REB-former interleaves the E-Branchformer and RWKV layers, combining different attention mechanisms to reduce computational complexity and enhance speech modeling. To overcome RWKV\u2019s unidirectional limitation, we introduce the GroupBiRWKV module for efficient contextual feature capture. The results show that the REB-former outperforms the E-Branchformer in terms of computational efficiency and inference speed, achieving a relative reduction of up to 7.1% in the word error rate (WER). On the LibriSpeech 100h dataset, our model achieves WER of 6.0%/15.8% on test-clean/test-other, setting a new state-of-the-art performance."
    },
    {
      "paperId": "99610ed917f5cc92c70017b13accbb7ef157ecc0",
      "externalIds": {
        "DBLP": "conf/interspeech/KimKC25",
        "DOI": "10.21437/interspeech.2025-1476",
        "CorpusId": 281301127
      },
      "corpusId": 281301127,
      "title": "Mamba-based Hybrid Model for Speech Enhancement",
      "venue": "Interspeech",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.21437/interspeech.2025-1476?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.21437/interspeech.2025-1476, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2374005476",
          "name": "Se-Ha Kim"
        },
        {
          "authorId": "2373971565",
          "name": "Tae-Gyeong Kim"
        },
        {
          "authorId": "2373850918",
          "name": "Chang-Jae Chun"
        }
      ],
      "abstract": "In this study, we propose MH-SENet, which is designed for speech enhancement by extracting the temporal and spectral features of speech signals in parallel. MH-SENet, which is based on the U-Net architecture, has an encoder and decoder consisting of a bi-directional Mamba and processes it more precisely by considering all the context of the input sequence. Furthermore, a cross-domain Mamba-Transformer block is constructed between the encoder and decoder to effectively fuse information between each time and frequency domains. We evaluated the performance of our proposed MH-SENet on the VCTK + DEMAND dataset and thus it outperformed existing methods by achieving the highest PESQ score. Despite being a hybrid model, the proposed MH-SENet has a lower number of parameters compared to the conventional models."
    },
    {
      "paperId": "93e79f3f394f2fe53bdd708749cf79b1b6697c1b",
      "externalIds": {
        "DBLP": "conf/interspeech/Shi0T25",
        "DOI": "10.21437/interspeech.2025-1433",
        "CorpusId": 281419635
      },
      "corpusId": 281419635,
      "title": "Who, When, and What: Leveraging the \"Three Ws\" Concept for Emotion Recognition in Conversation",
      "venue": "Interspeech",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.21437/interspeech.2025-1433?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.21437/interspeech.2025-1433, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2221271518",
          "name": "Xiaohan Shi"
        },
        {
          "authorId": "2259048344",
          "name": "Xingfeng Li"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        }
      ],
      "abstract": "Emotion Recognition in Conversation (ERC) is essential for dialogue systems in human-computer interaction. Most existing studies primarily focus on modeling contextual information from historical interactions but often overlook the effective integration of speaker and content information. To address these challenges, we propose the \u201cThree Ws\u201d concept\u2014Who, When, and What, representing speaker, context, and content informa-tion\u2014to comprehensively capture emotional cues from historical interactions. Building on this concept, we further introduce a novel model for ERC. Additionally, we incorporate a speaker similarity loss to enhance speaker information. Experimental results show that our model outperforms baselines, with each component making significant contributions\u2014especially context information. Additionally, the speaker similarity loss further improves ERC performance. Notably, the \u201cThree Ws\u201d concept demonstrates robustness across both single-modal and multimodal scenarios."
    },
    {
      "paperId": "02b13408f3fd58aa8f8bde133594c0d9ea2e881e",
      "externalIds": {
        "DBLP": "conf/interspeech/YuKXW25",
        "DOI": "10.21437/interspeech.2025-1448",
        "CorpusId": 281887984
      },
      "corpusId": 281887984,
      "title": "Online AV-CrossNet: a Causal and Efficient Audiovisual System for Speech Enhancement and Target Speaker Extraction",
      "venue": "Interspeech",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.21437/interspeech.2025-1448?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.21437/interspeech.2025-1448, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2306950387",
          "name": "Cheng Yu"
        },
        {
          "authorId": "2153125388",
          "name": "Vahid Ahmadi Kalkhorani"
        },
        {
          "authorId": "2279667302",
          "name": "Buye Xu"
        },
        {
          "authorId": "2290204603",
          "name": "DeLiang Wang"
        }
      ],
      "abstract": "This paper presents online AV-CrossNet, a computationally efficient audiovisual speech enhancement/extraction system capable of causal and real-time processing. We aim to improve the state-of-the-art AV-CrossNet by enabling causal, frame-by-frame processing. To achieve this, we incorporate causal layers and compression techniques, reduce model size, and employ only one-frame look-ahead, thereby substantially enhancing real-world applicability. Additionally, we analyze compression ratio in both audio and visual modules, providing valuable insights into audiovisual model compression. Experimental re-sults demonstrate an inference latency of 4.73 ms, capable of real-time processing. Moreover, the system maintains competitive performance while reducing size by a factor of 10. These findings highlight the efficiency and effectiveness of the proposed system, offering a promising solution for real-time audiovisual speech enhancement and speaker extraction in acous-tically adverse environments."
    },
    {
      "paperId": "de45a904fb456e9b67662e4ded5a61653be1c780",
      "externalIds": {
        "DBLP": "conf/interspeech/MoriyaMMSM25",
        "DOI": "10.21437/interspeech.2025-1079",
        "CorpusId": 281895852
      },
      "corpusId": 281895852,
      "title": "Attention-Free Dual-Mode ASR with Latency-Controlled Selective State Spaces",
      "venue": "Interspeech",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.21437/interspeech.2025-1079?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.21437/interspeech.2025-1079, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "3334164",
          "name": "Takafumi Moriya"
        },
        {
          "authorId": "2309247572",
          "name": "Masato Mimura"
        },
        {
          "authorId": "2056136055",
          "name": "Kiyoaki Matsui"
        },
        {
          "authorId": "153515534",
          "name": "Hiroshi Sato"
        },
        {
          "authorId": "2054405875",
          "name": "Kohei Matsuura"
        }
      ],
      "abstract": "This paper introduces a novel encoder architecture designed to enhance transducer-based dual-mode automatic speech recognition (ASR). Our approach leverages the selective state-space model, Mamba, to enable attention-free dual-mode ASR. While bidirectional Mamba (BiMamba) captures full context and enables constant-time inference, unlike attention-based models with quadratic complexity, it is limited to offline processing. In contrast, using only unidirectional Mamba for the dual-mode ASR degrades ASR performance in both offline and streaming modes due to its restricted access to future contexts. To address this issue, we propose the latency-controlled Bi-Mamba (LC-BiMamba); it enables chunk-wise processing in streaming mode while still accessing future contexts and functioning as standard BiMamba does in offline mode. Experimental results demonstrate that LC-BiMamba outperforms the base-line Conformer system, achieving faster and more accurate decoding in our dual-mode ASR framework"
    },
    {
      "paperId": "74c7be5b605fdc88e1f776affd59c3ca2f51d830",
      "externalIds": {
        "DBLP": "conf/interspeech/TranLGSDM25",
        "DOI": "10.21437/interspeech.2025-1703",
        "CorpusId": 281952157
      },
      "corpusId": 281952157,
      "title": "Leveraging SSL Speech Features and Mamba for Enhanced DeepFake Detection",
      "venue": "Interspeech",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.21437/interspeech.2025-1703?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.21437/interspeech.2025-1703, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2319450659",
          "name": "Hoan My Tran"
        },
        {
          "authorId": "2255493375",
          "name": "Damien Lolive"
        },
        {
          "authorId": "1830176",
          "name": "David Guennec"
        },
        {
          "authorId": "2915629",
          "name": "Aghilas Sini"
        },
        {
          "authorId": "2319368615",
          "name": "Arnaud Delhay"
        },
        {
          "authorId": "1794352",
          "name": "P. Marteau"
        }
      ],
      "abstract": "Large-scale self-supervised learning models have proven highly effective in extracting robust features for detecting both genuine and spoofed speech. However, leveraging these features remains challenging, particularly in balancing in-domain specialization with out-of-domain generalization. In this work, we propose an effective approach for automatically selecting features using self-gated mechanism, and aggregation of speech representations from a pretrained foundation model to enhance deepfake detection. Our approach integrates a multi-kernel gated convolution module to improve feature learning and facilitate the fusion of features. Additionally, we employ Mamba to effectively capture both short and long-range discriminative patterns in speech. The proposed method achieves strong performance in audio deepfake detection, demonstrating improved generalization across diverse datasets. The source code will be made available on Github 1 ."
    },
    {
      "paperId": "88936a168dcc26e06c590f3ee943ec0fb175ae6c",
      "externalIds": {
        "DBLP": "conf/interspeech/ShiM0T25",
        "DOI": "10.21437/interspeech.2025-1445",
        "CorpusId": 282065925
      },
      "corpusId": 282065925,
      "title": "Advancing Emotion Recognition via Ensemble Learning: Integrating Speech, Context, and Text Representations",
      "venue": "Interspeech",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.21437/interspeech.2025-1445?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.21437/interspeech.2025-1445, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2221271518",
          "name": "Xiaohan Shi"
        },
        {
          "authorId": "2323509142",
          "name": "Jinyi Mi"
        },
        {
          "authorId": "2259048344",
          "name": "Xingfeng Li"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        }
      ],
      "abstract": "Speech Emotion Recognition (SER) in real-world scenarios aims to identify a speaker\u2019s emotional states from spontaneous speech. While prior research has focused on noise reduction techniques within individual domains, integrating multi-domain noise-robust representations for SER remains underexplored. To address this challenge, we propose a novel Speech-Context-Text (SCT) model, which integrates speech, context, and text representations via ensemble learning. Specifically, we introduce the Mamba method for speech representation, employ a layer adapter to capture context representation, and adopt ASR correction to refine text representation. Extensive experiments demonstrate the effectiveness of SCT, achieving a 7.4% Macro-F1 improvement over the official baseline of the Speech Emotion Recognition in Naturalistic Conditions Challenge at IN-TERSPEECH 2025, securing 6th place in the competition. Ad-ditionally, SCT yields 7.37% and 7.95% gains on MSP-Podcast and IEMOCAP, respectively."
    },
    {
      "paperId": "84a6bdbb00f841fff46c278c94d1ab8be4733140",
      "externalIds": {
        "DBLP": "conf/interspeech/Fan0HM25",
        "DOI": "10.21437/interspeech.2025-504",
        "CorpusId": 282362701
      },
      "corpusId": 282362701,
      "title": "Band-Split Self-supervised Mamba for Infant-centered Audio Analysis",
      "venue": "Interspeech",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.21437/interspeech.2025-504?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.21437/interspeech.2025-504, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2176473115",
          "name": "Xulin Fan"
        },
        {
          "authorId": "2108961290",
          "name": "Jialu Li"
        },
        {
          "authorId": "1399115926",
          "name": "M. Hasegawa-Johnson"
        },
        {
          "authorId": "144368202",
          "name": "Nancy L. McElwain"
        }
      ],
      "abstract": "Infant-worn audio recorders provide a valuable means to analyze an infant\u2019s home environment and vocal interactions with family members. Recent advances in self-supervised learning on large unlabeled datasets and supervised training on limited annotated data have improved performance in this domain. However, data scarcity remains a challenge. We introduce Band-Split SSAMBA (BS-SSAMBA), a self-supervised representation learning method that incorporates band-specific projections and a band-agnostic Mamba encoder to model temporal relationships across frequency bands. Designed for data-efficient learning, BS-SSAMBA effectively leverages both unlabeled and labeled in-domain data. Through extensive experiments on family audio recordings, we show that BS-SSAMBA outperforms vanilla SSAMBA and wav2vec2-based models, demonstrating its effectiveness for infant-centered audio tagging."
    },
    {
      "paperId": "e562331dd1b3fbb794a770b8a9bd3da85522fafe",
      "externalIds": {
        "DBLP": "conf/interspeech/LeePCLKC25",
        "DOI": "10.21437/interspeech.2025-1084",
        "CorpusId": 282357601
      },
      "corpusId": 282357601,
      "title": "Efficient Streaming TTS Acoustic Model with Depthwise RVQ Decoding Strategies in a Mamba Framework",
      "venue": "Interspeech",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.21437/interspeech.2025-1084?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.21437/interspeech.2025-1084, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "3103493",
          "name": "Joun Yeop Lee"
        },
        {
          "authorId": "2387592267",
          "name": "Sangjun Park"
        },
        {
          "authorId": "73287614",
          "name": "Byoung Jin Choi"
        },
        {
          "authorId": "2254291118",
          "name": "Ji-Hyun Lee"
        },
        {
          "authorId": "2387645816",
          "name": "Min-Kyung Kim"
        },
        {
          "authorId": "2254262589",
          "name": "Hoon-Young Cho"
        }
      ],
      "abstract": "Recent advances in neural codec-based text-to-speech (TTS) systems have achieved remarkable synthesized speech quality. However, their reliance on large model size and heavy computational requirements limits CPU-based on-device deployment. In this work, we present a Mamba-based streaming acoustic model with two novel depthwise decoding strategies for residual vector quantization (RVQ)-based codec: a Masked Language Model (MLM) approach and an Implicit Neural Representation (INR) approach. The MLM strategy iteratively refines tokens along the code depth axis to enhance speech quality, whereas the INR approach predicts all quantization levels in parallel to reduce computational costs. We further incorporate a speaker embedding conditioning mechanism for a zero-shot scenario, enabling robust performance on unseen speakers. Experimental results demonstrate comparable or even superior improvements in both objective and subjective metrics compared to other larger TTS baseline models."
    },
    {
      "paperId": "dad943a2ca83d71cbb159ff8ec54308fedb18d4d",
      "externalIds": {
        "DBLP": "journals/corr/abs-2508-11849",
        "ArXiv": "2508.11849",
        "DOI": "10.1016/j.aei.2025.104230",
        "CorpusId": 280677834
      },
      "corpusId": 280677834,
      "title": "LocoMamba: Vision-Driven Locomotion via End-to-End Deep Reinforcement Learning with Mamba",
      "venue": "Advanced Engineering Informatics",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.11849, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2378185475",
          "name": "Yinuo Wang"
        },
        {
          "authorId": "2376201400",
          "name": "Gavin Tao"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "9e904e07eb71bec5c859ca12ad88756260929575",
      "externalIds": {
        "DBLP": "journals/corr/abs-2508-11990",
        "ArXiv": "2508.11990",
        "DOI": "10.48550/arXiv.2508.11990",
        "CorpusId": 280676878
      },
      "corpusId": 280676878,
      "title": "Universal Learning of Nonlinear Dynamics",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.11990, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2321406366",
          "name": "Evan Dogariu"
        },
        {
          "authorId": "2354172346",
          "name": "Anand Brahmbhatt"
        },
        {
          "authorId": "34840427",
          "name": "Elad Hazan"
        }
      ],
      "abstract": "We study the fundamental problem of learning a marginally stable unknown nonlinear dynamical system. We describe an algorithm for this problem, based on the technique of spectral filtering, which learns a mapping from past observations to the next based on a spectral representation of the system. Using techniques from online convex optimization, we prove vanishing prediction error for any nonlinear dynamical system that has finitely many marginally stable modes, with rates governed by a novel quantitative control-theoretic notion of learnability. The main technical component of our method is a new spectral filtering algorithm for linear dynamical systems, which incorporates past observations and applies to general noisy and marginally stable systems. This significantly generalizes the original spectral filtering algorithm to both asymmetric dynamics as well as incorporating noise correction, and is of independent interest."
    },
    {
      "paperId": "2a3db95fac9f7f33b8f3e2f562a79c1ca603a073",
      "externalIds": {
        "DBLP": "journals/corr/abs-2508-11921",
        "ArXiv": "2508.11921",
        "DOI": "10.48550/arXiv.2508.11921",
        "CorpusId": 280677602
      },
      "corpusId": 280677602,
      "title": "ENA: Efficient N-dimensional Attention",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.11921, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2376373629",
          "name": "Yibo Zhong"
        }
      ],
      "abstract": "Efficient modeling of long sequences of high-order data requires a more efficient architecture than Transformer. In this paper, we investigate two key aspects of extending linear recurrent models, especially those originally designed for language modeling, to high-order data (1D to ND): scanning strategies and attention-hybrid architectures. Empirical results suggest that scanning provides limited benefits, while attention-hybrid models yield promising results. Focusing on the latter, we further evaluate types of attention and find that tiled high-order sliding window attention (SWA) is efficient in both theory and practice. We term the resulting hybrid architecture of linear recurrence and high-order SWA as Efficient N-dimensional Attention (ENA). We then conduct several experiments to demonstrate its effectiveness. The intuition behind ENA is that linear recurrence compresses global information into a state, while SWA complements it by enforcing strict local modeling. Together, they form a simple framework that offers a promising and practical solution for ultra-long high-order data modeling."
    },
    {
      "paperId": "2f7a8808d3e472ae76662d99641eb0f3f0a02591",
      "externalIds": {
        "ArXiv": "2508.11935",
        "DBLP": "journals/corr/abs-2508-11935",
        "DOI": "10.48550/arXiv.2508.11935",
        "CorpusId": 280677637
      },
      "corpusId": 280677637,
      "title": "HPD: Hybrid Projection Decomposition for Robust State Space Models on Analog CIM Hardware",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.11935, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2344505221",
          "name": "Yuannuo Feng"
        },
        {
          "authorId": "2193072214",
          "name": "Wenyong Zhou"
        },
        {
          "authorId": "2376197390",
          "name": "Yuexi Lyu"
        },
        {
          "authorId": "2278199548",
          "name": "Hanjie Liu"
        },
        {
          "authorId": "2347731611",
          "name": "Zhengwu Liu"
        },
        {
          "authorId": "2292504995",
          "name": "Ngai Wong"
        },
        {
          "authorId": "2376197295",
          "name": "Wang Kang"
        }
      ],
      "abstract": "State Space Models (SSMs) are efficient alternatives to traditional sequence models, excelling at processing long sequences with lower computational complexity. Their reliance on matrix multiplications makes them ideal for compute-in-memory (CIM) architectures, which improve energy efficiency by computing within memory arrays. However, device non-idealities in CIM introduce weight perturbations that can degrade inference accuracy. In this paper, we systematically analyze the robustness of SSMs under noisy conditions, identifying that the final block and output projection layers are more susceptible to perturbations compared to other components. Building on these insights, we propose HPD, a Hybrid Projection Decomposition strategy for the last output projection layer. We replace the original weight matrix with the multiplication of U and {\\Sigma} in its SVD to ensure compatibility with existing hardware architectures, while offloading V>to digital hardware for precise and robust correction. Comprehensive tests on Mamba models show that our method reduces perplexity by up to 99.57% under various noise conditions compared to baseline models, with accuracy gains of up to 96.67% on the PIQA benchmark for commonsense reasoning."
    },
    {
      "paperId": "c087b5aca1b6be08f5625889b4a47fd85ed26a3e",
      "externalIds": {
        "DOI": "10.1109/CVAA66438.2025.11193297",
        "CorpusId": 282105448
      },
      "corpusId": 282105448,
      "title": "Integrated Mamba Block with Spatially-Aware Feature Fusion",
      "venue": "2025 5th International Conference on Computer Vision, Application and Algorithm (CVAA)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVAA66438.2025.11193297?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVAA66438.2025.11193297, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2385919392",
          "name": "Junyang Song"
        },
        {
          "authorId": "2385834450",
          "name": "Yifei Wang"
        }
      ],
      "abstract": "Visual modeling in CNNs has been widely enhanced by various attention mechanisms designed to integrate channelwise and spatial-wise information, achieving prominent results across a range of vision tasks. However, despite their efficiency in enhancing local feature interactions, these designs remain rooted in convolutional operations, which constrain their capacity for global spatial reasoning. Inspired by Mamba, an efficient longrange selective state space model, we propose a lightweight Integrated Mamba Block (IMB) comprising a bidirectional Mamba branch and a depth-wise convolution branch. These two pathways are combined through a spatially-aware fusion mechanism that exchanges channel descriptors and generates fine-grained spatial attention, which enables effective integration of global and local features with an acceptable computational cost. We conduct extensive experiments on CIFAR-100 with ResNet-50, demonstrating consistent performance gains and supporting the effectiveness of each component through ablation studies. The module also demonstrates transferability to lightweight architectures such as MobileNetV2, highlighting its general applicability. Code is available at https://github.com/undetectedatom/mamba-fusion-module"
    },
    {
      "paperId": "ebe6f4339cd4afb2243fbfc80248eddc3a52d844",
      "externalIds": {
        "DOI": "10.1109/ICICC66840.2025.11199503",
        "CorpusId": 282274314
      },
      "corpusId": 282274314,
      "title": "Mamba-Whisper Fusion: Efficient Low-Resource Thai Speech Recognition via Feature Fusion and Joint Decoding",
      "venue": "2025 5th International Conference on Intelligent Communications and Computing (ICICC)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICICC66840.2025.11199503?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICICC66840.2025.11199503, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2387245860",
          "name": "Yangshi Lin"
        },
        {
          "authorId": "2285688587",
          "name": "Minghui Zhong"
        },
        {
          "authorId": "2387364128",
          "name": "Yueer Wu"
        },
        {
          "authorId": "2387117211",
          "name": "Zhijun Li"
        },
        {
          "authorId": "2387647991",
          "name": "Zengsheng Lv"
        },
        {
          "authorId": "2387694404",
          "name": "Hanfang Yan"
        },
        {
          "authorId": "2387126066",
          "name": "Manli Zhu"
        },
        {
          "authorId": "2387875867",
          "name": "Jinting Tang"
        }
      ],
      "abstract": "Automatic speech recognition (ASR) for lowresource languages such as Thai faces significant challenges due to data scarcity and high computational demands. In this paper, we introduce a novel hybrid architecture that harnesses the pretrained encoder of OpenAI's Whisper small model to extract robust hidden features from audio inputs. To optimize for lowresource scenarios, we design a custom encoder based on the Mamba state-space model for efficient feature extraction. Features from the Whisper and Mamba encoders are fused and processed through a joint decoder for end-to-end ASR. We train and evaluate our model on the Thai subset of the CommonVoice dataset, offering a realistic low-resource benchmark. We compare against the following baselines: (1) direct finetuning of the full Whisper small model on CommonVoice Thai, achieving a word error rate (WER) of 12.1 %; (2) a from-scratch end-to-end ASR model using a classic Mamba architecture, yielding a WER of 13.5 %; and (3) the state-of-the-art Thonburian Whisper baseline from recent literature, reporting a WER of $\\mathbf{1 1. 2 \\%}$ on CommonVoice Thai. Experimental results show that our Mamba-Whisper fusion model (30 % fewer than Thonburian Whisper) outperforms all baselines, achieving a WER of $\\mathbf{1 0. 5 \\%}$-a relative reduction of $\\mathbf{2 2 \\%}$ over the from-scratch Mamba model and 6 % over Thonburian Whisper-while significantly accelerating inference via Mamba's linear complexity. This approach underscores the potential of efficient fusion strategies for low-resource ASR, paving the way for scalable multilingual systems."
    },
    {
      "paperId": "5826d98867f988b1f7b7df1352846ab67166093f",
      "externalIds": {
        "DBLP": "journals/iotj/LiuHSXZMQ25",
        "DOI": "10.1109/JIOT.2025.3574568",
        "CorpusId": 278972999
      },
      "corpusId": 278972999,
      "title": "TRIS-HAR: Transmissive Reconfigurable Intelligent Surfaces-Assisted Human Activity Recognition Using State Space Models",
      "venue": "IEEE Internet of Things Journal",
      "year": 2025,
      "citationCount": 3,
      "influentialCitationCount": 1,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JIOT.2025.3574568?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JIOT.2025.3574568, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2210834995",
          "name": "Junshuo Liu"
        },
        {
          "authorId": "2278978537",
          "name": "Yunlong Huang"
        },
        {
          "authorId": "2237417761",
          "name": "Xin Shi"
        },
        {
          "authorId": "1500391567",
          "name": "Rujing Xiong"
        },
        {
          "authorId": "2278984884",
          "name": "Jianan Zhang"
        },
        {
          "authorId": "1884948",
          "name": "Tiebin Mi"
        },
        {
          "authorId": "2296309720",
          "name": "Robert C. Qiu"
        }
      ],
      "abstract": "Human activity recognition (HAR) using radio frequency (RF) signals has attracted increasing interest due to its nonintrusive and privacy-preserving nature. However, traditional systems often suffer from multipath fading, environmental noise, and limited spatial diversity, particularly in through-the-wall scenarios. In this article, we propose transmissive reconfigurable intelligent surface-assisted through-the-wall HAR (TRIS-HAR), a novel HAR system that integrates a transmissive reconfigurable intelligent surface (TRIS) with an advanced dual-stream state space model, human intelligence mamba (HiMamba). The TRIS actively reshapes the propagation environment by constructing deterministic Quasi-Line-of-Sight (QLoS) paths across obstacles, significantly improving channel state information (CSI) quality. Complementing this, HiMamba leverages a lightweight structured state space architecture to jointly model temporal and spectral dynamics, enabling robust activity recognition under Non-Line-of-Sight conditions. Extensive experiments on both public and real-world datasets demonstrate that TRIS-HAR improves recognition accuracy from 85.00% to 98.06% and maintains strong generalizability across environments. The model is also deployed on a CPU-based edge device, achieving real-time inference at 108 frame per second with minimal memory cost. This work establishes a co-designed hardware-algorithm framework for RF-based HAR, offering a scalable and deployable solution for smart homes, healthcare, and next-generation pervasive sensing applications."
    },
    {
      "paperId": "8a28ff54e2019a196887e388928c4e76d2bb2e2b",
      "externalIds": {
        "DOI": "10.1109/JSEN.2025.3584953",
        "CorpusId": 280171923
      },
      "corpusId": 280171923,
      "title": "RFID-WiFi-Radar Fusion for Health Monitoring: A Cross-Modal Supervision Framework",
      "venue": "IEEE Sensors Journal",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JSEN.2025.3584953?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JSEN.2025.3584953, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2360839404",
          "name": "Xiangguo Li"
        },
        {
          "authorId": "2373501007",
          "name": "Yaohua Guo"
        },
        {
          "authorId": "2130282601",
          "name": "Xuehong Sun"
        },
        {
          "authorId": "2134798718",
          "name": "Liping Liu"
        },
        {
          "authorId": "2373427231",
          "name": "Xinjuan Wang"
        },
        {
          "authorId": "2278851215",
          "name": "Yanpeng Zhang"
        },
        {
          "authorId": "2360642330",
          "name": "Xiaoyong Song"
        }
      ],
      "abstract": "Wireless sensing technology, with its advantages in privacy protection and high recognition accuracy, has demonstrated remarkable performance in complex behavioral monitoring environments. However, single-modal sensing technologies often fail to meet the demands of multitask complex behavior recognition. Although multimodal fusion can significantly enhance system performance, challenges such as data heterogeneity and uneven modal contributions severely limit fusion efficiency. To address these challenges, this article proposes an innovative multimodal data fusion framework called the multimodal temporal-topology feature fusion network (MTTFNet). By combining the strengths of a convolutional neural network (CNN) and transformer architectures, the framework introduces a temporal feature enhancement module (TFEM) and incorporates an adaptive weighting fusion mechanism alongside a cross-modal supervision strategy. This design facilitates efficient collaborative optimization of modal features, effectively mitigating the challenges posed by heterogeneity and contribution imbalance. Experimental results demonstrate that the recognition accuracy of the radio frequency identification (RFID) modality has increased to 70.81%, while the accuracy of multimodal fusion consistently exceeds 90%. Notably, the fusion of all three modalities achieves an accuracy of 96.83%, significantly outperforming existing algorithms. These findings validate the effectiveness of MTTFNet and provide a robust solution for complex behavior recognition tasks."
    },
    {
      "paperId": "0a19b62fbef922ae43fa7eeebdb3f11a4d654e18",
      "externalIds": {
        "DBLP": "journals/corr/abs-2508-11531",
        "ArXiv": "2508.11531",
        "DOI": "10.1145/3746027.3755310",
        "CorpusId": 280671382
      },
      "corpusId": 280671382,
      "title": "Multi-State Tracker: Enhancing Efficient Object Tracking via Multi-State Specialization and Interaction",
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.11531, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2260298534",
          "name": "Shilei Wang"
        },
        {
          "authorId": "2178368663",
          "name": "Gong Cheng"
        },
        {
          "authorId": "2202629129",
          "name": "Pujian Lai"
        },
        {
          "authorId": "2376148210",
          "name": "Dong Gao"
        },
        {
          "authorId": "2156545584",
          "name": "Junwei Han"
        }
      ],
      "abstract": "Efficient trackers achieve faster runtime by reducing computational complexity and model parameters. However, this efficiency often compromises the expense of weakened feature representation capacity, thus limiting their ability to accurately capture target states using single-layer features. To overcome this limitation, we propose Multi-State Tracker (MST), which utilizes highly lightweight state-specific enhancement (SSE) to perform specialized enhancement on multi-state features produced by multi-state generation (MSG) and aggregates them in an interactive and adaptive manner using cross-state interaction (CSI). This design greatly enhances feature representation while incurring minimal computational overhead, leading to improved tracking robustness in complex environments. Specifically, the MSG generates multiple state representations at multiple stages during feature extraction, while SSE refines them to highlight target-specific features. The CSI module facilitates information exchange between these states and ensures the integration of complementary features. Notably, the introduced SSE and CSI modules adopt a highly lightweight hidden state adaptation-based state space duality (HSA-SSD) design, incurring only 0.1 GFLOPs in computation and 0.66 M in parameters. Experimental results demonstrate that MST outperforms all previous efficient trackers across multiple datasets, significantly improving tracking accuracy and robustness. In particular, it shows excellent runtime performance, with an AO score improvement of 4.5% over the previous SOTA efficient tracker HCAT on the GOT-10K dataset. The code is available at https://github.com/wsumel/MST."
    },
    {
      "paperId": "17b1d6bfd426be3c840f1ec8da72b2252d3df23a",
      "externalIds": {
        "ArXiv": "2508.11313",
        "DBLP": "conf/ijcai/LiuCMFZ00NM25",
        "DOI": "10.48550/arXiv.2508.11313",
        "CorpusId": 280671465
      },
      "corpusId": 280671465,
      "title": "Denoise-then-Retrieve: Text-Conditioned Video Denoising for Video Moment Retrieval",
      "venue": "International Joint Conference on Artificial Intelligence",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.11313, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2109235161",
          "name": "Weijia Liu"
        },
        {
          "authorId": "1705153",
          "name": "Jiuxin Cao"
        },
        {
          "authorId": "2368121411",
          "name": "Bo Miao"
        },
        {
          "authorId": "2376424592",
          "name": "Zhiheng Fu"
        },
        {
          "authorId": "2116570482",
          "name": "Xueling Zhu"
        },
        {
          "authorId": "1671426036",
          "name": "Jiawei Ge"
        },
        {
          "authorId": "2268797212",
          "name": "Bo Liu"
        },
        {
          "authorId": "2302402158",
          "name": "Mehwish Nasim"
        },
        {
          "authorId": "2292257468",
          "name": "Ajmal Mian"
        }
      ],
      "abstract": "Current text-driven Video Moment Retrieval (VMR) methods encode all video clips, including irrelevant ones, disrupting multimodal alignment and hindering optimization. To this end, we propose a denoise-then-retrieve paradigm that explicitly filters text-irrelevant clips from videos and then retrieves the target moment using purified multimodal representations. Following this paradigm, we introduce the Denoise-then-Retrieve Network (DRNet), comprising Text-Conditioned Denoising (TCD) and Text-Reconstruction Feedback (TRF) modules. TCD integrates cross-attention and structured state space blocks to dynamically identify noisy clips and produce a noise mask to purify multimodal video representations. TRF further distills a single query embedding from purified video representations and aligns it with the text embedding, serving as auxiliary supervision for denoising during training. Finally, we perform conditional retrieval using text embeddings on purified video representations for accurate VMR. Experiments on Charades-STA and QVHighlights demonstrate that our approach surpasses state-of-the-art methods on all metrics. Furthermore, our denoise-then-retrieve paradigm is adaptable and can be seamlessly integrated into advanced VMR models to boost performance."
    },
    {
      "paperId": "85a8b3d64aad27b0562a9ee86750e54da15a35c1",
      "externalIds": {
        "ArXiv": "2508.11569",
        "DBLP": "journals/corr/abs-2508-11569",
        "DOI": "10.1109/tcsvt.2025.3599544",
        "CorpusId": 280671324
      },
      "corpusId": 280671324,
      "title": "TrajSV: A Trajectory-based Model for Sports Video Representations and Applications",
      "venue": "IEEE transactions on circuits and systems for video technology (Print)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.11569, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2303565983",
          "name": "Zheng Wang"
        },
        {
          "authorId": "2261892883",
          "name": "Shihao Xu"
        },
        {
          "authorId": "2261895678",
          "name": "Wei Shi"
        }
      ],
      "abstract": "Sports analytics has received significant attention from both academia and industry in recent years. Despite the growing interest and efforts in this field, several issues remain unresolved, including (1) data unavailability, (2) lack of an effective trajectory-based framework, and (3) requirement for sufficient supervision labels. In this paper, we present TrajSV, a trajectory-based framework that addresses various issues in existing studies. TrajSV comprises three components: data preprocessing, Clip Representation Network (CRNet), and Video Representation Network (VRNet). The data preprocessing module extracts player and ball trajectories from sports broadcast videos. CRNet utilizes a trajectory-enhanced Transformer module to learn clip representations based on these trajectories. Additionally, VRNet learns video representations by aggregating clip representations and visual features with an encoder-decoder architecture. Finally, a triple contrastive loss is introduced to optimize both video and clip representations in an unsupervised manner. The experiments are conducted on three broadcast video datasets to verify the effectiveness of TrajSV for three types of sports (i.e., soccer, basketball, and volleyball) with three downstream applications (i.e., sports video retrieval, action spotting, and video captioning). The results demonstrate that TrajSV achieves state-of-the-art performance in sports video retrieval, showcasing a nearly 70% improvement. It outperforms baselines in action spotting, achieving state-of-the-art results in 9 out of 17 action categories, and demonstrates a nearly 20% improvement in video captioning. Additionally, we introduce a deployed system along with the three applications based on TrajSV."
    },
    {
      "paperId": "aeebc71828851dd6a64c8cb6f97114c49093d931",
      "externalIds": {
        "DOI": "10.1109/CCET66260.2025.11199647",
        "CorpusId": 282272984
      },
      "corpusId": 282272984,
      "title": "A New Hybrid Mamba Model for Significant Wave Height Prediction Enhanced by CEEMDAN and Channel Clustering",
      "venue": "2025 IEEE 8th International Conference on Computer and Communication Engineering Technology (CCET)",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CCET66260.2025.11199647?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CCET66260.2025.11199647, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2270920031",
          "name": "Yongmei Zhang"
        },
        {
          "authorId": "2387090163",
          "name": "Yan Zhang"
        }
      ],
      "abstract": "Accurate prediction of significant wave height (SWH) is crucial for offshore operations, marine resource utilization, and meteorological research. However, due to the fluctuating and nonlinear characteristics of ocean waves, its accurate prediction still faces significant challenges. Therefore, this paper proposes a hybrid model integrating Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN) and Channel Clustering Mamba (CCM-Mamba) for SWH prediction. First, the SWH time series is decomposed into intrinsic mode functions (IMFs) using CEEMDAN. Subsequently, a CCM-Mamba model is constructed to predicts each IMF. Finally, aggregation of these component predictions produces the SWH forecast. Experimental results demonstrate that this hybrid model achieves lower prediction error compared to single models, including S-Mamba, TimesNet, and Autoformer. The proposed hybrid model demonstrates promising application prospects for enhancing SWH prediction accuracy."
    },
    {
      "paperId": "50d15799714127990d46168b46b786a91eaeb46c",
      "externalIds": {
        "ArXiv": "2508.10542",
        "DBLP": "journals/corr/abs-2508-10542",
        "DOI": "10.48550/arXiv.2508.10542",
        "CorpusId": 280650027
      },
      "corpusId": 280650027,
      "title": "GCRPNet: Graph-Enhanced Contextual and Regional Perception Network For Salient Object Detection in Optical Remote Sensing Images",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.10542, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2351922391",
          "name": "Mengyu Ren"
        },
        {
          "authorId": "2352008067",
          "name": "Yutong Li"
        },
        {
          "authorId": "2319212539",
          "name": "Hua Li"
        },
        {
          "authorId": "3409475",
          "name": "Runmin Cong"
        },
        {
          "authorId": "2287942988",
          "name": "Sam Kwong"
        }
      ],
      "abstract": "Salient object detection (SOD) in optical remote sensing images (ORSIs) faces numerous challenges, including significant variations in target scales and low contrast between targets and the background. Existing methods based on vision transformers (ViTs) and convolutional neural networks (CNNs) architectures aim to leverage both global and local features, but the difficulty in effectively integrating these heterogeneous features limits their overall performance. To overcome these limitations, we propose a graph-enhanced contextual and regional perception network (GCRPNet), which builds upon the Mamba architecture to simultaneously capture long-range dependencies and enhance regional feature representation. Specifically, we employ the visual state space (VSS) encoder to extract multi-scale features. To further achieve deep guidance and enhancement of these features, we first design a difference-similarity guided hierarchical graph attention module (DS-HGAM). This module strengthens cross-layer interaction capabilities between features of different scales while enhancing the model's structural perception,allowing it to distinguish between foreground and background more effectively. Then, we design the LEVSS block as the decoder of GCRPNet. This module integrates our proposed adaptive scanning strategy and multi-granularity collaborative attention enhancement module (MCAEM). It performs adaptive patch scanning on feature maps processed via multi-scale convolutions, thereby capturing rich local region information and enhancing Mamba's local modeling capability. Extensive experimental results demonstrate that the proposed model achieves state-of-the-art performance, validating its effectiveness and superiority."
    },
    {
      "paperId": "93dadd6cc455e979a5efaf8481cfdae4395cc083",
      "externalIds": {
        "DOI": "10.1101/2025.06.04.656517",
        "CorpusId": 279170670
      },
      "corpusId": 279170670,
      "title": "Generanno: A Genomic Foundation Model for Metagenomic Annotation",
      "venue": "bioRxiv",
      "year": 2025,
      "citationCount": 2,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1101/2025.06.04.656517?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1101/2025.06.04.656517, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2329329854",
          "name": "Qiuyi Li"
        },
        {
          "authorId": "2329559942",
          "name": "Wei Wu"
        },
        {
          "authorId": "2366806663",
          "name": "Yiheng Zhu"
        },
        {
          "authorId": "2344920616",
          "name": "Fuli Feng"
        },
        {
          "authorId": "2266042580",
          "name": "Jieping Ye"
        },
        {
          "authorId": "2311493194",
          "name": "Zheng Wang"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "322ceac5a7447a2660cbf9242c1f1ca4227d4619",
      "externalIds": {
        "ArXiv": "2508.10453",
        "DBLP": "journals/corr/abs-2508-10453",
        "DOI": "10.48550/arXiv.2508.10453",
        "CorpusId": 280649761
      },
      "corpusId": 280649761,
      "title": "Trajectory-aware Shifted State Space Models for Online Video Super-Resolution",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.10453, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2292346462",
          "name": "Qiang Zhu"
        },
        {
          "authorId": "50123072",
          "name": "Xiandong Meng"
        },
        {
          "authorId": "2375953517",
          "name": "Yuxian Jiang"
        },
        {
          "authorId": "2374705362",
          "name": "Fan Zhang"
        },
        {
          "authorId": "2138393856",
          "name": "David R. Bull"
        },
        {
          "authorId": "1719428",
          "name": "Shuyuan Zhu"
        },
        {
          "authorId": "2290910857",
          "name": "Bing Zeng"
        }
      ],
      "abstract": "Online video super-resolution (VSR) is an important technique for many real-world video processing applications, which aims to restore the current high-resolution video frame based on temporally previous frames. Most of the existing online VSR methods solely employ one neighboring previous frame to achieve temporal alignment, which limits long-range temporal modeling of videos. Recently, state space models (SSMs) have been proposed with linear computational complexity and a global receptive field, which significantly improve computational efficiency and performance. In this context, this paper presents a novel online VSR method based on Trajectory-aware Shifted SSMs (TS-Mamba), leveraging both long-term trajectory modeling and low-complexity Mamba to achieve efficient spatio-temporal information aggregation. Specifically, TS-Mamba first constructs the trajectories within a video to select the most similar tokens from the previous frames. Then, a Trajectory-aware Shifted Mamba Aggregation (TSMA) module consisting of proposed shifted SSMs blocks is employed to aggregate the selected tokens. The shifted SSMs blocks are designed based on Hilbert scannings and corresponding shift operations to compensate for scanning losses and strengthen the spatial continuity of Mamba. Additionally, we propose a trajectory-aware loss function to supervise the trajectory generation, ensuring the accuracy of token selection when training our model. Extensive experiments on three widely used VSR test datasets demonstrate that compared with six online VSR benchmark models, our TS-Mamba achieves state-of-the-art performance in most cases and over 22.7\\% complexity reduction (in MACs). The source code for TS-Mamba will be available at https://github.com."
    },
    {
      "paperId": "dbc869cec0dd7d827f3d3607735ff333a4f60c82",
      "externalIds": {
        "ArXiv": "2508.10370",
        "DBLP": "journals/corr/abs-2508-10370",
        "DOI": "10.1145/3762190",
        "CorpusId": 280650110
      },
      "corpusId": 280650110,
      "title": "eMamba: Efficient Acceleration Framework for Mamba Models in Edge Computing",
      "venue": "ACM Transactions on Embedded Computing Systems",
      "year": 2025,
      "citationCount": 1,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.10370, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2376044184",
          "name": "Jiyong Kim"
        },
        {
          "authorId": "2375961819",
          "name": "Jaeho Lee"
        },
        {
          "authorId": "2329388478",
          "name": "Jiahao Lin"
        },
        {
          "authorId": "1874624023",
          "name": "Alish Kanani"
        },
        {
          "authorId": "2387923378",
          "name": "Sun Miao"
        },
        {
          "authorId": "117025233",
          "name": "U. Ogras"
        },
        {
          "authorId": "2303701150",
          "name": "Jaehyun Park"
        }
      ],
      "abstract": "State Space Model (SSM)-based machine learning architectures have recently gained significant attention for processing sequential data. Mamba, a recent sequence-to-sequence SSM, offers competitive accuracy with superior computational efficiency compared to state-of-the-art transformer models. While this advantage makes Mamba particularly promising for resource-constrained edge devices, no hardware acceleration frameworks are currently optimized for deploying it in such environments. This article presents eMamba, a comprehensive end-to-end hardware acceleration framework explicitly designed for deploying Mamba models on edge platforms. eMamba maximizes computational efficiency by replacing complex normalization layers with lightweight hardware-aware alternatives and approximating expensive operations, such as SiLU activation and exponentiation, considering the target applications. Then, it performs an approximation-aware neural architecture search (NAS) to tune the learnable parameters used during approximation. Evaluations with Fashion-MNIST, CIFAR-10, and MARS, an open-source human pose estimation dataset, show eMamba achieves comparable accuracy to state-of-the-art techniques using 1.63\u201319.9\u00d7 fewer parameters. In addition, it generalizes well to large-scale natural language tasks, demonstrating stable perplexity across varying sequence lengths on the WikiText2 dataset. We also quantize and implement the entire eMamba pipeline on an AMD ZCU102 FPGA and ASIC using GlobalFoundries (GF) 22 nm technology. Experimental results show 4.95\u20135.62\u00d7 lower latency and 2.22\u20139.95\u00d7 higher throughput, with 4.77\u00d7 smaller area, 9.84\u00d7 lower power, and 48.6\u00d7 lower energy consumption than baseline solutions while maintaining competitive accuracy."
    },
    {
      "paperId": "7c37e31e8de6ac87f221fa5c38d0a14be3e9fe8e",
      "externalIds": {
        "DBLP": "journals/corr/abs-2508-10737",
        "ArXiv": "2508.10737",
        "DOI": "10.48550/arXiv.2508.10737",
        "CorpusId": 280650010
      },
      "corpusId": 280650010,
      "title": "Privacy-enhancing Sclera Segmentation Benchmarking Competition: SSBC 2025",
      "venue": "arXiv.org",
      "year": 2025,
      "citationCount": 0,
      "influentialCitationCount": 0,
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.10737, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "70145792",
          "name": "Matej Vitek"
        },
        {
          "authorId": "2375903496",
          "name": "Darian Tomavsevi'c"
        },
        {
          "authorId": "2376431438",
          "name": "Abhijit Das"
        },
        {
          "authorId": "97786456",
          "name": "S. Nathan"
        },
        {
          "authorId": "2375904063",
          "name": "Gokhan Ozbulak"
        },
        {
          "authorId": "2375904065",
          "name": "Gozde Aycse Tatarouglu Ozbulak"
        },
        {
          "authorId": "1795889",
          "name": "Jean-Paul Calbimonte"
        },
        {
          "authorId": "2375904134",
          "name": "Andr'e Anjos"
        },
        {
          "authorId": "2375904407",
          "name": "Hariohm Hemant Bhatt"
        },
        {
          "authorId": "2361273972",
          "name": "Dhruv D. Premani"
        },
        {
          "authorId": "2375903445",
          "name": "Jay Chaudhari"
        },
        {
          "authorId": "2376319270",
          "name": "Caiyong Wang"
        },
        {
          "authorId": "2340027926",
          "name": "Jian Jiang"
        },
        {
          "authorId": "2334222695",
          "name": "C. Zhang"
        },
        {
          "authorId": "2210726898",
          "name": "Qi Zhang"
        },
        {
          "authorId": "41126013",
          "name": "I. Ganapathi"
        },
        {
          "authorId": "46244566",
          "name": "Syed Sadaf Ali"
        },
        {
          "authorId": "2375904383",
          "name": "Divya Velayudan"
        },
        {
          "authorId": "1423583013",
          "name": "Maregu Assefa"
        },
        {
          "authorId": "1802072",
          "name": "N. Werghi"
        },
        {
          "authorId": "2375903498",
          "name": "Zachary A. Daniels"
        },
        {
          "authorId": "2375904474",
          "name": "Leeon John"
        },
        {
          "authorId": "2321964526",
          "name": "Ritesh Vyas"
        },
        {
          "authorId": "145849213",
          "name": "J. Khiarak"
        },
        {
          "authorId": "2310341744",
          "name": "Taher Ak-bari Saeed"
        },
        {
          "authorId": "2005296630",
          "name": "Mahsa Nasehi"
        },
        {
          "authorId": "2348006185",
          "name": "Ali Kianfar"
        },
        {
          "authorId": "2375904306",
          "name": "Mobina Pashazadeh Panahi"
        },
        {
          "authorId": "2288596183",
          "name": "Geetanjali Sharma"
        },
        {
          "authorId": "2375904198",
          "name": "Pushp Raj Panth"
        },
        {
          "authorId": "151434651",
          "name": "Raghavendra Ramachandra"
        },
        {
          "authorId": "2285572000",
          "name": "Aditya Nigam"
        },
        {
          "authorId": "2249757578",
          "name": "Umapada Pal"
        },
        {
          "authorId": "2273840120",
          "name": "Peter Peer"
        },
        {
          "authorId": "2056771592",
          "name": "Vitomir vStruc"
        }
      ],
      "abstract": "This paper presents a summary of the 2025 Sclera Segmentation Benchmarking Competition (SSBC), which focused on the development of privacy-preserving sclera-segmentation models trained using synthetically generated ocular images. The goal of the competition was to evaluate how well models trained on synthetic data perform in comparison to those trained on real-world datasets. The competition featured two tracks: $(i)$ one relying solely on synthetic data for model development, and $(ii)$ one combining/mixing synthetic with (a limited amount of) real-world data. A total of nine research groups submitted diverse segmentation models, employing a variety of architectural designs, including transformer-based solutions, lightweight models, and segmentation networks guided by generative frameworks. Experiments were conducted across three evaluation datasets containing both synthetic and real-world images, collected under diverse conditions. Results show that models trained entirely on synthetic data can achieve competitive performance, particularly when dedicated training strategies are employed, as evidenced by the top performing models that achieved $F_1$ scores of over $0.8$ in the synthetic data track. Moreover, performance gains in the mixed track were often driven more by methodological choices rather than by the inclusion of real data, highlighting the promise of synthetic data for privacy-aware biometric development. The code and data for the competition is available at: https://github.com/dariant/SSBC_2025."
    }
  ],
  "abstract": "Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.",
  "references": []
}