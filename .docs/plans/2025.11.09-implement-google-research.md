# Plan: Add Google Research Blog Papers to System

**Date**: 2025-11-09
**Goal**: Enable processing of papers from Google Research blog, including non-arXiv papers

## Current System Analysis

### Database Schema
- Papers table has unique constraint on `arxiv_id`
- All papers currently required to have an arXiv ID
- Location: `papers/db/models.py`

### Paper Creation Flow
1. `create_paper()` in `papers/client.py` checks for duplicate by arXiv ID
2. Creates paper with status `'not_started'`
3. Paper processing worker picks it up for OCR/summarization

## Problem
Google Research blog has papers that:
- May have arXiv IDs (use existing flow)
- May NOT have arXiv IDs (need new identifier)
- Examples from scraper:
  - Post 1: PDF at `http://abehrouz.github.io/files/NL.pdf` - no arXiv
  - Post 10: DOI link `https://doi.org/10.1038/s41586-025-09526-6` - no arXiv

## Solution: PDF Content Hash

Use SHA-256 hash of PDF content as unique identifier for non-arXiv papers.

**Why content hash:**
- Simple - no external dependencies
- Reliable - same PDF = same hash
- Works for any PDF source
- No fallbacks needed

## Implementation Steps

### Step 1: Update Database Schema

**File**: `papers/db/models.py`

Add new field to PaperRecord:
```python
content_hash = Column(String(64), nullable=True)
```

Add index for content_hash:
```python
Index("ix_papers_content_hash", "content_hash", unique=True)
```

Make arxiv_id nullable:
```python
arxiv_id = Column(String(64), nullable=True)
```

**Migration**: Create Alembic migration to add column and index

### Step 2: Update Paper Models

**File**: `papers/models.py`

Add `content_hash` field to Paper DTO:
```python
class Paper(BaseModel):
    # ... existing fields ...
    arxiv_id: Optional[str] = None  # Make optional
    content_hash: Optional[str] = None  # Add new field
```

### Step 3: Create PDF Hash Utility

**File**: `shared/pdf_utils.py` (new file)

Functions:
- `download_pdf(url: str) -> bytes` - Download PDF from URL
- `calculate_pdf_hash(pdf_bytes: bytes) -> str` - Calculate SHA-256 hash

Keep it simple - just download and hash, no retries or fallbacks.

### Step 4: Update Paper Creation Logic

**File**: `papers/client.py`

Update `create_paper()` function:

```python
def create_paper(
    db: Session,
    arxiv_id: Optional[str] = None,
    pdf_url: Optional[str] = None,
    title: Optional[str] = None,
    authors: Optional[str] = None,
    external_popularity_signals: Optional[List[ExternalPopularitySignal]] = None,
    initiated_by_user_id: Optional[str] = None
) -> Paper:
    """
    Create a new paper and add to processing queue.

    For arXiv papers: provide arxiv_id
    For non-arXiv papers: provide pdf_url (hash will be calculated)

    Args:
        db: Database session
        arxiv_id: ArXiv ID if available
        pdf_url: Direct PDF URL if no arXiv ID
        ...

    Raises:
        ValueError: If paper already exists
        ValueError: If neither arxiv_id nor pdf_url provided
    """
    # Step 1: Validate inputs
    if not arxiv_id and not pdf_url:
        raise ValueError("Must provide either arxiv_id or pdf_url")

    # Step 2: Check for duplicates
    if arxiv_id:
        # Check by arXiv ID (fast)
        existing = get_paper_by_arxiv_id(db, arxiv_id)
        if existing:
            raise ValueError(f"Paper with arXiv ID {arxiv_id} already exists")
    else:
        # Download PDF and check by hash
        from shared.pdf_utils import download_pdf, calculate_pdf_hash

        pdf_bytes = download_pdf(pdf_url)
        content_hash = calculate_pdf_hash(pdf_bytes)

        existing = get_paper_by_content_hash(db, content_hash)
        if existing:
            raise ValueError(f"Paper with content hash {content_hash} already exists")

    # Step 3: Create paper record
    # ... rest of implementation
```

### Step 5: Update Database Client

**File**: `papers/db/client.py`

Add new function:
```python
def get_paper_by_content_hash(db: Session, content_hash: str) -> Optional[PaperRecord]:
    """Get paper by PDF content hash."""
    return db.query(PaperRecord).filter(
        PaperRecord.content_hash == content_hash
    ).first()
```

### Step 6: Create Google Research DAG

**File**: `airflow/dags/daily_google_research_papers_dag.py`

Similar to `daily_alphaxiv_papers_dag.py` but:
- Scrape Google Research blog using Selenium
- Extract paper URLs and arXiv IDs
- For papers with arXiv ID: call `create_paper(arxiv_id=...)`
- For papers without arXiv ID: call `create_paper(pdf_url=...)`

Structure:
1. Task: Scrape blog posts from Google Research
2. Task: Print found papers
3. Task: Add papers to queue (with duplicate handling)

### Step 7: Update Paper Processing Worker

**File**: `paperprocessor/client.py` (if needed)

Ensure worker can handle papers with:
- arXiv ID only
- Content hash only
- Both

Worker already downloads PDFs, so it should work without changes.
Just verify it handles `arxiv_id=None` case.

## Testing Plan

1. **Unit Tests**:
   - Test `calculate_pdf_hash()` with sample PDFs
   - Test `create_paper()` with arXiv ID
   - Test `create_paper()` with PDF URL
   - Test duplicate detection for both cases

2. **Integration Test**:
   - Add paper from Google Research with arXiv ID
   - Add paper from Google Research without arXiv ID
   - Verify both get processed correctly
   - Verify duplicate detection works

3. **Manual Test**:
   - Run Google Research DAG manually
   - Check papers added to database
   - Verify processing completes successfully

## Notes

- No fallbacks - if PDF download fails, throw error
- No retries - keep it simple
- Hash is calculated once during paper creation
- For arXiv papers, we still use arXiv ID as primary identifier
- Content hash only used when arXiv ID not available

## Files to Modify

1. `papers/db/models.py` - Add content_hash column
2. `papers/models.py` - Add content_hash to DTO
3. `shared/pdf_utils.py` - New file for PDF utilities
4. `papers/client.py` - Update create_paper()
5. `papers/db/client.py` - Add get_paper_by_content_hash()
6. `airflow/dags/daily_google_research_papers_dag.py` - New DAG
7. Database migration - Add content_hash column and index

## Dependencies

- `hashlib` (built-in Python) - for SHA-256
- `requests` (already installed) - for PDF download
- `selenium` (already installed) - for blog scraping
