# ArXiv Paper Discovery Database Storage

## What We Want To Do

Store discovered arXiv papers from the ranking pipeline in our database, including:
- Paper metadata (arxiv_id, version, title, abstract, published date)
- Primary category as indexed column + all categories as JSON array
- Authors with names and optional Semantic Scholar IDs as JSON array
- Paper embedding vector (768-dimensional SPECTER v1 from Semantic Scholar)
- Computed author credibility scores (avg h-index, avg citations per paper) - **only authors with known S2 profiles are included in averages**

This enables:
- Querying papers by category, author credibility, or date
- Vector similarity search for related papers
- Building feeds of high-quality new papers

**Note:** MySQL will be the source of truth. For vector similarity search, we'll create a Qdrant index on top of this table (implementation TBD).

---

## Database Table(s) & Schema

### `arxiv_papers` - Single table with all paper data

| Column | Type | Nullable | Description |
|--------|------|----------|-------------|
| id | BIGINT | NO | Primary key, auto-increment |
| arxiv_id | VARCHAR(64) | NO | ArXiv identifier (e.g., "2312.00752"), UNIQUE |
| version | INT | YES | ArXiv version number (e.g., 1, 2, 3) |
| title | VARCHAR(512) | NO | Paper title |
| abstract | TEXT | YES | Paper abstract |
| published_at | DATETIME | YES | Publication timestamp from arXiv |
| primary_category | VARCHAR(32) | YES | Primary arXiv category (e.g., "cs.LG") |
| categories | JSON | YES | All arXiv categories as array, e.g. `["cs.LG", "cs.AI"]` |
| authors | JSON | YES | Author list with optional S2 IDs, e.g. `[{"name": "Albert Gu", "semantic_scholar_id": "123"}, {"name": "Jane Doe"}]` |
| semantic_scholar_id | VARCHAR(64) | YES | S2 paper ID (e.g., "7bbc7595196a0606a07506c4fb1473e5e87f6082") |
| citation_count | INT | YES | Number of papers that cite this paper (from S2) |
| influential_citation_count | INT | YES | Citations where this paper had significant impact on the citing paper (from S2) |
| embedding_model | VARCHAR(32) | YES | Embedding model name (e.g., "specter_v1") |
| embedding_vector | JSON | YES | 768-dim float array as JSON |
| avg_author_h_index | FLOAT | YES | Computed: average h-index of authors (only authors with S2 profiles) |
| avg_author_citations_per_paper | FLOAT | YES | Computed: average citations/paper of authors (only authors with S2 profiles) |
| total_author_h_index | INT | YES | Sum of all author h-indices (only authors with S2 profiles) |
| created_at | DATETIME | NO | Record creation timestamp |
| updated_at | DATETIME | NO | Record update timestamp |

**Indexes:**
- UNIQUE on `arxiv_id`
- INDEX on `published_at`
- INDEX on `primary_category`
- INDEX on `avg_author_h_index`

**JSON column examples:**

`categories`:
```json
["cs.LG", "cs.AI"]
```

`authors` (semantic_scholar_id is optional):
```json
[
  {"name": "Albert Gu", "semantic_scholar_id": "2269161650"},
  {"name": "Tri Dao", "semantic_scholar_id": "2269146652"},
  {"name": "New Researcher"}
]
```

`embedding_vector`:
```json
[-2.023, -2.027, 0.595, 2.441, ...]
```

---

## High-Level Workflow

1. **Daily DAG scrapes arXiv**: A scheduled DAG runs daily and fetches ALL papers submitted to arXiv for a target date (all categories, no filtering). By default uses 5 days ago; can be overridden with `target_date` parameter.

2. **Enrich with Semantic Scholar**: The DAG attempts to enrich each paper with S2 data (authors with IDs, h-index, citations, embeddings). **If S2 doesn't have the paper yet, save anyway with null S2 fields** - no data is lost.

3. **Save papers to database**: Calls `save_arxiv_papers_batch()` to upsert papers to the `arxiv_papers` table.

4. **Configurable parameters**:
   - `target_date` (optional): Specific date to scrape (YYYY-MM-DD). Defaults to 5 days ago.
   - `max_papers` (optional): Limit number of papers for debugging/testing.

5. **Failure handling**: If the DAG fails, it should be clearly marked as failed. Manual intervention will handle retries.

---

## Implementation

### File: `papers/models.py`

Pydantic DTOs for arXiv paper operations.

```python
class ArxivPaperAuthor(BaseModel):
    """Single author with optional Semantic Scholar ID.

    Attributes:
        name: Author's full name as it appears on the paper.
        semantic_scholar_id: S2 author ID, or None if not found in S2.
    """
    name: str
    semantic_scholar_id: Optional[str] = None


class ArxivDiscoveredPaper(BaseModel):
    """
    Discovered arXiv paper DTO with metadata, authors, and credibility scores.

    Automatically converts from SQLAlchemy ArxivPaperRecord using model_validate().
    Use to_orm() to convert back to database model.

    Required fields:
        arxiv_id: ArXiv identifier without version (e.g., "2312.00752").
        title: Paper title.

    Optional fields (nullable in DB):
        version: ArXiv version number (e.g., 1, 2, 3).
        abstract: Paper abstract text.
        published_at: Publication timestamp from arXiv.
        primary_category: Primary arXiv category (e.g., "cs.LG").
        categories: All arXiv categories the paper belongs to.
        authors: List of authors with optional S2 IDs.
        semantic_scholar_id: S2 paper ID (40-char hex string).
        citation_count: Number of citations from S2.
        influential_citation_count: Influential citations from S2.
        embedding_model: Name of embedding model (e.g., "specter_v1").
        embedding_vector: 768-dimensional embedding as float list.
        avg_author_h_index: Average h-index across authors with S2 profiles (unknowns excluded).
        avg_author_citations_per_paper: Average citations per paper across authors with S2 profiles.
        total_author_h_index: Sum of h-indices for authors with S2 profiles.
    """
    arxiv_id: str
    title: str
    version: Optional[int] = None
    abstract: Optional[str] = None
    published_at: Optional[datetime] = None
    primary_category: Optional[str] = None
    categories: List[str] = Field(default_factory=list)
    authors: List[ArxivPaperAuthor] = Field(default_factory=list)
    semantic_scholar_id: Optional[str] = None
    citation_count: Optional[int] = None
    influential_citation_count: Optional[int] = None
    embedding_model: Optional[str] = None
    embedding_vector: Optional[List[float]] = None
    avg_author_h_index: Optional[float] = None
    avg_author_citations_per_paper: Optional[float] = None
    total_author_h_index: Optional[int] = None
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None

    def to_orm(self):
        """Convert ArxivDiscoveredPaper DTO to SQLAlchemy ArxivPaperRecord."""
        from papers.db.models import ArxivPaperRecord
        # ... serializes authors, categories, embedding_vector to JSON ...

    class Config:
        from_attributes = True


class ArxivPaperFilter(BaseModel):
    """Filter options for listing arXiv papers.

    Attributes:
        min_avg_h_index: Only return papers where avg_author_h_index >= this value.
        primary_category: Only return papers with this primary category.
        published_after: Only return papers published after this datetime.
        limit: Maximum number of papers to return (default 100).
        offset: Number of papers to skip for pagination (default 0).
    """
    min_avg_h_index: Optional[float] = None
    primary_category: Optional[str] = None
    published_after: Optional[datetime] = None
    limit: int = 100
    offset: int = 0
```

---

### File: `papers/db/models.py`

SQLAlchemy ORM model.

```python
class ArxivPaperRecord(Base):
    """SQLAlchemy model for arxiv_papers table."""
    __tablename__ = "arxiv_papers"
    # ... columns as defined in schema above ...
```

---

### File: `migrations/versions/20251222_create_arxiv_papers_table.py`

```python
def upgrade():
    """Create arxiv_papers table with JSON columns for categories, authors, and embedding."""

def downgrade():
    """Drop arxiv_papers table."""
```

---

### File: `papers/db/client_temp.py`

Database operations only. All public functions use DTOs. (Will be merged with `client.py` later.)

```python
def create_arxiv_paper(db: Session, paper: ArxivDiscoveredPaper) -> ArxivPaperRecord:
    """Create a new arXiv paper record.

    Args:
        db: Database session.
        paper: Paper DTO to insert.

    Returns:
        The created ArxivPaperRecord model instance.

    Raises:
        IntegrityError: If a paper with this arxiv_id already exists.
    """


def get_arxiv_paper_by_id(db: Session, arxiv_id: str) -> ArxivDiscoveredPaper | None:
    """Get a paper by its arXiv ID.

    Args:
        db: Database session.
        arxiv_id: ArXiv identifier (e.g., "2312.00752").

    Returns:
        ArxivDiscoveredPaper DTO if found, None otherwise.
    """


def list_arxiv_papers(db: Session, filters: ArxivPaperFilter) -> list[ArxivDiscoveredPaper]:
    """List papers with optional filters, ordered by avg_author_h_index descending.

    Args:
        db: Database session.
        filters: Filter and pagination options.

    Returns:
        List of ArxivDiscoveredPaper DTOs matching the filters.
    """


def save_arxiv_papers_batch(db: Session, papers: list[ArxivDiscoveredPaper]) -> int:
    """Upsert a batch of papers. Updates all fields unconditionally for existing papers.

    Partial success: commits papers that succeed, logs and skips papers that fail.

    Args:
        db: Database session.
        papers: List of ArxivDiscoveredPaper DTOs to upsert.

    Returns:
        Number of papers successfully upserted.
    """
```

---

### File: `dags/arxiv_discovery_dag.py`

DAG with inline arXiv/S2 fetching logic (similar to `testscripts/2025.12.22-explore-arxiv-data/5_rank_new_papers.py`).

- Fetches ALL papers from arXiv API for target date
- Accepts optional `target_date` parameter (YYYY-MM-DD). If not provided, defaults to 5 days ago.
- Enriches each paper with Semantic Scholar data (authors, h-index, citations, embeddings)
- If S2 doesn't have the paper, saves with null S2 fields
- Converts to `ArxivDiscoveredPaper` DTOs and calls `save_arxiv_papers_batch()` to upsert
- Accepts optional `max_papers` parameter for debugging
- Uses incremental backoff for S2 rate limiting (1s, 2s, 3s, 4s, 5s on consecutive failures, then stays at 5s)
- Paginates through all arXiv API results until complete
- If S2 enrichment fails for a paper after retries, saves with null S2 fields and continues (never fails the DAG due to S2 issues)
